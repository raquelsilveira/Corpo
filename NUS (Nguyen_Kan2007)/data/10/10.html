<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>sigproc-master.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2003-07-17T17:16:15+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Times;color:#000000;}
	.ft1{font-size:15px;font-family:Times;color:#000000;}
	.ft2{font-size:12px;font-family:Times;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:9px;font-family:Times;color:#000000;}
	.ft5{font-size:6px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Courier;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:12px;line-height:15px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft10{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10001.png" alt="background image">
<DIV style="position:absolute;top:109;left:89"><nobr><span class="ft0"><b>A Frequency-based and a Poisson-based Definition of the</b></span></nobr></DIV>
<DIV style="position:absolute;top:139;left:255"><nobr><span class="ft0"><b>Probability of Being Informative</b></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:388"><nobr><span class="ft1">Thomas Roelleke</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:346"><nobr><span class="ft8">Department of Computer Science<br>Queen Mary University of London</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:373"><nobr><span class="ft1">thor@dcs.qmul.ac.uk</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:81"><nobr><span class="ft1">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:81"><nobr><span class="ft9">This paper reports on theoretical investigations about the<br>assumptions underlying the inverse document frequency (idf ).<br>We show that an intuitive idf -based probability function for<br>the probability of a term being informative assumes disjoint<br>document events. By assuming documents to be indepen-<br>dent rather than disjoint, we arrive at a Poisson-based prob-<br>ability of being informative. The framework is useful for<br>understanding and deciding the parameter estimation and<br>combination in probabilistic retrieval models.</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:81"><nobr><span class="ft1">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:81"><nobr><span class="ft9">H.3.3 [Information Search and Retrieval]: Retrieval<br>models</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:81"><nobr><span class="ft1">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:81"><nobr><span class="ft3">Theory</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:81"><nobr><span class="ft1">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:81"><nobr><span class="ft9">Probabilistic information retrieval, inverse document fre-<br>quency (idf), Poisson distribution, information theory, in-<br>dependence assumption</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:81"><nobr><span class="ft1">1.</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:112"><nobr><span class="ft1">INTRODUCTION AND BACKGROUND</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:94"><nobr><span class="ft3">The inverse document frequency (idf ) is one of the most</span></nobr></DIV>
<DIV style="position:absolute;top:760;left:81"><nobr><span class="ft9">successful parameters for a relevance-based ranking of re-<br>trieved objects. With N being the total number of docu-<br>ments, and n(t) being the number of documents in which<br>term t occurs, the idf is defined as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:839;left:134"><nobr><span class="ft3">idf(t) := - log n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:229"><nobr><span class="ft3">N , 0 &lt;= idf(t) &lt; </span></nobr></DIV>
<DIV style="position:absolute;top:868;left:81"><nobr><span class="ft9">Ranking based on the sum of the idf -values of the query<br>terms that occur in the retrieved documents works well, this<br>has been shown in numerous applications. Also, it is well<br>known that the combination of a document-specific term</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft10">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br>SIGIR'03, July 28­August 1, 2003, Toronto, Canada.<br>Copyright 2003 ACM 1-58113-646-3/03/0007 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:315"><nobr><span class="ft3">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:322"><nobr><span class="ft4">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:475"><nobr><span class="ft9">weight and idf works better than idf alone. This approach<br>is known as tf-idf , where tf(t, d) (0 &lt;= tf(t, d) &lt;= 1) is<br>the so-called term frequency of term t in document d. The<br>idf reflects the discriminating power (informativeness) of a<br>term, whereas the tf reflects the occurrence of a term.</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:489"><nobr><span class="ft3">The idf alone works better than the tf alone does. An ex-</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:475"><nobr><span class="ft9">planation might be the problem of tf with terms that occur<br>in many documents; let us refer to those terms as "noisy"<br>terms. We use the notion of "noisy" terms rather than "fre-<br>quent" terms since frequent terms leaves open whether we<br>refer to the document frequency of a term in a collection or<br>to the so-called term frequency (also referred to as within-<br>document frequency) of a term in a document. We asso-<br>ciate "noise" with the document frequency of a term in a<br>collection, and we associate "occurrence" with the within-<br>document frequency of a term. The tf of a noisy term might<br>be high in a document, but noisy terms are not good candi-<br>dates for representing a document. Therefore, the removal<br>of noisy terms (known as "stopword removal") is essential<br>when applying tf . In a tf-idf approach, the removal of stop-<br>words is conceptually obsolete, if stopwords are just words<br>with a low idf .</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:489"><nobr><span class="ft3">From a probabilistic point of view, tf is a value with a</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:475"><nobr><span class="ft9">frequency-based probabilistic interpretation whereas idf has<br>an "informative" rather than a probabilistic interpretation.<br>The missing probabilistic interpretation of idf is a problem<br>in probabilistic retrieval models where we combine uncertain<br>knowledge of different dimensions (e.g.: informativeness of<br>terms, structure of documents, quality of documents, age<br>of documents, etc.) such that a good estimate of the prob-<br>ability of relevance is achieved. An intuitive solution is a<br>normalisation of idf such that we obtain values in the inter-<br>val [0; 1]. For example, consider a normalisation based on<br>the maximal idf -value. Let T be the set of terms occurring<br>in a collection.</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:504"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:513"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:531"><nobr><span class="ft3">(t is informative) := idf(t)</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:660"><nobr><span class="ft3">maxidf</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:504"><nobr><span class="ft3">maxidf := max(</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:598"><nobr><span class="ft3">{idf(t)|t  T }), maxidf &lt;= - log(1/N)</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:504"><nobr><span class="ft3">minidf := min(</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:594"><nobr><span class="ft3">{idf(t)|t  T }), minidf &gt;= 0</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:507"><nobr><span class="ft3">minidf</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:506"><nobr><span class="ft3">maxidf  P</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:575"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:593"><nobr><span class="ft3">(t is informative)  1.0</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:475"><nobr><span class="ft9">This frequency-based probability function covers the interval<br>[0; 1] if the minimal idf is equal to zero, which is the case<br>if we have at least one term that occurs in all documents.<br>Can we interpret P</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:587"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:605"><nobr><span class="ft3">, the normalised idf , as the probability</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:475"><nobr><span class="ft3">that the term is informative?</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:489"><nobr><span class="ft3">When investigating the probabilistic interpretation of the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">227</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft12{font-size:15px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft11">normalised idf , we made several observations related to dis-<br>jointness and independence of document events. These ob-<br>servations are reported in section 3. We show in section 3.1<br>that the frequency-based noise probability</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:345"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:350"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:372"><nobr><span class="ft3">used in the</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:81"><nobr><span class="ft9">classic idf -definition can be explained by three assumptions:<br>binary term occurrence, constant document containment and<br>disjointness of document containment events. In section 3.2<br>we show that by assuming independence of documents, we<br>obtain 1</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:134"><nobr><span class="ft3">- e</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:155"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:175"><nobr><span class="ft3"> 1 - 0.37 as the upper bound of the noise</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:81"><nobr><span class="ft3">probability of a term. The value e</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:289"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:309"><nobr><span class="ft3">is related to the loga-</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:81"><nobr><span class="ft9">rithm and we investigate in section 3.3 the link to informa-<br>tion theory. In section 4, we link the results of the previous<br>sections to probability theory. We show the steps from possi-<br>ble worlds to binomial distribution and Poisson distribution.<br>In section 5, we emphasise that the theoretical framework<br>of this paper is applicable for both idf and tf . Finally, in<br>section 6, we base the definition of the probability of be-<br>ing informative on the results of the previous sections and<br>compare frequency-based and Poisson-based definitions.</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:81"><nobr><span class="ft1">2.</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:112"><nobr><span class="ft1">BACKGROUND</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:94"><nobr><span class="ft3">The relationship between frequencies, probabilities and</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:81"><nobr><span class="ft9">information theory (entropy) has been the focus of many<br>researchers. In this background section, we focus on work<br>that investigates the application of the Poisson distribution<br>in IR since a main part of the work presented in this paper<br>addresses the underlying assumptions of Poisson.</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:94"><nobr><span class="ft3">[4] proposes a 2-Poisson model that takes into account</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:81"><nobr><span class="ft9">the different nature of relevant and non-relevant documents,<br>rare terms (content words) and frequent terms (noisy terms,<br>function words, stopwords). [9] shows experimentally that<br>most of the terms (words) in a collection are distributed<br>according to a low dimension n-Poisson model. [10] uses a<br>2-Poisson model for including term frequency-based proba-<br>bilities in the probabilistic retrieval model. The non-linear<br>scaling of the Poisson function showed significant improve-<br>ment compared to a linear frequency-based probability. The<br>Poisson model was here applied to the term frequency of a<br>term in a document. We will generalise the discussion by<br>pointing out that document frequency and term frequency<br>are dual parameters in the collection space and the docu-<br>ment space, respectively. Our discussion of the Poisson dis-<br>tribution focuses on the document frequency in a collection<br>rather than on the term frequency in a document.</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:94"><nobr><span class="ft3">[7] and [6] address the deviation of idf and Poisson, and</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft9">apply Poisson mixtures to achieve better Poisson-based esti-<br>mates. The results proved again experimentally that a one-<br>dimensional Poisson does not work for rare terms, therefore<br>Poisson mixtures and additional parameters are proposed.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:94"><nobr><span class="ft3">[3], section 3.3, illustrates and summarises comprehen-</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft9">sively the relationships between frequencies, probabilities<br>and Poisson. Different definitions of idf are put into con-<br>text and a notion of "noise" is defined, where noise is viewed<br>as the complement of idf . We use in our paper a different<br>notion of noise: we consider a frequency-based noise that<br>corresponds to the document frequency, and we consider a<br>term noise that is based on the independence of document<br>events.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:94"><nobr><span class="ft3">[11], [12], [8] and [1] link frequencies and probability esti-</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft9">mation to information theory. [12] establishes a framework<br>in which information retrieval models are formalised based<br>on probabilistic inference. A key component is the use of a<br>space of disjoint events, where the framework mainly uses</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">terms as disjoint events. The probability of being informa-<br>tive defined in our paper can be viewed as the probability<br>of the disjoint terms in the term space of [12].</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:489"><nobr><span class="ft3">[8] address entropy and bibliometric distributions. En-</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:475"><nobr><span class="ft9">tropy is maximal if all events are equiprobable and the fre-<br>quency-based Lotka law (N/i</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:657"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:670"><nobr><span class="ft3">is the number of scientists</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:475"><nobr><span class="ft9">that have written i publications, where N and  are distri-<br>bution parameters), Zipf and the Pareto distribution are re-<br>lated. The Pareto distribution is the continuous case of the<br>Lotka and Lotka and Zipf show equivalences. The Pareto<br>distribution is used by [2] for term frequency normalisation.<br>The Pareto distribution compares to the Poisson distribu-<br>tion in the sense that Pareto is "fat-tailed", i. e. Pareto as-<br>signs larger probabilities to large numbers of events than<br>Poisson distributions do.</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:645"><nobr><span class="ft3">This makes Pareto interesting</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:475"><nobr><span class="ft9">since Poisson is felt to be too radical on frequent events.<br>We restrict in this paper to the discussion of Poisson, how-<br>ever, our results show that indeed a smoother distribution<br>than Poisson promises to be a good candidate for improving<br>the estimation of probabilities in information retrieval.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:489"><nobr><span class="ft3">[1] establishes a theoretical link between tf-idf and infor-</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:475"><nobr><span class="ft9">mation theory and the theoretical research on the meaning<br>of tf-idf "clarifies the statistical model on which the different<br>measures are commonly based". This motivation matches<br>the motivation of our paper: We investigate theoretically<br>the assumptions of classical idf and Poisson for a better<br>understanding of parameter estimation and combination.</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:475"><nobr><span class="ft1">3.</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:507"><nobr><span class="ft1">FROM DISJOINT TO INDEPENDENT</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:489"><nobr><span class="ft3">We define and discuss in this section three probabilities:</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:475"><nobr><span class="ft9">The frequency-based noise probability (definition 1), the to-<br>tal noise probability for disjoint documents (definition 2).<br>and the noise probability for independent documents (defi-<br>nition 3).</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:475"><nobr><span class="ft1">3.1</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:515"><nobr><span class="ft12">Binary occurrence, constant containment<br>and disjointness of documents</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:489"><nobr><span class="ft3">We show in this section, that the frequency-based noise</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:475"><nobr><span class="ft3">probability</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:550"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:555"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:578"><nobr><span class="ft3">in the idf definition can be explained as</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:475"><nobr><span class="ft9">a total probability with binary term occurrence, constant<br>document containment and disjointness of document con-<br>tainments.</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:489"><nobr><span class="ft3">We refer to a probability function as binary if for all events</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:475"><nobr><span class="ft9">the probability is either 1.0 or 0.0. The occurrence proba-<br>bility P (t|d) is binary, if P (t|d) is equal to 1.0 if t  d, and<br>P (t|d) is equal to 0.0, otherwise.</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:490"><nobr><span class="ft3">P (t|d) is binary :  P (t|d) = 1.0  P (t|d) = 0.0</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:489"><nobr><span class="ft3">We refer to a probability function as constant if for all</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:475"><nobr><span class="ft9">events the probability is equal. The document containment<br>probability reflect the chance that a document occurs in a<br>collection. This containment probability is constant if we<br>have no information about the document containment or<br>we ignore that documents differ in containment. Contain-<br>ment could be derived, for example, from the size, quality,<br>age, links, etc. of a document. For a constant containment<br>in a collection with N documents,</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:698"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:696"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:713"><nobr><span class="ft3">is often assumed as</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft9">the containment probability. We generalise this definition<br>and introduce the constant  where 0    N. The con-<br>tainment of a document d depends on the collection c, this<br>is reflected by the notation P (d|c) used for the containment</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">228</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:-1px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10003.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft3">of a document.</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:121"><nobr><span class="ft3">P (d|c) is constant :  d : P (d|c) = </span></nobr></DIV>
<DIV style="position:absolute;top:124;left:358"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:81"><nobr><span class="ft9">For disjoint documents that cover the whole event space,<br>we set  = 1 and obtain</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:234"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:249"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:258"><nobr><span class="ft3">P (d|c) = 1.0. Next, we define</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:81"><nobr><span class="ft9">the frequency-based noise probability and the total noise<br>probability for disjoint documents. We introduce the event<br>notation t is noisy and t occurs for making the difference<br>between the noise probability P (t is noisy|c) in a collection<br>and the occurrence probability P (t occurs|d) in a document<br>more explicit, thereby keeping in mind that the noise prob-<br>ability corresponds to the occurrence probability of a term<br>in a collection.</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:96"><nobr><span class="ft3">Definition 1. The frequency-based term noise prob-</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:81"><nobr><span class="ft3">ability:</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:170"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:179"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:197"><nobr><span class="ft3">(t is noisy|c) := n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:303"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:96"><nobr><span class="ft3">Definition 2. The total term noise probability for</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:81"><nobr><span class="ft3">disjoint documents:</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:110"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:119"><nobr><span class="ft5">dis</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:134"><nobr><span class="ft3">(t is noisy|c) :=</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:240"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:255"><nobr><span class="ft3">P (t occurs|d) · P (d|c)</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft9">Now, we can formulate a theorem that makes assumptions<br>explicit that explain the classical idf .</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:96"><nobr><span class="ft3">Theorem 1. IDF assumptions: If the occurrence prob-</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:81"><nobr><span class="ft9">ability P (t|d) of term t over documents d is binary, and<br>the containment probability P (d|c) of documents d is con-<br>stant, and document containments are disjoint events, then<br>the noise probability for disjoint documents is equal to the<br>frequency-based noise probability.</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:136"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:144"><nobr><span class="ft5">dis</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:159"><nobr><span class="ft3">(t is noisy|c) = P</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:263"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:281"><nobr><span class="ft3">(t is noisy|c)</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:96"><nobr><span class="ft3">Proof. The assumptions are:</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:134"><nobr><span class="ft3">d : (P (t occurs|d) = 1  P (t occurs|d) = 0) </span></nobr></DIV>
<DIV style="position:absolute;top:686;left:134"><nobr><span class="ft3">P (d|c) = </span></nobr></DIV>
<DIV style="position:absolute;top:695;left:192"><nobr><span class="ft3">N </span></nobr></DIV>
<DIV style="position:absolute;top:734;left:141"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:156"><nobr><span class="ft3">P (d|c) = 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:81"><nobr><span class="ft3">We obtain:</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:81"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:89"><nobr><span class="ft5">dis</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:104"><nobr><span class="ft3">(t is noisy|c) =</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:200"><nobr><span class="ft5">d|td</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:234"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:231"><nobr><span class="ft3">N =</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:265"><nobr><span class="ft3">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:271"><nobr><span class="ft3">N = P</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:318"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:336"><nobr><span class="ft3">(t is noisy|c)</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft9">The above result is not a surprise but it is a mathemati-<br>cal formulation of assumptions that can be used to explain<br>the classical idf . The assumptions make explicit that the<br>different types of term occurrence in documents (frequency<br>of a term, importance of a term, position of a term, doc-<br>ument part where the term occurs, etc.) and the different<br>types of document containment (size, quality, age, etc.) are<br>ignored, and document containments are considered as dis-<br>joint events.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:94"><nobr><span class="ft3">From the assumptions, we can conclude that idf (frequency-</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:81"><nobr><span class="ft9">based noise, respectively) is a relatively simple but strict<br>estimate.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:147"><nobr><span class="ft3">Still, idf works well.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:289"><nobr><span class="ft3">This could be explained</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft9">by a leverage effect that justifies the binary occurrence and<br>constant containment: The term occurrence for small docu-<br>ments tends to be larger than for large documents, whereas</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">the containment for small documents tends to be smaller<br>than for large documents.</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:652"><nobr><span class="ft3">From that point of view, idf</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:475"><nobr><span class="ft9">means that P (t  d|c) is constant for all d in which t occurs,<br>and P (t  d|c) is zero otherwise. The occurrence and con-<br>tainment can be term specific. For example, set P (t  d|c) =<br>1/N</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:499"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:509"><nobr><span class="ft3">(c) if t occurs in d, where N</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:675"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:685"><nobr><span class="ft3">(c) is the number of doc-</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:475"><nobr><span class="ft9">uments in collection c (we used before just N). We choose a<br>document-dependent occurrence P (t|d) := 1/N</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:756"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:765"><nobr><span class="ft3">(d), i. e. the</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:475"><nobr><span class="ft3">occurrence probability is equal to the inverse of N</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:766"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:775"><nobr><span class="ft3">(d), which</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:475"><nobr><span class="ft9">is the total number of terms in document d. Next, we choose<br>the containment P (d|c) := N</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:647"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:656"><nobr><span class="ft3">(d)/N</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:691"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:699"><nobr><span class="ft3">(c)·N</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:731"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:740"><nobr><span class="ft3">(c)/N</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:774"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:784"><nobr><span class="ft3">(c) where</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:475"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:486"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:495"><nobr><span class="ft3">(d)/N</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:530"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:539"><nobr><span class="ft3">(c) is a document length normalisation (number</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:475"><nobr><span class="ft9">of terms in document d divided by the number of terms in<br>collection c), and N</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:598"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:607"><nobr><span class="ft3">(c)/N</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:640"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:651"><nobr><span class="ft3">(c) is a constant factor of the</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:475"><nobr><span class="ft9">collection (number of terms in collection c divided by the<br>number of documents in collection c). We obtain P (td|c) =<br>1/N</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:499"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:509"><nobr><span class="ft3">(c).</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:489"><nobr><span class="ft3">In a tf-idf -retrieval function, the tf -component reflects</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:475"><nobr><span class="ft9">the occurrence probability of a term in a document. This is<br>a further explanation why we can estimate the idf with a<br>simple P (t|d), since the combined tf-idf contains the occur-<br>rence probability. The containment probability corresponds<br>to a document normalisation (document length normalisa-<br>tion, pivoted document length) and is normally attached to<br>the tf -component or the tf-idf -product.</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:489"><nobr><span class="ft3">The disjointness assumption is typical for frequency-based</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:475"><nobr><span class="ft9">probabilities. From a probability theory point of view, we<br>can consider documents as disjoint events, in order to achieve<br>a sound theoretical model for explaining the classical idf .<br>But does disjointness reflect the real world where the con-<br>tainment of a document appears to be independent of the<br>containment of another document? In the next section, we<br>replace the disjointness assumption by the independence as-<br>sumption.</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:475"><nobr><span class="ft1">3.2</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:515"><nobr><span class="ft12">The upper bound of the noise probability<br>for independent documents</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:489"><nobr><span class="ft3">For independent documents, we compute the probability</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:475"><nobr><span class="ft9">of a disjunction as usual, namely as the complement of the<br>probability of the conjunction of the negated events:</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:515"><nobr><span class="ft3">P (d</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:539"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:548"><nobr><span class="ft3"> . . .  d</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:599"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:609"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:628"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:652"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:662"><nobr><span class="ft3">- P (¬d</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:709"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:718"><nobr><span class="ft3"> . . .  ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:778"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:788"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:628"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:652"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:662"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:682"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:694"><nobr><span class="ft3">(1</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:709"><nobr><span class="ft3">- P (d))</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:475"><nobr><span class="ft9">The noise probability can be considered as the conjunction<br>of the term occurrence and the document containment.</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:494"><nobr><span class="ft3">P (t is noisy|c) := P (t occurs  (d</span></nobr></DIV>
<DIV style="position:absolute;top:830;left:697"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:706"><nobr><span class="ft3"> . . .  d</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:757"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:767"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:773"><nobr><span class="ft3">|c)</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:475"><nobr><span class="ft9">For disjoint documents, this view of the noise probability<br>led to definition 2. For independent documents, we use now<br>the conjunction of negated events.</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:490"><nobr><span class="ft3">Definition 3. The term noise probability for inde-</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:475"><nobr><span class="ft3">pendent documents:</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:492"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:500"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:511"><nobr><span class="ft3">(t is noisy|c) :=</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:616"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:628"><nobr><span class="ft3">(1</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:643"><nobr><span class="ft3">- P (t occurs|d) · P (d|c))</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:475"><nobr><span class="ft9">With binary occurrence and a constant containment P (d|c) :=<br>/N, we obtain the term noise of a term t that occurs in n(t)<br>documents:</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:531"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:540"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:551"><nobr><span class="ft3">(t is noisy|c) = 1 - 1 - </span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:706"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:1047;left:731"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">229</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="10004.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft9">For binary occurrence and disjoint documents, the contain-<br>ment probability was 1/N. Now, with independent docu-<br>ments, we can use  as a collection parameter that controls<br>the average containment probability. We show through the<br>next theorem that the upper bound of the noise probability<br>depends on .</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:96"><nobr><span class="ft3">Theorem 2. The upper bound of being noisy: If the</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:81"><nobr><span class="ft9">occurrence P (t|d) is binary, and the containment P (d|c)<br>is constant, and document containments are independent<br>events, then 1</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:168"><nobr><span class="ft3">- e</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:189"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:210"><nobr><span class="ft3">is the upper bound of the noise proba-</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:81"><nobr><span class="ft3">bility.</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:154"><nobr><span class="ft3">t : P</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:187"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:198"><nobr><span class="ft3">(t is noisy|c) &lt; 1 - e</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:323"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:96"><nobr><span class="ft3">Proof. The upper bound of the independent noise prob-</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:81"><nobr><span class="ft3">ability follows from the limit lim</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:283"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:316"><nobr><span class="ft3">(1 +</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:350"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:348"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:359"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:365"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:381"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:404"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:416"><nobr><span class="ft3">(see</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:81"><nobr><span class="ft9">any comprehensive math book, for example, [5], for the con-<br>vergence equation of the Euler function). With x = -, we<br>obtain:</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:180"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:174"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:218"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:228"><nobr><span class="ft3">- </span></nobr></DIV>
<DIV style="position:absolute;top:404;left:244"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:268"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:282"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:303"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:81"><nobr><span class="ft3">For the term noise, we have:</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:136"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:145"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:156"><nobr><span class="ft3">(t is noisy|c) = 1 - 1 - </span></nobr></DIV>
<DIV style="position:absolute;top:472;left:312"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:336"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:81"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:89"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:100"><nobr><span class="ft3">(t is noisy|c) is strictly monotonous: The noise of a term</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:81"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:86"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:98"><nobr><span class="ft3">is less than the noise of a term t</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:296"><nobr><span class="ft5">n+1</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:318"><nobr><span class="ft3">, where t</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:372"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:385"><nobr><span class="ft3">occurs in</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft3">n documents and t</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:197"><nobr><span class="ft5">n+1</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:224"><nobr><span class="ft3">occurs in n + 1 documents. There-</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:81"><nobr><span class="ft9">fore, a term with n = N has the largest noise probability.<br>For a collection with infinite many documents, the upper<br>bound of the noise probability for terms t</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:329"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:343"><nobr><span class="ft3">that occur in all</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:81"><nobr><span class="ft3">documents becomes:</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:117"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:111"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:145"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:153"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:164"><nobr><span class="ft3">(t</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:175"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:190"><nobr><span class="ft3">is noisy)</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:255"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:285"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:279"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:313"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:323"><nobr><span class="ft3">- 1 - </span></nobr></DIV>
<DIV style="position:absolute;top:631;left:372"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:609;left:397"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:255"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:279"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:289"><nobr><span class="ft3">- e</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:309"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:81"><nobr><span class="ft9">By applying an independence rather a disjointness assump-<br>tion, we obtain the probability e</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:277"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:296"><nobr><span class="ft3">that a term is not noisy</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:81"><nobr><span class="ft9">even if the term does occur in all documents. In the disjoint<br>case, the noise probability is one for a term that occurs in<br>all documents.</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:94"><nobr><span class="ft3">If we view P (d|c) := /N as the average containment,</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft9">then  is large for a term that occurs mostly in large docu-<br>ments, and  is small for a term that occurs mostly in small<br>documents. Thus, the noise of a term t is large if t occurs in<br>n(t) large documents and the noise is smaller if t occurs in<br>small documents. Alternatively, we can assume a constant<br>containment and a term-dependent occurrence. If we as-<br>sume P (d|c) := 1, then P (t|d) := /N can be interpreted as<br>the average probability that t represents a document. The<br>common assumption is that the average containment or oc-<br>currence probability is proportional to n(t). However, here<br>is additional potential: The statistical laws (see [3] on Luhn<br>and Zipf) indicate that the average probability could follow<br>a normal distribution, i. e. small probabilities for small n(t)<br>and large n(t), and larger probabilities for medium n(t).</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:94"><nobr><span class="ft3">For the monotonous case we investigate here, the noise of</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft9">a term with n(t) = 1 is equal to 1 - (1 - /N) = /N and<br>the noise of a term with n(t) = N is close to 1 - e</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:377"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:393"><nobr><span class="ft3">. In the</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft3">next section, we relate the value e</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:286"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:306"><nobr><span class="ft3">to information theory.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:475"><nobr><span class="ft1">3.3</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:516"><nobr><span class="ft12">The probability of a maximal informative<br>signal</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:489"><nobr><span class="ft3">The probability e</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:596"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:617"><nobr><span class="ft3">is special in the sense that a signal</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:475"><nobr><span class="ft9">with that probability is a signal with maximal information as<br>derived from the entropy definition. Consider the definition<br>of the entropy contribution H(t) of a signal t.</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:571"><nobr><span class="ft3">H(t) := P (t) · - ln P (t)</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:475"><nobr><span class="ft3">We form the first derivation for computing the optimum.</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:549"><nobr><span class="ft3">H(t)</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:550"><nobr><span class="ft3">P (t)</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:601"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:625"><nobr><span class="ft3">- ln P (t) + -1</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:697"><nobr><span class="ft3">P (t) · P (t)</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:601"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:625"><nobr><span class="ft3">-(1 + ln P (t))</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:475"><nobr><span class="ft3">For obtaining optima, we use:</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:586"><nobr><span class="ft3">0 =</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:611"><nobr><span class="ft3">-(1 + ln P (t))</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:475"><nobr><span class="ft3">The entropy contribution H(t) is maximal for P (t) = e</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:815"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:830"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:475"><nobr><span class="ft9">This result does not depend on the base of the logarithm as<br>we see next:</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:515"><nobr><span class="ft3">H(t)</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:516"><nobr><span class="ft3">P (t) = - log</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:602"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:610"><nobr><span class="ft3">P (t) +</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:675"><nobr><span class="ft3">-1</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:655"><nobr><span class="ft3">P (t) · ln b · P (t)</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:526"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:551"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:582"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:576"><nobr><span class="ft3">ln b + log</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:632"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:640"><nobr><span class="ft3">P (t) = -</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:720"><nobr><span class="ft3">1 + ln P (t)</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:742"><nobr><span class="ft3">ln b</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:475"><nobr><span class="ft3">We summarise this result in the following theorem:</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:490"><nobr><span class="ft3">Theorem 3. The probability of a maximal informa-</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:475"><nobr><span class="ft3">tive signal: The probability P</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:658"><nobr><span class="ft5">max</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:683"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:704"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:722"><nobr><span class="ft3"> 0.37 is the prob-</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:475"><nobr><span class="ft9">ability of a maximal informative signal. The entropy of a<br>maximal informative signal is H</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:671"><nobr><span class="ft5">max</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:695"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:716"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:731"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:490"><nobr><span class="ft3">Proof. The probability and entropy follow from the deriva-</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:475"><nobr><span class="ft3">tion above.</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft3">The complement of the maximal noise probability is e</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:818"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:475"><nobr><span class="ft9">and we are looking now for a generalisation of the entropy<br>definition such that e</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:604"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:625"><nobr><span class="ft3">is the probability of a maximal in-</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:475"><nobr><span class="ft9">formative signal. We can generalise the entropy definition<br>by computing the integral of  + ln P (t), i. e. this derivation<br>is zero for e</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:546"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:562"><nobr><span class="ft3">. We obtain a generalised entropy:</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:504"><nobr><span class="ft3">-( + ln P (t)) d(P (t)) = P (t) · (1 -  - ln P (t))</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:475"><nobr><span class="ft9">The generalised entropy corresponds for  = 1 to the classi-<br>cal entropy. By moving from disjoint to independent docu-<br>ments, we have established a link between the complement<br>of the noise probability of a term that occurs in all docu-<br>ments and information theory. Next, we link independent<br>documents to probability theory.</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:475"><nobr><span class="ft1">4.</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:507"><nobr><span class="ft1">THE LINK TO PROBABILITY THEORY</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:489"><nobr><span class="ft3">We review for independent documents three concepts of</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:475"><nobr><span class="ft9">probability theory: possible worlds, binomial distribution<br>and Poisson distribution.</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:475"><nobr><span class="ft1">4.1</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:516"><nobr><span class="ft1">Possible Worlds</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:489"><nobr><span class="ft3">Each conjunction of document events (for each document,</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft9">we consider two document events: the document can be<br>true or false) is associated with a so-called possible world.<br>For example, consider the eight possible worlds for three<br>documents (N = 3).</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">230</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft14{font-size:11px;line-height:19px;font-family:Times;color:#000000;}
	.ft15{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10005.png" alt="background image">
<DIV style="position:absolute;top:83;left:179"><nobr><span class="ft3">world w</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:243"><nobr><span class="ft3">conjunction</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:188"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:243"><nobr><span class="ft3">d</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:250"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:96;left:259"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:279"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:96;left:288"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:308"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:188"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:243"><nobr><span class="ft3">d</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:250"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:259"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:279"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:288"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:317"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:188"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:243"><nobr><span class="ft3">d</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:250"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:259"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:288"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:297"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:317"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:188"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:243"><nobr><span class="ft3">d</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:250"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:259"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:288"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:297"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:326"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:188"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:243"><nobr><span class="ft3">¬d</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:259"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:269"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:288"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:297"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:317"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:188"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:243"><nobr><span class="ft3">¬d</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:259"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:269"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:288"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:297"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:326"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:188"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:243"><nobr><span class="ft3">¬d</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:259"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:269"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:297"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:307"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:326"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:188"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:243"><nobr><span class="ft3">¬d</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:259"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:269"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:297"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:307"><nobr><span class="ft3"> ¬d</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:335"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:81"><nobr><span class="ft9">With each world w, we associate a probability µ(w), which<br>is equal to the product of the single probabilities of the doc-<br>ument events.</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:179"><nobr><span class="ft14">world w probability µ(w)<br>w</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:189"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:312;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:322;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:267"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:314;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:322;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:333"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:189"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:267"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:333;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:333"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:189"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:349;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:359;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:267"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:351;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:359;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:333"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:189"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:368;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:378;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:267"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:370;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:378;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:333"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:189"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:386;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:396;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:267"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:389;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:396;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:333"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:189"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:405;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:267"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:333"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:189"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:423;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:267"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:426;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:333"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:179"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:189"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:241"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:442;left:251"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:249"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:260"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:267"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:276"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:283"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:289"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:299"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:316"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:315"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:326"><nobr><span class="ft13">¡</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:333"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:81"><nobr><span class="ft9">The sum over the possible worlds in which k documents are<br>true and N -k documents are false is equal to the probabil-<br>ity function of the binomial distribution, since the binomial<br>coefficient yields the number of possible worlds in which k<br>documents are true.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft1">4.2</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:121"><nobr><span class="ft1">Binomial distribution</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:94"><nobr><span class="ft3">The binomial probability function yields the probability</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:81"><nobr><span class="ft9">that k of N events are true where each event is true with<br>the single event probability p.</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:100"><nobr><span class="ft3">P (k) := binom(N, k, p) :=</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:272"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:274"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:300"><nobr><span class="ft3">p</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:307"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:318"><nobr><span class="ft3">(1</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:334"><nobr><span class="ft3">- p)N -k</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:81"><nobr><span class="ft9">The single event probability is usually defined as p := /N,<br>i. e. p is inversely proportional to N, the total number of<br>events. With this definition of p, we obtain for an infinite<br>number of documents the following limit for the product of<br>the binomial coefficient and p</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:260"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:267"><nobr><span class="ft3">:</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:105"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:98"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:143"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:146"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:171"><nobr><span class="ft3">p</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:178"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:189"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:112"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:142"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:136"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:172"><nobr><span class="ft3">N · (N -1) · . . . · (N -k +1)</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:249"><nobr><span class="ft3">k!</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:356"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:850;left:353"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:378"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:389"><nobr><span class="ft3">= </span></nobr></DIV>
<DIV style="position:absolute;top:830;left:413"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:407"><nobr><span class="ft3">k!</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:81"><nobr><span class="ft9">The limit is close to the actual value for k &lt;&lt; N. For large<br>k, the actual value is smaller than the limit.</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:94"><nobr><span class="ft3">The limit of (1</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:179"><nobr><span class="ft3">-p)N -k follows from the limit lim</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:386"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:419"><nobr><span class="ft3">(1+</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:84"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:82"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:94"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:99"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:114"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:135"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:141"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:145"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:139"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:170"><nobr><span class="ft3">(1</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:186"><nobr><span class="ft3">- p)</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:212"><nobr><span class="ft5">N-k</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:237"><nobr><span class="ft3">= lim</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:252"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:296"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:306"><nobr><span class="ft3">- </span></nobr></DIV>
<DIV style="position:absolute;top:979;left:321"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:346"><nobr><span class="ft3">N -k</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:152"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:183"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:177"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:221"><nobr><span class="ft3">e</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:228"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:247"><nobr><span class="ft3">· 1 - </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:289"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:314"><nobr><span class="ft5">-k</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:344"><nobr><span class="ft3">= e</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:365"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft9">Again, the limit is close to the actual value for k &lt;&lt; N. For<br>large k, the actual value is larger than the limit.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:475"><nobr><span class="ft1">4.3</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:516"><nobr><span class="ft1">Poisson distribution</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:489"><nobr><span class="ft3">For an infinite number of events, the Poisson probability</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:475"><nobr><span class="ft3">function is the limit of the binomial probability function.</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:575"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:569"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:603"><nobr><span class="ft3">binom(N, k, p) = </span></nobr></DIV>
<DIV style="position:absolute;top:144;left:718"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:712"><nobr><span class="ft3">k! · e</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:743"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:569"><nobr><span class="ft3">P (k) = poisson(k, ) := </span></nobr></DIV>
<DIV style="position:absolute;top:180;left:726"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:720"><nobr><span class="ft3">k! · e</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:751"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:475"><nobr><span class="ft3">The probability poisson (0, 1) is equal to e</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:735"><nobr><span class="ft5">-1</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:749"><nobr><span class="ft3">, which is the</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:475"><nobr><span class="ft3">probability of a maximal informative signal.</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:765"><nobr><span class="ft3">This shows</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:475"><nobr><span class="ft9">the relationship of the Poisson distribution and information<br>theory.</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:489"><nobr><span class="ft3">After seeing the convergence of the binomial distribution,</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:475"><nobr><span class="ft9">we can choose the Poisson distribution as an approximation<br>of the independent term noise probability. First, we define<br>the Poisson noise probability:</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:490"><nobr><span class="ft3">Definition 4. The Poisson term noise probability:</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:543"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:552"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:567"><nobr><span class="ft3">(t is noisy|c) := e</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:672"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:691"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:698"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:698"><nobr><span class="ft5">k=1</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:723"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:387;left:731"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:724"><nobr><span class="ft3">k!</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:475"><nobr><span class="ft9">For independent documents, the Poisson distribution ap-<br>proximates the probability of the disjunction for large n(t),<br>since the independent term noise probability is equal to the<br>sum over the binomial probabilities where at least one of<br>n(t) document containment events is true.</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:504"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:513"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:524"><nobr><span class="ft3">(t is noisy|c) =</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:639"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:639"><nobr><span class="ft5">k=1</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:672"><nobr><span class="ft3">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:680"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:712"><nobr><span class="ft3">p</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:719"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:730"><nobr><span class="ft3">(1</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:746"><nobr><span class="ft3">- p)N -k</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:504"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:513"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:524"><nobr><span class="ft3">(t is noisy|c)  P</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:648"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:663"><nobr><span class="ft3">(t is noisy|c)</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:475"><nobr><span class="ft9">We have defined a frequency-based and a Poisson-based prob-<br>ability of being noisy, where the latter is the limit of the<br>independence-based probability of being noisy. Before we<br>present in the final section the usage of the noise proba-<br>bility for defining the probability of being informative, we<br>emphasise in the next section that the results apply to the<br>collection space as well as to the the document space.</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:475"><nobr><span class="ft1">5.</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:507"><nobr><span class="ft15">THE COLLECTION SPACE AND THE<br>DOCUMENT SPACE</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:489"><nobr><span class="ft3">Consider the dual definitions of retrieval parameters in</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:475"><nobr><span class="ft9">table 1. We associate a collection space D × T with a col-<br>lection c where D is the set of documents and T is the set<br>of terms in the collection. Let N</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:681"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:697"><nobr><span class="ft3">:=</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:717"><nobr><span class="ft3">|D| and N</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:781"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:796"><nobr><span class="ft3">:=</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:816"><nobr><span class="ft3">|T |</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:475"><nobr><span class="ft9">be the number of documents and terms, respectively. We<br>consider a document as a subset of T and a term as a subset<br>of D. Let n</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:547"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:555"><nobr><span class="ft3">(d) := |{t|d  t}| be the number of terms that</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:475"><nobr><span class="ft3">occur in the document d, and let n</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:683"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:693"><nobr><span class="ft3">(t) := |{d|t  d}| be the</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:475"><nobr><span class="ft3">number of documents that contain the term t.</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:489"><nobr><span class="ft3">In a dual way, we associate a document space L × T with</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:475"><nobr><span class="ft9">a document d where L is the set of locations (also referred<br>to as positions, however, we use the letters L and l and not<br>P and p for avoiding confusion with probabilities) and T is<br>the set of terms in the document. The document dimension<br>in a collection space corresponds to the location (position)<br>dimension in a document space.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:489"><nobr><span class="ft3">The definition makes explicit that the classical notion of</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft9">term frequency of a term in a document (also referred to as<br>the within-document term frequency) actually corresponds<br>to the location frequency of a term in a document. For the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">231</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:5px;font-family:Times;color:#000000;}
	.ft17{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10006.png" alt="background image">
<DIV style="position:absolute;top:84;left:89"><nobr><span class="ft3">space</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:225"><nobr><span class="ft3">collection</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:543"><nobr><span class="ft3">document</span></nobr></DIV>
<DIV style="position:absolute;top:101;left:89"><nobr><span class="ft3">dimensions</span></nobr></DIV>
<DIV style="position:absolute;top:101;left:225"><nobr><span class="ft3">documents and terms</span></nobr></DIV>
<DIV style="position:absolute;top:101;left:543"><nobr><span class="ft3">locations and terms</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:89"><nobr><span class="ft9">document/location<br>frequency</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:225"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:234"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:244"><nobr><span class="ft3">(t, c): Number of documents in which term t</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:225"><nobr><span class="ft3">occurs in collection c</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:543"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:551"><nobr><span class="ft5">L</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:559"><nobr><span class="ft3">(t, d): Number of locations (positions) at which</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:543"><nobr><span class="ft3">term t occurs in document d</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:225"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:236"><nobr><span class="ft5">D</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:246"><nobr><span class="ft3">(c): Number of documents in collection c</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:543"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:554"><nobr><span class="ft5">L</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:562"><nobr><span class="ft3">(d): Number of locations (positions) in docu-</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:543"><nobr><span class="ft3">ment d</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:89"><nobr><span class="ft3">term frequency</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:225"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:234"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:242"><nobr><span class="ft3">(d, c): Number of terms that document d con-</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:225"><nobr><span class="ft3">tains in collection c</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:543"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:551"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:560"><nobr><span class="ft3">(l, d): Number of terms that location l contains</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:543"><nobr><span class="ft3">in document d</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:225"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:236"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:245"><nobr><span class="ft3">(c): Number of terms in collection c</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:543"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:554"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:563"><nobr><span class="ft3">(d): Number of terms in document d</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:89"><nobr><span class="ft3">noise/occurrence</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:225"><nobr><span class="ft3">P (t|c) (term noise)</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:543"><nobr><span class="ft3">P (t|d) (term occurrence)</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:89"><nobr><span class="ft3">containment</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:225"><nobr><span class="ft3">P (d|c) (document)</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:543"><nobr><span class="ft3">P (l|d) (location)</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:89"><nobr><span class="ft3">informativeness</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:225"><nobr><span class="ft3">- ln P (t|c)</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:543"><nobr><span class="ft3">- ln P (t|d)</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:89"><nobr><span class="ft3">conciseness</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:225"><nobr><span class="ft3">- ln P (d|c)</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:543"><nobr><span class="ft3">- ln P (l|d)</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:89"><nobr><span class="ft3">P(informative)</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:225"><nobr><span class="ft3">ln(P (t|c))/ ln(P (t</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:331"><nobr><span class="ft5">min</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:353"><nobr><span class="ft3">, c))</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:543"><nobr><span class="ft3">ln(P (t|d))/ ln(P (t</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:650"><nobr><span class="ft5">min</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:671"><nobr><span class="ft3">, d))</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:89"><nobr><span class="ft3">P(concise)</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:225"><nobr><span class="ft3">ln(P (d|c))/ ln(P (d</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:336"><nobr><span class="ft5">min</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:357"><nobr><span class="ft3">|c))</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:543"><nobr><span class="ft3">ln(P (l|d))/ ln(P (l</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:648"><nobr><span class="ft5">min</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:670"><nobr><span class="ft3">|d))</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:353"><nobr><span class="ft3">Table 1: Retrieval parameters</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:81"><nobr><span class="ft9">actual term frequency value, it is common to use the max-<br>imal occurrence (number of locations; let lf be the location<br>frequency).</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:90"><nobr><span class="ft3">tf(t, d) := lf(t, d) := P</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:220"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:238"><nobr><span class="ft3">(t occurs|d)</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:202"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:210"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:228"><nobr><span class="ft3">(t</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:239"><nobr><span class="ft5">max</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:264"><nobr><span class="ft3">occurs</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:301"><nobr><span class="ft3">|d) =</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:345"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:354"><nobr><span class="ft5">L</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:362"><nobr><span class="ft3">(t, d)</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:335"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:343"><nobr><span class="ft5">L</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:352"><nobr><span class="ft3">(t</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:362"><nobr><span class="ft5">max</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:383"><nobr><span class="ft3">, d)</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:81"><nobr><span class="ft9">A further duality is between informativeness and concise-<br>ness (shortness of documents or locations): informativeness<br>is based on occurrence (noise), conciseness is based on con-<br>tainment.</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:94"><nobr><span class="ft3">We have highlighted in this section the duality between</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:81"><nobr><span class="ft9">the collection space and the document space. We concen-<br>trate in this paper on the probability of a term to be noisy<br>and informative. Those probabilities are defined in the col-<br>lection space. However, the results regarding the term noise<br>and informativeness apply to their dual counterparts: term<br>occurrence and informativeness in a document. Also, the<br>results can be applied to containment of documents and lo-<br>cations.</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:81"><nobr><span class="ft1">6.</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:112"><nobr><span class="ft15">THE PROBABILITY OF BEING INFOR-<br>MATIVE</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:94"><nobr><span class="ft3">We showed in the previous sections that the disjointness</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:81"><nobr><span class="ft9">assumption leads to frequency-based probabilities and that<br>the independence assumption leads to Poisson probabilities.<br>In this section, we formulate a frequency-based definition<br>and a Poisson-based definition of the probability of being<br>informative and then we compare the two definitions.</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:96"><nobr><span class="ft3">Definition 5. The frequency-based probability of be-</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft3">ing informative:</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:115"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:124"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:142"><nobr><span class="ft3">(t is informative|c) := - ln</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:309"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:314"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:285"><nobr><span class="ft3">- ln</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:316"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:314"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:129"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:153"><nobr><span class="ft3">- log</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:184"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:198"><nobr><span class="ft3">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:204"><nobr><span class="ft3">N = 1 - log</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:284"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:297"><nobr><span class="ft3">n(t) = 1 - ln n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:371"><nobr><span class="ft3">ln N</span></nobr></DIV>
<DIV style="position:absolute;top:1007;left:94"><nobr><span class="ft3">We define the Poisson-based probability of being informa-</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:81"><nobr><span class="ft9">tive analogously to the frequency-based probability of being<br>informative (see definition 5).</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:96"><nobr><span class="ft3">Definition 6. The Poisson-based probability of be-</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:475"><nobr><span class="ft3">ing informative:</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:503"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:512"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:527"><nobr><span class="ft3">(t is informative|c) := -</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:678"><nobr><span class="ft3">ln e</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:707"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:726"><nobr><span class="ft3">·</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:733"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:747"><nobr><span class="ft17">n(t)<br>k=1 </span></nobr></DIV>
<DIV style="position:absolute;top:418;left:779"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:774"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:692"><nobr><span class="ft3">- ln(e</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:729"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:748"><nobr><span class="ft3">· )</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:601"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:627"><nobr><span class="ft3"> - ln</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:666"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:680"><nobr><span class="ft17">n(t)<br>k=1</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:705"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:712"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:707"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:650"><nobr><span class="ft3"> - ln </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:475"><nobr><span class="ft3">For the sum expression, the following limit holds:</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:582"><nobr><span class="ft3">lim</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:571"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:615"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:615"><nobr><span class="ft5">k=1</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:639"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:538;left:647"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:641"><nobr><span class="ft3">k! = e</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:680"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:546;left:691"><nobr><span class="ft3">- 1</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:475"><nobr><span class="ft9">For  &gt;&gt; 1, we can alter the noise and informativeness Pois-<br>son by starting the sum from 0, since e</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:717"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:601;left:729"><nobr><span class="ft3">&gt;&gt; 1. Then, the</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:475"><nobr><span class="ft3">minimal Poisson informativeness is poisson(0, ) = e</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:789"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:805"><nobr><span class="ft3">. We</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:475"><nobr><span class="ft3">obtain a simplified Poisson probability of being informative:</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:505"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:514"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:529"><nobr><span class="ft3">(t is informative|c)   - ln</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:703"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:717"><nobr><span class="ft17">n(t)<br>k=0 </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:749"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:744"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:707"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:601"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:626"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:636"><nobr><span class="ft3">- ln</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:665"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:680"><nobr><span class="ft17">n(t)<br>k=0 </span></nobr></DIV>
<DIV style="position:absolute;top:692;left:711"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:706"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:681"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:475"><nobr><span class="ft9">The computation of the Poisson sum requires an optimi-<br>sation for large n(t). The implementation for this paper<br>exploits the nature of the Poisson density: The Poisson den-<br>sity yields only values significantly greater than zero in an<br>interval around .</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:489"><nobr><span class="ft3">Consider the illustration of the noise and informative-</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:475"><nobr><span class="ft9">ness definitions in figure 1. The probability functions dis-<br>played are summarised in figure 2 where the simplified Pois-<br>son is used in the noise and informativeness graphs. The<br>frequency-based noise corresponds to the linear solid curve<br>in the noise figure. With an independence assumption, we<br>obtain the curve in the lower triangle of the noise figure. By<br>changing the parameter p := /N of the independence prob-<br>ability, we can lift or lower the independence curve. The<br>noise figure shows the lifting for the value  := ln N <br>9.2. The setting  = ln N is special in the sense that the<br>frequency-based and the Poisson-based informativeness have<br>the same denominator, namely ln N, and the Poisson sum<br>converges to . Whether we can draw more conclusions from<br>this setting is an open question.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft3">We can conclude, that the lifting is desirable if we know</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft3">for a collection that terms that occur in relatively few doc-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">232</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="10007.png" alt="background image">
<DIV style="position:absolute;top:276;left:160"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:149"><nobr><span class="ft3"> 0.2</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:149"><nobr><span class="ft3"> 0.4</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:149"><nobr><span class="ft3"> 0.6</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:149"><nobr><span class="ft3"> 0.8</span></nobr></DIV>
<DIV style="position:absolute;top:92;left:160"><nobr><span class="ft3"> 1</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:173"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:213"><nobr><span class="ft3"> 2000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:263"><nobr><span class="ft3"> 4000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:314"><nobr><span class="ft3"> 6000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:365"><nobr><span class="ft3"> 8000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:412"><nobr><span class="ft3"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:128"><nobr><span class="ft3">Probability of being noisy</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:193"><nobr><span class="ft3">n(t): Number of documents with term t</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:311"><nobr><span class="ft3">frequency</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:258"><nobr><span class="ft3">independence: 1/N</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:236"><nobr><span class="ft3">independence: ln(N)/N</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:287"><nobr><span class="ft18">poisson: 1000<br>poisson: 2000</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:253"><nobr><span class="ft3">poisson: 1000,2000</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:509"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:497"><nobr><span class="ft3"> 0.2</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:497"><nobr><span class="ft3"> 0.4</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:497"><nobr><span class="ft3"> 0.6</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:497"><nobr><span class="ft3"> 0.8</span></nobr></DIV>
<DIV style="position:absolute;top:92;left:509"><nobr><span class="ft3"> 1</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:522"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:562"><nobr><span class="ft3"> 2000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:612"><nobr><span class="ft3"> 4000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:663"><nobr><span class="ft3"> 6000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:714"><nobr><span class="ft3"> 8000</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:760"><nobr><span class="ft3"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:477"><nobr><span class="ft3">Probability of being informative</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:542"><nobr><span class="ft3">n(t): Number of documents with term t</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:660"><nobr><span class="ft3">frequency</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:606"><nobr><span class="ft3">independence: 1/N</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:585"><nobr><span class="ft3">independence: ln(N)/N</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:635"><nobr><span class="ft18">poisson: 1000<br>poisson: 2000</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:602"><nobr><span class="ft3">poisson: 1000,2000</span></nobr></DIV>
<DIV style="position:absolute;top:338;left:330"><nobr><span class="ft3">Figure 1: Noise and Informativeness</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:149"><nobr><span class="ft3">Probability function</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:421"><nobr><span class="ft3">Noise</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:599"><nobr><span class="ft3">Informativeness</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:149"><nobr><span class="ft3">Frequency P</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:224"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:301"><nobr><span class="ft3">Def</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:416"><nobr><span class="ft3">n(t)/N</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:586"><nobr><span class="ft3">ln(n(t)/N)/ ln(1/N)</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:301"><nobr><span class="ft3">Interval</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:384"><nobr><span class="ft3">1/N  P</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:437"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:459"><nobr><span class="ft3"> 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:597"><nobr><span class="ft3">0.0  P</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:642"><nobr><span class="ft5">freq</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:664"><nobr><span class="ft3"> 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:149"><nobr><span class="ft3">Independence P</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:244"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:301"><nobr><span class="ft3">Def</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:394"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:404"><nobr><span class="ft3">- (1 - p)</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:460"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:573"><nobr><span class="ft3">ln(1</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:600"><nobr><span class="ft3">- (1 - p)</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:656"><nobr><span class="ft5">n(t)</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:676"><nobr><span class="ft3">)/ ln(p)</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:301"><nobr><span class="ft3">Interval</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:383"><nobr><span class="ft3">p  P</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:417"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:431"><nobr><span class="ft3">&lt; 1 - e</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:476"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:595"><nobr><span class="ft3">ln(p)  P</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:651"><nobr><span class="ft5">in</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:666"><nobr><span class="ft3"> 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:149"><nobr><span class="ft3">Poisson P</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:207"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:301"><nobr><span class="ft3">Def</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:398"><nobr><span class="ft3">e</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:404"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:423"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:437"><nobr><span class="ft17">n(t)<br>k=1 </span></nobr></DIV>
<DIV style="position:absolute;top:462;left:469"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:464"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:562"><nobr><span class="ft3">( - ln</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:607"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:621"><nobr><span class="ft17">n(t)<br>k=1 </span></nobr></DIV>
<DIV style="position:absolute;top:462;left:653"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:648"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:661"><nobr><span class="ft3">)/( - ln )</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:301"><nobr><span class="ft3">Interval</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:364"><nobr><span class="ft3">e</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:370"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:389"><nobr><span class="ft3">·   P</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:431"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:450"><nobr><span class="ft3">&lt; 1 - e</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:495"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:527"><nobr><span class="ft3">( - ln(e</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:581"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:591"><nobr><span class="ft3">- 1))/( - ln )  P</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:715"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:734"><nobr><span class="ft3"> 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:149"><nobr><span class="ft3">Poisson P</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:207"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:227"><nobr><span class="ft3">simplified</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:301"><nobr><span class="ft3">Def</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:398"><nobr><span class="ft3">e</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:404"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:423"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:437"><nobr><span class="ft17">n(t)<br>k=0 </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:469"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:464"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:587"><nobr><span class="ft3">( - ln</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:631"><nobr><span class="ft13">È</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:646"><nobr><span class="ft17">n(t)<br>k=0 </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:677"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:673"><nobr><span class="ft5">k!</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:686"><nobr><span class="ft3">)/</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:301"><nobr><span class="ft3">Interval</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:387"><nobr><span class="ft3">e</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:393"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:413"><nobr><span class="ft3"> P</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:437"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:456"><nobr><span class="ft3">&lt; 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:599"><nobr><span class="ft3">0.0 &lt; P</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:643"><nobr><span class="ft5">poi</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:662"><nobr><span class="ft3"> 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:349"><nobr><span class="ft3">Figure 2: Probability functions</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:81"><nobr><span class="ft9">uments are no guarantee for finding relevant documents,<br>i. e. we assume that rare terms are still relatively noisy. On<br>the opposite, we could lower the curve when assuming that<br>frequent terms are not too noisy, i. e. they are considered as<br>being still significantly discriminative.</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:94"><nobr><span class="ft3">The Poisson probabilities approximate the independence</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:81"><nobr><span class="ft9">probabilities for large n(t); the approximation is better for<br>larger . For n(t) &lt; , the noise is zero whereas for n(t) &gt; <br>the noise is one. This radical behaviour can be smoothened<br>by using a multi-dimensional Poisson distribution. Figure 1<br>shows a Poisson noise based on a two-dimensional Poisson:</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:83"><nobr><span class="ft3">poisson(k, </span></nobr></DIV>
<DIV style="position:absolute;top:803;left:154"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:160"><nobr><span class="ft3">, </span></nobr></DIV>
<DIV style="position:absolute;top:803;left:175"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:181"><nobr><span class="ft3">) :=  · e</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:233"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:249"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:258"><nobr><span class="ft3">· </span></nobr></DIV>
<DIV style="position:absolute;top:789;left:275"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:275"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:269"><nobr><span class="ft3">k! + (1 - ) · e</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:360"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:375"><nobr><span class="ft16">2</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:385"><nobr><span class="ft3">· </span></nobr></DIV>
<DIV style="position:absolute;top:789;left:401"><nobr><span class="ft5">k</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:401"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:395"><nobr><span class="ft3">k!</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:81"><nobr><span class="ft3">The two dimensional Poisson shows a plateau between </span></nobr></DIV>
<DIV style="position:absolute;top:834;left:419"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:429"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft3">1000 and </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:150"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:162"><nobr><span class="ft3">= 2000, we used here  = 0.5. The idea be-</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:81"><nobr><span class="ft9">hind this setting is that terms that occur in less than 1000<br>documents are considered to be not noisy (i.e. they are in-<br>formative), that terms between 1000 and 2000 are half noisy,<br>and that terms with more than 2000 are definitely noisy.</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:94"><nobr><span class="ft3">For the informativeness, we observe that the radical be-</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:81"><nobr><span class="ft9">haviour of Poisson is preserved. The plateau here is ap-<br>proximately at 1/6, and it is important to realise that this<br>plateau is not obtained with the multi-dimensional Poisson<br>noise using  = 0.5. The logarithm of the noise is nor-<br>malised by the logarithm of a very small number, namely<br>0.5 · e</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:116"><nobr><span class="ft5">-1000</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:150"><nobr><span class="ft3">+ 0.5 · e</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:199"><nobr><span class="ft5">-2000</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:231"><nobr><span class="ft3">. That is why the informativeness</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft9">will be only close to one for very little noise, whereas for a<br>bit of noise, informativeness will drop to zero. This effect<br>can be controlled by using small values for  such that the</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:475"><nobr><span class="ft3">noise in the interval [</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:614"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:621"><nobr><span class="ft3">; </span></nobr></DIV>
<DIV style="position:absolute;top:613;left:635"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:641"><nobr><span class="ft3">] is still very little. The setting</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:475"><nobr><span class="ft3"> = e</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:509"><nobr><span class="ft5">-2000/6</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:555"><nobr><span class="ft3">leads to noise values of approximately e</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:791"><nobr><span class="ft5">-2000/6</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft3">in the interval [</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:579"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:585"><nobr><span class="ft3">; </span></nobr></DIV>
<DIV style="position:absolute;top:646;left:599"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:606"><nobr><span class="ft3">], the logarithms lead then to 1/6 for</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:475"><nobr><span class="ft3">the informativeness.</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:489"><nobr><span class="ft3">The indepence-based and frequency-based informativeness</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:475"><nobr><span class="ft9">functions do not differ as much as the noise functions do.<br>However, for the indepence-based probability of being infor-<br>mative, we can control the average informativeness by the<br>definition p := /N whereas the control on the frequency-<br>based is limited as we address next.</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:489"><nobr><span class="ft3">For the frequency-based idf , the gradient is monotonously</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:475"><nobr><span class="ft9">decreasing and we obtain for different collections the same<br>distances of idf -values, i. e. the parameter N does not affect<br>the distance. For an illustration, consider the distance be-<br>tween the value idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:599"><nobr><span class="ft5">n+1</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:620"><nobr><span class="ft3">) of a term t</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:694"><nobr><span class="ft5">n+1</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:720"><nobr><span class="ft3">that occurs in n+1</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:475"><nobr><span class="ft3">documents, and the value idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:658"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:666"><nobr><span class="ft3">) of a term t</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:739"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:751"><nobr><span class="ft3">that occurs in</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:475"><nobr><span class="ft3">n documents.</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:557"><nobr><span class="ft3">idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:583"><nobr><span class="ft5">n+1</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:604"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:613"><nobr><span class="ft3">- idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:652"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:660"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:679"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:703"><nobr><span class="ft3">ln</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:730"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:719"><nobr><span class="ft3">n + 1</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:475"><nobr><span class="ft3">The first three values of the distance function are:</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:553"><nobr><span class="ft3">idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:579"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:585"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:593"><nobr><span class="ft3">- idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:633"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:639"><nobr><span class="ft3">) = ln(1/(1 + 1)) = 0.69</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:553"><nobr><span class="ft3">idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:579"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:585"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:593"><nobr><span class="ft3">- idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:633"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:639"><nobr><span class="ft3">) = ln(1/(2 + 1)) = 0.41</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:553"><nobr><span class="ft3">idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:579"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:585"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:593"><nobr><span class="ft3">- idf(t</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:633"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:639"><nobr><span class="ft3">) = ln(1/(3 + 1)) = 0.29</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:475"><nobr><span class="ft9">For the Poisson-based informativeness, the gradient decreases<br>first slowly for small n(t), then rapidly near n(t)   and<br>then it grows again slowly for large n(t).</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft3">In conclusion, we have seen that the Poisson-based defini-</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft3">tion provides more control and parameter possibilities than</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">233</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="10008.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft9">the frequency-based definition does. Whereas more control<br>and parameter promises to be positive for the personalisa-<br>tion of retrieval systems, it bears at the same time the dan-<br>ger of just too many parameters. The framework presented<br>in this paper raises the awareness about the probabilistic<br>and information-theoretic meanings of the parameters. The<br>parallel definitions of the frequency-based probability and<br>the Poisson-based probability of being informative made<br>the underlying assumptions explicit. The frequency-based<br>probability can be explained by binary occurrence, constant<br>containment and disjointness of documents. Independence<br>of documents leads to Poisson, where we have to be aware<br>that Poisson approximates the probability of a disjunction<br>for a large number of events, but not for a small number.<br>This theoretical result explains why experimental investiga-<br>tions on Poisson (see [7]) show that a Poisson estimation<br>does work better for frequent (bad, noisy) terms than for<br>rare (good, informative) terms.</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:94"><nobr><span class="ft3">In addition to the collection-wide parameter setting, the</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:81"><nobr><span class="ft9">framework presented here allows for document-dependent<br>settings, as explained for the independence probability. This<br>is in particular interesting for heterogeneous and structured<br>collections, since documents are different in nature (size,<br>quality, root document, sub document), and therefore, bi-<br>nary occurrence and constant containment are less appro-<br>priate than in relatively homogeneous collections.</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:81"><nobr><span class="ft1">7.</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:112"><nobr><span class="ft1">SUMMARY</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:94"><nobr><span class="ft3">The definition of the probability of being informative trans-</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:81"><nobr><span class="ft9">forms the informative interpretation of the idf into a proba-<br>bilistic interpretation, and we can use the idf -based proba-<br>bility in probabilistic retrieval approaches. We showed that<br>the classical definition of the noise (document frequency) in<br>the inverse document frequency can be explained by three<br>assumptions: the term within-document occurrence prob-<br>ability is binary, the document containment probability is<br>constant, and the document containment events are disjoint.<br>By explicitly and mathematically formulating the assump-<br>tions, we showed that the classical definition of idf does not<br>take into account parameters such as the different nature<br>(size, quality, structure, etc.) of documents in a collection,<br>or the different nature of terms (coverage, importance, po-<br>sition, etc.) in a document. We discussed that the absence<br>of those parameters is compensated by a leverage effect of<br>the within-document term occurrence probability and the<br>document containment probability.</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:94"><nobr><span class="ft3">By applying an independence rather a disjointness as-</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:81"><nobr><span class="ft9">sumption for the document containment, we could estab-<br>lish a link between the noise probability (term occurrence<br>in a collection), information theory and Poisson. From the<br>frequency-based and the Poisson-based probabilities of be-<br>ing noisy, we derived the frequency-based and Poisson-based<br>probabilities of being informative. The frequency-based prob-<br>ability is relatively smooth whereas the Poisson probability<br>is radical in distinguishing between noisy or not noisy, and<br>informative or not informative, respectively. We showed how<br>to smoothen the radical behaviour of Poisson with a multi-<br>dimensional Poisson.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:94"><nobr><span class="ft3">The explicit and mathematical formulation of idf - and</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft9">Poisson-assumptions is the main result of this paper. Also,<br>the paper emphasises the duality of idf and tf , collection<br>space and document space, respectively. Thus, the result<br>applies to term occurrence and document containment in a</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">collection, and it applies to term occurrence and position<br>containment in a document. This theoretical framework is<br>useful for understanding and deciding the parameter estima-<br>tion and combination in probabilistic retrieval models. The<br>links between indepence-based noise as document frequency,<br>probabilistic interpretation of idf , information theory and<br>Poisson described in this paper may lead to variable proba-<br>bilistic idf and tf definitions and combinations as required<br>in advanced and personalised information retrieval systems.</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:475"><nobr><span class="ft9">Acknowledgment: I would like to thank Mounia Lalmas,<br>Gabriella Kazai and Theodora Tsikrika for their comments<br>on the as they said "heavy" pieces. My thanks also go to the<br>meta-reviewer who advised me to improve the presentation<br>to make it less "formidable" and more accessible for those<br>"without a theoretic bent".</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:659"><nobr><span class="ft3">This work was funded by a</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:475"><nobr><span class="ft3">research fellowship from Queen Mary University of London.</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:475"><nobr><span class="ft1">8.</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:507"><nobr><span class="ft1">REFERENCES</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:482"><nobr><span class="ft3">[1] A. Aizawa. An information-theoretic perspective of</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:503"><nobr><span class="ft9">tf-idf measures. Information Processing and<br>Management, 39:45­65, January 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:482"><nobr><span class="ft3">[2] G. Amati and C. J. Rijsbergen. Term frequency</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:503"><nobr><span class="ft9">normalization via Pareto distributions. In 24th<br>BCS-IRSG European Colloquium on IR Research,<br>Glasgow, Scotland, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:482"><nobr><span class="ft3">[3] R. K. Belew. Finding out about. Cambridge University</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:503"><nobr><span class="ft3">Press, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:482"><nobr><span class="ft3">[4] A. Bookstein and D. Swanson. Probabilistic models</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:503"><nobr><span class="ft9">for automatic indexing. Journal of the American<br>Society for Information Science, 25:312­318, 1974.</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:482"><nobr><span class="ft3">[5] I. N. Bronstein. Taschenbuch der Mathematik. Harri</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:503"><nobr><span class="ft3">Deutsch, Thun, Frankfurt am Main, 1987.</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:482"><nobr><span class="ft3">[6] K. Church and W. Gale. Poisson mixtures. Natural</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:503"><nobr><span class="ft3">Language Engineering, 1(2):163­190, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:482"><nobr><span class="ft3">[7] K. W. Church and W. A. Gale. Inverse document</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:503"><nobr><span class="ft9">frequency: A measure of deviations from poisson. In<br>Third Workshop on Very Large Corpora, ACL<br>Anthology, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:482"><nobr><span class="ft3">[8] T. Lafouge and C. Michel. Links between information</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:503"><nobr><span class="ft9">construction and information gain: Entropy and<br>bibliometric distribution. Journal of Information<br>Science, 27(1):39­49, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:482"><nobr><span class="ft3">[9] E. Margulis. N-poisson document modelling. In</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:503"><nobr><span class="ft9">Proceedings of the 15th Annual International ACM<br>SIGIR Conference on Research and Development in<br>Information Retrieval, pages 177­189, 1992.</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:475"><nobr><span class="ft3">[10] S. E. Robertson and S. Walker. Some simple effective</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:503"><nobr><span class="ft9">approximations to the 2-poisson model for<br>probabilistic weighted retrieval. In Proceedings of the<br>17th Annual International ACM SIGIR Conference on<br>Research and Development in Information Retrieval,<br>pages 232­241, London, et al., 1994. Springer-Verlag.</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:475"><nobr><span class="ft3">[11] S. Wong and Y. Yao. An information-theoric measure</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:503"><nobr><span class="ft9">of term specificity. Journal of the American Society<br>for Information Science, 43(1):54­61, 1992.</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:475"><nobr><span class="ft3">[12] S. Wong and Y. Yao. On modeling information</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:503"><nobr><span class="ft9">retrieval with probabilistic inference. ACM<br>Transactions on Information Systems, 13(1):38­68,<br>1995.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">234</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
