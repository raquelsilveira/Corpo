<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\126</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-08-15T11:05:38+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Times;color:#000000;}
	.ft1{font-size:15px;font-family:Times;color:#000000;}
	.ft2{font-size:12px;font-family:Times;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:9px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft7{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft8{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126001.png" alt="background image">
<DIV style="position:absolute;top:109;left:104"><nobr><span class="ft0"><b>Learning the Unified Kernel Machines for Classification</b></span></nobr></DIV>
<DIV style="position:absolute;top:177;left:172"><nobr><span class="ft1">Steven C. H. Hoi</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:148"><nobr><span class="ft2">CSE, Chinese University of</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:201"><nobr><span class="ft2">Hong Kong</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:143"><nobr><span class="ft1">chhoi@cse.cuhk.edu.hk</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:398"><nobr><span class="ft1">Michael R. Lyu</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:367"><nobr><span class="ft2">CSE, Chinese University of</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:420"><nobr><span class="ft2">Hong Kong</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:372"><nobr><span class="ft1">lyu@cse.cuhk.edu.hk</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:608"><nobr><span class="ft1">Edward Y. Chang</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:581"><nobr><span class="ft2">ECE, University of California,</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:629"><nobr><span class="ft2">Santa Barbara</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:585"><nobr><span class="ft1">echang@ece.ucsb.edu</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:81"><nobr><span class="ft1">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft7">Kernel machines have been shown as the state-of-the-art<br>learning techniques for classification. In this paper, we pro-<br>pose a novel general framework of learning the Unified Ker-<br>nel Machines (UKM) from both labeled and unlabeled data.<br>Our proposed framework integrates supervised learning, semi-<br>supervised kernel learning, and active learning in a unified<br>solution. In the suggested framework, we particularly fo-<br>cus our attention on designing a new semi-supervised ker-<br>nel learning method, i.e., Spectral Kernel Learning (SKL),<br>which is built on the principles of kernel target alignment<br>and unsupervised kernel design. Our algorithm is related<br>to an equivalent quadratic programming problem that can<br>be efficiently solved.</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:222"><nobr><span class="ft3">Empirical results have shown that</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:81"><nobr><span class="ft7">our method is more effective and robust to learn the semi-<br>supervised kernels than traditional approaches. Based on<br>the framework, we present a specific paradigm of unified<br>kernel machines with respect to Kernel Logistic Regresions<br>(KLR), i.e., Unified Kernel Logistic Regression (UKLR). We<br>evaluate our proposed UKLR classification scheme in com-<br>parison with traditional solutions.</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:304"><nobr><span class="ft3">The promising results</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:81"><nobr><span class="ft7">show that our proposed UKLR paradigm is more effective<br>than the traditional classification approaches.</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:81"><nobr><span class="ft1">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:81"><nobr><span class="ft7">I.5.2 [PATTERN RECOGNITION]: Design Methodol-<br>ogy--Classifier design and evaluation; H.2.8 [Database Man-<br>agement]: Database Applications--Data mining</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:81"><nobr><span class="ft1">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:81"><nobr><span class="ft3">Design, Algorithms, Experimentation</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft1">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:81"><nobr><span class="ft7">Classification, Kernel Machines, Spectral Kernel Learning,<br>Supervised Learning, Semi-Supervised Learning, Unsuper-<br>vised Kernel Design, Kernel Logistic Regressions, Active<br>Learning</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft8">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br>KDD'06, August 20­23, 2006, Philadelphia, Pennsylvania, USA.<br>Copyright 2006 ACM 1-59593-339-5/06/0008 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:315"><nobr><span class="ft3">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:322"><nobr><span class="ft4">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:475"><nobr><span class="ft1">1.</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:507"><nobr><span class="ft1">INTRODUCTION</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:489"><nobr><span class="ft3">Classification is a core data mining technique and has been</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:475"><nobr><span class="ft7">actively studied in the past decades. In general, the goal of<br>classification is to assign unlabeled testing examples with a<br>set of predefined categories. Traditional classification meth-<br>ods are usually conducted in a supervised learning way, in<br>which only labeled data are used to train a predefined clas-<br>sification model. In literature, a variety of statistical models<br>have been proposed for classification in the machine learn-<br>ing and data mining communities. One of the most popu-<br>lar and successful methodologies is the kernel-machine tech-<br>niques, such as Support Vector Machines (SVM) [25] and<br>Kernel Logistic Regressions (KLR) [29]. Like other early<br>work for classification, traditional kernel-machine methods<br>are usually performed in the supervised learning way, which<br>consider only the labeled data in the training phase.</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:489"><nobr><span class="ft3">It is obvious that a good classification model should take</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:475"><nobr><span class="ft7">advantages on not only the labeled data, but also the un-<br>labeled data when they are available. Learning on both la-<br>beled and unlabeled data has become an important research<br>topic in recent years. One way to exploit the unlabled data<br>is to use active learning [7]. The goal of active learning is<br>to choose the most informative example from the unlabeled<br>data for manual labeling. In the past years, active learning<br>has been studied for many classification tasks [16].</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:489"><nobr><span class="ft3">Another emerging popular technique to exploit unlabeled</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:475"><nobr><span class="ft7">data is semi-supervised learning [5], which has attracted<br>a surge of research attention recently [30].</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:757"><nobr><span class="ft3">A variety of</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:475"><nobr><span class="ft7">machine-learning techniques have been proposed for semi-<br>supervised learning, in which the most well-known approaches<br>are based on the graph Laplacians methodology [28, 31, 5].<br>While promising results have been popularly reported in<br>this research topic, there is so far few comprehensive semi-<br>supervised learning scheme applicable for large-scale classi-<br>fication problems.</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:489"><nobr><span class="ft3">Although supervised learning, semi-supervised learning</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:475"><nobr><span class="ft7">and active learning have been studied separately, so far<br>there is few comprehensive scheme to combine these tech-<br>niques effectively together for classification tasks. To this<br>end, we propose a general framework of learning the Uni-<br>fied Kernel Machines (UKM) [3, 4] by unifying supervised<br>kernel-machine learning, semi-supervised learning, unsuper-<br>vised kernel design and active learning together for large-<br>scale classification problems.</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:489"><nobr><span class="ft3">The rest of this paper is organized as follows. Section 2 re-</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:475"><nobr><span class="ft7">views related work of our framework and proposed solutions.<br>Section 3 presents our framework of learning the unified ker-</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">187</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:679"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft9{font-size:6px;font-family:Times;color:#000000;}
	.ft10{font-size:15px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft7">nel machines. Section 4 proposes a new algorithm of learning<br>semi-supervised kernels by Spectral Kernel Learning (SKL).<br>Section 5 presents a specific UKM paradigm for classifica-<br>tion, i.e., the Unified Kernel Logistic Regression (UKLR).<br>Section 6 evaluates the empirical performance of our pro-<br>posed algorithm and the UKLR classification scheme. Sec-<br>tion 7 sets out our conclusion.</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft1">2.</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:112"><nobr><span class="ft1">RELATED WORK</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:94"><nobr><span class="ft3">Kernel machines have been widely studied for data clas-</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:81"><nobr><span class="ft3">sification in the past decade.</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:276"><nobr><span class="ft3">Most of earlier studies on</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:81"><nobr><span class="ft7">kernel machines usually are based on supervised learning.<br>One of the most well-known techniques is the Support Vec-<br>tor Machines, which have achieved many successful stories<br>in a variety of applications [25].</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:296"><nobr><span class="ft3">In addition to SVM, a</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:81"><nobr><span class="ft7">series of kernel machines have also been actively studied,<br>such as Kernel Logistic Regression [29], Boosting [17], Reg-<br>ularized Least-Square (RLS) [12] and Minimax Probability<br>Machines (MPM) [15], which have shown comparable per-<br>formance with SVM for classification. The main theoretical<br>foundation behind many of the kernel machines is the the-<br>ory of regularization and reproducing kernel Hilbert space<br>in statistical learning [17, 25]. Some theoretical connections<br>between the various kernel machines have been explored in<br>recent studies [12].</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:94"><nobr><span class="ft3">Semi-supervised learning has recently received a surge of</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:81"><nobr><span class="ft7">research attention for classification [5, 30]. The idea of semi-<br>supervised learning is to use both labeled and unlabeled data<br>when constructing the classifiers for classification tasks. One<br>of the most popular solutions in semi-supervised learning<br>is based on the graph theory [6], such as Markov random<br>walks [22], Gaussian random fields [31], Diffusion models [13]<br>and Manifold learning [2]. They have demonstrated some<br>promising results on classification.</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:94"><nobr><span class="ft3">Some recent studies have begun to seek connections be-</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:81"><nobr><span class="ft7">tween the graph-based semi-supervised learning and the ker-<br>nel machine learning. Smola and Kondor showed some theo-<br>retical understanding between kernel and regularization based<br>on the graph theory [21]. Belkin et al. developed a frame-<br>work for regularization on graphs and provided some anal-<br>ysis on generalization error bounds [1]. Based on the emerg-<br>ing theoretical connections between kernels and graphs, some<br>recent work has proposed to learn the semi-supervised ker-<br>nels by graph Laplacians [32]. Zhang et al. recently pro-<br>vided a theoretical framework of unsupervised kernel design<br>and showed that the graph Laplacians solution can be con-<br>sidered as an equivalent kernel learning approach [27]. All<br>of the above studies have formed the solid foundation for<br>semi-supervised kernel learning in this work.</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:94"><nobr><span class="ft3">To exploit the unlabeled data, another research attention</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:81"><nobr><span class="ft7">is to employ active learning for reducing the labeling efforts<br>in classification tasks. Active learning, or called pool-based<br>active learning, has been proposed as an effective technique<br>for reducing the amount of labeled data in traditional super-<br>vised classification tasks [19]. In general, the key of active<br>learning is to choose the most informative unlabeled exam-<br>ples for manual labeling.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:249"><nobr><span class="ft3">A lot of active learning meth-</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:81"><nobr><span class="ft7">ods have been proposed in the community. Typically they<br>measure the classification uncertainty by the amount of dis-<br>agreement to the classification model [9, 10] or measure the<br>distance of each unlabeled example away from the classifi-<br>cation boundary [16, 24].</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:475"><nobr><span class="ft1">3.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:507"><nobr><span class="ft10">FRAMEWORK OF LEARNING UNIFIED<br>KERNEL MACHINES</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:489"><nobr><span class="ft3">In this section, we present the framework of learning the</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:475"><nobr><span class="ft7">unified kernel machines by combining supervised kernel ma-<br>chines, semi-supervised kernel learning and active learning<br>techniques into a unified solution. Figure 1 gives an overview<br>of our proposed scheme. For simplicity, we restrict our dis-<br>cussions to classification problems.</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:489"><nobr><span class="ft3">Let</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:514"><nobr><span class="ft3">M(K, ) denote a kernel machine that has some un-</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:475"><nobr><span class="ft7">derlying probabilistic model, such as kernel logistic regres-<br>sions (or support vector machines). In general, a kernel ma-<br>chine contains two components, i.e., the kernel K (either a<br>kernel function or simply a kernel matrix), and the model pa-<br>rameters . In traditional supervised kernel-machine learn-<br>ing, the kernel K is usually a known parametric kernel func-<br>tion and the goal of the learning task is usually to determine<br>the model parameter . This often limits the performance of<br>the kernel machine if the specified kernel is not appropriate.</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:489"><nobr><span class="ft3">To this end, we propose a unified scheme to learn the uni-</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:475"><nobr><span class="ft7">fied kernel machines by learning on both the kernel K and<br>the model parameters  together. In order to exploit the un-<br>labeled data, we suggest to combine semi-supervised kernel<br>learning and active learning techniques together for learn-<br>ing the unified kernel machines effectively from the labeled<br>and unlabeled data. More specifically, we outline a general<br>framework of learning the unified kernel machine as follows.</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:486"><nobr><span class="ft3">Figure 1: Learning the Unified Kernel Machines</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:489"><nobr><span class="ft3">Let L denote the labeled data and U denote the unlabeled</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:475"><nobr><span class="ft7">data. The goal of the unified kernel machine learning task is<br>to learn the kernel machine</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:642"><nobr><span class="ft3">M(K</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:676"><nobr><span class="ft9"></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:683"><nobr><span class="ft3">, </span></nobr></DIV>
<DIV style="position:absolute;top:847;left:698"><nobr><span class="ft9"></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:705"><nobr><span class="ft3">) that can classify the</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:475"><nobr><span class="ft7">data effectively. Specifically, it includes the following five<br>steps:</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:495"><nobr><span class="ft3">· Step 1. Kernel Initialization</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:509"><nobr><span class="ft3">The first step is to initialize the kernel component K</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:828"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:509"><nobr><span class="ft3">of the kernel machine</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:640"><nobr><span class="ft3">M(K</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:674"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:680"><nobr><span class="ft3">, </span></nobr></DIV>
<DIV style="position:absolute;top:946;left:695"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:701"><nobr><span class="ft3">). Typically, users can</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:509"><nobr><span class="ft3">specify the initial kernel K</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:665"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:674"><nobr><span class="ft3">(function or matrix) with a</span></nobr></DIV>
<DIV style="position:absolute;top:975;left:509"><nobr><span class="ft7">stanard kernel. When some domain knowledge is ava-<br>iable, users can also design some kernel with domain<br>knowledge (or some data-dependent kernels).</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:495"><nobr><span class="ft3">· Step 2. Semi-Supervised Kernel Learning</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:509"><nobr><span class="ft7">The initial kernel may not be good enough to clas-<br>sify the data correctly. Hence, we suggest to employ</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">188</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:81"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:-1px;font-family:Times;color:#000000;}
	.ft12{font-size:5px;font-family:Times;color:#000000;}
	.ft13{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126003.png" alt="background image">
<DIV style="position:absolute;top:86;left:114"><nobr><span class="ft7">the semi-supervised kernel learning technique to learn<br>a new kernel K by engaging both the labeled L and<br>unlabled data U available.</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:101"><nobr><span class="ft3">· Step 3. Model Parameter Estimation</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:114"><nobr><span class="ft7">When the kernel K is known, to estimate the param-<br>eters of the kernel machines based on some model as-<br>sumption, such as Kernel Logistic Regression or Sup-<br>port Vector Machines, one can simply employ the stan-<br>dard supervised kernel-machine learning to solve the<br>model parameters .</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:101"><nobr><span class="ft3">· Step 4. Active Learning</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:114"><nobr><span class="ft7">In many classification tasks, labeling cost is expensive.<br>Active learning is an important method to reduce hu-<br>man efforts in labeling. Typically, we can choose a<br>batch of most informative examples S that can most ef-<br>fectively update the current kernel machine</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:381"><nobr><span class="ft3">M(K, ).</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:101"><nobr><span class="ft3">· Step 5. Convergence Evaluation</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:114"><nobr><span class="ft7">The last step is the convergence evaluation in which we<br>check whether the kernel machine is good enough for<br>the classification task. If not, we will repeat the above<br>steps until a satisfied kernel machine is acquired.</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:94"><nobr><span class="ft3">This is a general framework of learning unified kernel ma-</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:81"><nobr><span class="ft7">chines. In this paper, we focus our main attention on the<br>the part of semi-supervised kernel learning technique, which<br>is a core component of learning the unified kernel machines.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft1">4.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:112"><nobr><span class="ft1">SPECTRAL KERNEL LEARNING</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:94"><nobr><span class="ft3">We propose a new semi-supervised kernel learning method,</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:81"><nobr><span class="ft7">which is a fast and robust algorithm for learning semi-supervised<br>kernels from labeled and unlabeled data. In the following<br>parts, we first introduce the theoretical motivations and then<br>present our spectral kernel learning algorithm. Finally, we<br>show the connections of our method to existing work and<br>justify the effectiveness of our solution from empirical ob-<br>servations.</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:81"><nobr><span class="ft1">4.1</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:121"><nobr><span class="ft1">Theoretical Foundation</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:94"><nobr><span class="ft3">Let us first consider a standard supservisd kernel learn-</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:81"><nobr><span class="ft7">ing problem. Assume that the data (X, Y ) are drawn from<br>an unknown distribution</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:237"><nobr><span class="ft3">D. The goal of supervised learn-</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:81"><nobr><span class="ft7">ing is to find a prediction function p(X) that minimizes the<br>following expected true loss:</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:195"><nobr><span class="ft3">E</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:205"><nobr><span class="ft9">(X,Y )D</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:253"><nobr><span class="ft3">L(p(X), Y ),</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:81"><nobr><span class="ft3">where E</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:132"><nobr><span class="ft9">(X,Y )D</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:185"><nobr><span class="ft3">denotes the expectation over the true un-</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:81"><nobr><span class="ft3">derlying distribution</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:209"><nobr><span class="ft3">D. In order to achieve a stable esimia-</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:81"><nobr><span class="ft7">tion, we usually need to restrict the size of hypothesis func-<br>tion family. Given l training examples (x</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:345"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:351"><nobr><span class="ft3">,y</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:362"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:368"><nobr><span class="ft3">),. . .,(x</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:411"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:415"><nobr><span class="ft3">,y</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:426"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:430"><nobr><span class="ft3">),</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:81"><nobr><span class="ft3">typically we train a predition function ^</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:324"><nobr><span class="ft3">p in a reproducing</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:81"><nobr><span class="ft3">Hilbert space</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:164"><nobr><span class="ft3">H by minimizing the empirical loss [25]. Since</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:81"><nobr><span class="ft7">the reproducing Hilbert space can be large, to avoid over-<br>fitting problems, we often consider a regularized method as<br>follow:</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:129"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:127"><nobr><span class="ft3">p = arg inf</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:174"><nobr><span class="ft9">pH</span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:212"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:213"><nobr><span class="ft3">l</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:231"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:224"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:246"><nobr><span class="ft3">L(p(x</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:281"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:286"><nobr><span class="ft3">), y</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:304"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:309"><nobr><span class="ft3">) + </span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:339"><nobr><span class="ft3">||p||</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:361"><nobr><span class="ft13">2<br>H</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:389"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:422"><nobr><span class="ft3">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft7">where  is a chosen positive regularization parameter. It<br>can be shown that the solution of (1) can be represented as</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft3">the following kernel method:</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:579"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:578"><nobr><span class="ft3">p(x) =</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:631"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:624"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:646"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:645"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:131;left:654"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:658"><nobr><span class="ft3">k(x</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:680"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:684"><nobr><span class="ft3">, x)</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:475"><nobr><span class="ft3"> = arg inf</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:526"><nobr><span class="ft9">R</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:548"><nobr><span class="ft12">l</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:568"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:568"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:588"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:581"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:602"><nobr><span class="ft3">L (p(x</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:640"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:645"><nobr><span class="ft3">), y</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:663"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:668"><nobr><span class="ft3">) + </span></nobr></DIV>
<DIV style="position:absolute;top:178;left:712"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:701"><nobr><span class="ft9">i,j=1</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:729"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:197;left:738"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:743"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:197;left:752"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:758"><nobr><span class="ft3">k(x</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:779"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:784"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:798"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:804"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:824"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:475"><nobr><span class="ft3">where</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:517"><nobr><span class="ft3"> is a parameter vector to be estimated from the</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:475"><nobr><span class="ft7">data and k is a kernel, which is known as kernel func-<br>tion. Typically a kernel returns the inner product between<br>the mapping images of two given data examples, such that<br>k(x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:497"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:501"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:516"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:522"><nobr><span class="ft3">) = (x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:575"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:579"><nobr><span class="ft3">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:615"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:620"><nobr><span class="ft3">) for x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:665"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:670"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:685"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:694"><nobr><span class="ft3"> X .</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:489"><nobr><span class="ft3">Let us now consider a semi-supervised learning setting.</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:475"><nobr><span class="ft3">Given labeled data</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:589"><nobr><span class="ft3">{(x</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:610"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:615"><nobr><span class="ft3">, y</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:628"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:633"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:638"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:645"><nobr><span class="ft13">l<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:667"><nobr><span class="ft3">and unlabeled data</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:783"><nobr><span class="ft3">{x</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:798"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:804"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:811"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:811"><nobr><span class="ft9">j=l+1</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:843"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:475"><nobr><span class="ft3">we consider to learn the real-valued vectors f</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:757"><nobr><span class="ft3"> R</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:780"><nobr><span class="ft9">m</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:795"><nobr><span class="ft3">by the</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:475"><nobr><span class="ft3">following semi-supervised learning method:</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:524"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:521"><nobr><span class="ft3">f = arg inf</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:569"><nobr><span class="ft9">f R</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:606"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:605"><nobr><span class="ft3">n</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:624"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:618"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:640"><nobr><span class="ft3">L(f</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:661"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:666"><nobr><span class="ft3">, y</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:679"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:683"><nobr><span class="ft3">) + f K</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:744"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:759"><nobr><span class="ft3">f</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:784"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:816"><nobr><span class="ft3">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:475"><nobr><span class="ft3">where K is an m</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:583"><nobr><span class="ft3">× m kernel matrix with K</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:745"><nobr><span class="ft9">i,j</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:763"><nobr><span class="ft3">= k(x</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:800"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:804"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:819"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:825"><nobr><span class="ft3">).</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:475"><nobr><span class="ft7">Zhang et al. [27] proved that the solution of the above semi-<br>supervised learning is equivelent to the solution of standard<br>supervised learning in (1), such that</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:580"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:577"><nobr><span class="ft3">f</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:584"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:593"><nobr><span class="ft3">= ^</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:608"><nobr><span class="ft3">p(x</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:629"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:635"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:654"><nobr><span class="ft3">j = 1, . . . , m.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:816"><nobr><span class="ft3">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:475"><nobr><span class="ft7">The theorem offers a princple of unsuperivsed kernel de-<br>sign: one can design a new kernel ¯</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:684"><nobr><span class="ft3">k(</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:697"><nobr><span class="ft3">·, ·) based on the unla-</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:475"><nobr><span class="ft3">beled data and then replace the orignal kernel k by ¯</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:787"><nobr><span class="ft3">k in the</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:475"><nobr><span class="ft7">standard supervised kernel learning. More specifically, the<br>framework of spectral kernel design suggests to design the<br>new kernel matrix ¯</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:589"><nobr><span class="ft3">K by a function g as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:597"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:593"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:631"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:625"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:647"><nobr><span class="ft3">g(</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:667"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:672"><nobr><span class="ft3">)v</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:686"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:690"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:699"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:712"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:816"><nobr><span class="ft3">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:475"><nobr><span class="ft3">where (</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:529"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:534"><nobr><span class="ft3">, v</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:548"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:553"><nobr><span class="ft3">) are the eigen-pairs of the original kernel ma-</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:475"><nobr><span class="ft3">trix K, and the function g(</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:635"><nobr><span class="ft3">·) can be regarded as a filter func-</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:475"><nobr><span class="ft7">tion or a transformation function that modifies the spectra<br>of the kernel. The authors in [27] show a theoretical justifi-<br>cation that designing a kernel matrix with faster spectral de-<br>cay rates should result in better generalization performance,<br>which offers an important pricinple in learning an effective<br>kernel matrix.</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:489"><nobr><span class="ft3">On the other hand, there are some recent papers that</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:475"><nobr><span class="ft7">have studied theoretical principles for learning effective ker-<br>nel functions or matrices from labeled and unlabeled data.<br>One important work is the kernel target alignment, which<br>can be used not only to assess the relationship between the<br>feature spaces by two kernels, but also to measure the simi-<br>larity between the feature space by a kernel and the feature<br>space induced by labels [8]. Specifically, given two kernel<br>matrices K</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:544"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:557"><nobr><span class="ft3">and K</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:598"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:604"><nobr><span class="ft3">, their relationship is defined by the</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:475"><nobr><span class="ft3">following score of alignment:</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:490"><nobr><span class="ft3">Definition 1. Kernel Alignment: The empirical align-</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:475"><nobr><span class="ft3">ment of two given kernels K</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:654"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:667"><nobr><span class="ft3">and K</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:707"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:720"><nobr><span class="ft3">with respect to the</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:475"><nobr><span class="ft3">sample set S is the quantity:</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:547"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:543"><nobr><span class="ft3">A(K</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:571"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:577"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:595"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:601"><nobr><span class="ft3">) =</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:670"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:681"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:688"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:706"><nobr><span class="ft9">2 F</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:627"><nobr><span class="ft11">Ô</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:646"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:657"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:664"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:681"><nobr><span class="ft9">1 F</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:708"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:719"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:726"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:743"><nobr><span class="ft9">2 F</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:816"><nobr><span class="ft3">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">189</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:679"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft14{font-size:11px;line-height:19px;font-family:Times;color:#000000;}
	.ft15{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126004.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft3">where K</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:132"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:142"><nobr><span class="ft3">is the kernel matrix induced by the kernel k</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:407"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:417"><nobr><span class="ft3">and</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:86"><nobr><span class="ft14">·, · is the Frobenius product between two matrices, i.e.,<br>K</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:98"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:104"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:122"><nobr><span class="ft9">2 F</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:147"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:161"><nobr><span class="ft11">È</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:176"><nobr><span class="ft15">n<br>i,j=1</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:205"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:212"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:219"><nobr><span class="ft3">(x</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:232"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:237"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:252"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:257"><nobr><span class="ft3">)k</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:270"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:276"><nobr><span class="ft3">(x</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:290"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:295"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:309"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:315"><nobr><span class="ft3">).</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:94"><nobr><span class="ft3">The above definition of kernel alignment offers a princi-</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft7">ple to learn the kernel matrix by assessing the relationship<br>between a given kernel and a target kernel induced by the<br>given labels. Let y =</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:221"><nobr><span class="ft3">{y</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:235"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:240"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:246"><nobr><span class="ft13">l<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:271"><nobr><span class="ft3">denote a vector of labels in</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft3">which y</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:127"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:135"><nobr><span class="ft3"> {+1, -1} for binary classification. Then the tar-</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:81"><nobr><span class="ft7">get kernel can be defined as T = yy . Let K be the kernel<br>matrix with the following structure</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:194"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:244"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:256"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:282"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:294"><nobr><span class="ft9">trt</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:242"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:254"><nobr><span class="ft9">trt</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:287"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:299"><nobr><span class="ft9">t</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:422"><nobr><span class="ft3">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:81"><nobr><span class="ft3">where K</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:132"><nobr><span class="ft9">ij</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:146"><nobr><span class="ft3">= (x</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:190"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:194"><nobr><span class="ft3">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:230"><nobr><span class="ft9">j</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:235"><nobr><span class="ft3">) , K</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:266"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:281"><nobr><span class="ft3">denotes the matrix part of</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:81"><nobr><span class="ft3">"train-data block" and K</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:234"><nobr><span class="ft9">t</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:243"><nobr><span class="ft3">denotes the matrix part of "test-</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:81"><nobr><span class="ft3">data block."</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:94"><nobr><span class="ft3">The theory in [8] provides the principle of learning the</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:81"><nobr><span class="ft7">kernel matrix, i.e., looking for a kernel matrix K with good<br>generalization performance is equivalent to finding the ma-<br>trix that maximizes the following empirical kernel alignment<br>score:</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:156"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:152"><nobr><span class="ft3">A(K</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:180"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:190"><nobr><span class="ft3">, T ) =</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:273"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:285"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:295"><nobr><span class="ft3">, T</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:317"><nobr><span class="ft9">F</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:232"><nobr><span class="ft11">Ô</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:251"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:263"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:273"><nobr><span class="ft3">, K</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:291"><nobr><span class="ft9">tr F</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:322"><nobr><span class="ft3">T, T</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:352"><nobr><span class="ft9">F</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:422"><nobr><span class="ft3">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:81"><nobr><span class="ft7">This principle has been used to learn the kernel matrices<br>with multiple kernel combinations [14] and also the semi-<br>supervised kernels from graph Laplacians [32]. Motivated by<br>the related theorecial work, we propose a new spectral ker-<br>nel learning (SKL) algorithm which learns spectrals of the<br>kernel matrix by obeying both the principle of unsupervised<br>kernel design and the principle of kernel target alignment.</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:81"><nobr><span class="ft1">4.2</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:121"><nobr><span class="ft1">Algorithm</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:94"><nobr><span class="ft3">Assume that we are given a set of labeled data L =</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:81"><nobr><span class="ft3">{x</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:96"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:101"><nobr><span class="ft3">, y</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:114"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:118"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:125"><nobr><span class="ft13">l<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:144"><nobr><span class="ft3">, a set of unlabeled data U =</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:348"><nobr><span class="ft3">{x</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:363"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:368"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:375"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:375"><nobr><span class="ft9">i=l+1</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:406"><nobr><span class="ft3">, and</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:81"><nobr><span class="ft3">an initial kernel matrix K.</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:266"><nobr><span class="ft3">We first conduct the eigen-</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:81"><nobr><span class="ft3">decomposition of the kernel matrix:</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:208"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:245"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:240"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:261"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:734;left:269"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:274"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:282"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:287"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:295"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:309"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:422"><nobr><span class="ft3">(8)</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:81"><nobr><span class="ft3">where (</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:136"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:140"><nobr><span class="ft3">, v</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:155"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:160"><nobr><span class="ft3">) are eigen pairs of K and are assumed in a</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:81"><nobr><span class="ft3">decreasing order, i.e., </span></nobr></DIV>
<DIV style="position:absolute;top:791;left:226"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:238"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:791;left:262"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:274"><nobr><span class="ft3"> . . .  </span></nobr></DIV>
<DIV style="position:absolute;top:793;left:338"><nobr><span class="ft9">n</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:345"><nobr><span class="ft3">. For efficiency</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:81"><nobr><span class="ft3">consideration, we select the top d eigen pairs, such that</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:189"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:201"><nobr><span class="ft9">d</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:211"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:834;left:233"><nobr><span class="ft9">d</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:227"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:248"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:852;left:256"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:261"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:269"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:274"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:283"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:296"><nobr><span class="ft3"> K ,</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:422"><nobr><span class="ft3">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:81"><nobr><span class="ft3">where the parameter d</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:237"><nobr><span class="ft3">n is a dimension cutoff factor that</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:81"><nobr><span class="ft7">can be determined by some criteria, such as the cumulative<br>eigen energy.</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:94"><nobr><span class="ft3">Based on the principle of unsupervised kernel design, we</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:81"><nobr><span class="ft3">consider to learn the kernel matrix as follows</span></nobr></DIV>
<DIV style="position:absolute;top:990;left:211"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:207"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:246"><nobr><span class="ft9">d</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:240"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:261"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:998;left:269"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:274"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:282"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:287"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:295"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:309"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:415"><nobr><span class="ft3">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft3">where </span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:129"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:138"><nobr><span class="ft3"> 0 are spectral coefficients of the new kernel ma-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft7">trix. The goal of spectral kernel learning (SKL) algorithm is<br>to find the optimal spectral coefficients </span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:331"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:340"><nobr><span class="ft3">for the following</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft3">optimization</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:563"><nobr><span class="ft3">max</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:569"><nobr><span class="ft9">¯</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:566"><nobr><span class="ft9">K,µ</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:659"><nobr><span class="ft3">^</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:655"><nobr><span class="ft3">A( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:671"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:682"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:693"><nobr><span class="ft3">, T )</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:809"><nobr><span class="ft3">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:528"><nobr><span class="ft3">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:640"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:636"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:667"><nobr><span class="ft11">È</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:682"><nobr><span class="ft15">d<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:703"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:156;left:711"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:716"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:724"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:729"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:737"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:645"><nobr><span class="ft3">trace( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:682"><nobr><span class="ft3">K) = 1</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:662"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:196;left:671"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:188;left:679"><nobr><span class="ft3"> 0,</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:602"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:611"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:619"><nobr><span class="ft3"> C</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:653"><nobr><span class="ft9">i+1</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:672"><nobr><span class="ft3">, i = 1 . . . d</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:739"><nobr><span class="ft3">- 1 ,</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:475"><nobr><span class="ft3">where C is introduced as a decay factor that satisfies C</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:809"><nobr><span class="ft3"> 1,</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:475"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:484"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:494"><nobr><span class="ft3">are top d eigen vectors of the original kernel matrix K,</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:479"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:475"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:487"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:502"><nobr><span class="ft3">is the kernel matrix restricted to the (labeled) training</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:475"><nobr><span class="ft3">data and T is the target kernel induced by labels.</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:805"><nobr><span class="ft3">Note</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:475"><nobr><span class="ft7">that C is introduced as an important parameter to control<br>the decay rate of spectral coefficients that will influence the<br>overall performance of the kernel machine.</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:489"><nobr><span class="ft3">The above optimization problem belongs to convex opti-</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:475"><nobr><span class="ft7">mization and is usually regarded as a semi-definite program-<br>ming problem (SDP) [14], which may not be computation-<br>ally efficient. In the following, we turn it into a Quadratic<br>Programming (QP) problem that can be solved much more<br>efficiently.</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:489"><nobr><span class="ft3">By the fact that the objective function (7) is invariant</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:475"><nobr><span class="ft3">to the constant term T, T</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:643"><nobr><span class="ft9">F</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:652"><nobr><span class="ft3">, we can rewrite the objective</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:475"><nobr><span class="ft3">function into the following form</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:628"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:625"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:636"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:647"><nobr><span class="ft3">, T</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:669"><nobr><span class="ft9">F</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:606"><nobr><span class="ft11">Ô</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:629"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:625"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:637"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:648"><nobr><span class="ft3">, ¯</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:654"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:665"><nobr><span class="ft9">tr F</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:696"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:809"><nobr><span class="ft3">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:475"><nobr><span class="ft7">The above alignment is invariant to scales. In order to re-<br>move the trace constraint in (11), we consider the following<br>alternative approach. Instead of maximizing the objective<br>function (12) directly, we can fix the numerator to 1 and<br>then minimize the denominator. Therefore, we can turn the<br>optimization problem into:</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:566"><nobr><span class="ft3">min</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:574"><nobr><span class="ft9">µ</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:643"><nobr><span class="ft11">Ô</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:665"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:662"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:674"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:684"><nobr><span class="ft3">, ¯</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:690"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:702"><nobr><span class="ft9">tr F</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:809"><nobr><span class="ft3">(13)</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:528"><nobr><span class="ft3">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:640"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:636"><nobr><span class="ft3">K =</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:667"><nobr><span class="ft11">È</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:682"><nobr><span class="ft15">d<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:703"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:715;left:711"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:716"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:724"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:729"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:737"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:652"><nobr><span class="ft3">¯</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:648"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:660"><nobr><span class="ft9">tr</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:671"><nobr><span class="ft3">, T</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:692"><nobr><span class="ft9">F</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:705"><nobr><span class="ft3">= 1</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:662"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:755;left:671"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:679"><nobr><span class="ft3"> 0,</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:602"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:775;left:611"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:619"><nobr><span class="ft3"> C</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:653"><nobr><span class="ft9">i+1</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:672"><nobr><span class="ft3">, i = 1 . . . d</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:739"><nobr><span class="ft3">- 1 .</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:475"><nobr><span class="ft7">This minimization problem without the trace constraint is<br>equivalent to the original maximization problem with the<br>trace constraint.</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:489"><nobr><span class="ft3">Let vec(A) denote the column vectorization of a matrix A</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:475"><nobr><span class="ft3">and let D = [vec(V</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:593"><nobr><span class="ft9">1,tr</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:612"><nobr><span class="ft3">) . . . vec(V</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:671"><nobr><span class="ft9">d,tr</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:691"><nobr><span class="ft3">)] be a constant matrix</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:475"><nobr><span class="ft3">with size of l</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:552"><nobr><span class="ft9">2</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:560"><nobr><span class="ft3">× d, in which the d matrices of V</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:757"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:765"><nobr><span class="ft3">= v</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:789"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:793"><nobr><span class="ft3">v</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:802"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:815"><nobr><span class="ft3">are</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:475"><nobr><span class="ft3">with size of l</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:557"><nobr><span class="ft3">× l. It is not difficult to show that the above</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:475"><nobr><span class="ft3">problem is equivalent to the following optimization</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:566"><nobr><span class="ft3">min</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:574"><nobr><span class="ft9">µ</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:667"><nobr><span class="ft3">||D||</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:809"><nobr><span class="ft3">(14)</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:528"><nobr><span class="ft3">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:637"><nobr><span class="ft3">vec(T ) D = 1</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:665"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:674"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:682"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:602"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:611"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:619"><nobr><span class="ft3"> C</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:653"><nobr><span class="ft9">i+1</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:672"><nobr><span class="ft3">, i = 1 . . . d</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:739"><nobr><span class="ft3">- 1 .</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:475"><nobr><span class="ft7">Minimizing the norm is then equivalent to minimizing the<br>squared norm. Hence, we can obtain the final optimization</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">190</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:81"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:3px;font-family:Times;color:#000000;}
	.ft17{font-size:2px;font-family:Times;color:#000000;}
	.ft18{font-size:4px;font-family:Times;color:#000000;}
	.ft19{font-size:3px;line-height:7px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126005.png" alt="background image">
<DIV style="position:absolute;top:266;left:235"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:268"><nobr><span class="ft16">5</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:299"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:331"><nobr><span class="ft16">15</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:363"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:396"><nobr><span class="ft16">25</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:428"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:227"><nobr><span class="ft16">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:227"><nobr><span class="ft16">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:227"><nobr><span class="ft16">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:227"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:227"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:227"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:232"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:312"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:223"><nobr><span class="ft12">Cumulative Energy</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:242"><nobr><span class="ft3">(a) Cumulative eigen energy</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:550"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:582"><nobr><span class="ft16">5</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:613"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:646"><nobr><span class="ft16">15</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:678"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:711"><nobr><span class="ft16">25</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:743"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:547"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:542"><nobr><span class="ft16">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:542"><nobr><span class="ft16">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:542"><nobr><span class="ft16">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:542"><nobr><span class="ft16">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:542"><nobr><span class="ft16">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:542"><nobr><span class="ft16">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:542"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:542"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:542"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:547"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:627"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:537"><nobr><span class="ft12">Scaled Coefficient</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:701"><nobr><span class="ft19">Original Kernel<br>SKL (C=1)<br>SKL (C=2)<br>SKL (C=3)</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:573"><nobr><span class="ft3">(b) Spectral coefficients</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:81"><nobr><span class="ft7">Figure 2: Illustration of cumulative eigen energy and the spectral coefficients of different decay factors on<br>the Ionosphere dataset. The initial kernel is a linear kernel and the number of labeled data is 20.</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:126"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:164"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:203"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:241"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:280"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:319"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:115"><nobr><span class="ft16">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:118"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:115"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:118"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:115"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:118"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:115"><nobr><span class="ft16">0.95</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:203"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:110"><nobr><span class="ft12">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:289"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:293"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:289"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:293"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:289"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:293"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:289"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:293"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:196"><nobr><span class="ft3">(a) C=1</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:384"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:402"><nobr><span class="ft16">5</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:418"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:436"><nobr><span class="ft16">15</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:454"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:471"><nobr><span class="ft16">25</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:489"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:507"><nobr><span class="ft16">35</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:525"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:543"><nobr><span class="ft16">45</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:560"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:374"><nobr><span class="ft16">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:377"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:374"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:377"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:374"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:377"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:374"><nobr><span class="ft16">0.95</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:455"><nobr><span class="ft18">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:370"><nobr><span class="ft18">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:533"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:536"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:533"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:536"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:533"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:536"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:533"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:536"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:445"><nobr><span class="ft3">(b) C=2</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:623"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:641"><nobr><span class="ft16">5</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:657"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:675"><nobr><span class="ft16">15</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:693"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:711"><nobr><span class="ft16">25</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:728"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:746"><nobr><span class="ft16">35</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:764"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:782"><nobr><span class="ft16">45</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:799"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:613"><nobr><span class="ft16">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:616"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:613"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:616"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:613"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:616"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:613"><nobr><span class="ft16">0.95</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:694"><nobr><span class="ft18">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:609"><nobr><span class="ft18">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:772"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:775"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:772"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:775"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:772"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:775"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:772"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:775"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:685"><nobr><span class="ft3">(c) C=3</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:81"><nobr><span class="ft7">Figure 3: Classification performance of semi-supervised kernels with different decay factors on the Ionosphere<br>dataset. The initial kernel is a linear kernel and the number of labeled data is 20.</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:81"><nobr><span class="ft3">problem as</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:171"><nobr><span class="ft3">min</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:179"><nobr><span class="ft9">µ</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:261"><nobr><span class="ft3"> D D</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:134"><nobr><span class="ft3">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:243"><nobr><span class="ft3">vec(T ) D = 1</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:271"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:800;left:279"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:288"><nobr><span class="ft3"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:208"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:820;left:216"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:225"><nobr><span class="ft3"> C</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:258"><nobr><span class="ft9">i+1</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:277"><nobr><span class="ft3">, i = 1 . . . d</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:344"><nobr><span class="ft3">- 1 .</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:81"><nobr><span class="ft7">This is a standard Quadratic Programming (QP) problem<br>that can be solved efficiently.</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:81"><nobr><span class="ft1">4.3</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:121"><nobr><span class="ft1">Connections and Justifications</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:94"><nobr><span class="ft3">The essential of our semi-supervised kernel learning method</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:81"><nobr><span class="ft7">is based on the theories of unsupervised kernel design and<br>kernel target alignment.</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:241"><nobr><span class="ft3">More specifically, we consider a</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:81"><nobr><span class="ft7">dimension-reduction effective method to learn the semi-supervised<br>kernel that maximizes the kernel alignment score. By exam-<br>ining the work on unsupervised kernel design, the following<br>two pieces of work can be summarized as a special case of<br>spectral kernel learning framework:</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:101"><nobr><span class="ft3">· Cluster Kernel</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:114"><nobr><span class="ft3">This method adopts a "[1. . . ,1,0,. . . ,0]" kernel that has</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:509"><nobr><span class="ft7">been used in spectral clustering [18]. It sets the top<br>spectral coefficients to 1 and the rest to 0, i.e.,</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:597"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:794;left:605"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:614"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:646"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:666"><nobr><span class="ft3">for</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:697"><nobr><span class="ft3">i</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:705"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:646"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:666"><nobr><span class="ft3">for</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:697"><nobr><span class="ft3">i &gt; d</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:742"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:809"><nobr><span class="ft3">(15)</span></nobr></DIV>
<DIV style="position:absolute;top:830;left:509"><nobr><span class="ft7">For a comparison, we refer to this method as "Cluster<br>kernel" denoted by K</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:640"><nobr><span class="ft9">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:674"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:495"><nobr><span class="ft3">· Truncated Kernel</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:509"><nobr><span class="ft7">Another method is called the truncated kernel that<br>keeps only the top d spectral coefficients</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:594"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:957;left:602"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:611"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:643"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:651"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:669"><nobr><span class="ft3">for</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:700"><nobr><span class="ft3">i</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:708"><nobr><span class="ft3"> d</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:646"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:666"><nobr><span class="ft3">for</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:697"><nobr><span class="ft3">i &gt; d</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:745"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:809"><nobr><span class="ft3">(16)</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:509"><nobr><span class="ft3">where </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:557"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:567"><nobr><span class="ft3">are the top eigen values of an initial kernel.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:509"><nobr><span class="ft7">We can see that this is exactly the method of ker-<br>nel principal component analysis [20] that keeps only<br>the d most significant principal components of a given<br>kernel. For a comparison, we denote this method as<br>K</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:521"><nobr><span class="ft9">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:545"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">191</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:679"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft20{font-size:1px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126006.png" alt="background image">
<DIV style="position:absolute;top:266;left:108"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:140"><nobr><span class="ft16">5</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:171"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:204"><nobr><span class="ft16">15</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:236"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:268"><nobr><span class="ft16">25</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:301"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:105"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:100"><nobr><span class="ft16">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:100"><nobr><span class="ft16">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:100"><nobr><span class="ft16">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:100"><nobr><span class="ft16">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:100"><nobr><span class="ft16">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:100"><nobr><span class="ft16">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:100"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:100"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:100"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:105"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:185"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:95"><nobr><span class="ft12">Scaled Coefficient</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:259"><nobr><span class="ft19">Original Kernel<br>SKL (C=1)<br>SKL (C=2)<br>SKL (C=3)</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:129"><nobr><span class="ft3">(a) Spectral coefficients</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:363"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:400"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:439"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:478"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:517"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:556"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:355"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:351"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:355"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:351"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:355"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:351"><nobr><span class="ft16">0.95</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:360"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:440"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:347"><nobr><span class="ft12">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:531"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:535"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:531"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:535"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:531"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:535"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:531"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:535"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:432"><nobr><span class="ft3">(b) C=1</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:618"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:655"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:694"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:733"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:772"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:811"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:610"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:607"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:610"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:607"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:610"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:607"><nobr><span class="ft16">0.95</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:615"><nobr><span class="ft16">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:695"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:602"><nobr><span class="ft12">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:782"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:786"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:782"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:786"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:782"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:786"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:782"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:786"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:688"><nobr><span class="ft3">(c) C=2</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:81"><nobr><span class="ft3">Figure 4:</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:158"><nobr><span class="ft3">Example of Spectral coefficients and performance impacted by different decay factors on the</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:81"><nobr><span class="ft3">Ionosphere dataset. The initial kernel is an RBF kernel and the number of labeled data is 20.</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:117"><nobr><span class="ft16">0</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:154"><nobr><span class="ft16">10</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:193"><nobr><span class="ft16">20</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:232"><nobr><span class="ft16">30</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:271"><nobr><span class="ft16">40</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:310"><nobr><span class="ft16">50</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:105"><nobr><span class="ft16">0.55</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:109"><nobr><span class="ft16">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:105"><nobr><span class="ft16">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:109"><nobr><span class="ft16">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:105"><nobr><span class="ft16">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:109"><nobr><span class="ft16">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:105"><nobr><span class="ft16">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:109"><nobr><span class="ft16">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:194"><nobr><span class="ft12">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:101"><nobr><span class="ft12">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:285"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:289"><nobr><span class="ft17">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:285"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:289"><nobr><span class="ft17">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:285"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:289"><nobr><span class="ft17">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:285"><nobr><span class="ft16">K</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:289"><nobr><span class="ft17">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:186"><nobr><span class="ft3">(a) C=1</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:371"><nobr><span class="ft17">0</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:390"><nobr><span class="ft17">5</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:407"><nobr><span class="ft17">10</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:426"><nobr><span class="ft17">15</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:445"><nobr><span class="ft17">20</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:463"><nobr><span class="ft17">25</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:482"><nobr><span class="ft17">30</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:501"><nobr><span class="ft17">35</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:519"><nobr><span class="ft17">40</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:538"><nobr><span class="ft17">45</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:557"><nobr><span class="ft17">50</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:365"><nobr><span class="ft17">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:362"><nobr><span class="ft17">0.55</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:365"><nobr><span class="ft17">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:362"><nobr><span class="ft17">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:365"><nobr><span class="ft17">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:362"><nobr><span class="ft17">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:365"><nobr><span class="ft17">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:362"><nobr><span class="ft17">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:365"><nobr><span class="ft17">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:449"><nobr><span class="ft16">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:359"><nobr><span class="ft16">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:537"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:540"><nobr><span class="ft20">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:537"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:540"><nobr><span class="ft20">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:537"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:540"><nobr><span class="ft20">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:537"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:540"><nobr><span class="ft20">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:436"><nobr><span class="ft3">(b) C=2</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:617"><nobr><span class="ft17">0</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:636"><nobr><span class="ft17">5</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:654"><nobr><span class="ft17">10</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:672"><nobr><span class="ft17">15</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:691"><nobr><span class="ft17">20</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:710"><nobr><span class="ft17">25</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:729"><nobr><span class="ft17">30</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:747"><nobr><span class="ft17">35</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:766"><nobr><span class="ft17">40</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:785"><nobr><span class="ft17">45</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:804"><nobr><span class="ft17">50</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:609"><nobr><span class="ft17">0.55</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:611"><nobr><span class="ft17">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:609"><nobr><span class="ft17">0.65</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:611"><nobr><span class="ft17">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:609"><nobr><span class="ft17">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:611"><nobr><span class="ft17">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:609"><nobr><span class="ft17">0.85</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:611"><nobr><span class="ft17">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:697"><nobr><span class="ft16">Dimension (d)</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:606"><nobr><span class="ft16">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:785"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:788"><nobr><span class="ft20">Origin</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:785"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:788"><nobr><span class="ft20">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:785"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:788"><nobr><span class="ft20">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:785"><nobr><span class="ft17">K</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:788"><nobr><span class="ft20">Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:683"><nobr><span class="ft3">(c) C=3</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:81"><nobr><span class="ft7">Figure 5: Classification performance of semi-supervised kernels with different decay factors on the Heart<br>dataset. The initial kernel is a linear kernel and the number of labeled data is 20.</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:94"><nobr><span class="ft3">In our case, in comparison with semi-supervised kernel</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:81"><nobr><span class="ft7">learning methods by graph Laplacians, our work is similar<br>to the approach in [32], which learns the spectral transfor-<br>mation of graph Laplacians by kernel target alignment with<br>order constraints. However, we should emphasize two im-<br>portant differences that will explain why our method can<br>work more effectively.</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:94"><nobr><span class="ft3">First, the work in [32] belongs to traditional graph based</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft7">semi-supervised learning methods which assume the kernel<br>matrix is derived from the spectral decomposition of graph<br>Laplacians.</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:161"><nobr><span class="ft3">Instead, our spectral kernel learning method</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft7">learns on any initial kernel and assume the kernel matrix is<br>derived from the spectral decomposition of the normalized<br>kernel.</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:94"><nobr><span class="ft3">Second, compared to the kernel learning method in [14],</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:81"><nobr><span class="ft7">the authors in [32] proposed to add order constraints into<br>the optimization of kernel target alignment [8] to enforce the<br>constraints of graph smoothness. In our case, we suggest<br>a decay factor C to constrain the relationship of spectral<br>coefficients in the optimization that can make the spectral<br>coefficients decay faster. In fact, if we ignore the difference<br>of graph Laplacians and assume that the initial kernel in our<br>method is given as K</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:219"><nobr><span class="ft3"> L</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:244"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:259"><nobr><span class="ft3">, we can see that the method</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:475"><nobr><span class="ft7">in [32] can be regarded as a special case of our method when<br>the decay factor C is set to 1 and the dimension cut-off<br>parameter d is set to n.</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:475"><nobr><span class="ft1">4.4</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:516"><nobr><span class="ft1">Empirical Observations</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:489"><nobr><span class="ft3">To argue that C = 1 in the spectral kernel learning al-</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:475"><nobr><span class="ft7">gorithm may not be a good choice for learning an effective<br>kernel, we illustrate some empirical examples to justifiy the<br>motivation of our spectral kernel learning algorithm. One<br>goal of our spectral kernel learning methodology is to attain<br>a fast decay rate of the spectral coefficients of the kernel<br>matrix. Figure 2 illustrates an example of the change of the<br>resulting spectral coefficients using different decay factors in<br>our spectral kernel learning algorithms. From the figure, we<br>can see that the curves with larger decay factors (C = 2, 3)<br>have faster decay rates than the original kernel and the one<br>using C = 1. Meanwhile, we can see that the cumulative<br>eigen energy score converges to 100% quickly when the num-<br>ber of dimensions is increased. This shows that we may use<br>much small number of eigen-pairs in our semi-supervised<br>kernel learning algorithm for large-scale problems.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft3">To examine more details in the impact of performance</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft3">with different decay factors, we evaluate the classification</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">192</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:81"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft21{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="126007.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft7">performance of spectral kernel learning methods with dif-<br>ferent decay factors in Figure 3. In the figure, we compare<br>the performance of different kernels with respect to spectral<br>kernel design methods. We can see that two unsupervised<br>kernels, K</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:145"><nobr><span class="ft9">Trunc</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:176"><nobr><span class="ft3">and K</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:217"><nobr><span class="ft9">Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:251"><nobr><span class="ft3">, tend to perform better than</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft7">the original kernel when the dimension is small. But their<br>performances are not very stable when the number of di-<br>mensions is increased. For comparison, the spectral kernel<br>learning method achieves very stable and good performance<br>when the decay factor C is larger than 1. When the decay<br>factor is equal to 1, the performance becomes unstable due<br>to the slow decay rates observed from our previous results<br>in Figure 3. This observation matches the theoretical jus-<br>tification [27] that a kernel with good performance usually<br>favors a faster decay rate of spectral coefficients.</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:94"><nobr><span class="ft3">Figure 4 and Figure 5 illustrate more empirical examples</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft7">based on different initial kernels, in which similar results<br>can be observed. Note that our suggested kernel learning<br>method can learn on any valid kernel, and different initial<br>kernels will impact the performance of the resulting spectral<br>kernels. It is usually helpful if the initial kernel is provided<br>with domain knowledge.</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:81"><nobr><span class="ft1">5.</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:112"><nobr><span class="ft21">UNIFIED KERNEL LOGISTIC<br>REGRESSION</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:94"><nobr><span class="ft3">In this section, we present a specific paradigm based on</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:81"><nobr><span class="ft7">the proposed framework of learning unified kernel machines.<br>We assume the underlying probabilistic model of the ker-<br>nel machine is Kernel Logistic Regression (KLR). Based on<br>the UKM framework, we develop the Unified Kernel Lo-<br>gistic Regression (UKLR) paradigm to tackle classification<br>tasks. Note that our framework is not restricted to the KLR<br>model, but also can be widely extended for many other ker-<br>nel machines, such as Support Vector Machine (SVM) and<br>Regularized Least-Square (RLS) classifiers.</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:94"><nobr><span class="ft3">Similar to other kernel machines, such as SVM, a KLR</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:81"><nobr><span class="ft7">problem can be formulated in terms of a stanard regularized<br>form of loss+penalty in the reproducing kernel Hilbert space<br>(RKHS):</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:141"><nobr><span class="ft3">min</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:136"><nobr><span class="ft9">f H</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:159"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:173"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:174"><nobr><span class="ft3">l</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:192"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:185"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:206"><nobr><span class="ft3">ln(1 + e</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:253"><nobr><span class="ft9">-y</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:268"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:272"><nobr><span class="ft9">f (x</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:290"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:294"><nobr><span class="ft9">)</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:299"><nobr><span class="ft3">) + </span></nobr></DIV>
<DIV style="position:absolute;top:749;left:324"><nobr><span class="ft3">2 ||f ||</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:356"><nobr><span class="ft13">2<br>H</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:366"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:381"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:415"><nobr><span class="ft3">(17)</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:81"><nobr><span class="ft3">where</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:122"><nobr><span class="ft3">H</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:133"><nobr><span class="ft9">K</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:149"><nobr><span class="ft3">is the RKHS by a kernel K and  is a regular-</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:81"><nobr><span class="ft7">ization parameter. By the representer theorem, the optimal<br>f (x) has the form:</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:190"><nobr><span class="ft3">f (x) =</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:244"><nobr><span class="ft9">l</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:237"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:258"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:855;left:267"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:271"><nobr><span class="ft3">K(x, x</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:312"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:317"><nobr><span class="ft3">) ,</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:415"><nobr><span class="ft3">(18)</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:81"><nobr><span class="ft3">where </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:129"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:138"><nobr><span class="ft3">are model parameters. Note that we omit the con-</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:81"><nobr><span class="ft7">stant term in f (x) for simplified notations. To solve the<br>KLR model parameters, there are a number of available<br>techniques for effective solutions [29].</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:94"><nobr><span class="ft3">When the kernel K and the model parameters  are avail-</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:81"><nobr><span class="ft7">able, we use the following solution for active learning, which<br>is simple and efficient for large-scale problems. More specifi-<br>cally, we measure the information entropy of each unlabeled<br>data example as follows</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:138"><nobr><span class="ft3">H(x; , K) =</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:222"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:236"><nobr><span class="ft9">N</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:244"><nobr><span class="ft12">C</span></nobr></DIV>
<DIV style="position:absolute;top:1074;left:236"><nobr><span class="ft9">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:257"><nobr><span class="ft3">p(C</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:279"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:284"><nobr><span class="ft3">|x)log(p(C</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:347"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:352"><nobr><span class="ft3">|x)) ,</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:415"><nobr><span class="ft3">(19)</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:493"><nobr><span class="ft7">Algorithm: Unified Kernel Logistic Regresssion<br>Input</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:513"><nobr><span class="ft3">· K</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:539"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:545"><nobr><span class="ft3">: Initial normalized kernel</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:513"><nobr><span class="ft3">· L: Set of labeled data</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:513"><nobr><span class="ft3">· U: Set of unlabeled data</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:493"><nobr><span class="ft3">Repeat</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:513"><nobr><span class="ft3">· Spectral Kernel Learning</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:527"><nobr><span class="ft3">K</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:544"><nobr><span class="ft3"> Spectral Kernel(K</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:674"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:680"><nobr><span class="ft3">, L, U );</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:513"><nobr><span class="ft3">· KLR Parameter Estimation</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:527"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:540"><nobr><span class="ft3"> KLR Solver(L, K);</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:513"><nobr><span class="ft3">· Convergence Test</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:527"><nobr><span class="ft3">If (converged), Exit Loop;</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:513"><nobr><span class="ft3">· Active Learning</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:527"><nobr><span class="ft3">x</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:535"><nobr><span class="ft9"></span></nobr></DIV>
<DIV style="position:absolute;top:372;left:546"><nobr><span class="ft3"> max</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:591"><nobr><span class="ft9">xU</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:618"><nobr><span class="ft3">H(x; , K)</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:527"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:541"><nobr><span class="ft3"> L  {x</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:602"><nobr><span class="ft9"></span></nobr></DIV>
<DIV style="position:absolute;top:388;left:609"><nobr><span class="ft3">}, U  U - {x</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:704"><nobr><span class="ft9"></span></nobr></DIV>
<DIV style="position:absolute;top:388;left:711"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:493"><nobr><span class="ft7">Until converged.<br>Output</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:513"><nobr><span class="ft3">· UKLR = M(K, ).</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:540"><nobr><span class="ft3">Figure 6: The UKLR Algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:475"><nobr><span class="ft3">where N</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:527"><nobr><span class="ft9">C</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:542"><nobr><span class="ft3">is the number of classes and C</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:732"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:742"><nobr><span class="ft3">denotes the i</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:822"><nobr><span class="ft9">th</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:475"><nobr><span class="ft3">class and p(C</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:559"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:563"><nobr><span class="ft3">|x) is the probability of the data example x</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:475"><nobr><span class="ft3">belonging to the i</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:585"><nobr><span class="ft9">th</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:602"><nobr><span class="ft3">class which can be naturally obtained</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:475"><nobr><span class="ft7">by the current KLR model (, K). The unlabeled data ex-<br>amples with maximum values of entropy will be considered<br>as the most informative data for labeling.</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:489"><nobr><span class="ft3">By unifying the spectral kernel learning method proposed</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:475"><nobr><span class="ft7">in Section 3, we summarize the proposed algorithm of Uni-<br>fied Kernel Logistic Regression (UKLR) in Figure 6. In the<br>algorithm, note that we can usually initialize a kernel by a<br>standard kernel with appropriate parameters determined by<br>cross validation or by a proper deisgn of the initial kernel<br>with domain knowledge.</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:475"><nobr><span class="ft1">6.</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:506"><nobr><span class="ft1">EXPERIMENTAL RESULTS</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:489"><nobr><span class="ft3">We discuss our empirical evaluation of the proposed frame-</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:475"><nobr><span class="ft7">work and algorithms for classification. We first evaluate the<br>effectiveness of our suggested spectral kernel learning algo-<br>rithm for learning semi-supervised kernels and then com-<br>pare the performance of our unified kernel logistic regression<br>paradigm with traditional classification schemes.</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:475"><nobr><span class="ft1">6.1</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:516"><nobr><span class="ft1">Experimental Testbed and Settings</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:489"><nobr><span class="ft3">We use the datasets from UCI machine learning reposi-</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:475"><nobr><span class="ft3">tory</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:500"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:507"><nobr><span class="ft3">. Four datasets are employed in our experiments. Ta-</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:475"><nobr><span class="ft7">ble 1 shows the details of four UCI datasets in our experi-<br>ments.</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:489"><nobr><span class="ft3">For experimental settings, to examine the influences of</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:475"><nobr><span class="ft7">different training sizes, we test the compared algorithms on<br>four different training set sizes for each of the four UCI<br>datasets. For each given training set size, we conduct 20<br>random trials in which a labeled set is randomly sampled</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:476"><nobr><span class="ft9">1</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:482"><nobr><span class="ft3">www.ics.uci.edu/ mlearn/MLRepository.html</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">193</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:679"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="126008.png" alt="background image">
<DIV style="position:absolute;top:96;left:94"><nobr><span class="ft3">Table 1: List of UCI machine learning datasets.</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:111"><nobr><span class="ft3">Dataset</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:192"><nobr><span class="ft3">#Instances</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:276"><nobr><span class="ft3">#Features</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:354"><nobr><span class="ft3">#Classes</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:111"><nobr><span class="ft3">Heart</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:215"><nobr><span class="ft3">270</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:300"><nobr><span class="ft3">13</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:378"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:111"><nobr><span class="ft3">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:215"><nobr><span class="ft3">351</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:300"><nobr><span class="ft3">34</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:378"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:111"><nobr><span class="ft3">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:215"><nobr><span class="ft3">208</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:300"><nobr><span class="ft3">60</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:378"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:111"><nobr><span class="ft3">Wine</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:215"><nobr><span class="ft3">178</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:300"><nobr><span class="ft3">13</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:378"><nobr><span class="ft3">3</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:81"><nobr><span class="ft7">from the whole dataset and all classes must be present in<br>the sampled labeled set.</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:246"><nobr><span class="ft3">The rest data examples of the</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:81"><nobr><span class="ft7">dataset are then used as the testing (unlabeled) data. To<br>train a classifier, we employ the standard KLR model for<br>classification. We choose the bounds on the regularization<br>parameters via cross validation for all compared kernels to<br>avoid an unfair comparison. For multi-class classification,<br>we perform one-against-all binary training and testing and<br>then pick the class with the maximum class probability.</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:81"><nobr><span class="ft1">6.2</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:121"><nobr><span class="ft1">Semi-Supervised Kernel Learning</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:94"><nobr><span class="ft3">In this part, we evaluate the performance of our spectral</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:81"><nobr><span class="ft7">kernel learning algorithm for learning semi-supervised ker-<br>nels. We implemented our algorithm by a standard Matlab<br>Quadratic Programming solver (quadprog). The dimension-<br>cut parameter d in our algorithm is simply fixed to 20 with-<br>out further optimizing. Note that one can easily determine<br>an appropriate value of d by examining the range of the<br>cumulative eigen energy score in order to reduce the com-<br>putational cost for large-scale problems. The decay factor<br>C is important for our spectral kernel learning algorithm.<br>As we have shown examples before, C must be a positive<br>real value greater than 1. Typically we favor a larger decay<br>factor to achieve better performance. But it must not be<br>set too large since the too large decay factor may result in<br>the overly stringent constraints in the optimization which<br>gives no solutions. In our experiments, C is simply fixed to<br>constant values (greater than 1) for the engaged datasets.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:94"><nobr><span class="ft3">For a comparison, we compare our SKL algorithms with</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:81"><nobr><span class="ft7">the state-of-the-art semi-supervised kernel learning method<br>by graph Laplacians [32], which is related to a quadrati-<br>cally constrained quaratic program (QCQP). More specif-<br>ically, we have implemented two graph Laplacians based<br>semi-supervised kernels by order constraints [32]. One is the<br>order-constrained graph kernel (denoted as "Order") and<br>the other is the improved order-constrained graph kernel<br>(denoted as "Imp-Order"), which removes the constraints<br>from constant eigenvectors. To carry a fair comparison, we<br>use the top 20 smallest eigenvalues and eigenvectors from<br>the graph Laplacian which is constructed with 10-NN un-<br>weighted graphs. We also include three standard kernels for<br>comparisons.</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:94"><nobr><span class="ft3">Table 2 shows the experimental results of the compared</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:81"><nobr><span class="ft7">kernels (3 standard and 5 semi-supervised kernels) based on<br>KLR classifiers on four UCI datasets with different sizes of<br>labeled data. Each cell in the table has two rows: the upper<br>row shows the average testing set accruacies with standard<br>errors; and the lower row gives the average run time in sec-<br>onds for learning the semi-supervised kernels on a 3GHz<br>desktop computer. We conducted a paired t-test at signifi-<br>cance level of 0.05 to assess the statistical significance of the<br>test set accuracy results. From the experimental results,<br>we found that the two order-constrained based graph ker-<br>nels perform well in the Ionosphere and Wine datasets, but<br>they do not achieve important improvements on the Heart</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft7">and Sonar datasets. Among all the compared kernels, the<br>semi-supervised kernels by our spectral kernel learning algo-<br>rithms achieve the best performances. The semi-supervised<br>kernel initialized with an RBF kernel outperforms other ker-<br>nels in most cases. For example, in Ionosphere dataset, an<br>RBF kernel with 10 initial training examples only achieves<br>73.56% test set accuracy, and the SKL algorithm can boost<br>the accuracy significantly to 83.36%. Finally, looking into<br>the time performance, the average run time of our algorithm<br>is less than 10% of the previous QCQP algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:475"><nobr><span class="ft1">6.3</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:516"><nobr><span class="ft1">Unified Kernel Logistic Regression</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:489"><nobr><span class="ft3">In this part, we evaluate the performance of our proposed</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:475"><nobr><span class="ft7">paradigm of unified kernel logistic regression (UKLR). As<br>a comparison, we implement two traditional classification<br>schemes: one is traditional KLR classification scheme that<br>is trained on randomly sampled labeled data, denoted as<br>"KLR+Rand." The other is the active KLR classification<br>scheme that actively selects the most informative examples<br>for labeling, denoted as "KLR+Active." The active learn-<br>ing strategy is based on a simple maximum entropy criteria<br>given in the pervious section. The UKLR scheme is imple-<br>mented based on the algorithm in Figure 6.</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:489"><nobr><span class="ft3">For active learning evaluation, we choose a batch of 10</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:475"><nobr><span class="ft7">most informative unlabeled examples for labeling in each<br>trial of evaluations. Table 3 summarizes the experimental<br>results of average test set accuarcy performances on four<br>UCI datasets. From the experimental results, we can ob-<br>serve that the active learning classification schems outper-<br>form the randomly sampled classification schemes in most<br>cases. This shows the suggested simple active learning strat-<br>egy is effectiveness. Further, among all compared schemes,<br>the suggsted UKLR solution significantly outperforms other<br>classification approaches in most cases. These results show<br>that the unified scheme is effective and promising to inte-<br>grate traditional learning methods together in a unified so-<br>lution.</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:475"><nobr><span class="ft1">6.4</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:516"><nobr><span class="ft1">Discussions</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:489"><nobr><span class="ft3">Although the experimental results have shown that our</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:475"><nobr><span class="ft7">scheme is promising, some open issues in our current solution<br>need be further explored in future work. One problem to in-<br>vestigate more effective active learning methods in selecting<br>the most informative examples for labeling. One solution to<br>this issue is to employ the batch mode active learning meth-<br>ods that can be more efficient for large-scale classification<br>tasks [11, 23, 24]. Moreover, we will study more effective ker-<br>nel learning algorithms without the assumption of spectral<br>kernels. Further, we may examine the theoretical analysis<br>of generalization performance of our method [27]. Finally,<br>we may combine some kernel machine speedup techniques to<br>deploy our scheme efficiently for large-scale applications [26].</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:475"><nobr><span class="ft1">7.</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:507"><nobr><span class="ft1">CONCLUSION</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:489"><nobr><span class="ft3">This paper presented a novel general framework of learn-</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:475"><nobr><span class="ft7">ing the Unified Kernel Machines (UKM) for classification.<br>Different from traditional classification schemes, our UKM<br>framework integrates supervised learning, semi-supervised<br>learning, unsupervised kernel design and active learning in<br>a unified solution, making it more effective for classification<br>tasks. For the proposed framework, we focus our attention<br>on tackling a core problem of learning semi-supervised ker-<br>nels from labeled and unlabled data. We proposed a Spectral</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">194</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:81"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="126009.png" alt="background image">
<DIV style="position:absolute;top:96;left:81"><nobr><span class="ft7">Table 2: Classification performance of different kernels using KLR classifiers on four datasets. The mean<br>accuracies and standard errors are shown in the table. 3 standard kernels and 5 semi-supervised kernels are<br>compared. Each cell in the table has two rows. The upper row shows the test set accuracy with standard<br>error; the lower row gives the average time used in learning the semi-supervised kernels ("Order" and "Imp-<br>Order" kernels are sovled by SeDuMi/YALMIP package; "SKL" kernels are solved directly by the Matlab<br>quadprog function.</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:89"><nobr><span class="ft4">Train</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:204"><nobr><span class="ft4">Standard Kernels</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:532"><nobr><span class="ft4">Semi-Supervised Kernels</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:94"><nobr><span class="ft4">Size</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:157"><nobr><span class="ft4">Linear</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:233"><nobr><span class="ft4">Quadratic</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:336"><nobr><span class="ft4">RBF</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:419"><nobr><span class="ft4">Order</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:491"><nobr><span class="ft4">Imp-Order</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:574"><nobr><span class="ft4">SKL(Linear)</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:665"><nobr><span class="ft4">SKL(Quad)</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:757"><nobr><span class="ft4">SKL(RBF)</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:89"><nobr><span class="ft4">Heart</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:140"><nobr><span class="ft4">67.19 ± 1.94</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:226"><nobr><span class="ft4">71.90 ± 1.23</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:312"><nobr><span class="ft4">70.04 ± 1.61</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:398"><nobr><span class="ft4">63.60 ± 1.94</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:484"><nobr><span class="ft4">63.60 ± 1.94</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:572"><nobr><span class="ft4">70.58 ± 1.63</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:660"><nobr><span class="ft4">72.33 ± 1.60</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:746"><nobr><span class="ft4">73.37 ± 1.50</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:412"><nobr><span class="ft4">( 0.67 )</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:498"><nobr><span class="ft4">( 0.81 )</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:586"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:675"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:765"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:140"><nobr><span class="ft4">67.40 ± 1.87</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:226"><nobr><span class="ft4">70.36 ± 1.51</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:312"><nobr><span class="ft4">72.64 ± 1.37</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:398"><nobr><span class="ft4">65.88 ± 1.69</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:484"><nobr><span class="ft4">65.88 ± 1.69</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:572"><nobr><span class="ft4">76.26 ± 1.29</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:660"><nobr><span class="ft4">75.36 ± 1.30</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:746"><nobr><span class="ft4">76.30 ± 1.33</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:412"><nobr><span class="ft4">( 0.71 )</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:498"><nobr><span class="ft4">( 0.81 )</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:586"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:675"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:765"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:140"><nobr><span class="ft4">75.42 ± 0.88</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:226"><nobr><span class="ft4">70.71 ± 0.83</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:312"><nobr><span class="ft4">74.40 ± 0.70</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:398"><nobr><span class="ft4">71.73 ± 1.14</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:484"><nobr><span class="ft4">71.73 ± 1.14</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:572"><nobr><span class="ft4">78.42 ± 0.59</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:660"><nobr><span class="ft4">78.65 ± 0.52</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:746"><nobr><span class="ft4">79.23 ± 0.58</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:412"><nobr><span class="ft4">( 0.95 )</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:498"><nobr><span class="ft4">( 0.97 )</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:586"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:675"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:765"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:140"><nobr><span class="ft4">78.24 ± 0.89</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:226"><nobr><span class="ft4">71.28 ± 1.10</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:312"><nobr><span class="ft4">78.48 ± 0.77</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:398"><nobr><span class="ft4">75.48 ± 0.69</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:484"><nobr><span class="ft4">75.48 ± 0.69</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:572"><nobr><span class="ft4">80.61 ± 0.45</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:660"><nobr><span class="ft4">80.26 ± 0.45</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:746"><nobr><span class="ft4">80.98 ± 0.51</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:412"><nobr><span class="ft4">( 1.35 )</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:498"><nobr><span class="ft4">( 1.34 )</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:586"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:675"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:765"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:89"><nobr><span class="ft4">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:140"><nobr><span class="ft4">73.71 ± 1.27</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:226"><nobr><span class="ft4">71.30 ± 1.70</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:312"><nobr><span class="ft4">73.56 ± 1.91</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:398"><nobr><span class="ft4">71.86 ± 2.79</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:484"><nobr><span class="ft4">71.86 ± 2.79</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:572"><nobr><span class="ft4">75.53 ± 1.75</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:660"><nobr><span class="ft4">71.22 ± 1.82</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:746"><nobr><span class="ft4">83.36 ± 1.31</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:412"><nobr><span class="ft4">( 0.90 )</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:498"><nobr><span class="ft4">( 0.87 )</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:586"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:675"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:765"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:140"><nobr><span class="ft4">75.62 ± 1.24</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:226"><nobr><span class="ft4">76.00 ± 1.58</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:312"><nobr><span class="ft4">81.71 ± 1.74</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:398"><nobr><span class="ft4">83.04 ± 2.10</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:484"><nobr><span class="ft4">83.04 ± 2.10</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:572"><nobr><span class="ft4">78.78 ± 1.60</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:660"><nobr><span class="ft4">80.30 ± 1.77</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:746"><nobr><span class="ft4">88.55 ± 1.32</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:412"><nobr><span class="ft4">( 0.87 )</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:498"><nobr><span class="ft4">( 0.79 )</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:586"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:675"><nobr><span class="ft4">( 0.06 )</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:765"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:140"><nobr><span class="ft4">76.59 ± 0.82</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:226"><nobr><span class="ft4">79.10 ± 1.46</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:312"><nobr><span class="ft4">86.21 ± 0.84</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:398"><nobr><span class="ft4">87.20 ± 1.16</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:484"><nobr><span class="ft4">87.20 ± 1.16</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:572"><nobr><span class="ft4">82.18 ± 0.56</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:660"><nobr><span class="ft4">83.08 ± 1.36</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:746"><nobr><span class="ft4">90.39 ± 0.84</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:412"><nobr><span class="ft4">( 0.93 )</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:498"><nobr><span class="ft4">( 0.97 )</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:586"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:675"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:765"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:140"><nobr><span class="ft4">77.97 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:226"><nobr><span class="ft4">82.93 ± 1.33</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:312"><nobr><span class="ft4">89.39 ± 0.65</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:398"><nobr><span class="ft4">90.56 ± 0.64</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:484"><nobr><span class="ft4">90.56 ± 0.64</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:572"><nobr><span class="ft4">83.26 ± 0.53</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:660"><nobr><span class="ft4">87.03 ± 1.02</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:746"><nobr><span class="ft4">92.14 ± 0.46</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:412"><nobr><span class="ft4">( 1.34 )</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:498"><nobr><span class="ft4">( 1.38 )</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:586"><nobr><span class="ft4">( 0.05 )</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:675"><nobr><span class="ft4">( 0.04 )</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:765"><nobr><span class="ft4">( 0.04 )</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:89"><nobr><span class="ft4">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:140"><nobr><span class="ft4">63.01 ± 1.47</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:226"><nobr><span class="ft4">62.85 ± 1.53</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:312"><nobr><span class="ft4">60.76 ± 1.80</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:398"><nobr><span class="ft4">59.67 ± 0.89</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:484"><nobr><span class="ft4">59.67 ± 0.89</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:572"><nobr><span class="ft4">64.27 ± 1.91</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:660"><nobr><span class="ft4">64.37 ± 1.64</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:746"><nobr><span class="ft4">65.30 ± 1.78</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:412"><nobr><span class="ft4">( 0.63 )</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:498"><nobr><span class="ft4">( 0.63 )</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:586"><nobr><span class="ft4">( 0.08 )</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:675"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:765"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:140"><nobr><span class="ft4">68.09 ± 1.11</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:226"><nobr><span class="ft4">69.55 ± 1.22</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:312"><nobr><span class="ft4">67.63 ± 1.15</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:398"><nobr><span class="ft4">64.68 ± 1.57</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:484"><nobr><span class="ft4">64.68 ± 1.57</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:572"><nobr><span class="ft4">70.61 ± 1.14</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:660"><nobr><span class="ft4">69.79 ± 1.30</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:746"><nobr><span class="ft4">71.76 ± 1.07</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:412"><nobr><span class="ft4">( 0.68 )</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:498"><nobr><span class="ft4">( 0.82 )</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:586"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:675"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:765"><nobr><span class="ft4">( 0.08 )</span></nobr></DIV>
<DIV style="position:absolute;top:563;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:140"><nobr><span class="ft4">66.40 ± 1.06</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:226"><nobr><span class="ft4">69.80 ± 0.93</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:312"><nobr><span class="ft4">68.23 ± 1.48</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:398"><nobr><span class="ft4">66.54 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:484"><nobr><span class="ft4">66.54 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:572"><nobr><span class="ft4">70.20 ± 1.48</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:660"><nobr><span class="ft4">68.48 ± 1.59</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:746"><nobr><span class="ft4">71.69 ± 0.87</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:412"><nobr><span class="ft4">( 0.88 )</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:498"><nobr><span class="ft4">( 1.02 )</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:586"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:675"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:765"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:140"><nobr><span class="ft4">64.94 ± 0.74</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:226"><nobr><span class="ft4">71.37 ± 0.52</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:312"><nobr><span class="ft4">71.61 ± 0.89</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:398"><nobr><span class="ft4">69.82 ± 0.82</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:484"><nobr><span class="ft4">69.82 ± 0.82</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:572"><nobr><span class="ft4">72.35 ± 1.06</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:660"><nobr><span class="ft4">71.28 ± 0.96</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:746"><nobr><span class="ft4">72.89 ± 0.68</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:412"><nobr><span class="ft4">( 1.14 )</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:498"><nobr><span class="ft4">( 1.20 )</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:586"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:675"><nobr><span class="ft4">( 0.08 )</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:765"><nobr><span class="ft4">( 0.07 )</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:89"><nobr><span class="ft4">Wine</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:140"><nobr><span class="ft4">82.26 ± 2.18</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:226"><nobr><span class="ft4">85.89 ± 1.73</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:312"><nobr><span class="ft4">87.80 ± 1.63</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:398"><nobr><span class="ft4">86.99 ± 1.98</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:484"><nobr><span class="ft4">86.99 ± 1.45</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:572"><nobr><span class="ft4">83.63 ± 2.62</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:660"><nobr><span class="ft4">83.21 ± 2.36</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:746"><nobr><span class="ft4">90.54 ± 1.08</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:412"><nobr><span class="ft4">( 1.02 )</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:498"><nobr><span class="ft4">( 0.86 )</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:586"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:675"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:765"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:140"><nobr><span class="ft4">86.39 ± 1.39</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:226"><nobr><span class="ft4">86.96 ± 1.30</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:312"><nobr><span class="ft4">93.77 ± 0.99</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:398"><nobr><span class="ft4">92.31 ± 1.39</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:484"><nobr><span class="ft4">92.31 ± 1.39</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:572"><nobr><span class="ft4">89.53 ± 2.32</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:660"><nobr><span class="ft4">92.56 ± 0.56</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:746"><nobr><span class="ft4">94.94 ± 0.50</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:412"><nobr><span class="ft4">( 0.92 )</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:498"><nobr><span class="ft4">( 0.91 )</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:586"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:675"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:765"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:140"><nobr><span class="ft4">92.50 ± 0.76</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:226"><nobr><span class="ft4">87.43 ± 0.63</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:312"><nobr><span class="ft4">94.63 ± 0.50</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:398"><nobr><span class="ft4">92.97 ± 0.54</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:484"><nobr><span class="ft4">92.97 ± 0.54</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:572"><nobr><span class="ft4">93.99 ± 1.09</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:660"><nobr><span class="ft4">94.29 ± 0.53</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:746"><nobr><span class="ft4">96.25 ± 0.30</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:412"><nobr><span class="ft4">( 1.28 )</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:498"><nobr><span class="ft4">( 1.27 )</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:586"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:675"><nobr><span class="ft4">( 0.10 )</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:765"><nobr><span class="ft4">( 0.09 )</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:140"><nobr><span class="ft4">94.96 ± 0.65</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:226"><nobr><span class="ft4">88.80 ± 0.93</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:312"><nobr><span class="ft4">96.38 ± 0.35</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:398"><nobr><span class="ft4">95.62 ± 0.37</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:484"><nobr><span class="ft4">95.62 ± 0.37</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:572"><nobr><span class="ft4">95.80 ± 0.47</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:660"><nobr><span class="ft4">95.36 ± 0.46</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:746"><nobr><span class="ft4">96.81 ± 0.28</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:168"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:254"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:340"><nobr><span class="ft4">--</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:412"><nobr><span class="ft4">( 1.41 )</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:498"><nobr><span class="ft4">( 1.39 )</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:586"><nobr><span class="ft4">( 0.08 )</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:675"><nobr><span class="ft4">( 0.08 )</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:765"><nobr><span class="ft4">( 0.10 )</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:81"><nobr><span class="ft7">Kernel Learning (SKL) algorithm, which is more effective<br>and efficient for learning kernels from labeled and unlabeled<br>data. Under the framework, we developed a paradigm of<br>unified kernel machine based on Kernel Logistic Regression,<br>i.e., Unified Kernel Logistic Regression (UKLR). Empirical<br>results demonstrated that our proposed solution is more ef-<br>fective than the traditional classification approaches.</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:81"><nobr><span class="ft1">8.</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:112"><nobr><span class="ft1">ACKNOWLEDGMENTS</span></nobr></DIV>
<DIV style="position:absolute;top:920;left:94"><nobr><span class="ft3">The work described in this paper was fully supported by</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:81"><nobr><span class="ft7">two grants, one from the Shun Hing Institute of Advanced<br>Engineering, and the other from the Research Grants Coun-<br>cil of the Hong Kong Special Administrative Region, China<br>(Project No. CUHK4205/04E).</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:81"><nobr><span class="ft1">9.</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:112"><nobr><span class="ft1">REFERENCES</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:88"><nobr><span class="ft3">[1] M. Belkin and I. M. andd P. Niyogi. Regularization</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:109"><nobr><span class="ft7">and semi-supervised learning on large graphs. In<br>COLT, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:482"><nobr><span class="ft3">[2] M. Belkin and P. Niyogi. Semi-supervised learning on</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:503"><nobr><span class="ft3">riemannian manifolds. Machine Learning, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:482"><nobr><span class="ft3">[3] E. Chang, S. C. Hoi, X. Wang, W.-Y. Ma, and</span></nobr></DIV>
<DIV style="position:absolute;top:819;left:503"><nobr><span class="ft7">M. Lyu. A unified machine learning framework for<br>large-scale personalized information management. In<br>The 5th Emerging Information Technology<br>Conference, NTU Taipei, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:482"><nobr><span class="ft3">[4] E. Chang and M. Lyu. Unified learning paradigm for</span></nobr></DIV>
<DIV style="position:absolute;top:899;left:503"><nobr><span class="ft7">web-scale mining. In Snowbird Machine Learning<br>Workshop, 2006.</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:482"><nobr><span class="ft3">[5] O. Chapelle, A. Zien, and B. Scholkopf.</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:503"><nobr><span class="ft3">Semi-supervised learning. MIT Press, 2006.</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:482"><nobr><span class="ft3">[6] F. R. K. Chung. Spectral Graph Theory. American</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:503"><nobr><span class="ft3">Mathematical Soceity, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:482"><nobr><span class="ft3">[7] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:503"><nobr><span class="ft7">learning with statistical models. In NIPS, volume 7,<br>pages 705­712, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:1047;left:482"><nobr><span class="ft3">[8] N. Cristianini, J. Shawe-Taylor, and A. Elisseeff. On</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:503"><nobr><span class="ft3">kernel-target alignment. JMLR, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">195</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:679"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="126010.png" alt="background image">
<DIV style="position:absolute;top:96;left:81"><nobr><span class="ft3">Table 3: Classification performance of different classification schemes on four UCI datasets.</span></nobr></DIV>
<DIV style="position:absolute;top:96;left:762"><nobr><span class="ft3">The mean</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft7">accuracies and standard errors are shown in the table. "KLR" represents the initial classifier with the initial<br>train size; other three methods are trained with additional 10 random/active examples.</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:89"><nobr><span class="ft4">Train</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:266"><nobr><span class="ft4">Linear Kernel</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:629"><nobr><span class="ft4">RBF Kernel</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:93"><nobr><span class="ft4">Size</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:161"><nobr><span class="ft4">KLR</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:227"><nobr><span class="ft4">KLR+Rand</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:314"><nobr><span class="ft4">KLR+Active</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:426"><nobr><span class="ft4">UKLR</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:519"><nobr><span class="ft4">KLR</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:585"><nobr><span class="ft4">KLR+Rand</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:673"><nobr><span class="ft4">KLR+Active</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:784"><nobr><span class="ft4">UKLR</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:89"><nobr><span class="ft4">Heart</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:139"><nobr><span class="ft4">67.19 ± 1.94</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:225"><nobr><span class="ft4">68.22 ± 2.16</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:316"><nobr><span class="ft4">69.22 ± 1.71</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:406"><nobr><span class="ft4">77.24 ± 0.74</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:498"><nobr><span class="ft4">70.04 ± 1.61</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:584"><nobr><span class="ft4">72.24 ± 1.23</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:674"><nobr><span class="ft4">75.36 ± 0.60</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:765"><nobr><span class="ft4">78.44 ± 0.88</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:139"><nobr><span class="ft4">67.40 ± 1.87</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:225"><nobr><span class="ft4">73.79 ± 1.29</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:316"><nobr><span class="ft4">73.77 ± 1.27</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:406"><nobr><span class="ft4">79.27 ± 1.00</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:498"><nobr><span class="ft4">72.64 ± 1.37</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:584"><nobr><span class="ft4">75.10 ± 0.74</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:674"><nobr><span class="ft4">76.23 ± 0.81</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:765"><nobr><span class="ft4">79.88 ± 0.90</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:139"><nobr><span class="ft4">75.42 ± 0.88</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:225"><nobr><span class="ft4">77.70 ± 0.92</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:316"><nobr><span class="ft4">78.65 ± 0.62</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:406"><nobr><span class="ft4">81.13 ± 0.42</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:498"><nobr><span class="ft4">74.40 ± 0.70</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:584"><nobr><span class="ft4">76.43 ± 0.68</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:674"><nobr><span class="ft4">76.61 ± 0.61</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:765"><nobr><span class="ft4">81.48 ± 0.41</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:139"><nobr><span class="ft4">78.24 ± 0.89</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:225"><nobr><span class="ft4">79.30 ± 0.75</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:316"><nobr><span class="ft4">80.18 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:406"><nobr><span class="ft4">82.55 ± 0.28</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:498"><nobr><span class="ft4">78.48 ± 0.77</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:584"><nobr><span class="ft4">78.50 ± 0.53</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:674"><nobr><span class="ft4">79.95 ± 0.62</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:765"><nobr><span class="ft4">82.66 ± 0.36</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:89"><nobr><span class="ft4">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:139"><nobr><span class="ft4">73.71 ± 1.27</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:225"><nobr><span class="ft4">74.89 ± 0.95</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:316"><nobr><span class="ft4">75.91 ± 0.96</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:406"><nobr><span class="ft4">77.31 ± 1.23</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:498"><nobr><span class="ft4">73.56 ± 1.91</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:584"><nobr><span class="ft4">82.57 ± 1.78</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:674"><nobr><span class="ft4">82.76 ± 1.37</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:765"><nobr><span class="ft4">90.48 ± 0.83</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:139"><nobr><span class="ft4">75.62 ± 1.24</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:225"><nobr><span class="ft4">77.09 ± 0.67</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:316"><nobr><span class="ft4">77.51 ± 0.66</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:406"><nobr><span class="ft4">81.42 ± 1.10</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:498"><nobr><span class="ft4">81.71 ± 1.74</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:584"><nobr><span class="ft4">85.95 ± 1.30</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:674"><nobr><span class="ft4">88.22 ± 0.78</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:765"><nobr><span class="ft4">91.28 ± 0.94</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:139"><nobr><span class="ft4">76.59 ± 0.82</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:225"><nobr><span class="ft4">78.41 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:316"><nobr><span class="ft4">77.91 ± 0.77</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:406"><nobr><span class="ft4">84.49 ± 0.37</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:498"><nobr><span class="ft4">86.21 ± 0.84</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:584"><nobr><span class="ft4">89.04 ± 0.66</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:674"><nobr><span class="ft4">90.32 ± 0.56</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:765"><nobr><span class="ft4">92.35 ± 0.59</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:139"><nobr><span class="ft4">77.97 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:225"><nobr><span class="ft4">79.05 ± 0.49</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:316"><nobr><span class="ft4">80.30 ± 0.79</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:406"><nobr><span class="ft4">84.49 ± 0.40</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:498"><nobr><span class="ft4">89.39 ± 0.65</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:584"><nobr><span class="ft4">90.55 ± 0.59</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:674"><nobr><span class="ft4">91.83 ± 0.49</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:765"><nobr><span class="ft4">93.89 ± 0.28</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:89"><nobr><span class="ft4">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:139"><nobr><span class="ft4">61.19 ± 1.56</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:225"><nobr><span class="ft4">63.72 ± 1.65</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:316"><nobr><span class="ft4">65.51 ± 1.55</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:406"><nobr><span class="ft4">66.12 ± 1.94</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:498"><nobr><span class="ft4">57.40 ± 1.48</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:584"><nobr><span class="ft4">60.19 ± 1.32</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:674"><nobr><span class="ft4">59.49 ± 1.46</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:765"><nobr><span class="ft4">67.13 ± 1.58</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:139"><nobr><span class="ft4">67.31 ± 1.07</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:225"><nobr><span class="ft4">68.85 ± 0.84</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:316"><nobr><span class="ft4">69.38 ± 1.05</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:406"><nobr><span class="ft4">71.60 ± 0.91</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:498"><nobr><span class="ft4">62.93 ± 1.36</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:584"><nobr><span class="ft4">64.72 ± 1.24</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:674"><nobr><span class="ft4">64.52 ± 1.07</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:765"><nobr><span class="ft4">72.30 ± 0.98</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:139"><nobr><span class="ft4">66.10 ± 1.08</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:225"><nobr><span class="ft4">67.59 ± 1.14</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:316"><nobr><span class="ft4">69.79 ± 0.86</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:406"><nobr><span class="ft4">71.40 ± 0.80</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:498"><nobr><span class="ft4">63.03 ± 1.32</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:584"><nobr><span class="ft4">63.72 ± 1.51</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:674"><nobr><span class="ft4">66.67 ± 1.53</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:765"><nobr><span class="ft4">72.26 ± 0.98</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:139"><nobr><span class="ft4">66.34 ± 0.82</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:225"><nobr><span class="ft4">68.16 ± 0.81</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:316"><nobr><span class="ft4">70.19 ± 0.90</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:406"><nobr><span class="ft4">73.04 ± 0.69</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:498"><nobr><span class="ft4">66.70 ± 1.25</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:584"><nobr><span class="ft4">68.70 ± 1.19</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:674"><nobr><span class="ft4">67.56 ± 0.90</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:765"><nobr><span class="ft4">73.16 ± 0.88</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:89"><nobr><span class="ft4">Wine</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:100"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:139"><nobr><span class="ft4">82.26 ± 2.18</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:225"><nobr><span class="ft4">87.31 ± 1.01</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:311"><nobr><span class="ft4">89.05 ± 1.07</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:409"><nobr><span class="ft4">87.31 ± 1.03</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:498"><nobr><span class="ft4">87.80 ± 1.63</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:584"><nobr><span class="ft4">92.75 ± 1.27</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:674"><nobr><span class="ft4">94.49 ± 0.54</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:765"><nobr><span class="ft4">94.87 ± 0.49</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:100"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:139"><nobr><span class="ft4">86.39 ± 1.39</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:225"><nobr><span class="ft4">93.99 ± 0.40</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:316"><nobr><span class="ft4">93.82 ± 0.71</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:406"><nobr><span class="ft4">94.43 ± 0.54</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:498"><nobr><span class="ft4">93.77 ± 0.99</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:584"><nobr><span class="ft4">95.57 ± 0.38</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:670"><nobr><span class="ft4">97.13 ± 0.18</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:767"><nobr><span class="ft4">96.76 ± 0.26</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:100"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:139"><nobr><span class="ft4">92.50 ± 0.76</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:225"><nobr><span class="ft4">95.25 ± 0.47</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:311"><nobr><span class="ft4">96.96 ± 0.40</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:409"><nobr><span class="ft4">96.12 ± 0.47</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:498"><nobr><span class="ft4">94.63 ± 0.50</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:584"><nobr><span class="ft4">96.27 ± 0.35</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:674"><nobr><span class="ft4">97.17 ± 0.38</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:765"><nobr><span class="ft4">97.21 ± 0.26</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:100"><nobr><span class="ft4">40</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:139"><nobr><span class="ft4">94.96 ± 0.65</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:225"><nobr><span class="ft4">96.21 ± 0.63</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:316"><nobr><span class="ft4">97.54 ± 0.37</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:406"><nobr><span class="ft4">97.70 ± 0.34</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:498"><nobr><span class="ft4">96.38 ± 0.35</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:584"><nobr><span class="ft4">96.33 ± 0.45</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:674"><nobr><span class="ft4">97.97 ± 0.23</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:765"><nobr><span class="ft4">98.12 ± 0.21</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:88"><nobr><span class="ft3">[9] S. Fine, R. Gilad-Bachrach, and E. Shamir. Query by</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:109"><nobr><span class="ft7">committee, linear separation and random walks.<br>Theor. Comput. Sci., 284(1):25­51, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:81"><nobr><span class="ft3">[10] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby.</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:109"><nobr><span class="ft7">Selective sampling using the query by committee<br>algorithm. Mach. Learn., 28(2-3):133­168, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft3">[11] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:109"><nobr><span class="ft7">categorization by batch mode active learning. In<br>WWW2006, Edinburg, 2006.</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:81"><nobr><span class="ft3">[12] S. B. C. M. J.A.K. Suykens, G. Horvath and</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:109"><nobr><span class="ft7">J. Vandewalle. Advances in Learning Theory:<br>Methods, Models and Applications. NATO Science<br>Series: Computer &amp; Systems Sciences, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft3">[13] R. Kondor and J. Lafferty. Diffusion kernels on graphs</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:109"><nobr><span class="ft3">and other discrete structures. 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:81"><nobr><span class="ft3">[14] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui,</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:109"><nobr><span class="ft7">and M. Jordan. Learning the kernel matrix with<br>semi-definite programming. JMLR, 5:27­72, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:81"><nobr><span class="ft3">[15] G. Lanckriet, L. Ghaoui, C. Bhattacharyya, and</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:109"><nobr><span class="ft7">M. Jordan. Minimax probability machine. In Advances<br>in Neural Infonation Processing Systems 14, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft3">[16] R. Liere and P. Tadepalli. Active learning with</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:109"><nobr><span class="ft7">committees for text categorization. In Proceedings 14th<br>Conference of the American Association for Artificial<br>Intelligence (AAAI), pages 591­596, MIT Press, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:81"><nobr><span class="ft3">[17] R. Meir and G. Ratsch. An introduction to boosting</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:109"><nobr><span class="ft7">and leveraging. In In Advanced Lectures on Machine<br>Learning (LNAI2600), 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:81"><nobr><span class="ft3">[18] A. Ng, M. Jordan, and Y. Weiss. On spectral</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:109"><nobr><span class="ft7">clustering: Analysis and an algorithm. In In Advances<br>in Neural Information Processing Systems 14, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1007;left:81"><nobr><span class="ft3">[19] N. Roy and A. McCallum. Toward optimal active</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:109"><nobr><span class="ft7">learning through sampling estimation of error<br>reduction. In 18th ICML, pages 441­448, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:81"><nobr><span class="ft3">[20] B. Scholkopf, A. Smola, and K.-R. Muller. Nonlinear</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:503"><nobr><span class="ft7">component analysis as a kernel eigenvalue problem.<br>Neural Computation, 10:1299­1319, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:475"><nobr><span class="ft3">[21] A. Smola and R. Kondor. Kernels and regularization</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:503"><nobr><span class="ft3">on graphs. In Intl. Conf. on Learning Theory, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:475"><nobr><span class="ft3">[22] M. Szummer and T. Jaakkola. Partially labeled</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:503"><nobr><span class="ft7">classification with markov random walks. In Advances<br>in Neural Information Processing Systems, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:475"><nobr><span class="ft3">[23] S. Tong and E. Chang. Support vector machine active</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:503"><nobr><span class="ft7">learning for image retrieval. In Proc ACM Multimedia<br>Conference, pages 107­118, New York, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:475"><nobr><span class="ft3">[24] S. Tong and D. Koller. Support vector machine active</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:503"><nobr><span class="ft7">learning with applications to text classification. In<br>Proc. 17th ICML, pages 999­1006, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:475"><nobr><span class="ft3">[25] V. N. Vapnik. Statistical Learning Theory. John Wiley</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:503"><nobr><span class="ft3">&amp; Sons, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:475"><nobr><span class="ft3">[26] G. Wu, Z. Zhang, and E. Y. Chang. Kronecker</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:503"><nobr><span class="ft7">factorization for speeding up kernel machines. In<br>SIAM Int. Conference on Data Mining (SDM), 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:475"><nobr><span class="ft3">[27] T. Zhang and R. K. Ando. Analysis of spectral kernel</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:503"><nobr><span class="ft3">design based semi-supervised learning. In NIPS, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:475"><nobr><span class="ft3">[28] D. Zhou, O. Bousquet, T. Lal, J. Weston, and</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:503"><nobr><span class="ft7">B. Schlkopf. Learning with local and global<br>consistency. In NIPS'16, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:475"><nobr><span class="ft3">[29] J. Zhu and T. Hastie. Kernel logistic regression and</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:503"><nobr><span class="ft7">the import vector machine. In NIPS 14, pages<br>1081­1088, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:475"><nobr><span class="ft3">[30] X. Zhu. Semi-supervised learning literature survey.</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:503"><nobr><span class="ft7">Technical report, Computer Sciences TR 1530,<br>University of Wisconsin - Madison, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:475"><nobr><span class="ft3">[31] X. Zhu, Z. Ghahramani, and J. Lafferty.</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:503"><nobr><span class="ft7">Semi-supervised learning using gaussian fields and<br>harmonic functions. In Proc. ICML'2003, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:475"><nobr><span class="ft3">[32] X. Zhu, J. Kandola, Z. Ghahramani, and J. Lafferty.</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:503"><nobr><span class="ft7">Nonparametric transforms of graph kernels for<br>semi-supervised learning. In NIPS2005, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:449"><nobr><span class="ft5">196</span></nobr></DIV>
<DIV style="position:absolute;top:43;left:81"><nobr><span class="ft6"><b>Research Track Paper</b></span></nobr></DIV>
</DIV>
</BODY>
</HTML>
