<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>focused-entity.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-07-08T13:47:12+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Courier;color:#000000;}
	.ft8{font-size:11px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft10{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="94001.png" alt="background image">
<DIV style="position:absolute;top:108;left:132"><nobr><span class="ft0"><b>Focused Named Entity Recognition Using Machine</b></span></nobr></DIV>
<DIV style="position:absolute;top:138;left:401"><nobr><span class="ft0"><b>Learning</b></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:203"><nobr><span class="ft1">Li Zhang</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:169"><nobr><span class="ft2">IBM China Research</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:202"><nobr><span class="ft2">Laboratory</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:150"><nobr><span class="ft2">No. 7, 5th Street, ShangDi</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:152"><nobr><span class="ft2">Beijing 100085, P.R.China</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:155"><nobr><span class="ft1">lizhang@cn.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:424"><nobr><span class="ft1">Yue Pan</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:388"><nobr><span class="ft2">IBM China Research</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:421"><nobr><span class="ft2">Laboratory</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:370"><nobr><span class="ft2">No. 7, 5th Street, ShangDi</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:371"><nobr><span class="ft2">Beijing 100085, P.R.China</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:373"><nobr><span class="ft1">panyue@cn.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:629"><nobr><span class="ft1">Tong Zhang</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:589"><nobr><span class="ft2">IBM T.J. Watson Research</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:654"><nobr><span class="ft2">Center</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:581"><nobr><span class="ft2">Route 134, Yorktown Heights</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:616"><nobr><span class="ft2">NY 10598, U.S. A.</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:581"><nobr><span class="ft1">tongz@watson.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:81"><nobr><span class="ft3"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:372;left:81"><nobr><span class="ft9">In this paper we study the problem of finding most topical<br>named entities among all entities in a document, which we<br>refer to as focused named entity recognition. We show that<br>these focused named entities are useful for many natural<br>language processing applications, such as document sum-<br>marization, search result ranking, and entity detection and<br>tracking. We propose a statistical model for focused named<br>entity recognition by converting it into a classification prob-<br>lem. We then study the impact of various linguistic fea-<br>tures and compare a number of classification algorithms.<br>From experiments on an annotated Chinese news corpus,<br>we demonstrate that the proposed method can achieve near<br>human-level accuracy.</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:81"><nobr><span class="ft3"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:619;left:81"><nobr><span class="ft9">I.2.7 [Artificial Intelligence]: Natural language Process-<br>ing--Text Analysis; H.3.1 [Information Storage And Re-<br>trieval]: Content Analysis and Indexing--Linguistic pro-<br>cessing</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:81"><nobr><span class="ft3"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:723;left:81"><nobr><span class="ft4">Algorithms, Experimentation</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:81"><nobr><span class="ft3"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:781;left:81"><nobr><span class="ft9">naive Bayes, decision tree, robust risk minimization, text<br>summarization, topic identification, information retrieval</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:81"><nobr><span class="ft3"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:831;left:112"><nobr><span class="ft3"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:853;left:94"><nobr><span class="ft4">With the rapid growth of online electronic documents,</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:81"><nobr><span class="ft9">many technologies have been developed to deal with the<br>enormous amount of information, such as automatic sum-<br>marization, topic detection and tracking, and information<br>retrieval. Among these technologies, a key component is to</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft10">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>SIGIR'04, </i>July 25­29, 2004, Sheffield, South Yorkshire, UK.<br>Copyright 2004 ACM 1-58113-881-4/04/0007 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:316"><nobr><span class="ft4">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:323"><nobr><span class="ft5">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:475"><nobr><span class="ft9">identify the main topics of a document, where topics can be<br>represented by words, sentences, concepts, and named en-<br>tities. A number of techniques for this purpose have been<br>proposed in the literature, including methods based on po-<br>sition [3], cue phrases [3], word frequency, lexical chains[1]<br>and discourse segmentation [13]. Although word frequency<br>is the easiest way to representing the topics of a document,<br>it was reported in [12] that position methods produce better<br>results than word counting based methods.</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:489"><nobr><span class="ft4">Important sentence extraction is the most popular method</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:475"><nobr><span class="ft9">studied in the literature. A recent trend in topic sentence<br>extraction is to employ machine learning methods. For ex-<br>ample, trainable classifiers have been used in [8, 21, 5, 11]<br>to select sentences based on features such as cue phrase,<br>location, sentence length, word frequency and title, etc.</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:489"><nobr><span class="ft4">All of the above methods share the same goal of extract-</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:475"><nobr><span class="ft9">ing important sentences from documents. However, for topic<br>representation, sentence-level document summaries may still<br>contain redundant information. For this reason, other rep-<br>resentations have also been suggested. For example, in [17],<br>the authors used structural features of technical papers to<br>identify important concepts rather than sentences. The au-<br>thors of [9] presented an efficient algorithm to choose topic<br>terms for hierarchical summarization according to a proba-<br>bilistic language model. Another hybrid system, presented<br>in [7], generated summarizations with the help of named en-<br>tity foci of an article. These named entities include people,<br>organizations, and places, and untyped names.</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:489"><nobr><span class="ft4">In this paper, we study the problem of finding impor-</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:475"><nobr><span class="ft9">tant named entities from news articles, which we call fo-<br>cused named entity recognition. A news article often reports<br>an event that can be effectively summarized by the five W<br>(who, what, when, where, and why) approach. Many of the<br>five W's can be associated with appropriate named entities<br>in the article. Our definition of focused named entities is<br>mainly concerned with Who and What. Therefore it is al-<br>most self-evident that the concept of focused named entity<br>is important for document understanding and automatic in-<br>formation extraction. In fact, a number of recent studies<br>have already suggested that named entities are useful for<br>text summarization [15, 4, 7, 16]. Moreover, we shall illus-<br>trate that focused named entities can be used in other text<br>processing tasks as well. For example, we can rank search<br>results by giving more weights to focused named entities.</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:489"><nobr><span class="ft4">We define focused named entities as named entities that</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:475"><nobr><span class="ft4">are most relevant to the main topic of a news article. Our</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">281</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:6px;font-family:Times;color:#000000;}
	.ft12{font-size:15px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="94002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft9">task is to automatically select these focused named enti-<br>ties from the set of all entities in a document. Since focused<br>named entity recognition is a newly proposed machine learn-<br>ing task, we need to determine whether it is well-posed.<br>That is, whether there exists a sufficient level of agree-<br>ment on focused named entities among human reviewers.<br>A detailed study on this matter will be reported in the sec-<br>tion 5.2. The conclusion of our study is that there is indeed<br>a sufficient level of agreement. Encouraged by this study,<br>we further investigated the machine learning approach to<br>this problem, which is the focus of the paper. We discuss<br>various issues encountered in the process of building a ma-<br>chine learning based system, and show that our method can<br>achieve near human performance.</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:94"><nobr><span class="ft4">The remainder of this paper is organized as follows. In</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:81"><nobr><span class="ft9">Section 2 we introduce the problem of focused named en-<br>tity recognition and illustrate its applications. Section 3 de-<br>scribes a general machine learning approach to this problem.<br>In Section 4, we present features used in our system. Sec-<br>tion 5 presents a study of human-level agreement on focused<br>named entities, and various experiments which illustrate the<br>importance of different features. Some final conclusions will<br>be given in section 6.</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:81"><nobr><span class="ft3"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:467;left:112"><nobr><span class="ft3"><b>THE PROBLEM</b></span></nobr></DIV>
<DIV style="position:absolute;top:489;left:94"><nobr><span class="ft4">Figure 1 is an example document.</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:305"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:321"><nobr><span class="ft4">This article reports</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:81"><nobr><span class="ft9">that Boeing Company would work with its new Research<br>and Technology Center to develop a new style of electric<br>airplane. On the upper half of the page, we list all named<br>entities appearing in the article and mark the focused enti-<br>ties. Among the twelve named entities, "Boeing Company"<br>and its "Research and Technology Center" are most relevant<br>to the main topic. Here we call "Boeing Company" and "Re-<br>search and Technology Center" the focuses. Clearly, focused<br>named entities are important for representing the main topic<br>of the content. In the following, we show that the concept<br>of focused named entity is useful for many natural language<br>processing applications, such as summarization, search rank-<br>ing and topic detection and tracking.</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:81"><nobr><span class="ft3"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:719;left:121"><nobr><span class="ft12"><b>Using Focused Named Entity for<br>Summarization</b></span></nobr></DIV>
<DIV style="position:absolute;top:758;left:94"><nobr><span class="ft4">We consider the task of automatic summarization of the</span></nobr></DIV>
<DIV style="position:absolute;top:774;left:81"><nobr><span class="ft9">sample document in Figure 1. A traditional method is to se-<br>lect sentences with highest weights, where sentence weights<br>are calculated by averaging term frequencies of words it con-<br>tains.</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:126"><nobr><span class="ft4">The resulting summarization is given in Figure 2.</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:81"><nobr><span class="ft9">Using focused named entities, we consider two methods to<br>refine the above summarization. The first method is to in-<br>crease the weight of the focused named entity "Boeing" in<br>the sentences, leading to the summary in Figure 3. The<br>other method simply picks sentences containing the focused<br>named entity "Boeing" as in Figure 4. From this example,<br>we can see that summarization using focused named entities<br>gives more indicative description of an article.</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:81"><nobr><span class="ft3"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:972;left:121"><nobr><span class="ft12"><b>Using Focused Named Entity for Ranking<br>Search Results</b></span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:94"><nobr><span class="ft4">Suppose we want to find news about World Cup football</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:81"><nobr><span class="ft4">match from a collection of news articles. First we search</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:88"><nobr><span class="ft4">The</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:136"><nobr><span class="ft4">original</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:206"><nobr><span class="ft4">article</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:268"><nobr><span class="ft4">can</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:314"><nobr><span class="ft4">be</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:352"><nobr><span class="ft4">accessed</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:427"><nobr><span class="ft4">at</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">http://www.boeing.com/news/releases/2001/q4/nr 011127a.html.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:475"><nobr><span class="ft9">Figure 1: Sample document with focused named en-<br>tities marked</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:492"><nobr><span class="ft4">Boeing To Explore Electric Airplane</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:492"><nobr><span class="ft9">Fuel cells and electric motors will not replace<br>jet engines on commercial transports, but they could<br>one day replace gas turbine auxiliary power units.</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:492"><nobr><span class="ft9">Unlike a battery, which needs to be recharged,<br>fuel cells keep working as long as the fuel lasts.</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:492"><nobr><span class="ft9">"Fuel cells show the promise of one day provid-<br>ing efficient, essentially pollution-free electrical power<br>for commercial airplane primary electrical power<br>needs," Daggett said.</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:475"><nobr><span class="ft4">Figure 2: Summary using term frequency weighting</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:492"><nobr><span class="ft4">Boeing To Explore Electric Airplane</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:492"><nobr><span class="ft4">Boeing</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:547"><nobr><span class="ft4">Commercial</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:632"><nobr><span class="ft4">Airplanes</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:702"><nobr><span class="ft4">will</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:736"><nobr><span class="ft4">develop</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:794"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:492"><nobr><span class="ft9">test an electrically powered demonstrator airplane as<br>part of a study to evaluate environmentally friendly<br>fuel cell technology for future Boeing products.</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:492"><nobr><span class="ft9">Fuel cells and electric motors will not replace<br>jet engines on commercial transports, but they could<br>one day replace gas turbine auxiliary power units.</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:492"><nobr><span class="ft9">"By adapting this technology for aviation, Boe-<br>ing intends to demonstrate its leadership in the<br>pursuit of delivering environmentally preferred prod-<br>ucts."</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:475"><nobr><span class="ft4">Figure 3: Summary weighted by focused named en-</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:499"><nobr><span class="ft4">tities</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">282</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="94003.png" alt="background image">
<DIV style="position:absolute;top:99;left:98"><nobr><span class="ft4">Boeing To Explore Electric Airplane</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:98"><nobr><span class="ft4">Boeing</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:152"><nobr><span class="ft4">Commercial</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:237"><nobr><span class="ft4">Airplanes</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:307"><nobr><span class="ft4">will</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:341"><nobr><span class="ft4">develop</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:400"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:98"><nobr><span class="ft9">test an electrically powered demonstrator airplane as<br>part of a study to evaluate environmentally friendly<br>fuel cell technology for future Boeing products.</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:98"><nobr><span class="ft9">The airplane manufacturer is working with Boe-<br>ing's new Research and Technology Center in Madrid,<br>Spain, to modify a small, single-engine airplane by<br>replacing its engine with fuel cells and an electric<br>motor that will turn a conventional propeller.</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:98"><nobr><span class="ft9">Boeing Madrid will design and integrate the ex-<br>perimental airplane's control system.</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:98"><nobr><span class="ft9">"By adapting this technology for aviation, Boe-<br>ing intends to demonstrate its leadership in the<br>pursuit of delivering environmentally preferred prod-<br>ucts."</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:81"><nobr><span class="ft9">Figure 4: Summary using sentences containing fo-<br>cused named entities</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:81"><nobr><span class="ft9">for documents containing the key phrase "World Cup". The<br>ranking function, which determines which document is more<br>relevant to the query, is very important to the search quality.</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:94"><nobr><span class="ft4">Since our query is a single phrase, the ranked search re-</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:81"><nobr><span class="ft9">sults, displayed in Table 1, are based on the term frequency<br>of the phrase "World Cup". It is clear that without deeper<br>text understanding, term frequency is a quite reasonable<br>measure of relevancy. However, although some articles may<br>contain more "World Cup" than others, they may actu-<br>ally focus less on the World Cup event which we are inter-<br>ested. Therefore a better indication of document relevancy<br>is whether a document focuses on the entity we are inter-<br>ested in. A simple method is to re-order the search results<br>first by whether the query entity is focused or not, and then<br>by its term-frequency. It is quite clear that this method<br>gives higher quality ranking.</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:94"><nobr><span class="ft4">In this example, we use Chinese corpus for demonstration,</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:81"><nobr><span class="ft9">so the original searching results are in Chinese, which we<br>have translated into English for reading convenience.</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:81"><nobr><span class="ft3"><b>2.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:121"><nobr><span class="ft3"><b>Other Uses of Focused Named Entity</b></span></nobr></DIV>
<DIV style="position:absolute;top:831;left:94"><nobr><span class="ft4">We believe that focused named entities are also helpful in</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft9">text clustering and categorization tasks such as topic detec-<br>tion and tracking. This is because if focused named entities<br>are automatically recognized, then the event for each docu-<br>ment can be described more precisely. Since focused named<br>entities characterize what an article talks about, it is natu-<br>ral to organize articles based on them. Therefore by giving<br>more weights to focused named entities, we believe that we<br>can potentially obtain better quality clustering and more<br>accurate topic detection and tracking.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:94"><nobr><span class="ft4">Our study of the focused named entity recognition prob-</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:81"><nobr><span class="ft9">lem is motivated by its potential applications as illustrated<br>above. Experiments in section 5.2 indicate that there is a<br>sufficient agreement on focused named entities among hu-<br>man reviewers. Therefore our goal is to build a system that<br>can automatically detect focused named entities among all</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">named entities in a document. We shall mention that al-<br>though this paper only studies named entities, the basic idea<br>can be extended to tasks such as finding important words,<br>noun-phrases in a document.</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:475"><nobr><span class="ft3"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:506"><nobr><span class="ft13"><b>LEARNING BASED FOCUSED NAMED<br>ENTITY RECOGNITION</b></span></nobr></DIV>
<DIV style="position:absolute;top:209;left:489"><nobr><span class="ft4">Focused named entity recognition can be regarded as a</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:475"><nobr><span class="ft9">binary classification problem. Consider the set of all named<br>entities in a document extracted by a named entity recogni-<br>tion system. Each entity in this set can be labeled yes if it<br>is a focused entity, or no if it is not. We formally define a<br>two-class categorization problem as one to determine a label<br>y</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:486"><nobr><span class="ft4"> {-1, 1} associated with a vector x of input variables.</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:489"><nobr><span class="ft4">However, in order to build a successful focused named en-</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:475"><nobr><span class="ft9">tity extractor, a number of issues have to be studied. First,<br>named entities that refer to the same person or organization<br>need to be grouped together; secondly what features are<br>useful; and thirdly, how well different learning algorithms<br>perform on this task. These issues will be carefully studied.</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:475"><nobr><span class="ft3"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:515"><nobr><span class="ft3"><b>Coreference Resolution</b></span></nobr></DIV>
<DIV style="position:absolute;top:445;left:489"><nobr><span class="ft4">Coreference is a common phenomenon in natural language.</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:475"><nobr><span class="ft9">It means that an entity can be referred to in different ways<br>and in different locations of the text. Therefore for focused<br>named entity recognition, it is useful to apply a coreference<br>resolution algorithm to merge entities with the same ref-<br>erents in a given document. There are different kinds of<br>coreference according to the basic coreference types, such<br>as pronominal coreference, proper name coreference, appo-<br>sition, predicate nominal, etc. Here in our system, we only<br>consider proper name coreference, which is to identify all<br>variations of a named entity in the text.</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:489"><nobr><span class="ft4">Although it is possible to use machine learning methods</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:475"><nobr><span class="ft9">for coreference resolution (see [20] as an example), we shall<br>use a simpler scheme, which works reasonably well. Our<br>coreference resolution method can be described as follows.</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:491"><nobr><span class="ft4">1. Partitioning: The set of named entities is divided into</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:509"><nobr><span class="ft9">sub-sets according to named entity types, because coref-<br>erence only occurs among entities with the same types.</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:491"><nobr><span class="ft4">2. Pair-wise comparison: Within each sub-set, pair-wise</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:509"><nobr><span class="ft9">comparison is performed to detect whether each entity-<br>pair is an instance of coreference. In this study, we use<br>a simple algorithm which is based on string-matching<br>only. Since we work with Chinese data, we split each<br>entity into single Chinese characters. We study two<br>different schemes here: using either exact string match-<br>ing or partial string matching to decide coreference.<br>In the case of exact string matching, two entities are<br>considered to be a coreference pair only when they<br>are identical. In the case of partial string matching, if<br>characters in the shorter entity form a (non-consecutive)<br>sub-string of the longer entity, then the two entities are<br>considered to be a coreference pair.</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:491"><nobr><span class="ft4">3. Clustering: Merge all coreference pairs created in the</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:509"><nobr><span class="ft9">second step into the same coreference chains. This step<br>can also be done differently. For example, by using a<br>sequential clustering method.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft4">Although the coreference resolution algorithm described</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">above is not perfect, it is not crucial since the results will</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">283</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft14{font-size:14px;font-family:Times;color:#000000;}
	.ft15{font-size:5px;font-family:Times;color:#000000;}
	.ft16{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
	.ft17{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="94004.png" alt="background image">
<DIV style="position:absolute;top:96;left:322"><nobr><span class="ft4">Table 1: Search result of "World Cup"</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:114"><nobr><span class="ft4">focus/not</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:190"><nobr><span class="ft4">tf</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:218"><nobr><span class="ft4">title</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:188"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:218"><nobr><span class="ft4">Uncover the Mystery of World Cup Draws</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:188"><nobr><span class="ft4">11</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:218"><nobr><span class="ft4">Brazil and Germany Qualified, Iran Kicked out</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:191"><nobr><span class="ft4">9</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:218"><nobr><span class="ft4">Preparing for World Cup, China Football Federation and Milutinovic Snatch the Time</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:191"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:218"><nobr><span class="ft4">Sun Wen Understands the Pressure Milutinovic and China Team Faced</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:191"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:218"><nobr><span class="ft4">Korea Leaves More Tickets to China Fans</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:191"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:218"><nobr><span class="ft4">Paraguay Qualified, but Head Coach Dismissed</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:191"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:218"><nobr><span class="ft4">LiXiang: Special Relationships between Milutinovic and I</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:191"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:218"><nobr><span class="ft4">Three Stars on Golden Eagle Festival</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:191"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:218"><nobr><span class="ft4">Adidas Fevernova, the Official 2002 FIFA World Cup Ball, Appears Before the Public in Beijing</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:191"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:218"><nobr><span class="ft4">China's World Top 10 Start to Vote</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:191"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:218"><nobr><span class="ft4">Qualified vs. Kicked out: McCarthy Stays on, Blazevic Demits</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:191"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:218"><nobr><span class="ft4">China Attends Group Match in Korea, But not in the Same Group With Korea</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:191"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:218"><nobr><span class="ft4">Don't Scare Peoples with Entering WTO</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:191"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:218"><nobr><span class="ft4">Kelon Tops China's Home Appliance Industry in CCTV Ads Bidding</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:191"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:218"><nobr><span class="ft4">Lou Lan: Great Secrets Behind</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:127"><nobr><span class="ft4">focus</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:191"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:218"><nobr><span class="ft4">Australia Beats Uruguay by One Goal</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:135"><nobr><span class="ft4">no</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:191"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:218"><nobr><span class="ft4">Chang Hong's "King of Precision Display": Good Friends of Football Fans</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:81"><nobr><span class="ft9">be passed to a machine learning algorithm in a later stage,<br>which can offset the mistakes made in the coreference stage.<br>Our experiment shows that by using coreference resolution,<br>the overall system performance can be improved apprecia-<br>bly.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft3"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:513;left:121"><nobr><span class="ft3"><b>Classification Methods</b></span></nobr></DIV>
<DIV style="position:absolute;top:535;left:94"><nobr><span class="ft4">In this paper, we compare three methods: a decision tree</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:81"><nobr><span class="ft9">based rule induction system, a naive Bayes classifier, and a<br>regularized linear classification method based on robust risk<br>minimization.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:85"><nobr><span class="ft14"><i>3.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:134"><nobr><span class="ft14"><i>Decision Tree</i></span></nobr></DIV>
<DIV style="position:absolute;top:628;left:94"><nobr><span class="ft4">In text-mining application, model interpretability is an</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:81"><nobr><span class="ft9">important characteristic to be considered in addition to the<br>accuracy achieved and the computational cost. The require-<br>ment of interpretability can be satisfied by using a rule-based<br>system, such as rules obtained from a decision tree. Rule-<br>based systems are particularly appealing since a person can<br>examine the rules and modify them. It is also much easier<br>to understand what a system does by examining its rules.</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:94"><nobr><span class="ft4">We shall thus include a decision tree based classifier in this</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:81"><nobr><span class="ft9">study. In a typical decision tree training algorithm, there are<br>usually two stages. The first stage is tree growing where a<br>tree is built by greedily splitting each tree node based on<br>a certain figure of merit. However after the first stage, the<br>tree can overfit the training data, therefore a second stage<br>involving tree pruning is invoked. In this stage, one removes<br>overfitted branches of the tree so that the remaining portion<br>has better predictive power. In our decision tree package,<br>the splitting criteria during tree growth is similar to that<br>of the standard C4.5 program [18], and the tree pruning is<br>done using a Bayesian model combination approach. See [6]<br>for detailed description.</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:85"><nobr><span class="ft14"><i>3.2.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:967;left:134"><nobr><span class="ft14"><i>Naive Bayes</i></span></nobr></DIV>
<DIV style="position:absolute;top:988;left:94"><nobr><span class="ft4">Another very popular binary classification method is naive</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:81"><nobr><span class="ft9">Bayes. In spite of its simplicity, it often achieves reasonable<br>performance in practical applications. It can be regarded as<br>a linear classification method, where we seek a weight vector<br>w and a threshold  such that w</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:280"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:289"><nobr><span class="ft4">x &lt;  if its label y =</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:422"><nobr><span class="ft4">-1</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">and w</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:119"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:127"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:140"><nobr><span class="ft4">  if its label y = 1. A score of value w</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:398"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:407"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:418"><nobr><span class="ft4">- </span></nobr></DIV>
<DIV style="position:absolute;top:426;left:475"><nobr><span class="ft9">can be assigned to each data point as a surrogate for the<br>likelihood of x to be in class.</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:489"><nobr><span class="ft4">In this work, we adopt the multinomial model described</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:475"><nobr><span class="ft4">in [14]. Let</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:557"><nobr><span class="ft4">{(x</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:577"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:583"><nobr><span class="ft4">, y</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:596"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:602"><nobr><span class="ft4">), . . . , (x</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:652"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:659"><nobr><span class="ft4">, y</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:672"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:680"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:685"><nobr><span class="ft4">} be the set of training</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:475"><nobr><span class="ft4">data. The linear weight w is given by w = w</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:753"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:763"><nobr><span class="ft4">- w</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:788"><nobr><span class="ft11">-1</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:802"><nobr><span class="ft4">, and</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft4"> = </span></nobr></DIV>
<DIV style="position:absolute;top:502;left:512"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:523"><nobr><span class="ft4">- </span></nobr></DIV>
<DIV style="position:absolute;top:500;left:544"><nobr><span class="ft11">-1</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:559"><nobr><span class="ft4">. Denote by x</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:651"><nobr><span class="ft11">i,j</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:670"><nobr><span class="ft4">the j-th component of the</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:475"><nobr><span class="ft4">data vector x</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:556"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:561"><nobr><span class="ft4">, then the j-th component w</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:734"><nobr><span class="ft16">c<br>j</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:745"><nobr><span class="ft4">of w</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:771"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:781"><nobr><span class="ft4">(c =</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:811"><nobr><span class="ft4">±1)</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:475"><nobr><span class="ft4">is given by</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:554"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:565"><nobr><span class="ft16">c<br>j</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:574"><nobr><span class="ft4">= log</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:633"><nobr><span class="ft4"> +</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:672"><nobr><span class="ft11">i:y</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:685"><nobr><span class="ft15">i</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:690"><nobr><span class="ft11">=c</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:706"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:714"><nobr><span class="ft11">i,j</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:611"><nobr><span class="ft4">d +</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:657"><nobr><span class="ft17">d<br>j=1</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:694"><nobr><span class="ft11">i:y</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:707"><nobr><span class="ft15">i</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:712"><nobr><span class="ft11">=c</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:728"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:736"><nobr><span class="ft11">i,j</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:751"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:475"><nobr><span class="ft4">and </span></nobr></DIV>
<DIV style="position:absolute;top:614;left:509"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:519"><nobr><span class="ft4">(c =</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:549"><nobr><span class="ft4">±1) is given by </span></nobr></DIV>
<DIV style="position:absolute;top:614;left:653"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:662"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:677"><nobr><span class="ft4">- log</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:712"><nobr><span class="ft11">|{i:y</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:734"><nobr><span class="ft15">i</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:739"><nobr><span class="ft11">=c}|</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:733"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:763"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:489"><nobr><span class="ft4">The parameter  &gt; 0 in the above formulation is a smooth-</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:475"><nobr><span class="ft9">ing (regularization) parameter. [14] fixed  to be 1, which<br>corresponds to the Laplacian smoothing.</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:479"><nobr><span class="ft14"><i>3.2.3</i></span></nobr></DIV>
<DIV style="position:absolute;top:692;left:528"><nobr><span class="ft14"><i>Robust Risk Minimization Method</i></span></nobr></DIV>
<DIV style="position:absolute;top:714;left:489"><nobr><span class="ft4">Similar to naive Bayes, this method is also a linear pre-</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:475"><nobr><span class="ft4">diction method.</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:586"><nobr><span class="ft4">Given a linear model p(x) = w</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:788"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:796"><nobr><span class="ft4">x + b,</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:475"><nobr><span class="ft9">we consider the following prediction rule: predict y = 1 if<br>p(x)</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:505"><nobr><span class="ft4"> 0, and predict y = -1 otherwise. The classification</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:475"><nobr><span class="ft9">error (we shall ignore the point p(x) = 0, which is assumed<br>to occur rarely) is</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:557"><nobr><span class="ft4">I(p(x), y) =</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:649"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:669"><nobr><span class="ft4">if p(x)y</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:719"><nobr><span class="ft4"> 0,</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:649"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:669"><nobr><span class="ft4">if p(x)y &gt; 0.</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:475"><nobr><span class="ft9">A very natural way to compute a linear classifier is by find-<br>ing a weight ( ^</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:558"><nobr><span class="ft4">w, ^</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:575"><nobr><span class="ft4">b) that minimizes the average classification</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:475"><nobr><span class="ft4">error in the training set:</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:540"><nobr><span class="ft4">( ^</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:546"><nobr><span class="ft4">w, ^</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:562"><nobr><span class="ft4">b) = arg min</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:617"><nobr><span class="ft11">w,b</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:641"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:640"><nobr><span class="ft4">n</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:660"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:654"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:675"><nobr><span class="ft4">I(w</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:698"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:707"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:715"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:722"><nobr><span class="ft4">+ b, y</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:755"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:760"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft9">Unfortunately this problem is typically NP-hard computa-<br>tionally. It is thus desirable to replace the classification error<br>loss I(p, y) with another formulation that is computation-<br>ally more desirable. Large margin methods such as SVM<br>employ modified loss functions that are convex. Many loss<br>functions work well for related classification problems such<br>as text-categorization [24, 10].</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:676"><nobr><span class="ft4">The specific loss function</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">284</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:11px;line-height:24px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="94005.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft4">consider here is</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:144"><nobr><span class="ft4">h(p, y) =</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:201"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:106;left:201"><nobr><span class="ft18"><br></span></nobr></DIV>
<DIV style="position:absolute;top:105;left:220"><nobr><span class="ft4">-2py</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:298"><nobr><span class="ft4">py &lt;</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:331"><nobr><span class="ft4">-1</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:222"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:230"><nobr><span class="ft4">(py</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:252"><nobr><span class="ft4">- 1)</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:278"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:298"><nobr><span class="ft4">py</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:316"><nobr><span class="ft4"> [-1, 1]</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:221"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:298"><nobr><span class="ft4">py &gt; 1.</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:81"><nobr><span class="ft9">That is, our linear weights are computed by minimizing the<br>following average loss on the training data:</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:145"><nobr><span class="ft4">( ^</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:150"><nobr><span class="ft4">w, ^</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:167"><nobr><span class="ft4">b) = arg min</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:226"><nobr><span class="ft11">w</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:246"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:246"><nobr><span class="ft4">n</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:264"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:259"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:280"><nobr><span class="ft4">h(w</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:304"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:313"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:321"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:328"><nobr><span class="ft4">+ b, y</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:361"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:366"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:81"><nobr><span class="ft9">This method, which we refer to as RRM (robust risk mini-<br>mization), has been applied to linguistic processing [23] and<br>text categorization [2] with good results. Detailed algorithm<br>was introduced in [22].</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft3"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:337;left:112"><nobr><span class="ft3"><b>FEATURES</b></span></nobr></DIV>
<DIV style="position:absolute;top:359;left:94"><nobr><span class="ft4">We assume that named entities are extracted by a named</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:81"><nobr><span class="ft9">entity recognition system. Many named entity recognition<br>techniques have been reported in the literal, most of them<br>use machine learning approach. An overview of these meth-<br>ods can be found in [19]. In our system, for the purpose of<br>simplicity, we use human annotated named entities in the<br>experiments. In the learning phase, each named entity is<br>considered as an independent learning instance. Features<br>must reflect properties of an individual named entity, such<br>as its type and frequency, and various global statistical mea-<br>sures either at the document scale or at the corpus scale.<br>This section describes features we have considered in our<br>system, our motivations, and how their values are encoded.</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:81"><nobr><span class="ft3"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:573;left:121"><nobr><span class="ft3"><b>Entity Type</b></span></nobr></DIV>
<DIV style="position:absolute;top:595;left:94"><nobr><span class="ft4">Four entity types are defined: person, organization, place,</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:81"><nobr><span class="ft9">and proper nouns. The type of a named entity is a very<br>useful feature. For example, person and organization are<br>more likely to be the focus than a place. Each entity type<br>corresponds to a binary feature-component in the feature<br>vector, taking a value of either one or zero. For example,<br>a person type is encoded as [1 0 0 0], and an organization<br>type is encoded as [0 1 0 0].</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:81"><nobr><span class="ft3"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:730;left:121"><nobr><span class="ft3"><b>In Title or Not</b></span></nobr></DIV>
<DIV style="position:absolute;top:752;left:94"><nobr><span class="ft4">Whether a named entity appears in the title or not is an</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:81"><nobr><span class="ft9">important indicator of whether it is a focused entity. This<br>is because title is a concise summary of what an article is<br>about. The value of this feature is binary (0 or 1).</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:81"><nobr><span class="ft3"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:825;left:121"><nobr><span class="ft3"><b>Entity Frequency</b></span></nobr></DIV>
<DIV style="position:absolute;top:847;left:94"><nobr><span class="ft4">This feature is the number of times that the named entity</span></nobr></DIV>
<DIV style="position:absolute;top:862;left:81"><nobr><span class="ft9">occurs in the document. Generally speaking, the more fre-<br>quent it occurs, the more important it is. The value of this<br>feature is just the frequency of the named entity.</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:81"><nobr><span class="ft3"><b>4.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:919;left:121"><nobr><span class="ft3"><b>Entity Distribution</b></span></nobr></DIV>
<DIV style="position:absolute;top:941;left:94"><nobr><span class="ft4">This feature is somewhat complicated. The motivation is</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:81"><nobr><span class="ft9">that if a named entity occurs in many different parts of a<br>document, then it is more likely to be an important entity.<br>Therefore we use the entropy of the probability distribution<br>that measures how evenly an entity is distributed in a doc-<br>ument.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:94"><nobr><span class="ft4">Consider a document which is divided into m sections.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft9">Suppose that each named entity's probability distribution is<br>given by</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:133"><nobr><span class="ft4">{p</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:147"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:153"><nobr><span class="ft4">, ..., p</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:184"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:188"><nobr><span class="ft4">, ..., p</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:219"><nobr><span class="ft11">m</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:230"><nobr><span class="ft4">}, where p</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:289"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:298"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:320"><nobr><span class="ft11">occurrence in ith section</span></nobr></DIV>
<DIV style="position:absolute;top:1074;left:314"><nobr><span class="ft11">total occurrence in the doc</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:446"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">The entropy of the named entity distribution is computed<br>by entropy =</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:562"><nobr><span class="ft4">-</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:590"><nobr><span class="ft17">m<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:611"><nobr><span class="ft4">p</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:618"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:624"><nobr><span class="ft4">log p</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:652"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:656"><nobr><span class="ft4">. In our experiments, we se-</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:475"><nobr><span class="ft9">lect m = 10. This feature contributes a real valued feature-<br>component to the feature vector.</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:475"><nobr><span class="ft3"><b>4.5</b></span></nobr></DIV>
<DIV style="position:absolute;top:160;left:515"><nobr><span class="ft3"><b>Entity Neighbor</b></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:489"><nobr><span class="ft4">The context in which a certain named entity appears is</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:475"><nobr><span class="ft9">quite useful. In this study, we only consider a simple fea-<br>ture which counts its left and right neighboring entity types.<br>If several named entities of the same type are listed side by<br>side, then it is likely that the purpose is for enumeration, and<br>the listed named entities are not important. Each neighbor-<br>ing side has five possible types -- four named entity types<br>plus a normal-word (not a named entity) type. For example,<br>consider a person mentioned three times in the document.<br>Among the three mentions, the left neighbors are two person<br>names and one common word, and the right neighbors are<br>one place name and two common words. Then the entity<br>neighbor feature components are [2 0 0 0 1 0 0 1 0 2].</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:475"><nobr><span class="ft3"><b>4.6</b></span></nobr></DIV>
<DIV style="position:absolute;top:398;left:515"><nobr><span class="ft3"><b>First Sentence Occurrence</b></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:489"><nobr><span class="ft4">This feature is inspired by the position method [3, 12] in</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:475"><nobr><span class="ft9">sentence extraction. Its value is the occurrences of the entity<br>appearing in the beginning sentence of a paragraph.</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:475"><nobr><span class="ft3"><b>4.7</b></span></nobr></DIV>
<DIV style="position:absolute;top:478;left:515"><nobr><span class="ft3"><b>Document Has Entity in Title or Not</b></span></nobr></DIV>
<DIV style="position:absolute;top:500;left:489"><nobr><span class="ft4">This feature indicates whether any entity exists in the title</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:475"><nobr><span class="ft4">of the document, and thus takes binary value of 0 or 1.</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:475"><nobr><span class="ft3"><b>4.8</b></span></nobr></DIV>
<DIV style="position:absolute;top:542;left:515"><nobr><span class="ft3"><b>Total Entity Count</b></span></nobr></DIV>
<DIV style="position:absolute;top:565;left:489"><nobr><span class="ft4">This feature is the total number of entities in the doc-</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:475"><nobr><span class="ft9">ument, which takes integer value. The feature reflects the<br>relative importance of an entity in the entity set.</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:475"><nobr><span class="ft3"><b>4.9</b></span></nobr></DIV>
<DIV style="position:absolute;top:623;left:515"><nobr><span class="ft3"><b>Document Frequency in the Corpus</b></span></nobr></DIV>
<DIV style="position:absolute;top:645;left:489"><nobr><span class="ft4">This is a corpus level feature. If a named entity has a low</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:475"><nobr><span class="ft9">frequency in the document collection, but relatively high<br>frequency in the current document, then it is likely to be a<br>focused entity. When this feature is used, the term frequency<br>feature in section 4.3 will be computed using (tf /docsize)</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:827"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:723;left:475"><nobr><span class="ft9">log(N/df ), where df is the number of documents that a<br>named entity occurs in.</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:475"><nobr><span class="ft3"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:776;left:506"><nobr><span class="ft3"><b>EXPERIMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:798;left:489"><nobr><span class="ft4">In this section, we study the following issues: corpus an-</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:475"><nobr><span class="ft9">notation, human-level agreement on focused named entities,<br>performance of machine learning methods compared with a<br>baseline, influence of different features, and the impact of<br>coreference module to the overall performance.</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:475"><nobr><span class="ft3"><b>5.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:888;left:515"><nobr><span class="ft3"><b>Corpus Annotation</b></span></nobr></DIV>
<DIV style="position:absolute;top:910;left:489"><nobr><span class="ft4">We select fifteen days of Beijing Youth Daily news in</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:475"><nobr><span class="ft9">November 2001 as our testing corpus, which contains 1,325<br>articles. The text, downloaded from http://bjyouth.ynet.com,<br>is in Chinese. The articles belong to a variety of categories,<br>including politics, economy, laws, education, science, enter-<br>tainments, and sports.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:489"><nobr><span class="ft4">Since different people may have different opinions on the</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft9">focused named entities, a common set of rules should be<br>agreed upon before the whole corpus is to be annotated.<br>We use the following method to come up with a general<br>guideline for annotating focused named entities.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">285</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="94006.png" alt="background image">
<DIV style="position:absolute;top:86;left:94"><nobr><span class="ft4">First, the named entities in each document were anno-</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:81"><nobr><span class="ft9">tated by human. Then, we selected twenty documents from<br>the corpus and invited twelve people to mark focused named<br>entities. Nine of the twelve people are experts in natural<br>language processing, so their opinions are very valuable to<br>define focused named entities. Based on the survey result,<br>entities marked by more than half of the survey participants<br>were defined as focused named entities. We obtained fifty<br>focused named entities for the twenty articles. By studying<br>the focused named entities in those articles, we were able to<br>design specifications for focused named entity annotation.<br>The whole corpus was then marked according to the speci-<br>fications.</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:81"><nobr><span class="ft3"><b>5.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:299;left:121"><nobr><span class="ft3"><b>Human Agreement Statistics</b></span></nobr></DIV>
<DIV style="position:absolute;top:321;left:94"><nobr><span class="ft4">In our survey, fifty entities were identified as focused en-</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft9">tities from the total number of 341 entities in the 20 doc-<br>uments. Table 2 shows, among the 50 focused entities, 5<br>entities are agreed as focus by all 12 persons, and 7 entities<br>are agreed by 11 persons, etc.</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:131"><nobr><span class="ft4">Table 2: Human agreement statistics</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:89"><nobr><span class="ft4">num of focused named entities</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:293"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:322"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:352"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:379"><nobr><span class="ft4">8</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:402"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:425"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:455"><nobr><span class="ft4">8</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:110"><nobr><span class="ft4">num of person agreeing</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:289"><nobr><span class="ft4">12</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:319"><nobr><span class="ft4">11</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:349"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:379"><nobr><span class="ft4">9</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:402"><nobr><span class="ft4">8</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:428"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:455"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:94"><nobr><span class="ft4">Let N</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:130"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:142"><nobr><span class="ft4">denotes the number of person with agreement on</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:81"><nobr><span class="ft9">focused named entity k, then the human agreement level<br>Agree</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:118"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:130"><nobr><span class="ft4">on the k-th focused named entity is Agree</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:390"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:401"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:419"><nobr><span class="ft11">N</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:427"><nobr><span class="ft15">k</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:421"><nobr><span class="ft11">12</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:436"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:81"><nobr><span class="ft4">The average agreement on the 50 focused named entities</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft4">is Average Agree =</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:223"><nobr><span class="ft15">50</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:223"><nobr><span class="ft15">k=1</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:244"><nobr><span class="ft11">Agree</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:275"><nobr><span class="ft15">k</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:241"><nobr><span class="ft11">50</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:288"><nobr><span class="ft4">= 72.17%, with variance</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:81"><nobr><span class="ft9">2.65%. We also computed the precision and the recall for the<br>survey participants with respect to the fifty focused named<br>entities.</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:142"><nobr><span class="ft4">Table 3 shows that the best human annotator</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:81"><nobr><span class="ft4">achieves an F</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:167"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:179"><nobr><span class="ft4">measure of 81.32%.</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:313"><nobr><span class="ft4">Some of the partici-</span></nobr></DIV>
<DIV style="position:absolute;top:639;left:81"><nobr><span class="ft9">pants marked either too many or too few named entities,<br>and thus had much lower performance numbers. This prob-<br>lem was fixed when the whole corpus was annotated using<br>specifications induced from this small-scale experiment.</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:117"><nobr><span class="ft4">Table 3: Human annotation performance</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:155"><nobr><span class="ft4">user id</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:212"><nobr><span class="ft4">precision</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:281"><nobr><span class="ft4">recall</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:338"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:346"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:172"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:222"><nobr><span class="ft4">90.24</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:281"><nobr><span class="ft4">74.00</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:329"><nobr><span class="ft4">81.32</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:172"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:222"><nobr><span class="ft4">86.05</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:281"><nobr><span class="ft4">74.00</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:329"><nobr><span class="ft4">79.57</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:172"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:222"><nobr><span class="ft4">83.33</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:281"><nobr><span class="ft4">70.00</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:329"><nobr><span class="ft4">76.09</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:172"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:222"><nobr><span class="ft4">84.21</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:281"><nobr><span class="ft4">64.00</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:329"><nobr><span class="ft4">72.73</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:172"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:222"><nobr><span class="ft4">96.55</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:281"><nobr><span class="ft4">56.00</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:329"><nobr><span class="ft4">70.89</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:172"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:222"><nobr><span class="ft4">90.63</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:281"><nobr><span class="ft4">58.00</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:329"><nobr><span class="ft4">70.73</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:172"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:222"><nobr><span class="ft4">71.74</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:281"><nobr><span class="ft4">66.00</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:329"><nobr><span class="ft4">68.75</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:172"><nobr><span class="ft4">8</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:222"><nobr><span class="ft4">73.81</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:281"><nobr><span class="ft4">62.00</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:329"><nobr><span class="ft4">67.39</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:172"><nobr><span class="ft4">9</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:222"><nobr><span class="ft4">57.14</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:281"><nobr><span class="ft4">80.00</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:329"><nobr><span class="ft4">66.67</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:168"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:222"><nobr><span class="ft4">48.19</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:281"><nobr><span class="ft4">80.00</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:329"><nobr><span class="ft4">60.15</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:168"><nobr><span class="ft4">11</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:222"><nobr><span class="ft4">38.60</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:281"><nobr><span class="ft4">88.00</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:329"><nobr><span class="ft4">53.66</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:168"><nobr><span class="ft4">12</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:222"><nobr><span class="ft4">33.33</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:281"><nobr><span class="ft4">94.00</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:329"><nobr><span class="ft4">49.21</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:81"><nobr><span class="ft3"><b>5.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:982;left:121"><nobr><span class="ft3"><b>Corpus Named Entity Statistics</b></span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:94"><nobr><span class="ft4">We consider two data sets in our experiments: one is the</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft9">whole corpus of 1,325 articles, and the other is a subset of<br>726 articles with named entities in their titles. Table 4 shows<br>there are totally 3,001 focused entities among 18,371 entities<br>in the whole corpus, which means that 16.34 percent of the</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft9">entities are marked as focused. On average, there are 2.26<br>focused named entities for each article, which is consistent<br>with the small-scale survey result.</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:500"><nobr><span class="ft4">Table 4: Corpus statistics on named entities</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:483"><nobr><span class="ft4">set</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:516"><nobr><span class="ft4">docnum</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:580"><nobr><span class="ft4">entities</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:640"><nobr><span class="ft4">focuses</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:698"><nobr><span class="ft4">focus percent</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:795"><nobr><span class="ft4">focus/doc</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:488"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:525"><nobr><span class="ft4">1,325</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:583"><nobr><span class="ft4">18,371</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:645"><nobr><span class="ft4">3,001</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:717"><nobr><span class="ft4">16.34%</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:812"><nobr><span class="ft4">2.26</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:488"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:530"><nobr><span class="ft4">726</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:583"><nobr><span class="ft4">10,697</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:645"><nobr><span class="ft4">1,669</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:717"><nobr><span class="ft4">15.60%</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:812"><nobr><span class="ft4">2.30</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:475"><nobr><span class="ft3"><b>5.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:255;left:515"><nobr><span class="ft3"><b>Baseline Results</b></span></nobr></DIV>
<DIV style="position:absolute;top:277;left:489"><nobr><span class="ft4">Since named entities in title or with high frequency are</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:475"><nobr><span class="ft9">more likely to be the focal entities, we consider three base-<br>line methods. The first method marks entities in titles to<br>be the foci; the second method marks most frequent entities<br>in each article to be the focal entities; the third method is<br>a combination of the above two, which selects those entities<br>either in title or occurring most frequently. We use par-<br>tial string matching for coreference resolution in the three<br>baseline experiments.</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:489"><nobr><span class="ft4">Named entities occurring in the title are more likely to be</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:475"><nobr><span class="ft9">the focus of the document, but they only represent a small<br>portion of all focal entities. Baseline experiment 1 shows the<br>precision of this method is quite high but the recall is very<br>low.</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:489"><nobr><span class="ft4">Baseline experiment 2 implies that most of the top 1</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:475"><nobr><span class="ft9">named entities are focused entities, but again the recall is<br>very low. However, if more named entities are selected, the<br>precision is decreased significantly, so that the F</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:773"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:785"><nobr><span class="ft4">measure</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:475"><nobr><span class="ft9">does not improve. The top-3 performance is the worst, with<br>an F</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:503"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:513"><nobr><span class="ft4">measure of only 50.47%. Note that several named en-</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:475"><nobr><span class="ft9">tities may have the same occurrence frequency in one docu-<br>ment, which introduces uncertainty into the method.</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:489"><nobr><span class="ft4">By combining named entities from the title and with high</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:475"><nobr><span class="ft9">frequency, we obtain better results than either of the two<br>basic baseline methods. The best performance is achieved<br>by combining the in-title and top 1 named entities, which<br>achieves F</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:538"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:549"><nobr><span class="ft4">measures of 66.68% for data set 1, and 70.51%</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:475"><nobr><span class="ft4">for data set 2.</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:475"><nobr><span class="ft3"><b>5.5</b></span></nobr></DIV>
<DIV style="position:absolute;top:726;left:515"><nobr><span class="ft3"><b>Machine Learning Results</b></span></nobr></DIV>
<DIV style="position:absolute;top:748;left:489"><nobr><span class="ft4">Since in our implementation, decision tree and naive Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:475"><nobr><span class="ft9">methods only take integer features, we encode the float-<br>ing features to integer values using a simple equal interval<br>binning method. If a feature x is observed to have values<br>bounded by x</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:557"><nobr><span class="ft11">min</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:582"><nobr><span class="ft4">and x</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:616"><nobr><span class="ft11">max</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:639"><nobr><span class="ft4">, then the bin width is computed</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:475"><nobr><span class="ft4">by  =</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:521"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:528"><nobr><span class="ft15">max</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:549"><nobr><span class="ft11">-x</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:564"><nobr><span class="ft15">min</span></nobr></DIV>
<DIV style="position:absolute;top:834;left:550"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:591"><nobr><span class="ft4">and the bin boundaries are at x</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:784"><nobr><span class="ft11">min</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:809"><nobr><span class="ft4">+ i</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:475"><nobr><span class="ft4">where i = 1, ..., k</span></nobr></DIV>
<DIV style="position:absolute;top:839;left:583"><nobr><span class="ft4">- 1. The method is applied to each con-</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:475"><nobr><span class="ft9">tinuous feature independently and k is set to 10. Although<br>more sophisticated discretization methods exist, the equal<br>interval binning method performs quite well in practice.</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:489"><nobr><span class="ft4">Machine learning results are obtained from five-fold cross-</span></nobr></DIV>
<DIV style="position:absolute;top:920;left:475"><nobr><span class="ft9">validation. Coreference resolution is done with partial string-<br>matching. The test results are reported in Table 6.</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:489"><nobr><span class="ft4">This experiment shows that good performance can be</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:475"><nobr><span class="ft9">achieved by using machine learning techniques. The RRM<br>performance on both data sets are significantly better than<br>the base line results, and comparable to that of the best hu-<br>man annotator we observed from our small-scale experiment<br>in Section 5.2.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">286</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="94007.png" alt="background image">
<DIV style="position:absolute;top:96;left:372"><nobr><span class="ft4">Table 5: Baseline results</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:240"><nobr><span class="ft4">Corpus</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:314"><nobr><span class="ft4">Method</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:385"><nobr><span class="ft4">Focuses</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:447"><nobr><span class="ft4">focus/doc</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:522"><nobr><span class="ft4">Precision</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:593"><nobr><span class="ft4">Recall</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:654"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:663"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:238"><nobr><span class="ft4">726docs</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:325"><nobr><span class="ft4">title</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:397"><nobr><span class="ft4">992</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:464"><nobr><span class="ft4">1.36</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:534"><nobr><span class="ft4">83.47</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:596"><nobr><span class="ft4">49.61</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:646"><nobr><span class="ft4">62.23</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:232"><nobr><span class="ft4">1,325docs</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:324"><nobr><span class="ft4">top1</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:392"><nobr><span class="ft4">1,580</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:464"><nobr><span class="ft4">1.19</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:534"><nobr><span class="ft4">88.54</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:596"><nobr><span class="ft4">46.62</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:646"><nobr><span class="ft4">61.08</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:324"><nobr><span class="ft4">top2</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:392"><nobr><span class="ft4">4,194</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:464"><nobr><span class="ft4">3.17</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:534"><nobr><span class="ft4">54.48</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:596"><nobr><span class="ft4">76.14</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:646"><nobr><span class="ft4">63.52</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:324"><nobr><span class="ft4">top3</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:392"><nobr><span class="ft4">7,658</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:464"><nobr><span class="ft4">5.78</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:534"><nobr><span class="ft4">35.13</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:596"><nobr><span class="ft4">89.64</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:646"><nobr><span class="ft4">50.47</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:238"><nobr><span class="ft4">726docs</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:307"><nobr><span class="ft4">title+top1</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:392"><nobr><span class="ft4">1,247</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:464"><nobr><span class="ft4">1.72</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:534"><nobr><span class="ft4">82.44</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:596"><nobr><span class="ft4">61.59</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:646"><nobr><span class="ft4">70.51</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:307"><nobr><span class="ft4">title+top2</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:392"><nobr><span class="ft4">2,338</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:464"><nobr><span class="ft4">3.22</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:534"><nobr><span class="ft4">56.93</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:596"><nobr><span class="ft4">79.75</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:646"><nobr><span class="ft4">66.43</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:307"><nobr><span class="ft4">title+top3</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:392"><nobr><span class="ft4">4,165</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:464"><nobr><span class="ft4">5.74</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:534"><nobr><span class="ft4">36.06</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:596"><nobr><span class="ft4">89.99</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:646"><nobr><span class="ft4">51.49</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:232"><nobr><span class="ft4">1,325docs</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:307"><nobr><span class="ft4">title+top1</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:392"><nobr><span class="ft4">2,011</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:464"><nobr><span class="ft4">1.52</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:534"><nobr><span class="ft4">83.09</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:596"><nobr><span class="ft4">55.68</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:646"><nobr><span class="ft4">66.68</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:307"><nobr><span class="ft4">title+top2</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:392"><nobr><span class="ft4">4,388</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:464"><nobr><span class="ft4">3.31</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:534"><nobr><span class="ft4">53.78</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:596"><nobr><span class="ft4">78.64</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:646"><nobr><span class="ft4">63.88</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:307"><nobr><span class="ft4">title+top3</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:392"><nobr><span class="ft4">7,738</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:464"><nobr><span class="ft4">5.84</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:534"><nobr><span class="ft4">34.94</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:596"><nobr><span class="ft4">90.10</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:646"><nobr><span class="ft4">50.36</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:340"><nobr><span class="ft4">Table 6: Machine learning results</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:209"><nobr><span class="ft4">Dataset</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:335"><nobr><span class="ft4">RRM</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:453"><nobr><span class="ft4">Decision Tree</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:600"><nobr><span class="ft4">Naive Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:299"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:347"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:392"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:401"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:442"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:489"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:534"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:543"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:585"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:632"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:677"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:686"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:209"><nobr><span class="ft4">726 docs</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:288"><nobr><span class="ft4">88.51</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:336"><nobr><span class="ft4">80.54</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:383"><nobr><span class="ft4">84.27</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:431"><nobr><span class="ft4">87.29</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:479"><nobr><span class="ft4">78.02</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:526"><nobr><span class="ft4">82.37</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:574"><nobr><span class="ft4">69.32</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:622"><nobr><span class="ft4">90.28</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:669"><nobr><span class="ft4">78.37</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:209"><nobr><span class="ft4">1,325 docs</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:288"><nobr><span class="ft4">84.70</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:336"><nobr><span class="ft4">78.23</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:383"><nobr><span class="ft4">81.32</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:431"><nobr><span class="ft4">83.83</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:479"><nobr><span class="ft4">74.61</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:526"><nobr><span class="ft4">78.89</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:574"><nobr><span class="ft4">69.14</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:622"><nobr><span class="ft4">89.08</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:669"><nobr><span class="ft4">77.82</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:81"><nobr><span class="ft3"><b>5.6</b></span></nobr></DIV>
<DIV style="position:absolute;top:426;left:121"><nobr><span class="ft3"><b>Influence of Features</b></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:94"><nobr><span class="ft4">The goal of this section is to study the impact of different</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:81"><nobr><span class="ft9">features with different algorithms. Results are reported in<br>Table 7. Feature id corresponds to the feature subsection<br>number in section 4.</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:94"><nobr><span class="ft4">Experiment A uses frequency-based features only. It is</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft9">quite similar to the bag-of-word document model for text<br>categorization, with the entity-frequency and in-title infor-<br>mation. By adding more sophisticated document-level fea-<br>tures, the performance can be significantly improved. For<br>the RRM method, F</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:206"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:217"><nobr><span class="ft4">finally reaches 81.32%. It is interest-</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:81"><nobr><span class="ft9">ing to observe that the corpus-level feature (experiment F<br>versus G) has different impacts on the three algorithms. It<br>is a good feature for naive Bayes, but not for the RRM and<br>decision tree. Whether corpus-level features can appreciably<br>enhance the classification performance requires more careful<br>investigation.</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:94"><nobr><span class="ft4">The experiments also indicate that the three learning al-</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft9">gorithms do not perform equally well. RRM appears to have<br>the best overall performance. The naive Bayes method re-<br>quires all features to be independent, which is a quite unreal-<br>istic assumption in practice. The main problem for decision<br>tree is that it easily fragments the data, so that the proba-<br>bility estimate at the leaf-nodes become unreliable. This is<br>also the reason why voted decision trees (using procedures<br>like boosting or bagging) perform better.</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:94"><nobr><span class="ft4">The decision tree can find rules readable by a human. For</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:81"><nobr><span class="ft9">example, one such rule reads as: if a named entities appears<br>at least twice, its left and right neighbors are normal words,<br>its discrete distribution entropy is greater than 2, and the<br>entity appears in the title, then the probability of it being a<br>focused entity is 0.87.</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:81"><nobr><span class="ft3"><b>5.7</b></span></nobr></DIV>
<DIV style="position:absolute;top:951;left:121"><nobr><span class="ft3"><b>Coreference Resolution</b></span></nobr></DIV>
<DIV style="position:absolute;top:973;left:94"><nobr><span class="ft4">In order to understand the impact of coreference resolu-</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:81"><nobr><span class="ft9">tion on the performance of focused named entity recognition,<br>we did the same set of experiments as in section 5.5, but with<br>exact string matching only for coreference resolution in the<br>feature extraction process. Table 8 reports the five-fold cross<br>validation results. On average the performance is decreased<br>by about 3 to 5 percent. This means coreference resolution</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:475"><nobr><span class="ft9">plays an important role in the task. The reason is that it<br>maps variations of a named entity into a single group, so<br>that features such as occurrence frequency and entity distri-<br>bution can be estimated more reliably. We believe that with<br>more sophisticated analysis such as pronominal coreference<br>resolution, the classification performance can be further im-<br>proved.</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:475"><nobr><span class="ft3"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:553;left:506"><nobr><span class="ft3"><b>CONCLUSIONS AND FUTURE WORK</b></span></nobr></DIV>
<DIV style="position:absolute;top:576;left:489"><nobr><span class="ft4">In this paper, we studied the problem of focused named</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:475"><nobr><span class="ft9">entity recognition. We gave examples to illustrate that fo-<br>cused named entities are useful for many natural language<br>processing applications.</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:634"><nobr><span class="ft4">The task can be converted into</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:475"><nobr><span class="ft9">a binary classification problem. We focused on designing<br>linguistic features, and compared the performance of three<br>machine learning algorithms. Our results show that the ma-<br>chine learning approach can achieve near human-level accu-<br>racy. Because our system is trainable and features we use<br>are language independent, it is easy for us to build a similar<br>classification model for other languages. Our method can<br>also be generalized to related tasks such as finding impor-<br>tant words and noun-phrases in a document.</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:489"><nobr><span class="ft4">In the future, we will integrate focused named entity recog-</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:475"><nobr><span class="ft9">nition into real applications, such as information retrieval,<br>automatic summarization, and topic detection and tracking,<br>so that we can further study and evaluate its influences to<br>these systems.</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:475"><nobr><span class="ft3"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:873;left:506"><nobr><span class="ft3"><b>ACKNOWLEDGMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:895;left:489"><nobr><span class="ft4">We thank Honglei Guo for providing the original corpus</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:475"><nobr><span class="ft9">with named entity annotation. Jianmin Jiang helped us to<br>set up named entity annotation server, which made it much<br>easier to view and annotate the corpus. We thank Zhaoming<br>Qiu, Shixia Liu, Zhili Guo for their valuable comments on<br>the experiments. We are also grateful to our colleagues who<br>spent their time to participate in the focused named entity<br>survey.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft3"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:506"><nobr><span class="ft3"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:482"><nobr><span class="ft4">[1] R. Barzilay and M. Elhadad. Using lexical chains for</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:503"><nobr><span class="ft4">text summarization. In Proceedings of the ACL</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">287</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="94008.png" alt="background image">
<DIV style="position:absolute;top:96;left:221"><nobr><span class="ft4">Table 7: Performance of different features with different algorithms</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:174"><nobr><span class="ft4">ID</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:228"><nobr><span class="ft4">Features</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:367"><nobr><span class="ft4">RRM</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:489"><nobr><span class="ft4">Decision Tree</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:636"><nobr><span class="ft4">Naive Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:328"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:379"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:427"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:436"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:478"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:525"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:570"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:579"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:620"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:668"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:713"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:722"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:176"><nobr><span class="ft4">A</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:232"><nobr><span class="ft4">2+3+7</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:317"><nobr><span class="ft4">79.11</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:368"><nobr><span class="ft4">20.86</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:419"><nobr><span class="ft4">32.96</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:467"><nobr><span class="ft4">77.48</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:514"><nobr><span class="ft4">61.81</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:562"><nobr><span class="ft4">68.70</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:609"><nobr><span class="ft4">96.39</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:657"><nobr><span class="ft4">33.47</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:705"><nobr><span class="ft4">49.67</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:177"><nobr><span class="ft4">B</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:232"><nobr><span class="ft4">1+2+3</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:317"><nobr><span class="ft4">71.95</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:368"><nobr><span class="ft4">82.08</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:419"><nobr><span class="ft4">76.60</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:467"><nobr><span class="ft4">71.06</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:514"><nobr><span class="ft4">72.31</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:562"><nobr><span class="ft4">71.23</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:609"><nobr><span class="ft4">93.29</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:657"><nobr><span class="ft4">42.91</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:705"><nobr><span class="ft4">58.76</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:177"><nobr><span class="ft4">C</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:223"><nobr><span class="ft4">1+2+3+7</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:317"><nobr><span class="ft4">73.32</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:365"><nobr><span class="ft4">0.8143</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:419"><nobr><span class="ft4">76.87</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:467"><nobr><span class="ft4">70.90</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:514"><nobr><span class="ft4">78.63</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:562"><nobr><span class="ft4">74.54</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:609"><nobr><span class="ft4">92.58</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:657"><nobr><span class="ft4">48.65</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:705"><nobr><span class="ft4">63.74</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:176"><nobr><span class="ft4">D</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:214"><nobr><span class="ft4">1+2+3+7+5</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:317"><nobr><span class="ft4">70.60</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:368"><nobr><span class="ft4">84.99</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:419"><nobr><span class="ft4">76.98</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:467"><nobr><span class="ft4">74.42</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:514"><nobr><span class="ft4">75.85</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:562"><nobr><span class="ft4">75.09</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:609"><nobr><span class="ft4">85.44</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:657"><nobr><span class="ft4">61.96</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:705"><nobr><span class="ft4">71.71</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:177"><nobr><span class="ft4">E</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:205"><nobr><span class="ft4">1+2+3+7+5+8</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:317"><nobr><span class="ft4">86.15</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:368"><nobr><span class="ft4">75.89</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:419"><nobr><span class="ft4">80.68</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:467"><nobr><span class="ft4">74.42</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:514"><nobr><span class="ft4">75.85</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:562"><nobr><span class="ft4">75.09</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:609"><nobr><span class="ft4">66.56</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:657"><nobr><span class="ft4">86.14</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:705"><nobr><span class="ft4">75.07</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:177"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:208"><nobr><span class="ft4">1+2+</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:244"><nobr><span class="ft4">· · · +7+8</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:317"><nobr><span class="ft4">85.98</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:368"><nobr><span class="ft4">77.37</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:419"><nobr><span class="ft4">81.44</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:467"><nobr><span class="ft4">79.62</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:514"><nobr><span class="ft4">78.30</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:562"><nobr><span class="ft4">78.92</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:609"><nobr><span class="ft4">66.40</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:657"><nobr><span class="ft4">89.44</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:705"><nobr><span class="ft4">76.19</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:176"><nobr><span class="ft4">G</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:208"><nobr><span class="ft4">1+2+</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:244"><nobr><span class="ft4">· · · +8+9</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:317"><nobr><span class="ft4">84.70</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:368"><nobr><span class="ft4">78.23</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:419"><nobr><span class="ft4">81.32</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:467"><nobr><span class="ft4">83.83</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:514"><nobr><span class="ft4">74.61</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:562"><nobr><span class="ft4">78.89</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:609"><nobr><span class="ft4">69.14</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:657"><nobr><span class="ft4">89.08</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:705"><nobr><span class="ft4">77.82</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:140"><nobr><span class="ft4">Table 8: Machine learning test result with exact string-matching for coreference resolution</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:209"><nobr><span class="ft4">data set</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:335"><nobr><span class="ft4">RRM</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:453"><nobr><span class="ft4">Decision Tree</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:600"><nobr><span class="ft4">Naive Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:299"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:347"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:392"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:401"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:442"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:489"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:534"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:543"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:585"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:632"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:677"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:686"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:209"><nobr><span class="ft4">726 docs</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:288"><nobr><span class="ft4">84.43</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:336"><nobr><span class="ft4">75.21</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:383"><nobr><span class="ft4">79.49</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:431"><nobr><span class="ft4">83.13</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:479"><nobr><span class="ft4">73.68</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:526"><nobr><span class="ft4">78.10</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:574"><nobr><span class="ft4">67.85</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:622"><nobr><span class="ft4">85.64</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:669"><nobr><span class="ft4">75.64</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:209"><nobr><span class="ft4">1,325 docs</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:288"><nobr><span class="ft4">81.67</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:336"><nobr><span class="ft4">72.60</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:383"><nobr><span class="ft4">76.74</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:431"><nobr><span class="ft4">79.60</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:479"><nobr><span class="ft4">70.45</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:526"><nobr><span class="ft4">74.69</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:574"><nobr><span class="ft4">66.77</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:622"><nobr><span class="ft4">83.56</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:669"><nobr><span class="ft4">74.20</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:109"><nobr><span class="ft9">Intelligent Scalable Text Summarization Workshop<br>(ISTS'97), pages 10­17, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:88"><nobr><span class="ft4">[2] F. J. Damerau, T. Zhang, S. M. Weiss, and</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:109"><nobr><span class="ft9">N. Indurkhya. Text categorization for a comprehensive<br>time-dependent benchmark. Information Processing &amp;<br>Management, 40(2):209­221, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:88"><nobr><span class="ft4">[3] H. P. Edmundson. New methods in automatic</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:109"><nobr><span class="ft9">abstracting. Journal of The Association for<br>Computing Machinery, 16(2):264­285, 1969.</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:88"><nobr><span class="ft4">[4] J. Y. Ge, X. J. Huang, and L. Wu. Approaches to</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:109"><nobr><span class="ft9">event-focused summarization based on named entities<br>and query words. In DUC 2003 Workshop on Text<br>Summarization, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:88"><nobr><span class="ft4">[5] E. Hovy and C.-Y. Lin. Automated text</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:109"><nobr><span class="ft9">summarization in summarist. In I. Mani and<br>M. Maybury, editors, Advances in Automated Text<br>Summarization, pages 81­94. MIT Press, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:88"><nobr><span class="ft4">[6] D. E. Johnson, F. J. Oles, T. Zhang, and T. Goetz. A</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:109"><nobr><span class="ft9">decision-tree-based symbolic rule induction system for<br>text categorization. IBM Systems Journal, 41:428­437,<br>2002.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:88"><nobr><span class="ft4">[7] M.-Y. Kan and K. R. McKeown. Information</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:109"><nobr><span class="ft9">extraction and summarization: domain independence<br>through focus types. Columbia University Computer<br>Science Technical Report CUCS-030-99.</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:88"><nobr><span class="ft4">[8] J. M. Kupiec, J. Pedersen, and F. Chen. A trainable</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:109"><nobr><span class="ft9">document summarizer. In SIGIR '95, pages 68­73,<br>1995.</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:88"><nobr><span class="ft4">[9] D. Lawrie, W. B. Croft, and A. Rosenberg. Finding</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:109"><nobr><span class="ft9">topic words for hierarchical summarization. In SIGIR<br>'01, pages 349­357, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:81"><nobr><span class="ft4">[10] F. Li and Y. Yang. A loss function analysis for</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:109"><nobr><span class="ft9">classification methods in text categorization. In ICML<br>03, pages 472­479, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:81"><nobr><span class="ft4">[11] C.-Y. Lin. Training a selection function for extraction.</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:109"><nobr><span class="ft4">In CIKM '99, pages 1­8, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:81"><nobr><span class="ft4">[12] C.-Y. Lin and E. Hovy. Identifying topics by position.</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:109"><nobr><span class="ft9">In Proceedings of the Applied Natural Language<br>Processing Conference (ANLP-97), pages 283­290,<br>1997.</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:81"><nobr><span class="ft4">[13] D. Marcu. From discourse structures to text</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:109"><nobr><span class="ft4">summaries. In Proceedings of the ACL'97/EACL'97</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:503"><nobr><span class="ft9">Workshop on Intelligent Scalable Text Summarization,<br>pages 82­88. ACL, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:475"><nobr><span class="ft4">[14] A. McCallum and K. Nigam. A comparison of event</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:503"><nobr><span class="ft9">models for naive bayes text classification. In<br>AAAI/ICML-98 Workshop on Learning for Text<br>Categorization, pages 41­48, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:475"><nobr><span class="ft4">[15] J. L. Neto, A. Santos, C. Kaestner, A. Freitas, and</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:503"><nobr><span class="ft9">J. Nievola. A trainable algorithm for summarizing<br>news stories. In Proceedings of PKDD'2000 Workshop<br>on Machine Learning and Textual Information Access,<br>September 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:475"><nobr><span class="ft4">[16] C. Nobata, S. Sekine, H. Isahara, and R. Grishman.</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:503"><nobr><span class="ft9">Summarization system integrated with named entity<br>tagging and ie pattern discovery. In Proceedings of<br>Third International Conference on Language<br>Resources and Evaluation (LREC 2002), 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:475"><nobr><span class="ft4">[17] C. D. Paice and P. A. Jones. The identification of</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:503"><nobr><span class="ft9">important concepts in highly structured technical<br>papers. In SIGIR '93, pages 69­78. ACM, 1993.</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:475"><nobr><span class="ft4">[18] J. R. Quinlan. C4.5: Programs for Machine Learning.</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:503"><nobr><span class="ft4">Morgan Kaufmann, 1993.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:475"><nobr><span class="ft4">[19] E. F. T. K. Sang and F. D. Meulder. Introduction to</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:503"><nobr><span class="ft9">the conll-2003 shared task: Language-independent<br>named entity recognition. In Proceedings of<br>CoNLL-2003, pages 142­147, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:475"><nobr><span class="ft4">[20] W.-M. Soon, H.-T. Ng, and C.-Y. Lim. A machine</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:503"><nobr><span class="ft9">learning approach to coreference resolution of noun<br>phrases. Computational Linguistics, 27(4):521­544,<br>2001.</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:475"><nobr><span class="ft4">[21] S. Teufel and M. Moens. Sentence extraction as a</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:503"><nobr><span class="ft9">classification task. In ACL/EACL-97 Workshop on<br>Intelligent and Scalable Text Summarization, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:475"><nobr><span class="ft4">[22] T. Zhang. On the dual formulation of regularized</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:503"><nobr><span class="ft4">linear systems. Machine Learning, 46:91­129, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:475"><nobr><span class="ft4">[23] T. Zhang, F. Damerau, and D. E. Johnson. Text</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:503"><nobr><span class="ft9">chunking based on a generalization of Winnow.<br>Journal of Machine Learning Research, 2:615­637,<br>2002.</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:475"><nobr><span class="ft4">[24] T. Zhang and F. J. Oles. Text categorization based on</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:503"><nobr><span class="ft9">regularized linear classification methods. Information<br>Retrieval, 4:5­31, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft8">288</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
