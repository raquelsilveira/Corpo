<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>sp8038-trnka.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-01-18T15:54:25+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:15px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Courier;color:#000000;}
	.ft8{font-size:11px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:14px;font-family:Times;color:#000000;}
	.ft11{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft12{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="194001.png" alt="background image">
<DIV style="position:absolute;top:108;left:139"><nobr><span class="ft0"><b>Topic Modeling in Fringe Word Prediction for AAC</b></span></nobr></DIV>
<DIV style="position:absolute;top:177;left:169"><nobr><span class="ft1">Keith Trnka, Debra Yarrington,</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:222"><nobr><span class="ft1">Kathleen McCoy</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:185"><nobr><span class="ft2">Computer Science Department</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:213"><nobr><span class="ft2">University of Delaware</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:225"><nobr><span class="ft2">Newark, DE 19716</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:143"><nobr><span class="ft3">{</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:150"><nobr><span class="ft1">trnka,yarringt,mccoy</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:311"><nobr><span class="ft3">}</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:318"><nobr><span class="ft1">@cis.udel.edu</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:533"><nobr><span class="ft1">Christopher Pennington</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:579"><nobr><span class="ft2">AgoraNet, Inc.</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:548"><nobr><span class="ft2">314 E. Main St., Suite 1</span></nobr></DIV>
<DIV style="position:absolute;top:226;left:564"><nobr><span class="ft2">Newark, DE 19711</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:526"><nobr><span class="ft1">penningt@agora-net.com</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:81"><nobr><span class="ft4"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:336;left:81"><nobr><span class="ft9">Word prediction can be used for enhancing the communi-<br>cation ability of persons with speech and language impair-<br>ments. In this work, we explore two methods of adapting<br>a language model to the topic of conversation, and apply<br>these methods to the prediction of fringe words.</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:81"><nobr><span class="ft4">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:81"><nobr><span class="ft10">I.7.m Document and Text Processing: Miscellaneous</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:81"><nobr><span class="ft4">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:194"><nobr><span class="ft3">Algorithms</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:81"><nobr><span class="ft3">Keywords:</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:159"><nobr><span class="ft3">Word prediction, topic modeling, language mod-</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:81"><nobr><span class="ft3">eling, AAC</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:527;left:112"><nobr><span class="ft4"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:549;left:94"><nobr><span class="ft3">Alternative and Augmentative Communication (AAC) is</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:81"><nobr><span class="ft9">the field of research concerned with finding ways to help<br>those with speech difficulties communicate more easily and<br>completely. Today there are approximately 2 million peo-<br>ple in the United States with some form of communication<br>difficulty. One means to help ease communication is the<br>use of an electronic communication device, which may have<br>synthetic speech as output. However, one issue in using an<br>AAC device is communication rate. Whereas speaking rate<br>is estimated at 180 words per minute (wpm), many AAC<br>users' communication rates are lower than 15 wpm [3, 7,<br>16]. Thus one goal of developers is to find ways to increase<br>the rate of communication, by making AAC devices easier<br>to use and more intelligent.</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:94"><nobr><span class="ft3">Some researchers have attempted to speed communica-</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:81"><nobr><span class="ft9">tion rate by providing quick access to the core vocabulary<br>­ the relatively small set of frequently used words. Meth-<br>ods for doing this include abbreviation expansion and iconic<br>methods such as semantic compaction [1]. In contrast, in<br>this work we attempt to speed access to the much larger<br>set of words often called fringe vocabulary. This set is of<br>interest because although each individual word occurs less<br>frequently, the set of fringe words on the whole is very sig-<br>nificant.</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:94"><nobr><span class="ft3">Suppose that the user wants to enter "I want a home</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:81"><nobr><span class="ft11">in the country." After typing, "I want a h", they might<br>see something like shown below. The system has created a<br>prediction window</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:197"><nobr><span class="ft3">containing the five words that it thinks</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:81"><nobr><span class="ft9">the user may be trying to type. In this example, the user can<br>press F5 to complete the word "home" and the system will</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:81"><nobr><span class="ft12">Copyright is held by the author/owner.<br><i>IUI'06, </i>January 29­February 1, 2006, Sydney, Australia.<br>ACM 1-59593-287-9/06/0001.</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:475"><nobr><span class="ft9">enter the word with a space afterwards. So in this example,<br>the user needed 2 keystrokes to enter what would normally<br>take 5 keystrokes.</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:575"><nobr><span class="ft3">I want a</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:642"><nobr><span class="ft11">h<br>hundred</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:707"><nobr><span class="ft3">(F1)</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:642"><nobr><span class="ft3">half</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:707"><nobr><span class="ft3">(F2)</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:642"><nobr><span class="ft3">house</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:707"><nobr><span class="ft3">(F3)</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:642"><nobr><span class="ft3">hard</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:707"><nobr><span class="ft3">(F4)</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:642"><nobr><span class="ft3">home</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:707"><nobr><span class="ft3">(F5)</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:489"><nobr><span class="ft3">It is difficult to judge how much word prediction can speed</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:475"><nobr><span class="ft9">communication rate. Much of this determination is depen-<br>dent on the accuracy of the prediction method, the char-<br>acteristics of the user, such as their physical and cognitive<br>abilities, and the characteristics of the user interface, such<br>as where the prediction list is displayed and how a word in<br>the list is selected. Here, the prediction method is evaluated<br>separately from the rest of a word prediction system by sim-<br>ulating what a user would type in a conversation if he/she<br>were taking full advantage of the prediction list. This the-<br>oretical evaluation measures the percentage of keystrokes<br>that were saved by word prediction over typing out every<br>character.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:489"><nobr><span class="ft3">In this paper we first describe related work and give some</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:475"><nobr><span class="ft9">background in statistical approaches to word prediction. We<br>present approaches to topic modeling and compare the re-<br>sults of topic modeling to a baseline method. For a more<br>thorough account of this work, visit<br>http://www.cis.udel.edu/fringe/.</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:475"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:507"><nobr><span class="ft4"><b>RELATED WORK</b></span></nobr></DIV>
<DIV style="position:absolute;top:799;left:489"><nobr><span class="ft3">Several previous researchers have used n-gram models in</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:475"><nobr><span class="ft9">word prediction for AAC [4, 5, 12, 18]. For example, Lesher<br>et al. [12] show how increasing training set size and unigrams<br>to bigrams (going from 47% to 54.7%) to trigrams (another<br>.8%). These evaluations used a window size of 6.</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:489"><nobr><span class="ft3">Other researchers have integrated grammatical informa-</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:475"><nobr><span class="ft9">tion into n-gram word prediction systems. Garay-Vitoria<br>and Gonzalez-Abascal [10] integrated a statistical chart parser,<br>while Fazly and Hirst [8] and Copestake [7] used part-of-<br>speech (POS) tagging. These yielded improvements of 1-5%<br>keystroke savings.</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:489"><nobr><span class="ft3">There have been several attempts at topic modeling in</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:475"><nobr><span class="ft9">the language modeling community, particularly for speech<br>recognition [2, 14, 17, 6, 9, 13]. Some of the evaluations<br>of topic modeling have found different variants of it to be<br>very beneficial [2, 14, 9]. Lesher and Rinkus [13] is an at-<br>tempt at topic modeling for word prediction, but does not<br>use dynamic topic modeling like [9, 2] and this work.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft8">276</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:8px;font-family:Times;color:#000000;}
	.ft14{font-size:6px;font-family:Times;color:#000000;}
	.ft15{font-size:5px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="194002.png" alt="background image">
<DIV style="position:absolute;top:82;left:107"><nobr><span class="ft13">Window</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:166"><nobr><span class="ft13">Bigrams</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:225"><nobr><span class="ft13">Trigrams</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:287"><nobr><span class="ft13">Method A</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:357"><nobr><span class="ft13">Method B</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:143"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:177"><nobr><span class="ft13">41.5%</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:240"><nobr><span class="ft13">42.3%</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:310"><nobr><span class="ft13">43.1%</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:378"><nobr><span class="ft13">42.5%</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:143"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:177"><nobr><span class="ft13">50.6%</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:240"><nobr><span class="ft13">51.1%</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:310"><nobr><span class="ft13">52.3%</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:378"><nobr><span class="ft13">51.4%</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:143"><nobr><span class="ft13">3</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:177"><nobr><span class="ft13">54.7%</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:240"><nobr><span class="ft13">55.1%</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:310"><nobr><span class="ft13">56.4%</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:378"><nobr><span class="ft13">55.4%</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:143"><nobr><span class="ft13">4</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:177"><nobr><span class="ft13">57.0%</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:240"><nobr><span class="ft13">57.3%</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:310"><nobr><span class="ft13">58.7%</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:378"><nobr><span class="ft13">57.7%</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:143"><nobr><span class="ft13">5</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:177"><nobr><span class="ft13">58.6%</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:240"><nobr><span class="ft13">58.8%</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:310"><nobr><span class="ft13">60.2%</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:378"><nobr><span class="ft13">59.1%</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:143"><nobr><span class="ft13">6</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:177"><nobr><span class="ft13">59.8%</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:240"><nobr><span class="ft13">60.0%</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:310"><nobr><span class="ft13">61.4%</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:378"><nobr><span class="ft13">60.3%</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:143"><nobr><span class="ft13">7</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:177"><nobr><span class="ft13">60.6%</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:240"><nobr><span class="ft13">60.8%</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:310"><nobr><span class="ft13">62.2%</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:378"><nobr><span class="ft13">61.1%</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:143"><nobr><span class="ft13">8</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:177"><nobr><span class="ft13">61.3%</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:240"><nobr><span class="ft13">61.5%</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:310"><nobr><span class="ft13">62.9%</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:378"><nobr><span class="ft13">61.8%</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:143"><nobr><span class="ft13">9</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:177"><nobr><span class="ft13">61.9%</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:240"><nobr><span class="ft13">62.0%</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:310"><nobr><span class="ft13">63.5%</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:378"><nobr><span class="ft13">62.3%</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:138"><nobr><span class="ft13">10</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:177"><nobr><span class="ft13">62.4%</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:240"><nobr><span class="ft13">62.5%</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:310"><nobr><span class="ft13">64.0%</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:378"><nobr><span class="ft13">62.8%</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:81"><nobr><span class="ft9">Table 1: The keystroke savings of topic modeling is<br>shown compared to a bigram and trigram baseline.</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:81"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:295;left:112"><nobr><span class="ft4"><b>METHODS</b></span></nobr></DIV>
<DIV style="position:absolute;top:316;left:94"><nobr><span class="ft3">Like several of the aforementioned word prediction re-</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:81"><nobr><span class="ft9">searchers, we use n-gram methods for language modeling.<br>Our baseline word prediction methods use bigram and trigram-<br>based n-gram models with backoff with Good-Turing smooth-<br>ing, the current best practice in statistical language model-<br>ing according to Manning and Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:295"><nobr><span class="ft3">utze [15]. Additionally,</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:81"><nobr><span class="ft9">we incorporate a special unigram model for the first word<br>of each sentence. In word prediction, these language models<br>our used to rank all the words that the user could possi-<br>bly be typing. The top W words are presented to the user,<br>where W is the prediction window size.</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:94"><nobr><span class="ft3">Statistical approaches require a collection of text to con-</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:81"><nobr><span class="ft9">struct a language model. Ideally, our corpus would be a large<br>collection of conversations involving one or more people us-<br>ing an AAC system. Such a corpus is unavailable, so we<br>follow [13] in using the Switchboard corpus, which is a col-<br>lection of telephone conversations and their transcriptions.</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:433"><nobr><span class="ft14">1</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:81"><nobr><span class="ft9">The training section contains a randomly pre-selected 2217<br>conversations and the testing section contains the remain-<br>ing 221 conversations. We perform preprocessing to remove<br>some speech repairs in accordance with Hindle [11]. These<br>editing rules bring the Switchboard conversations closer to<br>what we envision an AAC user would type.</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:81"><nobr><span class="ft4"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:688;left:121"><nobr><span class="ft4"><b>Evaluation</b></span></nobr></DIV>
<DIV style="position:absolute;top:710;left:94"><nobr><span class="ft3">We compare the number of keystrokes required for a user</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:81"><nobr><span class="ft9">taking full advantage of our word prediction system to the<br>number of keystrokes required to enter each character of the<br>conversation. We use immediate prediction for our evalua-<br>tions, which allows use of the prediction list before the first<br>character of a word has been entered. We assume that one<br>keystroke is required to "speak" each turn of input and that<br>a space is automatically inserted after a word is selected<br>from the prediction list.</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:122"><nobr><span class="ft3">KS</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:148"><nobr><span class="ft3">= keys</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:192"><nobr><span class="ft14">normal</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:233"><nobr><span class="ft3">- keys</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:275"><nobr><span class="ft14">withprediction</span></nobr></DIV>
<DIV style="position:absolute;top:877;left:225"><nobr><span class="ft3">keys</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:253"><nobr><span class="ft14">normal</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:355"><nobr><span class="ft3"> 100%</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:81"><nobr><span class="ft9">Because we are interested in the prediction of fringe words,<br>our evaluations are measured on fringe words only. Core<br>words are excluded from the list of predictions. The par-<br>ticular core vocabulary we chose is available from the AAC<br>Centers at the University of Nebraska at Lincoln, available<br>from http://aac.unl.edu/. We used the "Young Adult Con-<br>versation Regular" core vocabulary list, as it is the most<br>similar to the type of conversations in the Switchboard cor-<br>pus.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft14">1</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:88"><nobr><span class="ft3">The Switchboard transcriptions were available from</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:81"><nobr><span class="ft3">http://www.isip.msstate.edu/projects/switchboard/</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:475"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:84;left:507"><nobr><span class="ft4"><b>TOPIC MODELING</b></span></nobr></DIV>
<DIV style="position:absolute;top:105;left:489"><nobr><span class="ft3">The goal of topic modeling is to identify the current topic</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:475"><nobr><span class="ft9">of conversation, then increase the probability of related words<br>and decrease the probability of unrelated words. Some words<br>will be unaffected by topic modeling, such as function words,<br>which are used similarly in all topics. It is for this reason<br>that we chose to improve fringe word prediction with topic<br>modeling: we feel that topic modeling specifically improves<br>fringe word prediction.</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:489"><nobr><span class="ft3">Researchers are consistent in representing a topic by creat-</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:475"><nobr><span class="ft9">ing a collection of representative text of the topic. However,<br>researchers differ on the best way to organize a collection of<br>topics. Some researchers have created a hierarchical collec-<br>tion of topics [9], while others have created a disjoint set of<br>topics [14, 2, 17]. We feel that the primary lure of a hierar-<br>chical approach, the ability to generalize, can be captured<br>in the set approach as well, by giving varying weight to all<br>topics and not just the most likely topic. For this reason,<br>we represent topics as disjoint sets of conversations.</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:489"><nobr><span class="ft3">The current topic of conversation must be identified from</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:475"><nobr><span class="ft9">the part of the conversation that has taken place so far, and<br>updated periodically in the conversation. Thus, we must<br>devise a representation for a partial conversation for assess-<br>ing the similarity of the conversation to each topic. In rep-<br>resenting the conversation so far, we choose to implement<br>an exponentially decayed cache, like [2], using TF-IDF val-<br>ues rather than raw frequencies. This follows the work of<br>Mahajan et. al. [14] in considering the inverse document<br>frequency of a word to proportional to its utility in identi-<br>fying the current topic. Because our approach is for topic<br>identification, we ignore words that occur in 85% or more<br>of the topics, with the intuition that such words are irrele-<br>vant to selection of topic. As a step to convert our model of<br>the current conversation to a model of the current topic, we<br>compute the document similarity between the cache and the<br>unigram model for each topic. We chose to use the cosine<br>metric, following [9].</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:489"><nobr><span class="ft3">Given that we have computed a similarity score between</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:475"><nobr><span class="ft9">each topic and the current conversation, there are two main<br>variations on how to construct a new language model. Ma-<br>hajan et. al. [14] implemented a k-nearest solution, con-<br>structing the topic model from the most similar k topics.<br>Each topic's language model was weighted equally for their<br>experiments. Instead, we chose to follow Florian and Yarowsky's<br>approach [9]. They expand the probability for a word (w)<br>given a history (h) as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:537"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:548"><nobr><span class="ft3">(w | h) =</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:618"><nobr><span class="ft3">X</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:607"><nobr><span class="ft14">ttopics</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:651"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:661"><nobr><span class="ft3">(t | h)  P (w | t, h)</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:475"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:486"><nobr><span class="ft3">(w | t, h) is simply the probability of w taken from the</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:475"><nobr><span class="ft9">language model constructed for topic t. The probability of<br>the topic is estimated as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:567"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:578"><nobr><span class="ft3">(t | h) </span></nobr></DIV>
<DIV style="position:absolute;top:924;left:667"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:676"><nobr><span class="ft3">(t, h)</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:634"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:648"><nobr><span class="ft14">t</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:653"><nobr><span class="ft15"></span></nobr></DIV>
<DIV style="position:absolute;top:950;left:656"><nobr><span class="ft14"></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:664"><nobr><span class="ft14">topics</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:697"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:706"><nobr><span class="ft3">(t</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:716"><nobr><span class="ft14"></span></nobr></DIV>
<DIV style="position:absolute;top:942;left:720"><nobr><span class="ft3">, h</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:734"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:475"><nobr><span class="ft9">where S(t, h) is the cosine similarity of the topic to the cur-<br>rent part of the conversation.</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:475"><nobr><span class="ft4"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:516"><nobr><span class="ft4"><b>Method A</b></span></nobr></DIV>
<DIV style="position:absolute;top:1034;left:489"><nobr><span class="ft3">Our first method of topic modeling is most similar in</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:475"><nobr><span class="ft9">spirit to the work of Mahajan et. al. [14] and Florian and<br>Yarowsky [9]. In training, a bigram model is computed for</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft8">277</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:8px;line-height:13px;font-family:Times;color:#000000;}
	.ft17{font-size:8px;line-height:12px;font-family:Times;color:#000000;}
	.ft18{font-size:8px;line-height:11px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="194003.png" alt="background image">
<DIV style="position:absolute;top:85;left:81"><nobr><span class="ft9">each topic in Switchboard. In testing, the cache represen-<br>tation of the current conversation is compared against the<br>unigram representation of each topic and similarity scores<br>are computed. The similarity scores are then used to weight<br>the frequencies obtained from each topic in a linear inter-<br>polation. Then this interpolated bigram model is used to<br>compute the probabilities used for word prediction.</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:94"><nobr><span class="ft3">Topic modeling shows a sizable improvement over the the</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:81"><nobr><span class="ft9">bigram baseline: 1.6% ­ 1.7%. We've included the com-<br>parison to a bigram baseline because it is the most natural<br>baseline in terms of language understanding. However, a tri-<br>gram baseline is also a natural comparison when considering<br>that it can run with the same or less computational resources<br>than topic modeling. When compared against the trigram<br>baseline, the topic model gives 0.8% ­ 1.5% improvement.</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:81"><nobr><span class="ft4"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:335;left:121"><nobr><span class="ft4"><b>Method B</b></span></nobr></DIV>
<DIV style="position:absolute;top:356;left:94"><nobr><span class="ft3">Our second method of topic modeling is more similar</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:81"><nobr><span class="ft9">to the work of Bellegarda [2]. Like Bellegarda, we com-<br>pute topic-dependent unigram probabilities. These topic-<br>dependent probabilities are multiplied with probabilities from<br>a trigram backoff model. Additionally, we weight the topic<br>component with a tuning parameter. After manual tuning<br>on a two conversations, we found that  = .15 worked well.</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:94"><nobr><span class="ft3">Method B is an improvement over a trigram baseline, but</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:81"><nobr><span class="ft9">only a minor improvement. We feel that the problem is that<br>a low  value was necessary to avoid overriding the word<br>preference that is due to context, but that it also reduced<br>the ability of the overall model to adapt to a particular topic.</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:81"><nobr><span class="ft4"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:558;left:121"><nobr><span class="ft4"><b>Comparison</b></span></nobr></DIV>
<DIV style="position:absolute;top:579;left:94"><nobr><span class="ft3">Method A offers an additional 1% or more keystroke sav-</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:81"><nobr><span class="ft9">ings over Method B for most window sizes. This is due to<br>the low weight of the tuning parameter for Method B. How-<br>ever, as previously mentioned, the low weight was necessary.<br>Additionally, notice that Method A becomes comparatively<br>better as the window size is increased. The trigram model<br>component in Method B can be thought of as a stronger<br>source of knowledge than the interpolated bigram model of<br>Method A. Because of this, when the trigram history exists<br>in the language model, Method B's predictions are more ac-<br>curate. However, because the trigram model is sparse, it<br>can only contribute to the top few predictions. Thus, it has<br>a much greater effect on the top few window sizes.</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:94"><nobr><span class="ft3">For real world systems, however, absolute performance is</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:81"><nobr><span class="ft9">not the only factor. The computational demands of each<br>approach are often considered when selecting a practical so-<br>lution. The trigram baseline processed at 1,325 words per<br>minute (wpm). Method A processed conversations in test-<br>ing at 32 wpm and Method B processed 1,267 words per<br>minute. Method B uses barely more processing time than<br>the trigram baseline model.</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:81"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:935;left:112"><nobr><span class="ft4"><b>CONCLUSIONS</b></span></nobr></DIV>
<DIV style="position:absolute;top:956;left:94"><nobr><span class="ft3">Topic modeling can be implemented in many different</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:81"><nobr><span class="ft9">ways. We've demonstrated two such methods for topic mod-<br>eling: one for computationally limited devices and another<br>for computationally rich devices. Both methods show a clear<br>improvement over a trigram model with backoff. Before the<br>advent of word prediction, a user would've pressed 6.4 keys<br>per fringe word on average. Now, with topic modeling for<br>word prediction, only 2.5 keys per word are required.</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:475"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:84;left:507"><nobr><span class="ft4"><b>ACKNOWLEDGMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:105;left:489"><nobr><span class="ft3">We would like to thank the US Department of Education</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:475"><nobr><span class="ft9">for funding this research under grant H113G040051, under<br>the National Institute on Disability and Rehabilitation Re-<br>search program. We would also like to thank Dr. Gregory<br>Lesher for correspondence regarding his work and Dr. David<br>Saunders for lending us a compute server.</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:475"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:217;left:507"><nobr><span class="ft4"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:234;left:481"><nobr><span class="ft16">[1] B. Baker. Minspeak. Byte, pages 186­202, 1982.<br>[2] J. Bellegarda. Large vocabulary speech recognition with</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:501"><nobr><span class="ft17">multispan language models. IEEE Trans. On Speech and<br>Audio Processing</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:593"><nobr><span class="ft13">, 8(1), 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:481"><nobr><span class="ft13">[3] D. R. Beukelman and P. Mirenda. Augmentative and</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:501"><nobr><span class="ft17">alternative communication: Management of severe<br>communication disorders in children and adults</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:757"><nobr><span class="ft13">. P.H. Brookes</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:501"><nobr><span class="ft13">Pub. Co., 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:481"><nobr><span class="ft13">[4] L. Boggess. Two simple prediction algorithms to facilitate text</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:501"><nobr><span class="ft17">production. In Proceedings of the second conference on<br>Applied natural language processing</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:693"><nobr><span class="ft13">, pages 33­40, Morristown,</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:501"><nobr><span class="ft13">NJ, USA, 1988. Association for Computational Linguistics.</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:481"><nobr><span class="ft13">[5] A. Carlberger, J. Carlberger, T. Magnuson, M. S. Hunnicutt,</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:501"><nobr><span class="ft18">S. Palazuelos-Cagigas, and S. A. Navarro. Profet, a new<br>generation of word prediction: An evaluation study. In<br>Proceedings of Natural Language Processing for<br>Communication Aids</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:614"><nobr><span class="ft13">, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:481"><nobr><span class="ft13">[6] S. Chen, K. Seymore, and R. Rosenfeld. Topic adaptation for</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:501"><nobr><span class="ft17">language modeling using unnormalized exponential models. In<br>Proc. Int'l Conf. on Acoustics, Speech and Signal Processing</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:828"><nobr><span class="ft13">,</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:501"><nobr><span class="ft13">1998.</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:481"><nobr><span class="ft13">[7] A. Copestake. Augmented and alternative nlp techniques for</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:501"><nobr><span class="ft18">augmentative and alternative communication. In Proceedings<br>of the ACL workshop on Natural Language Processing for<br>Communication Aids</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:614"><nobr><span class="ft13">, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:481"><nobr><span class="ft13">[8] A. Fazly and G. Hirst. Testing the efficacy of part-of-speech</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:501"><nobr><span class="ft18">information in word completion. In Proceedings of the 10th<br>Conference of the European Chapter of the Association for<br>Computational Linguistics</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:643"><nobr><span class="ft13">, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:481"><nobr><span class="ft13">[9] R. Florian and D. Yarowsky. Dynamic nonlocal language</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:501"><nobr><span class="ft17">modeling via hierarchical topic-based adaptation. In<br>Proceedings of ACL'99</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:623"><nobr><span class="ft13">, pages 167­174, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:475"><nobr><span class="ft13">[10] N. Garay-Vitoria and J. Gonz´</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:657"><nobr><span class="ft13">alez-Abascal. Intelligent</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:501"><nobr><span class="ft17">word-prediction to enhance text input rate. In Proceedings of<br>the second international conference on Intelligent User<br>Interfaces</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:553"><nobr><span class="ft13">, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:475"><nobr><span class="ft13">[11] D. Hindle. Deterministic parsing of syntactic non-fluencies. In</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:501"><nobr><span class="ft18">Proceedings of the 21st Annual Meeting of the Association for<br>Computational Linguistics</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:643"><nobr><span class="ft13">, 1983.</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:475"><nobr><span class="ft13">[12] G. Lesher, B. Moulton, and J. Higgonbotham. Effects of ngram</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:501"><nobr><span class="ft17">order and training text size on word prediction. In Proceedings<br>of the RESNA '99 Annual Conference</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:706"><nobr><span class="ft13">, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:475"><nobr><span class="ft13">[13] G. Lesher and G. Rinkus. Domain-specific word prediction for</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:501"><nobr><span class="ft17">augmentative communication. In Proceedings of the RESNA<br>'01 Annual Conference</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:625"><nobr><span class="ft13">, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:791;left:475"><nobr><span class="ft13">[14] M. Mahajan, D. Beeferman, and X. D. Huang. Improved</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:501"><nobr><span class="ft17">topic-dependent language modeling using information retrieval<br>techniques. In Proceedings of the International Conference on<br>Acoustics, Speech, and Signal Processing</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:720"><nobr><span class="ft13">, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:475"><nobr><span class="ft13">[15] C. Manning and H. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:624"><nobr><span class="ft13">utze. Foundations of Statistical</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:501"><nobr><span class="ft13">Natural Language Processing</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:656"><nobr><span class="ft13">. MIT Press, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:475"><nobr><span class="ft13">[16] A. Newell, S. Langer, and M. Hickey. The r^</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:728"><nobr><span class="ft13">ole of natural</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:501"><nobr><span class="ft17">language processing in alternative and augmentative<br>communication. Natural Language Engineering, 4(1):1­16,<br>1996.</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:475"><nobr><span class="ft13">[17] K. Seymore and R. Rosenfeld. Using story topics for language</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:501"><nobr><span class="ft18">model adaptation. In Proceedings of Eurospeech '97, pages<br>1987­1990, Rhodes, Greece, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:475"><nobr><span class="ft13">[18] A. L. Swiffin, J. A. Pickering, J. L. Arnott, and A. F. Newell.</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:501"><nobr><span class="ft17">Pal: An effort efficient portable communication aid and<br>keyboard emulator. In Proceedings of the 8th Annual<br>Conference on Rehabilitation Techonology</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:727"><nobr><span class="ft13">, pages 197­199,</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:501"><nobr><span class="ft13">1985.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft8">278</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
