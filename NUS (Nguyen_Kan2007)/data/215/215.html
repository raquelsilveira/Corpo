<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - f112-Zhuang1.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="Rick">
<META name="date" content="2005-05-31T19:27:28+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Times;color:#000000;}
	.ft1{font-size:16px;font-family:Times;color:#000000;}
	.ft2{font-size:13px;font-family:Times;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:9px;font-family:Times;color:#000000;}
	.ft5{font-size:16px;font-family:Courier;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;line-height:25px;font-family:Times;color:#000000;}
	.ft8{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft9{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215001.png" alt="background image">
<DIV style="position:absolute;top:109;left:144"><nobr><span class="ft0"><b>What's There and What's Not? Focused Crawling  </b></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:188"><nobr><span class="ft0"><b>for Missing Documents in Digital Libraries </b></span></nobr></DIV>
<DIV style="position:absolute;top:175;left:148"><nobr><span class="ft1">Ziming Zhuang </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:92"><nobr><span class="ft2">School of Information Sciences and </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:169"><nobr><span class="ft2">Technology </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:96"><nobr><span class="ft2">The Pennsylvania State University </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:102"><nobr><span class="ft2">University Park, PA 16802, USA  </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:122"><nobr><span class="ft1">zzhuang@ist.psu.edu</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:411"><nobr><span class="ft1">Rohit Wagle </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:341"><nobr><span class="ft2">School of Information Sciences and </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:420"><nobr><span class="ft2">Technology </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:345"><nobr><span class="ft2">The Pennsylvania State University </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:352"><nobr><span class="ft2">University Park, PA 16802, USA </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:391"><nobr><span class="ft1">rohitsw@psu.edu </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:663"><nobr><span class="ft1">C. Lee Giles </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:593"><nobr><span class="ft2">School of Information Sciences and </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:672"><nobr><span class="ft2">Technology </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:597"><nobr><span class="ft2">The Pennsylvania State University </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:604"><nobr><span class="ft2">University Park, PA 16802, USA </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:640"><nobr><span class="ft1">giles@ist.psu.edu </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:711"><nobr><span class="ft7"> <br> </span></nobr></DIV>
<DIV style="position:absolute;top:343;left:81"><nobr><span class="ft1">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:179"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:81"><nobr><span class="ft8">Some large scale topical digital libraries, such as CiteSeer, harvest <br>online academic documents by crawling open-access archives, <br>university and author homepages, and authors' self-submissions. <br>While these approaches have so far built reasonable size libraries, <br>they can suffer from having only a portion of the documents from <br>specific publishing venues. We propose to use alternative online <br>resources and techniques that maximally exploit other resources to <br>build the complete document collection of any given publication <br>venue.  </span></nobr></DIV>
<DIV style="position:absolute;top:515;left:81"><nobr><span class="ft8">We investigate the feasibility of using publication metadata to <br>guide the crawler towards authors' homepages to harvest what is <br>missing from a digital library collection. We collect a real-world <br>dataset from two Computer Science publishing venues, involving <br>a total of 593 unique authors over a time frame of 1998 to 2004. <br>We then identify the missing papers that are not indexed by <br>CiteSeer. Using a fully automatic heuristic-based system that has <br>the capability of locating authors' homepages and then using <br>focused crawling to download the desired papers, we demonstrate <br>that it is practical to harvest using a focused crawler academic <br>papers that are missing from our digital library. Our harvester <br>achieves a performance with an average recall level of 0.82 <br>overall and 0.75 for those missing documents. Evaluation of the <br>crawler's performance based on the harvest rate shows definite <br>advantages over other crawling approaches and consistently <br>outperforms a defined baseline crawler on a number of measures. </span></nobr></DIV>
<DIV style="position:absolute;top:782;left:81"><nobr><span class="ft1">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:353"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:803;left:81"><nobr><span class="ft8">H.3.7 [Information Systems]: Information Storage and Retrieval <br>­ Digital Libraries </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:81"><nobr><span class="ft1">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:198"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:869;left:81"><nobr><span class="ft3">Algorithms, Performance, Design, Experimentation. </span></nobr></DIV>
<DIV style="position:absolute;top:899;left:81"><nobr><span class="ft1">Keywords: </span></nobr></DIV>
<DIV style="position:absolute;top:903;left:178"><nobr><span class="ft3">Digital libraries, focused crawler, CiteSeer, </span></nobr></DIV>
<DIV style="position:absolute;top:920;left:81"><nobr><span class="ft3">DBLP, ACM, harvesting. </span></nobr></DIV>
<DIV style="position:absolute;top:343;left:477"><nobr><span class="ft1">1.  INTRODUCTION </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:477"><nobr><span class="ft8">Digital libraries that are based on active crawling methods such as <br>CiteSeer often have missing documents in collections of archived <br>publications, such as ACM and IEEE. How do such digital <br>libraries find and obtain those missing? We propose using <br>external resources of publication metadata and focused crawlers <br>to search the Web for those missing. </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:477"><nobr><span class="ft8">The basic concept of a focused crawler (also known as a topical <br>crawlers) [1], is based on a crawling strategy that relevant Web <br>pages contain more relevant links, and these relevant links should <br>be explored first. Initially, the measure of relevancy was based on <br>keywords matching; connectivity-based metrics were later <br>introduced [2]. In [3] the concept of a focused crawler was <br>formally introduced: a crawler that seeks, acquires, indexes, and <br>maintains pages on a specific set of topics that represent a <br>relatively narrow segment of the Web. </span></nobr></DIV>
<DIV style="position:absolute;top:619;left:477"><nobr><span class="ft8">Today, focused crawling techniques have become more important <br>for building specialty and niche (vertical) search engines While <br>both the sheer volume of the Web and its highly dynamic content <br>increasingly challenge the task of document collection, digital <br>libraries based on crawling benefit from focused crawlers since <br>they can quickly harvest a high-quality subset of the relevant <br>online documents. </span></nobr></DIV>
<DIV style="position:absolute;top:739;left:477"><nobr><span class="ft8">Current approaches to harvesting online academic documents <br>normally consist of focused crawling of open-access archives, <br>author and institution web sites and directories of authors' self-<br>submissions. A random sample of 150 journals and conferences in <br>Computer Science show that less than 10% have websites that are <br>open to crawlers. Many of the top publishing venues that have <br>their documents electronically available to subscribers such as the <br>ACM Digital Library, the IEEE Digital, Library or the Springer-<br>Verlag Digital Library, normally use access permission <br>techniques and robots.txt to ban crawlers. A recent study indicates <br>that CiteSeer indexes 425, 000 unique research documents related <br>to Computer Science, DBLP contains 500,464 records and there <br>are 141,345 records in the Association for Computing Machinery <br>(ACM) Digital Library and 825,826 records in the more <br>comprehensive ACM Guide [4]. The study also shows that in <br>CiteSeer there is an overlapping portion of 86, 467 documents <br>(20.2% of CiteSeer's total archive) comprising 17.3% of the <br>Digital Bibliography &amp; Library Project (DBLP) archive. </span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:477"><nobr><span class="ft8">This research investigates alternative online resources and <br>focused crawling techniques to build a complete document </span></nobr></DIV>
<DIV style="position:absolute;top:946;left:87"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:960;left:87"><nobr><span class="ft8">Permission to make digital or hard copies of all or part of this <br>work for personal or classroom use is granted without fee <br>provided that copies are not made or distributed for profit or <br>commercial advantage and that copies bear this notice and the full <br>citation on the first page. To copy otherwise, or republish, to post <br>on servers or to redistribute to lists, requires prior specific <br>permission and/or a fee. </span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:87"><nobr><span class="ft9">JCDL'05, June 7­11, 2005, Denver, Colorado, USA <br>Copyright 2005 ACM 1-58113-876-8/05/0006...$5.00. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">301</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft10{font-size:16px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215002.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">collection for any given publication venue. We propose to answer <br>the following: </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:81"><nobr><span class="ft8">Q1 - What are the best focused crawling techniques to maximally <br>exploit online resources, in order to harvest the desired papers   <br>effectively and efficiently? </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:81"><nobr><span class="ft8">Q2 ­ Is it effective to use authors' homepages as alternative <br>online resources to find the missing documents? </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:81"><nobr><span class="ft8">Q3 ­ How can the above methods be automated to effectively <br>obtain missing documents? </span></nobr></DIV>
<DIV style="position:absolute;top:287;left:81"><nobr><span class="ft8">The rest of the paper is organized as follows. In section 2 we <br>present a review of related work. In Section 3 we cover in much <br>detail the design rationale of the system. In Section 4 we describe <br>how we collect data and perform the evaluation, and present the <br>results with discussion. Finally, we conclude the paper with future <br>work proposed in Section 5. </span></nobr></DIV>
<DIV style="position:absolute;top:395;left:81"><nobr><span class="ft1">2.  RELATED WORK </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:81"><nobr><span class="ft8">The focused crawling literature shows that much has been focused <br>on enhancing the dynamic performance, scalability, effectiveness, <br>and efficiency of the crawler, namely, harvesting higher-quality <br>documents in a shorter period of time.  </span></nobr></DIV>
<DIV style="position:absolute;top:488;left:81"><nobr><span class="ft8">Breadth-first searching is probably the simplest strategy for <br>crawling, i.e. traversing the Web in a way that a directed graph is <br>traveled using a breadth-first search algorithm. Interestingly, a <br>breadth-first crawler is found to be capable of yielding high-<br>quality documents at an early stage of the crawl [5]. Although <br>more sophisticated crawlers tend to retrieve even higher quality <br>pages than their breadth-first counterparts, they are usually <br>computationally more expensive. In our study, we use a multi-<br>threaded breadth-first crawler as a baseline to compare to our own <br>crawling method. </span></nobr></DIV>
<DIV style="position:absolute;top:655;left:81"><nobr><span class="ft8">Best-first crawling attempts to direct the crawler towards the best <br>(i.e. most relevant in terms of topic relevance) documents. <br>Different heuristics, such as link-based criteria, lexical similarity <br>measures, contextual knowledge, and fine-tuned combinations of <br>such have been explored in a number of studies over the years. In <br>[2], the authors find that PageRank [6] can yield the best <br>performance when ordering seed URLs. However, a more recent <br>study [7] shows that PageRank metrics may just be too general in <br>context without regard to the specific target topic. An updated <br>version of PageRank algorithm which reflects the importance with <br>respect to a particular topic has been proposed [8]. </span></nobr></DIV>
<DIV style="position:absolute;top:838;left:81"><nobr><span class="ft8">In [3], a Bayesian classifier is used to estimate the probability that <br>a page belongs to the target topic, in a way that a node belongs to <br>a certain position in an existing taxonomy hierarchy. In [9], a <br>keyword-based vector space model is used to calculate the <br>similarity of Web pages to the seed URLs, and if the similarity is <br>above a certain threshold, the pages are downloaded and indexed, <br>and their out-going links are followed.  </span></nobr></DIV>
<DIV style="position:absolute;top:958;left:81"><nobr><span class="ft8">A focused crawler [10] based on context graphs is proposed by so <br>that the crawler can extract information about the context within <br>which desired documents are usually found.  A set of classifiers <br>are then trained to classify in-degree Web pages according to an <br>estimation of their relevance to the topic. The relevance <br>estimation then navigates the crawler towards desired documents. </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">Crawlers with a probability model are used for calculating <br>priorities, which combines Web page content-based learning, <br>URL token-based learning, and link-based learning [11]. In a later <br>work, [12] takes into account the users' access behavior and re-<br>tunes the previous model to connect this behavior with the <br>predicate satisfaction probability of the candidate Web pages <br>waiting to be crawled.  </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:477"><nobr><span class="ft8">An interesting "reversed" approach is proposed in [13], which <br>suggests a given scientific document from a digital library be used <br>as an input to the focused crawler. The main title and reference <br>titles of the document are extracted and used to train a classifier to <br>learn topical knowledge. The crawler is then guided by such <br>knowledge to discover other topic-relevant documents on the Web. </span></nobr></DIV>
<DIV style="position:absolute;top:332;left:477"><nobr><span class="ft8">More up-to-date reviews of focused crawling algorithms are <br>presented in [14] and [15]. In [14], five different methods are <br>implemented and evaluated within a unified evaluation <br>framework on small and large datasets. </span></nobr></DIV>
<DIV style="position:absolute;top:405;left:477"><nobr><span class="ft8">Here we discuss two studies that bear similarities to ours. The <br>HPSearch and Mops presented in [16] support the search for <br>research papers close to the homepages of certain scientists. <br>However, their system does not investigate the issues of document <br>harvesting for digital libraries for different publishing venues. <br>Furthermore, our system outperforms theirs in terms of the <br>percentage of correct homepages returned. In a more recent study <br>[17], a Paper Search Engine (PaSE) is proposed, which uses <br>citation information to locate online copies of scientific <br>documents. While their study addresses a different research <br>question, the PaSE system employs similar heuristics as we do to <br>favor certain out-going links in order to quickly locate academic <br>papers.  </span></nobr></DIV>
<DIV style="position:absolute;top:629;left:477"><nobr><span class="ft10">3.  SYSTEM DESIGN <br>3.1  System Overview </span></nobr></DIV>
<DIV style="position:absolute;top:671;left:477"><nobr><span class="ft8">We develop an automated system in which document metadata is <br>used to automatically locate the homepages of the authors and <br>focused crawl these homepages with the intent of finding missing <br>documents. Our system, shown in Figure 1, consists of a <br>Homepage Aggregator and a smart Focused Crawler. </span></nobr></DIV>
<DIV style="position:absolute;top:759;left:477"><nobr><span class="ft8">The system accepts a user's request to harvest the desired papers <br>published in a specific venue (e.g. a conference or a journal). The <br>Homepage Aggregator will query a Public Metadata Repository <br>and extract useful metadata heuristics to assist in quickly and <br>accurately locating URLs of the authors' homepages. A list of <br>such URLs will be inserted into the Homepage URL Database. <br>The Crawler uses focused crawling techniques to search the <br>domains for desired publications. It accepts the seed URLs as an <br>input and uses them as starting points for the crawl. The Crawler <br>uses anchor text to determine link priorities and quickly navigates <br>through the websites using to get to the desired academic papers. <br>The harvested documents will be stored in the Document <br>Database. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">302</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:6px;font-family:Times;color:#000000;}
	.ft12{font-size:5px;font-family:Times;color:#000000;}
	.ft13{font-size:7px;font-family:Times;color:#000000;}
	.ft14{font-size:6px;line-height:9px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215003.png" alt="background image">
<DIV style="position:absolute;top:314;left:174"><nobr><span class="ft3">Figure 1. System Architecture </span></nobr></DIV>
<DIV style="position:absolute;top:339;left:81"><nobr><span class="ft1">3.2  Using Metadata to Locate Homepages </span></nobr></DIV>
<DIV style="position:absolute;top:360;left:81"><nobr><span class="ft8">Crawling authors' homepages first requires the system to be able <br>to locate such websites quickly and accurately. A study of the <br>literature indicates that personal website and homepage finding <br>have been studied a lot since the birth of WWW. In [18], the <br>authors present AHOY! as the first working system for personal <br>homepage finding, which can filter irrelevant pages based on <br>pattern matching heuristics. Later, the TREC (Text REtrieval <br>Conference) hosted the task of Web homepage finding in 2001 <br>and its subsequent years, and algorithms based on link analysis, <br>linguistic cues, and machine learning etc. are proposed [19, 20, <br>21]. Examples of current working systems include <br>HomePageSearch (hpsearch.uni-trier.de) which is a Homepage <br>Aggregator mainly for computer scientists, and compiled <br>directories (e.g. Google Directory) </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:81"><nobr><span class="ft8">See Figure 2 for the architecture of the Homepage Aggregator <br>component. </span></nobr></DIV>
<DIV style="position:absolute;top:809;left:441"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:822;left:111"><nobr><span class="ft3">Figure 2. Architecture of the Homepage Aggregator </span></nobr></DIV>
<DIV style="position:absolute;top:846;left:81"><nobr><span class="ft8">The goal of the Homepage Aggregator is to look for homepages <br>of the authors and save them as seed URLs to feed the Focused <br>Crawler. First it queries the Metadata Repository and retrieves the <br>document metadata. For each author, it extracts from metadata a <br>value pair of (N, P), where N is the name of the author and P is <br>the name of the venue (with a number of variations) in which the <br>paper is published. A list of such pairs is then submitted to a Web <br>search engine. Pages returned by the search engine will go <br>through a Homepage Filter where we use metadata heuristics to <br>remove false positives (pages that are not likely to be the <br>homepages of the authors) and disambiguate among namesakes, if <br>there is any. Different priority weights are assigned to the <br>remaining pages according to their likelihood of being the <br>homepage of the author. The more likely it's the homepage of the </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">author, the higher priority it receives. Eventually the page with <br>the highest priority weights will be inserted into the Homepage <br>URL Database, and will be crawled later.  </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft8">Recall that we extract from metadata a pair value of (N, P). Now <br>let U be the URL and T be the title of a Web page P returned by <br>the Web search engine. When there are more than two authors for <br>the same paper, assume Ui are the URLs of the homepages of <br>other authors already found by the system. We have incorporated <br>the findings in [16] about major characteristics of personal <br>homepages. The metadata heuristics employed in the Homepage <br>Filter are explained in Table 1. </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:514"><nobr><span class="ft3">Table 1. Heuristics Employed in Homepage Filter </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:504"><nobr><span class="ft3">Function Heuristic </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:731"><nobr><span class="ft3">Rules </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:493"><nobr><span class="ft3">Remove false </span></nobr></DIV>
<DIV style="position:absolute;top:356;left:506"><nobr><span class="ft3">positives</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:554"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:631"><nobr><span class="ft8">Remove U if U or T indicates a <br>publisher's website. </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:372;left:631"><nobr><span class="ft8">Remove U if U or T indicates a <br>digital library. </span></nobr></DIV>
<DIV style="position:absolute;top:403;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:403;left:631"><nobr><span class="ft8">Remove U if U points to a file other <br>than .htm/.html </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:493"><nobr><span class="ft3">Disambiguate </span></nobr></DIV>
<DIV style="position:absolute;top:452;left:508"><nobr><span class="ft3">between  </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:501"><nobr><span class="ft3">namesakes</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:559"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:631"><nobr><span class="ft8">Choose U among the candidates if U <br>is in the same domain as Ui.  </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:613"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:471;left:631"><nobr><span class="ft8">Remove U if its parent-domain is <br>already found by the system.</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:788"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:503;left:489"><nobr><span class="ft3">Assign priority</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:571"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:503;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:503;left:631"><nobr><span class="ft8">U receives high priority if T contains <br>N and any of the following: <br>homepage (home page), web <br>(website), research, publication, <br>papers. </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:610"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:583;left:631"><nobr><span class="ft8">U receives medium priority if T <br>contains any of the following: <br>homepage (home page), web <br>(website), research, publication, <br>papers. </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:613"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:665;left:631"><nobr><span class="ft8">U receives low priority when neither <br>one of the above two rules is fired.</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:819"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:707;left:477"><nobr><span class="ft1">3.3  Crawler Architecture </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:477"><nobr><span class="ft8">The Focused Crawler crawls web pages, using heuristics to <br>quickly navigate to the publications. The architecture of the <br>component is shown in Figure 3. </span></nobr></DIV>
<DIV style="position:absolute;top:784;left:477"><nobr><span class="ft8">The crawler accepts two primary sets of inputs that vary for each <br>crawl. The first is a set of seed URLs that are the starting points of <br>the crawl. These are added to the crawl queue at low priority. The <br>second set of inputs is a collection of domain names that the <br>crawler is permitted to crawl.  </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:477"><nobr><span class="ft8">Once the seed URLs are entered into the queue, the crawler <br>threads are started. Each thread gets one URL from the priority <br>queue, and downloads the page that it points to. </span></nobr></DIV>
<DIV style="position:absolute;top:929;left:477"><nobr><span class="ft8">After a page is downloaded, the out-going links are examined and <br>those matched with the ignored list are removed, either because <br>they are out of the target domain or because their MIME types are <br>not processed by the crawler. At this point, if a PDF/PostScript <br>document is found, it will be inserted into the Document Database. <br>The rest of the out-going links will each be classified as high, <br>medium, or low priority, and inserted into different priority <br>queues.  </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:245"><nobr><span class="ft14">Metadata <br>Extractor </span></nobr></DIV>
<DIV style="position:absolute;top:711;left:253"><nobr><span class="ft11">Web </span></nobr></DIV>
<DIV style="position:absolute;top:727;left:237"><nobr><span class="ft11">Search Engine </span></nobr></DIV>
<DIV style="position:absolute;top:779;left:233"><nobr><span class="ft11">Homepage Filter </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:364"><nobr><span class="ft11">Homepage </span></nobr></DIV>
<DIV style="position:absolute;top:660;left:363"><nobr><span class="ft11">Aggregator </span></nobr></DIV>
<DIV style="position:absolute;top:654;left:106"><nobr><span class="ft11">Public </span></nobr></DIV>
<DIV style="position:absolute;top:664;left:101"><nobr><span class="ft11">Metadata </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:98"><nobr><span class="ft11">Repository </span></nobr></DIV>
<DIV style="position:absolute;top:689;left:96"><nobr><span class="ft12"> </span></nobr></DIV>
<DIV style="position:absolute;top:761;left:388"><nobr><span class="ft11">Homepage </span></nobr></DIV>
<DIV style="position:absolute;top:770;left:397"><nobr><span class="ft11">URL </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:390"><nobr><span class="ft11">Database </span></nobr></DIV>
<DIV style="position:absolute;top:258;left:255"><nobr><span class="ft13">Focused  </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:256"><nobr><span class="ft13">Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:212;left:166"><nobr><span class="ft13">Metadata </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:164"><nobr><span class="ft13">Heuristics </span></nobr></DIV>
<DIV style="position:absolute;top:139;left:106"><nobr><span class="ft13">Public </span></nobr></DIV>
<DIV style="position:absolute;top:151;left:100"><nobr><span class="ft13">Metadata </span></nobr></DIV>
<DIV style="position:absolute;top:162;left:97"><nobr><span class="ft13">Repository </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:407"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:270;left:387"><nobr><span class="ft13">Document </span></nobr></DIV>
<DIV style="position:absolute;top:281;left:389"><nobr><span class="ft13">Database</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:360"><nobr><span class="ft13">Homepage </span></nobr></DIV>
<DIV style="position:absolute;top:151;left:371"><nobr><span class="ft13">URL </span></nobr></DIV>
<DIV style="position:absolute;top:162;left:363"><nobr><span class="ft13">Database</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:233"><nobr><span class="ft13">Homepage </span></nobr></DIV>
<DIV style="position:absolute;top:141;left:232"><nobr><span class="ft13">Aggregator </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">303</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft15{font-size:8px;font-family:Times;color:#000000;}
	.ft16{font-size:11px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215004.png" alt="background image">
<DIV style="position:absolute;top:400;left:127"><nobr><span class="ft3">Figure 3. Architecture of the Focused Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:424;left:81"><nobr><span class="ft8">In order to concentrate or limit the crawls towards only desirable <br>content, the crawler is provided with three lists for reference. The <br>contents of the lists may be changed depending on the types of <br>domains being crawled.  <br>The Ignore List is a set of file types that are to be ignored by the <br>crawler. The most common types of URLs that are ignored by the <br>crawler are links to image files. The list can also include parts of <br>the domain(s) being crawled, which the crawler is not supposed to <br>visit. Table 2 shows a sample Ignore List.  </span></nobr></DIV>
<DIV style="position:absolute;top:579;left:181"><nobr><span class="ft3">Table 2. Sample Ignore List </span></nobr></DIV>
<DIV style="position:absolute;top:602;left:95"><nobr><span class="ft3">File Types </span></nobr></DIV>
<DIV style="position:absolute;top:601;left:184"><nobr><span class="ft3">.jpg, .bmp, .gif, .png, .jpeg, .mpg, .mpeg, .avi </span></nobr></DIV>
<DIV style="position:absolute;top:626;left:208"><nobr><span class="ft3">http://clgiles.ist.psu.edu/picture.html </span></nobr></DIV>
<DIV style="position:absolute;top:627;left:125"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:99"><nobr><span class="ft3">Domains </span></nobr></DIV>
<DIV style="position:absolute;top:645;left:206"><nobr><span class="ft3">http://clgiles.ist.psu.edu/courses.html </span></nobr></DIV>
<DIV style="position:absolute;top:662;left:81"><nobr><span class="ft8">Files of type JPG, BMP etc will be ignored during the crawl. Also <br>any outgoing links to pages within the ignored domains will not <br>be considered for crawling. <br>The Allow List on the other hand is a collection of domain names <br>that make up the crawl space of the crawler. Links pointing <br>outside the specified domains are ignored by the crawler (unless <br>they are determined to be research documents). This list is useful <br>to limit the breadth of the crawl to only those domains that are of <br>interest. Table 3 shows a sample Allow List.  </span></nobr></DIV>
<DIV style="position:absolute;top:817;left:183"><nobr><span class="ft3">Table 3. Sample Allow List </span></nobr></DIV>
<DIV style="position:absolute;top:839;left:92"><nobr><span class="ft3">Domains </span></nobr></DIV>
<DIV style="position:absolute;top:839;left:231"><nobr><span class="ft3">http://clgiles.ist.psu.edu </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:81"><nobr><span class="ft8">So the link http://clgiles.ist.psu.edu will be considered for <br>crawling if it's discovered. <br>Priority lists contain a set of keywords and their assigned weights <br>that are used to determine the priorities of the extracted links. The <br>links will be visited by the crawler in the order of their assigned <br>priority.  <br>The Crawl Queue holds the discovered URLs that are yet to be <br>crawled. This queue consists of three sub-queues: High-priority, <br>Medium-priority and Low-priority queue. The Low-priority queue <br>is the default queue. The seed URLs are entered into this queue.  </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">We adopt a simple yet very effective heuristics to make the <br>priority classification based upon the likelihood of the link <br>eventually leading to academic publications.  </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft16">We first train a classifier with data collected from two publishing <br>venues: the Very Large Data Bases (VLDB) Conference and the <br>Text REtrieval Conference (TREC). Several crawls are carried <br>out with a breadth-first policy. The logs of the crawls are <br>analyzed and a traverse tree is generated for each of the crawl that <br>indicates the URLs visited and the link path that is followed by <br>the crawler to reach the desired publications.  <br>Consider a small website having 11 pages as shown in Figure 4. </span></nobr></DIV>
<DIV style="position:absolute;top:548;left:820"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:567;left:583"><nobr><span class="ft3">Figure 4. Sample Website </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:477"><nobr><span class="ft16">The circles represent URL's in the website and the arrows are the <br>hyperlinks from one page to another. The link structure shown is <br>that which is followed by the breadth-first crawler to visit each <br>URL. All other links such as those that may point outside the <br>domain are ignored in the above diagram. <br>The node marked with `S' is the seed or start URL. The nodes <br>marked with `P' are research document files that are detected by <br>the crawler. Now the links that are of interest to us are S A P <br>and S C D P. The anchor text contained in these links `SA', <br>`AP', `SC', `SD', `DP' is extracted and marked as `interesting'. <br>The text in the remainder of the links is also noted, but goes in <br>`not interesting' set. <br>Similar analysis is done on all the logs that are generated by the <br>breadth-first crawl. All the keywords that are commonly <br>occurring in the "interesting" class and not so commonly <br>occurring in the "non-interesting" class are extracted. Weights are <br>assigned to each of these keywords depending on their placement <br>in the link structure. The keywords closer to the documents are <br>given more weight that those closer to the seed URL. For e.g. <br>keyword `SA' has a lesser weight than keyword `DP' as `DP' is <br>closer to P than to S as opposed to `SA'. <br>The formula for calculating keyword weight is: </span></nobr></DIV>
<DIV style="position:absolute;top:964;left:571"><nobr><span class="ft3">W (OT</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:609"><nobr><span class="ft13">oq</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:627"><nobr><span class="ft3">) = D (Q) / D (P)  </span></nobr></DIV>
<DIV style="position:absolute;top:964;left:747"><nobr><span class="ft3">       </span></nobr></DIV>
<DIV style="position:absolute;top:964;left:801"><nobr><span class="ft3">    (I) </span></nobr></DIV>
<DIV style="position:absolute;top:986;left:477"><nobr><span class="ft3">where OT</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:531"><nobr><span class="ft13">oq </span></nobr></DIV>
<DIV style="position:absolute;top:986;left:551"><nobr><span class="ft3"> is the anchor text of the out-going link from page O </span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:477"><nobr><span class="ft8">to page Q; P is the desired academic paper found by following the <br>link from O to Q;  D(P) denotes the distance (number of hops) <br>between  P and the starting URL S on the path </span></nobr></DIV>
<DIV style="position:absolute;top:334;left:627"><nobr><span class="ft15">S</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:534"><nobr><span class="ft15">A</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:627"><nobr><span class="ft15">B</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:712"><nobr><span class="ft15">C </span></nobr></DIV>
<DIV style="position:absolute;top:458;left:496"><nobr><span class="ft15">P</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:700"><nobr><span class="ft15">E</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:735"><nobr><span class="ft15">D</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:735"><nobr><span class="ft15">P</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:767"><nobr><span class="ft15">Depth 1 </span></nobr></DIV>
<DIV style="position:absolute;top:453;left:767"><nobr><span class="ft15">Depth 2 </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:767"><nobr><span class="ft15">Depth 3 </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:767"><nobr><span class="ft15">Depth 0 </span></nobr></DIV>
<DIV style="position:absolute;top:133;left:114"><nobr><span class="ft11">Homepage </span></nobr></DIV>
<DIV style="position:absolute;top:142;left:123"><nobr><span class="ft11">URL </span></nobr></DIV>
<DIV style="position:absolute;top:152;left:116"><nobr><span class="ft11">Database </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:386"><nobr><span class="ft11">Document </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:388"><nobr><span class="ft11">Database</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:363"><nobr><span class="ft11">Ignore </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:367"><nobr><span class="ft11">List </span></nobr></DIV>
<DIV style="position:absolute;top:186;left:173"><nobr><span class="ft11">Priority Queues </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:193"><nobr><span class="ft11">Priority </span></nobr></DIV>
<DIV style="position:absolute;top:264;left:189"><nobr><span class="ft11">Heuristics </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:275"><nobr><span class="ft11">Download Pages </span></nobr></DIV>
<DIV style="position:absolute;top:237;left:275"><nobr><span class="ft11">Link Extractor </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:281"><nobr><span class="ft11">Link Filter</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:269"><nobr><span class="ft11">Link </span></nobr></DIV>
<DIV style="position:absolute;top:299;left:264"><nobr><span class="ft11">Priority </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:262"><nobr><span class="ft11">Analyzer </span></nobr></DIV>
<DIV style="position:absolute;top:348;left:330"><nobr><span class="ft11">PDF/PS </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:325"><nobr><span class="ft11">Documents </span></nobr></DIV>
<DIV style="position:absolute;top:139;left:360"><nobr><span class="ft11">Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:362"><nobr><span class="ft11">Thread </span></nobr></DIV>
<DIV style="position:absolute;top:260;left:363"><nobr><span class="ft11">Allow </span></nobr></DIV>
<DIV style="position:absolute;top:270;left:367"><nobr><span class="ft11">List </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">304</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft17{font-size:6px;line-height:14px;font-family:Times;color:#000000;}
	.ft18{font-size:6px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215005.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">S...OQ...P;  D(Q) denotes the distance between Q <br>and the starting URL S on the path S...OQ. <br>Now that a list of anchor texts and their corresponding priority <br>weights has been compiled during the training process, we can <br>classify each of them into different priority categories according <br>to the weights. Table 4 shows a few samples extracted from our <br>list. </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:174"><nobr><span class="ft3">Table 4. Sample Anchor Texts </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:108"><nobr><span class="ft3">Priority Anchor </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:327"><nobr><span class="ft3">Texts </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:111"><nobr><span class="ft3">p_High </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:204"><nobr><span class="ft3">volume, pub, paper, conf, journal, content, </span></nobr></DIV>
<DIV style="position:absolute;top:287;left:259"><nobr><span class="ft3">program, research, list </span></nobr></DIV>
<DIV style="position:absolute;top:303;left:101"><nobr><span class="ft3">p_Medium </span></nobr></DIV>
<DIV style="position:absolute;top:303;left:231"><nobr><span class="ft3">topic, faculty, people, group, lab </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:81"><nobr><span class="ft8">We now need to consider how to prioritize out-going links that <br>are more likely to lead to desired academic publications. The <br>anchor text in these links is compared against the weighted <br>keywords. If any of the weighted keywords are present in the text, <br>the comparison is considered to be successful. There are no <br>keywords having more than one weight. The final priority of the <br>link is calculated by the following function. </span></nobr></DIV>
<DIV style="position:absolute;top:713;left:81"><nobr><span class="ft8">The priority of a link may also depend on the priority of its parent. <br>This is mainly due to the fact that not all the links that emerge <br>from a page with a medium or high priority may lead to a research <br>document. For e.g. in Figure 4 the node `C' will be crawled with a <br>medium priority, however only node `D' leads to a research <br>document. The priority of the node `E' is thus reduced to low as it <br>will not have a weighted keyword attached to it and that of `D' is <br>increased to high. The priorities of links thus established are used <br>to insert the link in the proper priority queue for crawling. In <br>order to achieve high efficiency, the crawler spawns multiple <br>threads which will be fed with URLs on the descending order of <br>priority. When there is no URL left in the priority queues and no <br>crawler thread is currently running, the crawling task is finished. </span></nobr></DIV>
<DIV style="position:absolute;top:931;left:81"><nobr><span class="ft1">4.  RESULTS AND DISCUSSION </span></nobr></DIV>
<DIV style="position:absolute;top:952;left:81"><nobr><span class="ft8">We have collected data from two Computer Science publication <br>venues: the ACM SIGMOD International Workshop on the Web <br>and Databases (WebDB), first held in 1998 and then each year in <br>conjunction with the annual ACM SIGMOD Conference, and <br>Journal of Artificial Intelligence Research (JAIR), which was <br>established in 1993 both as an electronic scientific journals and a <br>hard-copy semiyearly published by AAAI Press. We choose these <br>two venues because both of them are highly selective venues with </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">less than a 25% acceptance rate and we want to observe if there is <br>a major difference of performance between conferences and <br>journals. </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft8">We have extracted the metadata of WebDB and JAIR from the <br>DBLP repository. By analyzing these metadata, we successfully <br>identify the 593 unique authors who have in total published 289 <br>papers in either one of these two venues during the period from <br>1998 to 2004. Please see Table 5 for more details of the dataset. </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:547"><nobr><span class="ft3">Table 5. Statistics of the collected data </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:578"><nobr><span class="ft3">WebDB JAIR </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:479"><nobr><span class="ft3">Year</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:535"><nobr><span class="ft3">Unique </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:532"><nobr><span class="ft3">Authors </span></nobr></DIV>
<DIV style="position:absolute;top:293;left:606"><nobr><span class="ft3">Publication Unique </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:698"><nobr><span class="ft3">Authors </span></nobr></DIV>
<DIV style="position:absolute;top:293;left:769"><nobr><span class="ft3">Publication</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:480"><nobr><span class="ft3">1998</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:549"><nobr><span class="ft3">32 13 40 20 </span></nobr></DIV>
<DIV style="position:absolute;top:341;left:480"><nobr><span class="ft3">1999</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:549"><nobr><span class="ft3">51 17 50 28 </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:480"><nobr><span class="ft3">2000</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:549"><nobr><span class="ft3">61 20 33 20 </span></nobr></DIV>
<DIV style="position:absolute;top:375;left:480"><nobr><span class="ft3">2001</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:549"><nobr><span class="ft3">51 18 45 25 </span></nobr></DIV>
<DIV style="position:absolute;top:391;left:480"><nobr><span class="ft3">2002</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:549"><nobr><span class="ft3">47 17 64 27 </span></nobr></DIV>
<DIV style="position:absolute;top:408;left:480"><nobr><span class="ft3">2003</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:549"><nobr><span class="ft3">56 17 72 30 </span></nobr></DIV>
<DIV style="position:absolute;top:424;left:480"><nobr><span class="ft3">2004</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:549"><nobr><span class="ft3">51 16 57 21 </span></nobr></DIV>
<DIV style="position:absolute;top:441;left:479"><nobr><span class="ft3">Total</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:546"><nobr><span class="ft3">285 118 308 171 </span></nobr></DIV>
<DIV style="position:absolute;top:457;left:477"><nobr><span class="ft8"> <br>In order to examine whether our approach is effective in <br>recovering those missing documents from a digital library, we use <br>the CiteSeer Scientific Digital Library as another data source. <br>Cross-referencing the metadata of each of the two venues from <br>DBLP, we successfully identified 30 out of 118 (25.42%) WebDB <br>papers and 46 out of 171 (26.90%) JAIR papers that are not <br>indexed by CiteSeer (see Figure 5 for details). This is done by <br>exact title-matching between the records in the DBLP metadata <br>repository and the CiteSeer document archive. </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:640"><nobr><span class="ft15">WebDB</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:535"><nobr><span class="ft11">8</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:569"><nobr><span class="ft11">12</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:605"><nobr><span class="ft11">17</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:640"><nobr><span class="ft11">13</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:676"><nobr><span class="ft11">10</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:712"><nobr><span class="ft11">14</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:747"><nobr><span class="ft11">14</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:535"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:571"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:607"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:643"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:678"><nobr><span class="ft11">7</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:714"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:750"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:503"><nobr><span class="ft11">0%</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:499"><nobr><span class="ft11">10%</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:499"><nobr><span class="ft11">20%</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:499"><nobr><span class="ft11">30%</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:499"><nobr><span class="ft11">40%</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:499"><nobr><span class="ft11">50%</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:499"><nobr><span class="ft11">60%</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:499"><nobr><span class="ft11">70%</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:499"><nobr><span class="ft11">80%</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:499"><nobr><span class="ft11">90%</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:495"><nobr><span class="ft11">100%</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:529"><nobr><span class="ft11">1998</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:565"><nobr><span class="ft11">1999</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:601"><nobr><span class="ft11">2000</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:637"><nobr><span class="ft11">2001</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:672"><nobr><span class="ft11">2002</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:708"><nobr><span class="ft11">2003</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:744"><nobr><span class="ft11">2004</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:786"><nobr><span class="ft17">NOT Indexed<br>Indexed</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:836"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:835;left:644"><nobr><span class="ft15">JAIR</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:541"><nobr><span class="ft11">17</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:573"><nobr><span class="ft11">25</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:604"><nobr><span class="ft11">20</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:636"><nobr><span class="ft11">21</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:667"><nobr><span class="ft11">17</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:699"><nobr><span class="ft11">16</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:730"><nobr><span class="ft11">15</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:544"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:576"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:607"><nobr><span class="ft11">0</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:639"><nobr><span class="ft11">4</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:667"><nobr><span class="ft11">10</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:699"><nobr><span class="ft11">14</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:733"><nobr><span class="ft11">6</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:510"><nobr><span class="ft11">0%</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:505"><nobr><span class="ft11">10%</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:505"><nobr><span class="ft11">20%</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:505"><nobr><span class="ft11">30%</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:505"><nobr><span class="ft11">40%</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:505"><nobr><span class="ft11">50%</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:505"><nobr><span class="ft11">60%</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:505"><nobr><span class="ft11">70%</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:505"><nobr><span class="ft11">80%</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:505"><nobr><span class="ft11">90%</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:500"><nobr><span class="ft11">100%</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:537"><nobr><span class="ft11">1998</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:569"><nobr><span class="ft11">1999</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:600"><nobr><span class="ft11">2000</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:632"><nobr><span class="ft11">2001</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:663"><nobr><span class="ft11">2002</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:694"><nobr><span class="ft11">2003</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:726"><nobr><span class="ft11">2004</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:771"><nobr><span class="ft18">NOT Indexed<br>Indexed</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:837"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:515"><nobr><span class="ft3">Figure 5. Coverage of the two venues by CiteSeer </span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:477"><nobr><span class="ft8">The metadata extracted from DBLP are also used as heuristics to <br>locate the homepages of the 593 authors. The name of the author </span></nobr></DIV>
<DIV style="position:absolute;top:457;left:94"><nobr><span class="ft3">// Get_Priority(): Returns the priority for link L</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:360"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:366"><nobr><span class="ft3"> with anchor </span></nobr></DIV>
<DIV style="position:absolute;top:473;left:94"><nobr><span class="ft3">text T which has weight W</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:240"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:246"><nobr><span class="ft3">. </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:94"><nobr><span class="ft3">// Low=0, Medium=1, High=2 (for weight and priority) </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:94"><nobr><span class="ft3">Get_Priority { </span></nobr></DIV>
<DIV style="position:absolute;top:548;left:94"><nobr><span class="ft3">If W</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:119"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:125"><nobr><span class="ft3"> = 0 and (Priority(Parent(L</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:270"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:276"><nobr><span class="ft3">)) &gt; 0 then </span></nobr></DIV>
<DIV style="position:absolute;top:569;left:148"><nobr><span class="ft3">Priority(L</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:202"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:208"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:212"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:569;left:215"><nobr><span class="ft3">=   Priority(Parent(L</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:326"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:331"><nobr><span class="ft3">)) -1; </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:94"><nobr><span class="ft3">Else if W</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:145"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:151"><nobr><span class="ft3"> &gt; 0 </span></nobr></DIV>
<DIV style="position:absolute;top:613;left:94"><nobr><span class="ft3"> Priority(L</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:202"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:208"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:212"><nobr><span class="ft13"> </span></nobr></DIV>
<DIV style="position:absolute;top:613;left:215"><nobr><span class="ft3">= W</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:238"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:244"><nobr><span class="ft3"> ; </span></nobr></DIV>
<DIV style="position:absolute;top:635;left:94"><nobr><span class="ft3">End IF </span></nobr></DIV>
<DIV style="position:absolute;top:660;left:94"><nobr><span class="ft3">Return (Priority(L</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:193"><nobr><span class="ft13">T</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:199"><nobr><span class="ft3">)); </span></nobr></DIV>
<DIV style="position:absolute;top:685;left:94"><nobr><span class="ft3">} </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">305</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft19{font-size:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215006.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">and the corresponding venue (with a number of variations) are <br>submitted to Google API and the first 10 URLs returned are <br>parsed automatically by the Homepage Filter component. Using <br>the heuristics discussed in the previous section, we assign priority <br>weights to each of the URLs. For each author, URLs with the <br>highest priority weights are inserted into the URL Database and <br>crawled by the Focused Crawler at a later stage. </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:81"><nobr><span class="ft8">We have manually examined the records in the URL Database in <br>order to evaluate the effectiveness of the Homepage Aggregator. <br>In total, homepages of 539 authors (90.89%) have been found. <br>Details about the 54 authors whose homepages cannot be found <br>by the system are shown in Table 6. Here we define Non-U.S. <br>authors to be those whose affiliations are not currently in the <br>States. </span></nobr></DIV>
<DIV style="position:absolute;top:349;left:87"><nobr><span class="ft3">Table 6. Number of authors whose homepages are not found </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:152"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:267"><nobr><span class="ft3">WebDB JAIR </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:115"><nobr><span class="ft3">U.S. Authors </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:283"><nobr><span class="ft3">13 6 </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:100"><nobr><span class="ft3">Non-U.S. Authors </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:283"><nobr><span class="ft3">25 10 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:98"><nobr><span class="ft3">Total (Percentage) </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:256"><nobr><span class="ft3">38 (13.33%) </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:368"><nobr><span class="ft3">16 (5.19%) </span></nobr></DIV>
<DIV style="position:absolute;top:448;left:81"><nobr><span class="ft8">There are only 2 papers ([22], [23]) of which all the authors' <br>homepages are not found by the system, which account for less <br>than 1% of the 289 papers in our data set. In other words, <br>although the system fails to locate the homepages of about 9% of <br>the authors, it is not a major performance impact on the document <br>recall and the crawler should still be able to find 99.31% of all the <br>papers. </span></nobr></DIV>
<DIV style="position:absolute;top:568;left:81"><nobr><span class="ft8">For the cases where the system fails to locate some of the <br>homepages, we notice that most of the 19 U.S. authors whose <br>homepages are not found were actually in their graduate programs <br>when they co-authored the paper, and their Web presences seem <br>to have disappeared after graduation. In addition, there's a <br>significant difference between the numbers of U.S. and non-U.S. <br>authors whose homepages cannot be found, with non-U.S. almost <br>twice the number of U.S. authors. Since this is our initial attempt <br>limited to only the domain of computer science, whether this <br>difference holds true for other disciplines and the reason behind <br>remain an open question.  Finally, there are several cases where <br>the homepages of those with famous names actually show up <br>instead of the desired authors. For example, a search via Google <br>API for the first author in [24] returns the homepage of a comic <br>artist. The top 5 websites for George Russell, the first author of <br>[25], happen to belong to that of a famous Jazz musician. There <br>are also a few cases where the search engine actually returns the <br>homepage of the co-author instead of the author himself, because <br>the author's name is listed on the co-author's page as a <br>collaborator and the co-author's page receives a higher page <br>ranking. All these indicate that the disambiguation capability <br>needs to be improved. </span></nobr></DIV>
<DIV style="position:absolute;top:926;left:81"><nobr><span class="ft1">4.1  Finding Desired Academic Publications </span></nobr></DIV>
<DIV style="position:absolute;top:947;left:81"><nobr><span class="ft8">When the crawl is finished, we manually examine the <br>downloaded PDF/PostScript documents in order to evaluate the <br>performance of the crawler. In total, the crawler has acquired 236 <br>out of the 289 papers (81.66%) published in WebDB (100 out of <br>118, 84.75%) and JAIR (136 out of 171, 79.53%) from 1998 to <br>2004. For details of the results for each venue, please see Figure 6 <br>and 7. </span></nobr></DIV>
<DIV style="position:absolute;top:122;left:610"><nobr><span class="ft19">WebDB, 1998 - 2004</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:519"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:519"><nobr><span class="ft19">5</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:513"><nobr><span class="ft19">10</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:513"><nobr><span class="ft19">15</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:513"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:513"><nobr><span class="ft19">25</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:543"><nobr><span class="ft19">1998</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:583"><nobr><span class="ft19">1999</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:624"><nobr><span class="ft19">2000</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:664"><nobr><span class="ft19">2001</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:705"><nobr><span class="ft19">2002</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:745"><nobr><span class="ft19">2003</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:786"><nobr><span class="ft19">2004</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:505"><nobr><span class="ft19">Numb</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:505"><nobr><span class="ft19">er</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:707"><nobr><span class="ft19">Papers found</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:707"><nobr><span class="ft19">Papers published</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:552"><nobr><span class="ft3">Figure 6. Number of WebDB Papers </span></nobr></DIV>
<DIV style="position:absolute;top:378;left:622"><nobr><span class="ft4">JAIR,1998 - 2004</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:523"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:523"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:517"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:517"><nobr><span class="ft4">15</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:517"><nobr><span class="ft4">20</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:517"><nobr><span class="ft4">25</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:517"><nobr><span class="ft4">30</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:517"><nobr><span class="ft4">35</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:546"><nobr><span class="ft4">1998</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:588"><nobr><span class="ft4">1999</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:629"><nobr><span class="ft4">2000</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:671"><nobr><span class="ft4">2001</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:713"><nobr><span class="ft4">2002</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:754"><nobr><span class="ft4">2003</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:796"><nobr><span class="ft4">2004</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:507"><nobr><span class="ft4">Number</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:669"><nobr><span class="ft4">Papers found</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:751"><nobr><span class="ft4">Papers published</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:557"><nobr><span class="ft3"> Figure 7. Number of JAIR Papers </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:477"><nobr><span class="ft8">Here we adopt one of the performance metrics, recall level, first <br>proposed in [16] and used in [17]. Recall level is defined as: </span></nobr></DIV>
<DIV style="position:absolute;top:663;left:597"><nobr><span class="ft3">(i) = | S(i) </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:660"><nobr><span class="ft3"> T | / | T | </span></nobr></DIV>
<DIV style="position:absolute;top:688;left:477"><nobr><span class="ft8">where  S(i) is the set of documents downloaded by the crawler <br>during a crawl on the dataset of a calendar year i; T is the set of <br>desired documents, which in this study are the papers published <br>by a specific venue in the same calendar year. This measure <br>represents the capability of the system to capture desired <br>academic papers. </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:477"><nobr><span class="ft8">Overall, our system has achieved a recall level of 0.8475 for <br>WebDB and 0.7953 for JAIR documents. See Figure 8 for more <br>details. </span></nobr></DIV>
<DIV style="position:absolute;top:848;left:477"><nobr><span class="ft8">It's interesting to note that while the recall level of WebDB is <br>constantly increasing until reaching 1.0 in the last two years, the <br>recall level of JAIR seems to fluctuate around 0.8 over the 7-<br>years period. We find that 29 out of the 35 (82.86%) JAIR papers <br>not found by the system are actually downloadable via a link from <br>the authors' homepages to the publisher's website. Yet we miss <br>these papers simply because we limit our crawler not to go <br>beyond the domain of authors' homepages. We believe that a <br>more sophisticated domain restriction for the crawler can be <br>easily employed in order to achieve an even higher recall level. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">306</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft20{font-size:7px;line-height:-2px;font-family:Times;color:#000000;}
	.ft21{font-size:7px;line-height:-8px;font-family:Times;color:#000000;}
	.ft22{font-size:7px;line-height:-4px;font-family:Times;color:#000000;}
	.ft23{font-size:10px;line-height:-9px;font-family:Times;color:#000000;}
	.ft24{font-size:10px;line-height:-6px;font-family:Times;color:#000000;}
	.ft25{font-size:10px;line-height:-5px;font-family:Times;color:#000000;}
	.ft26{font-size:10px;line-height:-4px;font-family:Times;color:#000000;}
	.ft27{font-size:10px;line-height:-3px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215007.png" alt="background image">
<DIV style="position:absolute;top:314;left:132"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:120"><nobr><span class="ft4">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:120"><nobr><span class="ft4">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:120"><nobr><span class="ft4">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:120"><nobr><span class="ft4">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:132"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:120"><nobr><span class="ft4">1.2</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:155"><nobr><span class="ft4">1998</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:194"><nobr><span class="ft4">1999</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:233"><nobr><span class="ft4">2000</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:272"><nobr><span class="ft4">2001</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:311"><nobr><span class="ft4">2002</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:351"><nobr><span class="ft4">2003</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:390"><nobr><span class="ft4">2004</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:112"><nobr><span class="ft4">Reca</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:112"><nobr><span class="ft4">ll Level</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:377"><nobr><span class="ft4">WebDB</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:377"><nobr><span class="ft4">JAIR</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:138"><nobr><span class="ft3">Figure 8. Overall Recall Level, 1998 - 2004 </span></nobr></DIV>
<DIV style="position:absolute;top:391;left:81"><nobr><span class="ft8">We calculate the recall level for the documents published in <br>WebDB and JAIR yet missing from CiteSeer's collection (see <br>Figure 9). In this case, S(i) is the set of missing documents <br>downloaded by the crawler, and T is the set of the papers not <br>indexed by CiteSeer and missing from the collection. On average, <br>the recall level has achieved 0.78 for WebDB and 0.72 for JAIR. <br>Especially WebDB's recall level is constantly increasing, <br>reaching 1.0 for the last three years. This proves that it's practical <br>to harvest the missing documents for a given publishing venue.  </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:125"><nobr><span class="ft13">0</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:117"><nobr><span class="ft13">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:117"><nobr><span class="ft13">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:117"><nobr><span class="ft13">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:117"><nobr><span class="ft13">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:125"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:117"><nobr><span class="ft13">1.2</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:149"><nobr><span class="ft13">1998</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:190"><nobr><span class="ft13">1999</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:231"><nobr><span class="ft13">2000</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:272"><nobr><span class="ft13">2001</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:312"><nobr><span class="ft13">2002</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:353"><nobr><span class="ft13">2003</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:394"><nobr><span class="ft13">2004</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:107"><nobr><span class="ft13">Reca</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:107"><nobr><span class="ft22">l<br>l<br> L<br>e<br>v<br>e<br>l</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:379"><nobr><span class="ft13">WebDB</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:379"><nobr><span class="ft13">JAIR</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:441"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:784;left:119"><nobr><span class="ft3">Figure 9. Recall Level for the Missing Documents </span></nobr></DIV>
<DIV style="position:absolute;top:808;left:81"><nobr><span class="ft8">The trends shown in Figure 8 and 9 seem to indicate that a rising <br>number of academic papers have been put online, especially in <br>and after the year 2000. However, it's interesting to note that it <br>seems conference/workshop authors favor putting their <br>publications on their homepages, while journal authors don't. Due <br>to the limited size of our sample, we feel this is an open question <br>to be answered with more data across multiple venues. </span></nobr></DIV>
<DIV style="position:absolute;top:929;left:81"><nobr><span class="ft1">4.2  Crawler Comparison: BF Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:949;left:81"><nobr><span class="ft8">In order to further evaluate the performance of our system, we <br>also compare our work to other crawling approaches. First we <br>crawled three conference websites using our system and a <br>breadth-first (BF) crawler. Figures 10, 11 and 12 show the results <br>of crawls on different conference websites. The BF crawls are <br>shown by the dashed line while the results of the focused crawler <br>are shown by the solid line on the figures. The horizontal axis <br>indicates the number of pages crawled and the vertical axis </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">represents the number of research documents found by searching <br>those pages. The number of documents found is a cumulative sum <br>of all PDF, PS and GZ files found on those sites. Since they may <br>contain duplicate files or the same content in different file types, <br>the numbers shown do not indicate unique papers. The number of <br>pages crawled does not include academic papers. The same crawl <br>restrictions applied to both the crawlers. </span></nobr></DIV>
<DIV style="position:absolute;top:396;left:526"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:513"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:513"><nobr><span class="ft19">400</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:513"><nobr><span class="ft19">600</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:513"><nobr><span class="ft19">800</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:506"><nobr><span class="ft19">1000</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:506"><nobr><span class="ft19">1200</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:506"><nobr><span class="ft19">1400</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:506"><nobr><span class="ft19">1600</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:506"><nobr><span class="ft19">1800</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:541"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:626"><nobr><span class="ft19">10</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:714"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:802"><nobr><span class="ft19">30</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:637"><nobr><span class="ft19">Pages Crawled</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:498"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:498"><nobr><span class="ft26">m<br>b<br>e<br>r<br> o<br>f<br> Do</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:498"><nobr><span class="ft27">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:783"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:783"><nobr><span class="ft19">BF</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:829"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:557"><nobr><span class="ft3">Figure 10. ACL Conference Crawl </span></nobr></DIV>
<DIV style="position:absolute;top:488;left:477"><nobr><span class="ft8">Figure 10 shows the crawls done on parts of the Association for <br>Computational Linguistics (ACL) conference website. The total <br>number of pages crawled on this site were less than 30. Both <br>crawls overlap which indicates that there is virtually no difference <br>between the document detection rate of the BF crawler and our <br>focused crawler. For such a small website, both crawlers detect <br>the same number of documents after crawling the same number of <br>pages on the website. </span></nobr></DIV>
<DIV style="position:absolute;top:806;left:529"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:508"><nobr><span class="ft19">1000</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:508"><nobr><span class="ft19">2000</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:508"><nobr><span class="ft19">3000</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:508"><nobr><span class="ft19">4000</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:508"><nobr><span class="ft19">5000</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:508"><nobr><span class="ft19">6000</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:508"><nobr><span class="ft19">7000</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:543"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:602"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:666"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:731"><nobr><span class="ft19">300</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:795"><nobr><span class="ft19">400</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:637"><nobr><span class="ft19">Pages Crawled</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:498"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:498"><nobr><span class="ft27">m<br>b<br>e<br>r<br> o<br>f<br> <br>Do</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:498"><nobr><span class="ft25">c<br>u<br>m<br>e<br>n<br>t<br>s<br> </span></nobr></DIV>
<DIV style="position:absolute;top:767;left:776"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:776"><nobr><span class="ft19">BF</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:831"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:870;left:553"><nobr><span class="ft3">Figure 11. TREC Conference Crawl </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:477"><nobr><span class="ft8">Figure 11 shows the crawls done on the Text Retrieval <br>Conference (TREC) pages. Here the total of pages crawled is <br>about 1000 (only first half of the crawl is shown in the graph). <br>Both crawlers start detecting documents at the same rate. After <br>detecting around 1393 documents (35 pages crawled) the <br>document detection rate of the focused crawler becomes slightly <br>better than the BF crawler. Although the difference is not very <br>significant, the focused crawler does detect the research <br>documents slightly earlier in the crawl as compared to the BF <br>crawler. The BF crawler detects the same amount of documents <br>(4800 documents) as the focused crawler but after crawling 20-30 </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">307</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft28{font-size:11px;font-family:Times;color:#ff0000;}
	.ft29{font-size:11px;line-height:-10px;font-family:Times;color:#000000;}
	.ft30{font-size:11px;line-height:-6px;font-family:Times;color:#000000;}
	.ft31{font-size:11px;line-height:-5px;font-family:Times;color:#000000;}
	.ft32{font-size:11px;line-height:-4px;font-family:Times;color:#000000;}
	.ft33{font-size:11px;line-height:-3px;font-family:Times;color:#000000;}
	.ft34{font-size:10px;line-height:14px;font-family:Times;color:#000000;}
	.ft35{font-size:10px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215008.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">pages more than the focused crawler. The total number of <br>documents found by both the crawlers is around 6000. </span></nobr></DIV>
<DIV style="position:absolute;top:331;left:128"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:115"><nobr><span class="ft3">500</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:108"><nobr><span class="ft3">1000</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:108"><nobr><span class="ft3">1500</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:108"><nobr><span class="ft3">2000</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:108"><nobr><span class="ft3">2500</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:108"><nobr><span class="ft3">3000</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:108"><nobr><span class="ft3">3500</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:108"><nobr><span class="ft3">4000</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:143"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:174"><nobr><span class="ft3">500 1000 1500 2000 2500 3000 3500</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:241"><nobr><span class="ft3">Pages Crawled</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:102"><nobr><span class="ft3">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:102"><nobr><span class="ft32">m<br>b<br>e<br>r<br> o<br>f<br> Do</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:102"><nobr><span class="ft33">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:386"><nobr><span class="ft3">FC</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:386"><nobr><span class="ft3">BF</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:435"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:157"><nobr><span class="ft3">Figure 12. VLDB Conference Crawl </span></nobr></DIV>
<DIV style="position:absolute;top:431;left:81"><nobr><span class="ft8">The crawls performed on the Very Large Database (VLDB) <br>conference pages as shown in Figure 12 indicate that the focused <br>crawler detects the documents much earlier in the crawl. Here the <br>total number of pages crawled is about 3500. Approximately 28% <br>(1000 out of 3500) of the documents are located by both the <br>crawlers after crawling around 8.5% (300 out of 3500) of the <br>domain. At this point the focused crawler continues to locate <br>more documents while the BF crawler does not uncover any new <br>documents until 28% (1000 out of 3500) of the total crawl. 85% <br>(3000 out of 3500) of the documents are located by the focused <br>crawler after completing just 33% (1189 out of 3500) of the total <br>crawl, while the breadth first crawler locates the same amount of <br>documents after completing 50% (1781 out of 3500) of the total <br>crawl. Towards the end of the crawl the breadth-first crawler <br>detects more papers as compared to the focused crawler. It takes <br>the focused crawler around 1000 more pages of crawls until it <br>makes up the difference. This seems to be due to the lack of <br>keywords associated with the links that eventually led to the <br>documents. The focused crawler evaluates other papers that have <br>a higher priority values before eventually discovering the <br>remaining documents. <br>The behavior of the BF crawler is consistent for all the three <br>crawls. Most of the documents located were in crawl depths 2, 3, <br>4 and 5. The BF crawler detects them after completing search of <br>the previous crawl depths.  As the focused crawler prioritizes the <br>links for crawling, the higher depths with more priority are <br>crawled before the lower depths with less priority. <br>The above experiment indicates that the document harvest rate is <br>almost the same for smaller websites. The difference becomes <br>apparent when the size of the website being crawled is large. The <br>focused crawler is able to detect the documents much earlier in <br>the crawl as compared to the BF crawler. Since the crawls are not <br>terminated early for the focused crawler, the number of <br>documents found and the relevance of documents are same for <br>both the crawlers. Therefore as the size of websites being crawled <br>increases, the focused crawler detects more documents earlier <br>during the crawls as compared to the BF crawler.  <br>We assess the crawler's capability of harvesting academic <br>publications in a more general sense which is not only limited to a </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">specific venue. We have manually examined the first 500 <br>PDF/PostScript documents found by the two crawlers, classified <br>the documents into academic publications which are desirable <br>(papers published in conferences and journals; technical reports; <br>degree thesis, etc.), and non-publication documents which are <br>considered  noise for a publication collection (course material; <br>presentation slides; project schedule; etc.) Percentage of both <br>categories is compared side-by-side and shown in Figure 13. Our <br>crawler has outperformed the breadth-first counterpart by having <br>much less of this noise. </span></nobr></DIV>
<DIV style="position:absolute;top:405;left:600"><nobr><span class="ft19">423</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:671"><nobr><span class="ft19">480</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:603"><nobr><span class="ft19">77</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:675"><nobr><span class="ft19">22</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:552"><nobr><span class="ft19">0%</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:546"><nobr><span class="ft19">10%</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:546"><nobr><span class="ft19">20%</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:546"><nobr><span class="ft19">30%</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:546"><nobr><span class="ft19">40%</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:546"><nobr><span class="ft19">50%</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:546"><nobr><span class="ft19">60%</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:546"><nobr><span class="ft19">70%</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:546"><nobr><span class="ft19">80%</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:546"><nobr><span class="ft19">90%</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:539"><nobr><span class="ft19">100%</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:603"><nobr><span class="ft19">BF</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:675"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:519"><nobr><span class="ft19">Total: 50</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:519"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:744"><nobr><span class="ft34">Non-<br>Publication<br>Documents<br>Academic<br>Publications</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:485"><nobr><span class="ft3">Figure 13. Composition of the First 500 PDF/PS Documents </span></nobr></DIV>
<DIV style="position:absolute;top:555;left:477"><nobr><span class="ft1">4.3  Crawler Comparison: Nutch Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:477"><nobr><span class="ft8">We compare the performance of our system with Nutch <br>(http://www.nutch.org/docs/en/), an open source Web crawler and <br>search engine built upon Lucene. In our experiment, we run the <br>Nutch crawler on the official websites of WebDB and JAIR, and <br>identify those papers published between 1998 and 2004 from the <br>downloaded documents. We then compare the number of papers <br>harvested by Nutch and FC crawler (see Figure 14 for details). <br>Results show that guided by certain heuristics, crawling authors' <br>homepages can actually achieve almost the same recall level as <br>crawling publishers' websites. </span></nobr></DIV>
<DIV style="position:absolute;top:878;left:527"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:521"><nobr><span class="ft4">50</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:515"><nobr><span class="ft4">100</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:515"><nobr><span class="ft4">150</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:515"><nobr><span class="ft4">200</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:577"><nobr><span class="ft4">WebDB</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:678"><nobr><span class="ft4">JAIR</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:507"><nobr><span class="ft4">Number of</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:507"><nobr><span class="ft4"> Documents</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:767"><nobr><span class="ft4">Nutch</span></nobr></DIV>
<DIV style="position:absolute;top:813;left:767"><nobr><span class="ft4">FC</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:767"><nobr><span class="ft9">Total<br>Publications</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:482"><nobr><span class="ft3">Figure 14. Comparison between Nutch and Focused Crawler </span></nobr></DIV>
<DIV style="position:absolute;top:954;left:477"><nobr><span class="ft3">Figure 15</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:530"><nobr><span class="ft28"> </span></nobr></DIV>
<DIV style="position:absolute;top:954;left:534"><nobr><span class="ft3">indicates the progress of the crawls conducted by both </span></nobr></DIV>
<DIV style="position:absolute;top:970;left:477"><nobr><span class="ft8">the Focused Crawler and the Nutch Crawler on the ACL <br>conference website. The documents found are of PDF and PS <br>only. The focused crawler starts discovering documents earlier in <br>the crawl and the process continues gradually. Nutch on the other <br>hand discovers most of the documents after crawling around 84% <br>(22 out of 26) of the website. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">308</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft36{font-size:10px;line-height:-10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="215009.png" alt="background image">
<DIV style="position:absolute;top:318;left:160"><nobr><span class="ft3">Figure 6. ACL Conference Crawl</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:129"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:115"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:115"><nobr><span class="ft19">400</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:115"><nobr><span class="ft19">600</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:115"><nobr><span class="ft19">800</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:109"><nobr><span class="ft19">1000</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:109"><nobr><span class="ft19">1200</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:109"><nobr><span class="ft19">1400</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:109"><nobr><span class="ft19">1600</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:109"><nobr><span class="ft19">1800</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:142"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:230"><nobr><span class="ft19">10</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:321"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:413"><nobr><span class="ft19">30</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:243"><nobr><span class="ft19">Pages Crawled</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:101"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:101"><nobr><span class="ft27">m<br>b<br>e<br>r<br> o<br>f<br> <br>Do</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:101"><nobr><span class="ft27">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:209"><nobr><span class="ft19">Nutch</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:209"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:438"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:352;left:125"><nobr><span class="ft3">Figure 15. Crawling ACL Conference Websites </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:81"><nobr><span class="ft8">Documents found during the ACL conference crawl are classified <br>into two categories: relevant (i.e. academic publications) and non-<br>relevant (non-publication). Figure 16 shows the number of <br>documents in each category. Note that determining documents' <br>relevancy is an offline process. Here R indicates relevant and NR <br>indicated non-relevant documents. </span></nobr></DIV>
<DIV style="position:absolute;top:579;left:146"><nobr><span class="ft19">R, 1588</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:278"><nobr><span class="ft19">R, 1404</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:156"><nobr><span class="ft19">NR, 0</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:291"><nobr><span class="ft19">NR, 0</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:128"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:114"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:114"><nobr><span class="ft19">400</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:114"><nobr><span class="ft19">600</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:114"><nobr><span class="ft19">800</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:107"><nobr><span class="ft19">1000</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:107"><nobr><span class="ft19">1200</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:107"><nobr><span class="ft19">1400</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:107"><nobr><span class="ft19">1600</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:107"><nobr><span class="ft19">1800</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:208"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:340"><nobr><span class="ft19">Nutch</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:266"><nobr><span class="ft19">Crawler</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:102"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:102"><nobr><span class="ft27">m<br>b<br>e<br>r<br> o<br>f<br> <br>Do</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:102"><nobr><span class="ft27">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:402"><nobr><span class="ft19">NR</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:402"><nobr><span class="ft19">R</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:438"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:716;left:112"><nobr><span class="ft3">Figure 16. Relevancy of the ACL Conference Crawl  </span></nobr></DIV>
<DIV style="position:absolute;top:741;left:81"><nobr><span class="ft8">Figure 16 indicates that all the documents (PDF and PS) found by <br>both the crawlers are academic publications (thus NR = 0). <br>However, the 184 documents Nutch failed to detect are <br>determined to be all relevant research publications. </span></nobr></DIV>
<DIV style="position:absolute;top:813;left:81"><nobr><span class="ft8">The same comparison is also conducted by crawling the official <br>WebDB conference websites. Figure 17 shows that the Focused <br>Crawler starts detecting desired documents at an earlier stage as <br>compared to the Nutch crawler. Yet due to the small number of <br>pages crawled, a rigorous comparison cannot be made in this case. <br>Figure 18 shows that the focused crawler locates two more <br>academic publications than the Nutch crawler, both of which are <br>marked as relevant documents. </span></nobr></DIV>
<DIV style="position:absolute;top:955;left:81"><nobr><span class="ft1">5.  CONCLUSION AND FUTURE WORK  </span></nobr></DIV>
<DIV style="position:absolute;top:976;left:81"><nobr><span class="ft8">We have shown the feasibility of using authors' homepages as <br>alternative online resources to harvest the academic papers <br>missing from a collection of digital libraries, as well as the <br>techniques to maximize the crawler's performance in doing so. <br>We have designed and implemented a heuristic-based system <br>which utilizes document metadata to accurately locate authors' </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft8">homepages and performs a focused crawling to quickly navigate <br>to the desired publications. Evaluation has been conducted using a <br>large dataset collected from several publishing venues in the <br>Computer Science domain, and detailed results are presented and <br>discussed.  </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:551"><nobr><span class="ft19">Figure 10. WEDB Conference Crawl</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:529"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:522"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:522"><nobr><span class="ft19">40</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:522"><nobr><span class="ft19">60</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:522"><nobr><span class="ft19">80</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:515"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:515"><nobr><span class="ft19">120</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:541"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:605"><nobr><span class="ft19">10</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:671"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:739"><nobr><span class="ft19">30</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:805"><nobr><span class="ft19">40</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:642"><nobr><span class="ft19">Pages Crawled</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:504"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:504"><nobr><span class="ft26">m<br>b<br>e<br>r<br> o<br>f<br> Do</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:504"><nobr><span class="ft27">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:768"><nobr><span class="ft19">Nutch</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:768"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:833"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:433;left:512"><nobr><span class="ft3">Figure 17. Crawling WebDB Conference Websites </span></nobr></DIV>
<DIV style="position:absolute;top:578;left:776"><nobr><span class="ft19">R, 104</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:636"><nobr><span class="ft19">R, 106</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:638"><nobr><span class="ft19">NR, 1</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:779"><nobr><span class="ft19">NR, 1</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:516"><nobr><span class="ft19">0</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:509"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:509"><nobr><span class="ft19">40</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:509"><nobr><span class="ft19">60</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:509"><nobr><span class="ft19">80</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:502"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:502"><nobr><span class="ft19">120</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:596"><nobr><span class="ft19">FC</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:727"><nobr><span class="ft19">Nutch</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:654"><nobr><span class="ft19">Crawler</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:497"><nobr><span class="ft19">Nu</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:497"><nobr><span class="ft26">m<br>b<br>e<br>r<br> o<br>f<br> Do</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:497"><nobr><span class="ft27">c<br>u<br>m<br>e<br>n<br>t<br>s</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:790"><nobr><span class="ft19">NR</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:790"><nobr><span class="ft19">R</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:830"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:698;left:499"><nobr><span class="ft3">Figure 18. Relevancy of the WebDB Conference Crawl  </span></nobr></DIV>
<DIV style="position:absolute;top:723;left:477"><nobr><span class="ft8">For the academic venues investigated in this study, we are able to <br>fill many of the missing documents in the CiteSeer digital library.  </span></nobr></DIV>
<DIV style="position:absolute;top:764;left:477"><nobr><span class="ft8">The designed focused crawling technique efficiently locates <br>desired publications on authors' homepages as well as conference <br>websites. The Homepage Aggregator detects homepages well and <br>the Focused Crawler outperforms the baseline crawler in a <br>number of measures. </span></nobr></DIV>
<DIV style="position:absolute;top:852;left:477"><nobr><span class="ft8">Future work includes a more rigorous disambiguation scheme for <br>the Homepage Aggregator and a more sophisticated weighting <br>scheme for the Focused Crawler. In addition, we are now <br>developing a training process for the crawler to learn the URL <br>patterns of alternative resources other than author homepages, <br>such as institutional archives. Also, the automation of the process <br>cycle of crawling, log analysis, and heuristics generation can help <br>search engine based digital libraries scale and significantly reduce <br>costs. The actual URL of the web pages can also be used to assist <br>in priority assignment instead of just using the anchor text of the <br>link. A comparison of this approach to techniques other than a <br>Breadth-first crawl is currently underway. Furthermore, we plan <br>to evaluate the validity of this approach by expanding our <br>experiment on to disciplines other than the Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">309</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="215010.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">domain. We believe our study and its consequents will shed lights <br>on the question of finding missing papers for our digital library, <br>or "what's there and what's not". </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:81"><nobr><span class="ft1">6.  ACKNOWLEDGEMENTS </span></nobr></DIV>
<DIV style="position:absolute;top:195;left:81"><nobr><span class="ft8">We gratefully acknowledge P. Mitra and the anonymous <br>reviewers for their comments, I. Councill and P. Teregowda for <br>their work on the CiteSeer metadata, and E. Maldonado and D. <br>Hellar for the crawl list. This work is partially supported by <br>Microsoft. </span></nobr></DIV>
<DIV style="position:absolute;top:293;left:81"><nobr><span class="ft1">7.  REFERENCES </span></nobr></DIV>
<DIV style="position:absolute;top:313;left:81"><nobr><span class="ft3">[1]  De Bra, P., Houben, G., Kornatzky, Y., and Post, R </span></nobr></DIV>
<DIV style="position:absolute;top:329;left:108"><nobr><span class="ft8">Information Retrieval in Distributed Hypertexts. In <br>Proceedings of the 4th RIAO (Computer-Assisted Information <br>Retrieval) Conference, pp. 481-491, 1994. </span></nobr></DIV>
<DIV style="position:absolute;top:383;left:81"><nobr><span class="ft3">[2]  Cho J., Garcia-Molina, H., and Page, L. Efficient Crawling </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:108"><nobr><span class="ft8">Through URL Ordering. In Proceedings of the 7th World Wide <br>Web Conference, Brisbane, Australia, pp. 161-172. April 1998. </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:81"><nobr><span class="ft3">[3]  Chakrabarti, S., Van den Berg, M., and Dom, B. Focused </span></nobr></DIV>
<DIV style="position:absolute;top:452;left:108"><nobr><span class="ft8">Crawling: A New Approach to Topic-Specific Web Resource <br>Discovery. In Proceedings of the 8th International WWW <br>Conference, pp. 545-562, Toronto, Canada, May 1999. </span></nobr></DIV>
<DIV style="position:absolute;top:506;left:81"><nobr><span class="ft3">[4]  Giles, C. L. and Councill, I. G. Who gets acknowledged: </span></nobr></DIV>
<DIV style="position:absolute;top:522;left:108"><nobr><span class="ft8">Measuring scientific contributions through automatic <br>acknowledgement indexing. In Proceedings of the National <br>Academy of Sciences 101(51) pp. 17599-17604, Dec. 21, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:81"><nobr><span class="ft3">[5]  Najork, M. and Wiener, J. L. Breadth-First Search Crawling </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:108"><nobr><span class="ft8">Yields High-Quality Pages. In Proceedings of the 10th <br>International World Wide Web Conference, pp. 114-118, 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:629;left:81"><nobr><span class="ft3">[6]  Page, L., Brin, S., Motwani, R., and Winograd, T. The </span></nobr></DIV>
<DIV style="position:absolute;top:645;left:108"><nobr><span class="ft8">pagerank citation ranking: Bringing order to the web. <br>Technical report, Stanford University Database Group, 1998. <br>Available at http://dbpubs.stanford.edu: 8090/pub/1999-66 </span></nobr></DIV>
<DIV style="position:absolute;top:698;left:81"><nobr><span class="ft3">[7]  Menczer, F., Pant, G., Ruiz, M., and Srinivasan, P. Evaluating </span></nobr></DIV>
<DIV style="position:absolute;top:714;left:108"><nobr><span class="ft8">Topic-Driven Web Crawlers.' In Proceedings of the 2001 <br>Annual Conference of the Association of Computing <br>Machinery, Special Interest Group in Information Retrieval, <br>241-249. New Orleans, September 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:783;left:81"><nobr><span class="ft3">[8]  Haveliwala, T. H. Topic-Sensitive PageRank. In Proceedings </span></nobr></DIV>
<DIV style="position:absolute;top:799;left:108"><nobr><span class="ft8">of the 11th International World Wide Web Conference, pp. <br>517-526. Honolulu, Hawaii, USA. May 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:837;left:81"><nobr><span class="ft3">[9]  Mukherjea, S. WTMS: a system for collecting and analyzing </span></nobr></DIV>
<DIV style="position:absolute;top:853;left:108"><nobr><span class="ft8">topic-specific Web information. Computer Networks 33(1-6): <br>457-471, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:890;left:81"><nobr><span class="ft3">[10]  Diligenti, M., Coetzee, F.M., Lawrence, S., Giles, C. L., and </span></nobr></DIV>
<DIV style="position:absolute;top:906;left:108"><nobr><span class="ft8">Gori, M. Focused Crawling Using Context Graphs. In <br>Proceedings of the 26th International Conference on Very <br>Large Data Bases, pp. 527-534, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:960;left:81"><nobr><span class="ft3">[11]  Aggarwal, C. C., Al-Garawi, F., and Yu, P. S. Intelligent </span></nobr></DIV>
<DIV style="position:absolute;top:976;left:108"><nobr><span class="ft8">Crawling on the World Wide Web with Arbitary Predicates. In <br>Proceedings of the Tenth International Conference on World <br>Wide Web, pp. 96-105, 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:81"><nobr><span class="ft3">[12]  Aggarwal, C. C. On Learning Strategies for Topic Specific </span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:108"><nobr><span class="ft8">Web Crawling. Next Generation Data Mining Applications, <br>January 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft3">[13]  Pant, G., Tsjoutsiouliklis, K., Johnson, J., and Giles, C. L. </span></nobr></DIV>
<DIV style="position:absolute;top:124;left:504"><nobr><span class="ft8">Panorama: Extending Digital Libraries with Topical Crawlers. <br>In  Proceedings of the 2004 Joint ACM/IEEE Conference on <br>Digital Libraries, pp. 142-150, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:178;left:477"><nobr><span class="ft3">[14]  Menczer, F., Pant, G., and Srinivasan, P. Topical Web </span></nobr></DIV>
<DIV style="position:absolute;top:194;left:504"><nobr><span class="ft8">Crawlers: Evaluating Adaptive Algorithms. ACM TOIT 4(4): <br>378-419, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:477"><nobr><span class="ft3">[15]  Pant, G., Srinivasan, P., and Menczer, F. Crawling the Web. In </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:504"><nobr><span class="ft8">M. Levene and A. Poulovassilis, eds.: Web Dynamics, Springer, <br>2004. </span></nobr></DIV>
<DIV style="position:absolute;top:285;left:477"><nobr><span class="ft3">[16]  Hoff, G. and Mundhenk, M. Finding scientific papers with </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:504"><nobr><span class="ft8">homepagesearch and MOPS. In Proceedings of the Nineteenth <br>Annual International Conference of Computer Documentation, <br>Communicating in the New Millennium, pp. 201-207. October <br>21-24, 2001, Santa Fe, New Mexico, USA. </span></nobr></DIV>
<DIV style="position:absolute;top:370;left:477"><nobr><span class="ft3">[17]  On, B. and Lee, D. PaSE: Locating Online Copy of Scientific </span></nobr></DIV>
<DIV style="position:absolute;top:386;left:504"><nobr><span class="ft8">Documents Effectively. In Proceedings of the 7th International <br>Conference of Asian Digital Libraries (ICADL), pp. 408-418. <br>Shanghai, China, December 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:440;left:477"><nobr><span class="ft3">[18]  Shakes, J., Langheinrich, M., and Etzioni, O. Dynamic </span></nobr></DIV>
<DIV style="position:absolute;top:456;left:504"><nobr><span class="ft8">Reference Sifting: a Case Study in the Homepage Domain. In <br>Proceedings of the Sixth International World Wide Web <br>Conference, pp. 189-200, 1997. </span></nobr></DIV>
<DIV style="position:absolute;top:509;left:477"><nobr><span class="ft3">[19]  Xi, W. and Fox, E. A. Machine Learning Approach for </span></nobr></DIV>
<DIV style="position:absolute;top:525;left:504"><nobr><span class="ft8">Homepage Finding Task. In Proceedings of  the Tenth Text <br>REtrieval Conference (TREC 2001), pp. 686-698, 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:563;left:477"><nobr><span class="ft3">[20]  Anh, V. N. and Moffat, A. Homepage Finding and Topic </span></nobr></DIV>
<DIV style="position:absolute;top:578;left:504"><nobr><span class="ft8">Distillation using a Common Retrieval Strategy. In <br>Proceedings of the Eleventh Text REtrieval Conference (TREC <br>2002), 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:632;left:477"><nobr><span class="ft3">[21]  Ogilvie, P. and Callan, J. Combining Structural Information </span></nobr></DIV>
<DIV style="position:absolute;top:648;left:504"><nobr><span class="ft8">and the Use of Priors in Mixed Named-Page and Homepage <br>Finding. In Proceedings of  the Twelfth Text REtrieval <br>Conference (TREC 2003), pp. 177-184, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:701;left:477"><nobr><span class="ft3">[22]  Sundaresan, N., Yi, J., and Huang, A. W. Using Metadata to </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:504"><nobr><span class="ft8">Enhance a Web Information Gathering System. In Proceedings <br>of the Third International Workshop on the Web and <br>Databases (WebDB 2000), pp. 11-16, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:771;left:477"><nobr><span class="ft3">[23]  Flesca, S., Furfaro, F., and Greco, S. Weighted Path Queries on </span></nobr></DIV>
<DIV style="position:absolute;top:787;left:504"><nobr><span class="ft8">Web Data. In Proceedings of the Fourth International <br>Workshop on the Web and Databases (WebDB 2001), pp. 7-12, <br>2001. </span></nobr></DIV>
<DIV style="position:absolute;top:840;left:477"><nobr><span class="ft3">[24]  Ruiz, A., López-de-Teruel, P. E., and Garrido, M. C. </span></nobr></DIV>
<DIV style="position:absolute;top:856;left:504"><nobr><span class="ft8">Probabilistic Inference from Arbitrary Uncertainty using <br>Mixtures of Factorized Generalized Gaussians. Journal of <br>Artificial Intelligence Research (JAIR), Volume 9, pp. 167-217, <br>1998. </span></nobr></DIV>
<DIV style="position:absolute;top:925;left:477"><nobr><span class="ft3">[25] Russell, G., Neumüller, M., and Connor, R. C. H. TypEx: A </span></nobr></DIV>
<DIV style="position:absolute;top:941;left:504"><nobr><span class="ft8">Type Based Approach to XML Stream Querying. In <br>Proceedings of the Sixth International Workshop on the Web <br>and Databases (WebDB 2003), pp. 55-60, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">310</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
