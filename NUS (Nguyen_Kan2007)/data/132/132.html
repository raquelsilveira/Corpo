<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\132</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-05-28T20:49:50+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:6px;font-family:Times;color:#000000;}
	.ft2{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:15px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:9px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Courier;color:#000000;}
	.ft9{font-size:11px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft11{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="132001.png" alt="background image">
<DIV style="position:absolute;top:144;left:94"><nobr><span class="ft0"><b>Machine Learning for Information Architecture in a Large</b></span></nobr></DIV>
<DIV style="position:absolute;top:174;left:309"><nobr><span class="ft0"><b>Governmental Website</b></span></nobr></DIV>
<DIV style="position:absolute;top:181;left:606"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:243;left:243"><nobr><span class="ft2">Miles Efron</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:156"><nobr><span class="ft3">School of Information &amp; Library Science</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:194"><nobr><span class="ft3">CB#3360, 100 Manning Hall</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:196"><nobr><span class="ft3">University of North Carolina</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:195"><nobr><span class="ft3">Chapel Hill, NC 27599-3360</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:213"><nobr><span class="ft2">efrom@ils.unc.edu</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:565"><nobr><span class="ft2">Jonathan Elsas</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:495"><nobr><span class="ft3">School of Information &amp; Library Science</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:533"><nobr><span class="ft3">CB#3360, 100 Manning Hall</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:535"><nobr><span class="ft3">University of North Carolina</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:534"><nobr><span class="ft3">Chapel Hill, NC 27599-3360</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:539"><nobr><span class="ft2">jelsas@email.unc.edu</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:220"><nobr><span class="ft2">Gary Marchionini</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:156"><nobr><span class="ft3">School of Information &amp; Library Science</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:194"><nobr><span class="ft3">CB#3360, 100 Manning Hall</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:196"><nobr><span class="ft3">University of North Carolina</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:195"><nobr><span class="ft3">Chapel Hill, NC 27599-3360</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:211"><nobr><span class="ft2">march@ils.unc.edu</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:566"><nobr><span class="ft2">Junliang Zhang</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:495"><nobr><span class="ft3">School of Information &amp; Library Science</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:533"><nobr><span class="ft3">CB#3360, 100 Manning Hall</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:535"><nobr><span class="ft3">University of North Carolina</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:534"><nobr><span class="ft3">Chapel Hill, NC 27599-3360</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:531"><nobr><span class="ft2">junliang@email.unc.edu</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft4"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:538;left:81"><nobr><span class="ft10">This paper describes ongoing research into the application<br>of machine learning techniques for improving access to gov-<br>ernmental information in complex digital libraries. Under<br>the auspices of the GovStat Project, our goal is to identify a<br>small number of semantically valid concepts that adequately<br>spans the intellectual domain of a collection. The goal of this<br>discovery is twofold. First we desire a practical aid for infor-<br>mation architects. Second, automatically derived document-<br>concept relationships are a necessary precondition for real-<br>world deployment of many dynamic interfaces. The current<br>study compares concept learning strategies based on three<br>document representations: keywords, titles, and full-text. In<br>statistical and user-based studies, human-created keywords<br>provide significant improvements in concept learning over<br>both title-only and full-text representations.</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:81"><nobr><span class="ft4"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:819;left:81"><nobr><span class="ft10">H.3.7 [Information Storage and Retrieval]: Digital Li-<br>braries--Systems Issues, User Issues; H.3.3 [Information<br>Storage and Retrieval]: Information Search and Retrieval--<br>Clustering</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:81"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:88"><nobr><span class="ft5">This research was supported by NSF EIA grant 0131824.</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:81"><nobr><span class="ft11">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>JCDL'04, </i>June 7­11, 2004, Tucson, Arizona, USA.<br>Copyright 2004 ACM 1-58113-832-6/04/0006 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:315"><nobr><span class="ft5">$</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:322"><nobr><span class="ft6">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:475"><nobr><span class="ft4"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:538;left:475"><nobr><span class="ft5">Design, Experimentation</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:475"><nobr><span class="ft4"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:600;left:475"><nobr><span class="ft10">Machine Learning, Information Architecture, Interface De-<br>sign</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:475"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:653;left:507"><nobr><span class="ft4"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:675;left:489"><nobr><span class="ft5">The GovStat Project is a joint effort of the University</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:475"><nobr><span class="ft10">of North Carolina Interaction Design Lab and the Univer-<br>sity of Maryland Human-Computer Interaction Lab</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:792"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:798"><nobr><span class="ft5">. Cit-</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:475"><nobr><span class="ft10">ing end-user difficulty in finding governmental information<br>(especially statistical data) online, the project seeks to cre-<br>ate an integrated model of user access to US government<br>statistical information that is rooted in realistic data mod-<br>els and innovative user interfaces. To enable such models<br>and interfaces, we propose a data-driven approach, based<br>on data mining and machine learning techniques. In par-<br>ticular, our work analyzes a particular digital library--the<br>website of the Bureau of Labor Statistics</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:720"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:730"><nobr><span class="ft5">(BLS)--in efforts</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:475"><nobr><span class="ft10">to discover a small number of linguistically meaningful con-<br>cepts, or "bins," that collectively summarize the semantic<br>domain of the site.</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:489"><nobr><span class="ft5">The project goal is to classify the site's web content ac-</span></nobr></DIV>
<DIV style="position:absolute;top:926;left:475"><nobr><span class="ft10">cording to these inferred concepts as an initial step towards<br>data filtering via active user interfaces (cf.</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:756"><nobr><span class="ft5">[13]).</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:800"><nobr><span class="ft5">Many</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:475"><nobr><span class="ft10">digital libraries already make use of content classification,<br>both explicitly and implicitly; they divide their resources<br>manually by topical relation; they organize content into hi-<br>erarchically oriented file systems. The goal of the present</span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:476"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:482"><nobr><span class="ft5">http://www.ils.unc.edu/govstat</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:476"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:482"><nobr><span class="ft5">http://www.bls.gov</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">151</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft12{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="132002.png" alt="background image">
<DIV style="position:absolute;top:122;left:81"><nobr><span class="ft10">research is to develop another means of browsing the content<br>of these collections. By analyzing the distribution of terms<br>across documents, our goal is to supplement the agency's<br>pre-existing information structures. Statistical learning tech-<br>nologies are appealing in this context insofar as they stand<br>to define a data-driven--as opposed to an agency-driven--<br>navigational structure for a site.</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:94"><nobr><span class="ft5">Our approach combines supervised and unsupervised learn-</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:81"><nobr><span class="ft10">ing techniques. A pure document clustering [12] approach<br>to such a large, diverse collection as BLS led to poor results<br>in early tests [6]. But strictly supervised techniques [5] are<br>inappropriate, too. Although BLS designers have defined<br>high-level subject headings for their collections, as we dis-<br>cuss in Section 2, this scheme is less than optimal. Thus we<br>hope to learn an additional set of concepts by letting the<br>data speak for themselves.</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:94"><nobr><span class="ft5">The remainder of this paper describes the details of our</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:81"><nobr><span class="ft10">concept discovery efforts and subsequent evaluation. In Sec-<br>tion 2 we describe the previously existing, human-created<br>conceptual structure of the BLS website. This section also<br>describes evidence that this structure leaves room for im-<br>provement. Next (Sections 3­5), we turn to a description<br>of the concepts derived via content clustering under three<br>document representations: keyword, title only, and full-text.<br>Section 6 describes a two-part evaluation of the derived con-<br>ceptual structures. Finally, we conclude in Section 7 by out-<br>lining upcoming work on the project.</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:81"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:583;left:112"><nobr><span class="ft12"><b>STRUCTURING ACCESS TO THE BLS<br>WEBSITE</b></span></nobr></DIV>
<DIV style="position:absolute;top:626;left:94"><nobr><span class="ft5">The Bureau of Labor Statistics is a federal government</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:81"><nobr><span class="ft10">agency charged with compiling and publishing statistics per-<br>taining to labor and production in the US and abroad. Given<br>this broad mandate, the BLS publishes a wide array of in-<br>formation, intended for diverse audiences.</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:358"><nobr><span class="ft5">The agency's</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:81"><nobr><span class="ft10">website acts as a clearinghouse for this process. With over<br>15,000 text/html documents (and many more documents if<br>spreadsheets and typeset reports are included), providing<br>access to the collection provides a steep challenge to infor-<br>mation architects.</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:81"><nobr><span class="ft4"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:810;left:121"><nobr><span class="ft4"><b>The Relation Browser</b></span></nobr></DIV>
<DIV style="position:absolute;top:832;left:94"><nobr><span class="ft5">The starting point of this work is the notion that access</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:81"><nobr><span class="ft10">to information in the BLS website could be improved by<br>the addition of a dynamic interface such as the relation<br>browser described by Marchionini and Brunk [13]. The re-<br>lation browser allows users to traverse complex data sets by<br>iteratively slicing the data along several topics. In Figure<br>1 we see a prototype instantiation of the relation browser,<br>applied to the FedStats website</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:271"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:277"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:94"><nobr><span class="ft5">The relation browser supports information seeking by al-</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:81"><nobr><span class="ft10">lowing users to form queries in a stepwise fashion, slicing and<br>re-slicing the data as their interests dictate. Its motivation<br>is in keeping with Shneiderman's suggestion that queries<br>and their results should be tightly coupled [2]. Thus in Fig-</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:81"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:88"><nobr><span class="ft5">http://www.fedstats.gov</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:521"><nobr><span class="ft5">Figure 1: Relation Browser Prototype</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:475"><nobr><span class="ft10">ure 1, users might limit their search set to those documents<br>about "energy." Within this subset of the collection, they<br>might further eliminate documents published more than a<br>year ago. Finally, they might request to see only documents<br>published in PDF format.</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:489"><nobr><span class="ft5">As Marchionini and Brunk discuss, capturing the publica-</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:475"><nobr><span class="ft10">tion date and format of documents is trivial. But successful<br>implementations of the relation browser also rely on topical<br>classification. This presents two stumbling blocks for system<br>designers:</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:495"><nobr><span class="ft5">· Information architects must define the appropriate set</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:509"><nobr><span class="ft5">of topics for their collection</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:495"><nobr><span class="ft5">· Site maintainers must classify each document into its</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:509"><nobr><span class="ft5">appropriate categories</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:489"><nobr><span class="ft5">These tasks parallel common problems in the metadata</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:475"><nobr><span class="ft10">community: defining appropriate elements and marking up<br>documents to support metadata-aware information access.<br>Given a collection of over 15,000 documents, these hur-<br>dles are especially daunting, and automatic methods of ap-<br>proaching them are highly desirable.</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:475"><nobr><span class="ft4"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:948;left:516"><nobr><span class="ft4"><b>A Pre-Existing Structure</b></span></nobr></DIV>
<DIV style="position:absolute;top:970;left:489"><nobr><span class="ft5">Prior to our involvement with the project, designers at</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:475"><nobr><span class="ft10">BLS created a shallow classificatory structure for the most<br>important documents in their website. As seen in Figure 2,<br>the BLS home page organizes 65 "top-level" documents into<br>15 categories. These include topics such as Employment and<br>Unemployment, Productivity, and Inflation and Spending.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">152</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:10px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="132003.png" alt="background image">
<DIV style="position:absolute;top:516;left:151"><nobr><span class="ft5">Figure 2: The BLS Home Page</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:94"><nobr><span class="ft5">We hoped initially that these pre-defined categories could</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:81"><nobr><span class="ft10">be used to train a 15-way document classifier, thus automat-<br>ing the process of populating the relation browser altogether.<br>However, this approach proved unsatisfactory. In personal<br>meetings, BLS officials voiced dissatisfaction with the ex-<br>isting topics. Their form, it was argued, owed as much to<br>the institutional structure of BLS as it did to the inherent<br>topology of the website's information space. In other words,<br>the topics reflected official divisions rather than semantic<br>clusters. The BLS agents suggested that re-designing this<br>classification structure would be desirable.</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:94"><nobr><span class="ft5">The agents' misgivings were borne out in subsequent anal-</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:81"><nobr><span class="ft10">ysis. The BLS topics comprise a shallow classificatory struc-<br>ture; each of the 15 top-level categories is linked to a small<br>number of related pages. Thus there are 7 pages associated<br>with Inflation. Altogether, the link structure of this clas-<br>sificatory system contains 65 documents; that is, excluding<br>navigational links, there are 65 documents linked from the<br>BLS home page, where each hyperlink connects a document<br>to a topic (pages can be linked to multiple topics). Based on<br>this hyperlink structure, we defined M, a symmetric 65 × 65<br>matrix, where m</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:180"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:193"><nobr><span class="ft5">counts the number of topics in which doc-</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:81"><nobr><span class="ft10">uments i and j are both classified on the BLS home page. To<br>analyze the redundancy inherent in the pre-existing struc-<br>ture, we derived the principal components of M (cf. [11]).<br>Figure 3 shows the resultant scree plot</span></nobr></DIV>
<DIV style="position:absolute;top:956;left:315"><nobr><span class="ft1">4</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:321"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:94"><nobr><span class="ft5">Because all 65 documents belong to at least one BLS topic,</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:81"><nobr><span class="ft1">4</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:88"><nobr><span class="ft5">A scree plot shows the magnitude of the k</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:360"><nobr><span class="ft1">th</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:378"><nobr><span class="ft5">eigenvalue</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:81"><nobr><span class="ft14">versus its rank. During principal component analysis scree<br>plots visualize the amount of variance captured by each com-<br>ponent.</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:530"><nobr><span class="ft13">0</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:570"><nobr><span class="ft13">10</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:613"><nobr><span class="ft13">20</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:657"><nobr><span class="ft13">30</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:700"><nobr><span class="ft13">40</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:744"><nobr><span class="ft13">50</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:787"><nobr><span class="ft13">60</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:513"><nobr><span class="ft13">0</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:513"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:513"><nobr><span class="ft13">4</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:513"><nobr><span class="ft13">6</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:513"><nobr><span class="ft13">8</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:513"><nobr><span class="ft13">10</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:513"><nobr><span class="ft13">12</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:513"><nobr><span class="ft13">14</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:629"><nobr><span class="ft13">Eigenvalue Rank</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:488"><nobr><span class="ft13">Eigenvlue Magnitude</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:518"><nobr><span class="ft5">Figure 3: Scree Plot of BLS Categories</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:475"><nobr><span class="ft10">the rank of M is guaranteed to be less than or equal to<br>15 (hence, eigenvalues 16 . . . 65 = 0). What is surprising<br>about Figure 3, however, is the precipitous decline in mag-<br>nitude among the first four eigenvalues. The four largest<br>eigenvlaues account for 62.2% of the total variance in the<br>data. This fact suggests a high degree of redundancy among<br>the topics. Topical redundancy is not in itself problematic.<br>However, the documents in this very shallow classificatory<br>structure are almost all gateways to more specific informa-<br>tion. Thus the listing of the Producer Price Index under<br>three categories could be confusing to the site's users. In<br>light of this potential for confusion and the agency's own re-<br>quest for redesign, we undertook the task of topic discovery<br>described in the following sections.</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:475"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:817;left:507"><nobr><span class="ft12"><b>A HYBRID APPROACH TO TOPIC<br>DISCOVERY</b></span></nobr></DIV>
<DIV style="position:absolute;top:860;left:489"><nobr><span class="ft5">To aid in the discovery of a new set of high-level topics for</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:475"><nobr><span class="ft10">the BLS website, we turned to unsupervised machine learn-<br>ing methods. In efforts to let the data speak for themselves,<br>we desired a means of concept discovery that would be based<br>not on the structure of the agency, but on the content of the<br>material. To begin this process, we crawled the BLS web-<br>site, downloading all documents of MIME type text/html.<br>This led to a corpus of 15,165 documents. Based on this<br>corpus, we hoped to derive k  10 topical categories, such<br>that each document d</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:607"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:617"><nobr><span class="ft5">is assigned to one or more classes.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">153</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="132004.png" alt="background image">
<DIV style="position:absolute;top:122;left:94"><nobr><span class="ft5">Document clustering (cf. [16]) provided an obvious, but</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:81"><nobr><span class="ft10">only partial solution to the problem of automating this type<br>of high-level information architecture discovery. The prob-<br>lems with standard clustering are threefold.</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:97"><nobr><span class="ft5">1. Mutually exclusive clusters are inappropriate for iden-</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:114"><nobr><span class="ft10">tifying the topical content of documents, since docu-<br>ments may be about many subjects.</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:97"><nobr><span class="ft5">2. Due to the heterogeneity of the data housed in the</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:114"><nobr><span class="ft10">BLS collection (tables, lists, surveys, etc.), many doc-<br>uments' terms provide noisy topical information.</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:97"><nobr><span class="ft5">3. For application to the relation browser, we require a</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:114"><nobr><span class="ft10">small number (k  10) of topics. Without significant<br>data reduction, term-based clustering tends to deliver<br>clusters at too fine a level of granularity.</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:94"><nobr><span class="ft5">In light of these problems, we take a hybrid approach to</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:81"><nobr><span class="ft5">topic discovery.</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:188"><nobr><span class="ft5">First, we limit the clustering process to</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:81"><nobr><span class="ft10">a sample of the entire collection, described in Section 4.<br>Working on a focused subset of the data helps to overcome<br>problems two and three, listed above. To address the prob-<br>lem of mutual exclusivity, we combine unsupervised with<br>supervised learning methods, as described in Section 5.</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:81"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:112"><nobr><span class="ft12"><b>FOCUSING ON CONTENT-RICH<br>DOCUMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:94"><nobr><span class="ft5">To derive empirically evidenced topics we initially turned</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:81"><nobr><span class="ft10">to cluster analysis. Let A be the n×p data matrix with n ob-<br>servations in p variables. Thus a</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:281"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:295"><nobr><span class="ft5">shows the measurement</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:81"><nobr><span class="ft5">for the i</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:134"><nobr><span class="ft1">th</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:152"><nobr><span class="ft5">observation on the j</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:280"><nobr><span class="ft1">th</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:298"><nobr><span class="ft5">variable. As described</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:81"><nobr><span class="ft10">in [12], the goal of cluster analysis is to assign each of the<br>n observations to one of a small number k groups, each of<br>which is characterized by high intra-cluster correlation and<br>low inter-cluster correlation. Though the algorithms for ac-<br>complishing such an arrangement are legion, our analysis<br>focuses on k-means clustering</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:258"><nobr><span class="ft1">5</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:265"><nobr><span class="ft5">, during which, each observa-</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:81"><nobr><span class="ft5">tion o</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:115"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:124"><nobr><span class="ft5">is assigned to the cluster C</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:284"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:294"><nobr><span class="ft5">whose centroid is closest</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:81"><nobr><span class="ft10">to it, in terms of Euclidean distance. Readers interested in<br>the details of the algorithm are referred to [12] for a thor-<br>ough treatment of the subject.</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:94"><nobr><span class="ft5">Clustering by k-means is well-studied in the statistical</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:81"><nobr><span class="ft10">literature, and has shown good results for text analysis (cf.<br>[8, 16]). However, k-means clustering requires that the re-<br>searcher specify k, the number of clusters to define. When<br>applying k-means to our 15,000 document collection, in-<br>dicators such as the gap statistic [17] and an analysis of<br>the mean-squared distance across values of k suggested that<br>k  80 was optimal. This paramterization led to semanti-<br>cally intelligible clusters. However, 80 clusters are far too<br>many for application to an interface such as the relation</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:81"><nobr><span class="ft1">5</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:88"><nobr><span class="ft5">We have focused on k-means as opposed to other clustering</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:81"><nobr><span class="ft5">algorithms for several reasons.</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:282"><nobr><span class="ft5">Chief among these is the</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:81"><nobr><span class="ft14">computational efficiency enjoyed by the k-means approach.<br>Because we need only a flat clustering there is little to be<br>gained by the more expensive hierarchical algorithms. In<br>future work we will turn to model-based clustering [7] as a<br>more principled method of selecting the number of clusters<br>and of representing clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:475"><nobr><span class="ft10">browser. Moreover, the granularity of these clusters was un-<br>suitably fine. For instance, the 80-cluster solution derived<br>a cluster whose most highly associated words (in terms of<br>log-odds ratio [1]) were drug, pharmacy, and chemist. These<br>words are certainly related, but they are related at a level<br>of specificity far below what we sought.</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:489"><nobr><span class="ft5">To remedy the high dimensionality of the data, we re-</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:475"><nobr><span class="ft10">solved to limit the algorithm to a subset of the collection.<br>In consultation with employees of the BLS, we continued<br>our analysis on documents that form a series titled From<br>the Editor's Desk</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:580"><nobr><span class="ft1">6</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:587"><nobr><span class="ft5">. These are brief articles, written by BLS</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:475"><nobr><span class="ft10">employees. BLS agents suggested that we focus on the Ed-<br>itor's Desk because it is intended to span the intellectual<br>domain of the agency. The column is published daily, and<br>each entry describes an important current issue in the BLS<br>domain. The Editor's Desk column has been written daily<br>(five times per week) since 1998. As such, we operated on a<br>set of N = 1279 documents.</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:489"><nobr><span class="ft5">Limiting attention to these 1279 documents not only re-</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:475"><nobr><span class="ft10">duced the dimensionality of the problem. It also allowed<br>the clustering process to learn on a relatively clean data set.<br>While the entire BLS collection contains a great deal of non-<br>prose text (i.e. tables, lists, etc.), the Editor's Desk docu-<br>ments are all written in clear, journalistic prose. Each docu-<br>ment is highly topical, further aiding the discovery of term-<br>topic relations. Finally, the Editor's Desk column provided<br>an ideal learning environment because it is well-supplied<br>with topical metadata. Each of the 1279 documents con-<br>tains a list of one or more keywords. Additionally, a subset<br>of the documents (1112) contained a subject heading. This<br>metadata informed our learning and evaluation, as described<br>in Section 6.1.</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:475"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:645;left:507"><nobr><span class="ft12"><b>COMBINING SUPERVISED AND<br>UNSUPERVISED LEARNING FOR TOPIC<br>DISCOVERY</b></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:489"><nobr><span class="ft5">To derive suitably general topics for the application of a</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:475"><nobr><span class="ft10">dynamic interface to the BLS collection, we combined doc-<br>ument clustering with text classification techniques. Specif-<br>ically, using k-means, we clustered each of the 1279 docu-<br>ments into one of k clusters, with the number of clusters<br>chosen by analyzing the within-cluster mean squared dis-<br>tance at different values of k (see Section 6.1). Construct-<br>ing mutually exclusive clusters violates our assumption that<br>documents may belong to multiple classes. However, these<br>clusters mark only the first step in a two-phase process of<br>topic identification. At the end of the process, document-<br>cluster affinity is measured by a real-valued number.</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:489"><nobr><span class="ft5">Once the Editor's Desk documents were assigned to clus-</span></nobr></DIV>
<DIV style="position:absolute;top:912;left:475"><nobr><span class="ft10">ters, we constructed a k-way classifier that estimates the<br>strength of evidence that a new document d</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:748"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:758"><nobr><span class="ft5">is a member</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:475"><nobr><span class="ft5">of class C</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:535"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:542"><nobr><span class="ft5">. We tested three statistical classification tech-</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:475"><nobr><span class="ft10">niques: probabilistic Rocchio (prind), naive Bayes, and sup-<br>port vector machines (SVMs). All were implemented using<br>McCallum's BOW text classification library [14]. Prind is a<br>probabilistic version of the Rocchio classification algorithm<br>[9]. Interested readers are referred to Joachims' article for</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:476"><nobr><span class="ft1">6</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:482"><nobr><span class="ft5">http://www.bls.gov/opub/ted</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">154</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="132005.png" alt="background image">
<DIV style="position:absolute;top:122;left:81"><nobr><span class="ft10">further details of the classification method. Like prind, naive<br>Bayes attempts to classify documents into the most proba-<br>ble class. It is described in detail in [15]. Finally, support<br>vector machines were thoroughly explicated by Vapnik [18],<br>and applied specifically to text in [10]. They define a deci-<br>sion boundary by finding the maximally separating hyper-<br>plane in a high-dimensional vector space in which document<br>classes become linearly separable.</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:94"><nobr><span class="ft5">Having clustered the documents and trained a suitable</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:81"><nobr><span class="ft10">classifier, the remaining 14,000 documents in the collection<br>are labeled by means of automatic classification. That is, for<br>each document d</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:184"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:195"><nobr><span class="ft5">we derive a k-dimensional vector, quan-</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:81"><nobr><span class="ft5">tifying the association between d</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:281"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:290"><nobr><span class="ft5">and each class C</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:392"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:400"><nobr><span class="ft5">. . . C</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:429"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:436"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:81"><nobr><span class="ft10">Deriving topic scores via naive Bayes for the entire 15,000-<br>document collection required less than two hours of CPU<br>time. The output of this process is a score for every docu-<br>ment in the collection on each of the automatically discov-<br>ered topics. These scores may then be used to populate a<br>relation browser interface, or they may be added to a tradi-<br>tional information retrieval system. To use these weights in<br>the relation browser we currently assign to each document<br>the two topics on which it scored highest. In future work we<br>will adopt a more rigorous method of deriving document-<br>topic weight thresholds. Also, evaluation of the utility of<br>the learned topics for users will be undertaken.</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:81"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:565;left:112"><nobr><span class="ft12"><b>EVALUATION OF CONCEPT<br>DISCOVERY</b></span></nobr></DIV>
<DIV style="position:absolute;top:608;left:94"><nobr><span class="ft5">Prior to implementing a relation browser interface and</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:81"><nobr><span class="ft10">undertaking the attendant user studies, it is of course im-<br>portant to evaluate the quality of the inferred concepts, and<br>the ability of the automatic classifier to assign documents<br>to the appropriate subjects. To evaluate the success of the<br>two-stage approach described in Section 5, we undertook<br>two experiments. During the first experiment we compared<br>three methods of document representation for the cluster-<br>ing task. The goal here was to compare the quality of doc-<br>ument clusters derived by analysis of full-text documents,<br>documents represented only by their titles, and documents<br>represented by human-created keyword metadata. During<br>the second experiment, we analyzed the ability of the statis-<br>tical classifiers to discern the subject matter of documents<br>from portions of the database in addition to the Editor's<br>Desk.</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:81"><nobr><span class="ft4"><b>6.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:901;left:121"><nobr><span class="ft4"><b>Comparing Document Representations</b></span></nobr></DIV>
<DIV style="position:absolute;top:923;left:94"><nobr><span class="ft5">Documents from The Editor's Desk column came sup-</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:81"><nobr><span class="ft10">plied with human-generated keyword metadata. Addition-<br>ally, The titles of the Editor's Desk documents tend to be<br>germane to the topic of their respective articles. With such<br>an array of distilled evidence of each document's subject<br>matter, we undertook a comparison of document represen-<br>tations for topic discovery by clustering. We hypothesized<br>that keyword-based clustering would provide a useful model.<br>But we hoped to see whether comparable performance could</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:475"><nobr><span class="ft10">be attained by methods that did not require extensive hu-<br>man indexing, such as the title-only or full-text representa-<br>tions. To test this hypothesis, we defined three modes of<br>document representation--full-text, title-only, and keyword<br>only--we generated three sets of topics, T</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:739"><nobr><span class="ft1">f ull</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:760"><nobr><span class="ft5">, T</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:779"><nobr><span class="ft1">title</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:801"><nobr><span class="ft5">, and</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:475"><nobr><span class="ft5">T</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:483"><nobr><span class="ft1">kw</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:498"><nobr><span class="ft5">, respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:489"><nobr><span class="ft5">Topics based on full-text documents were derived by appli-</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:475"><nobr><span class="ft10">cation of k-means clustering to the 1279 Editor's Desk doc-<br>uments, where each document was represented by a 1908-<br>dimensional vector.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:606"><nobr><span class="ft5">These 1908 dimensions captured the</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:475"><nobr><span class="ft5">TF.IDF weights [3] of each term t</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:684"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:694"><nobr><span class="ft5">in document d</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:782"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:788"><nobr><span class="ft5">, for all</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:475"><nobr><span class="ft10">terms that occurred at least three times in the data. To ar-<br>rive at the appropriate number of clusters for these data, we<br>inspected the within-cluster mean-squared distance for each<br>value of k = 1 . . . 20. As k approached 10 the reduction in<br>error with the addition of more clusters declined notably,<br>suggesting that k  10 would yield good divisions. To se-<br>lect a single integer value, we calculated which value of k led<br>to the least variation in cluster size. This metric stemmed<br>from a desire to suppress the common result where one large<br>cluster emerges from the k-means algorithm, accompanied<br>by several accordingly small clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:719"><nobr><span class="ft5">Without reason to</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:475"><nobr><span class="ft10">believe that any single topic should have dramatically high<br>prior odds of document membership, this heuristic led to<br>k</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:482"><nobr><span class="ft1">f ull</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:507"><nobr><span class="ft5">= 10.</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:489"><nobr><span class="ft5">Clusters based on document titles were constructed simi-</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:475"><nobr><span class="ft10">larly. However, in this case, each document was represented<br>in the vector space spanned by the 397 terms that occur<br>at least twice in document titles. Using the same method<br>of minimizing the variance in cluster membership k</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:786"><nobr><span class="ft1">title</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:808"><nobr><span class="ft5">­the</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:475"><nobr><span class="ft10">number of clusters in the title-based representation­was also<br>set to 10.</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:489"><nobr><span class="ft5">The dimensionality of the keyword-based clustering was</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:475"><nobr><span class="ft10">very similar to that of the title-based approach. There were<br>299 keywords in the data, all of which were retained. The<br>median number of keywords per document was 7, where a<br>keyword is understood to be either a single word, or a multi-<br>word term such as "consumer price index." It is worth noting<br>that the keywords were not drawn from any controlled vo-<br>cabulary; they were assigned to documents by publishers at<br>the BLS. Using the keywords, the documents were clustered<br>into 10 classes.</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:489"><nobr><span class="ft5">To evaluate the clusters derived by each method of docu-</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:475"><nobr><span class="ft10">ment representation, we used the subject headings that were<br>included with 1112 of the Editor's Desk documents. Each<br>of these 1112 documents was assigned one or more subject<br>headings, which were withheld from all of the cluster appli-<br>cations. Like the keywords, subject headings were assigned<br>to documents by BLS publishers. Unlike the keywords, how-<br>ever, subject headings were drawn from a controlled vocab-<br>ulary. Our analysis began with the assumption that doc-<br>uments with the same subject headings should cluster to-<br>gether. To facilitate this analysis, we took a conservative<br>approach; we considered multi-subject classifications to be<br>unique. Thus if document d</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:648"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:657"><nobr><span class="ft5">was assigned to a single sub-</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:475"><nobr><span class="ft5">ject prices, while document d</span></nobr></DIV>
<DIV style="position:absolute;top:990;left:650"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:659"><nobr><span class="ft5">was assigned to two subjects,</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:475"><nobr><span class="ft5">international comparisons, prices, documents d</span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:761"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:771"><nobr><span class="ft5">and d</span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:805"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:815"><nobr><span class="ft5">are</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:475"><nobr><span class="ft5">not considered to come from the same class.</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:489"><nobr><span class="ft5">Table 1 shows all Editor's Desk subject headings that were</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:475"><nobr><span class="ft5">assigned to at least 10 documents. As noted in the table,</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">155</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="132006.png" alt="background image">
<DIV style="position:absolute;top:132;left:102"><nobr><span class="ft5">Table 1: Top Editor's Desk Subject Headings</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:134"><nobr><span class="ft5">Subject</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:349"><nobr><span class="ft5">Count</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:134"><nobr><span class="ft5">prices</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:373"><nobr><span class="ft5">92</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:134"><nobr><span class="ft5">unemployment</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:373"><nobr><span class="ft5">55</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:134"><nobr><span class="ft5">occupational safety &amp; health</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:373"><nobr><span class="ft5">53</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:134"><nobr><span class="ft5">international comparisons, prices</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:373"><nobr><span class="ft5">48</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:134"><nobr><span class="ft5">manufacturing, prices</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:373"><nobr><span class="ft5">45</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:134"><nobr><span class="ft5">employment</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:373"><nobr><span class="ft5">44</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:134"><nobr><span class="ft5">productivity</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:373"><nobr><span class="ft5">40</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:134"><nobr><span class="ft5">consumer expenditures</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:373"><nobr><span class="ft5">36</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:134"><nobr><span class="ft5">earnings &amp; wages</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:373"><nobr><span class="ft5">27</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:134"><nobr><span class="ft5">employment &amp; unemployment</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:373"><nobr><span class="ft5">27</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:134"><nobr><span class="ft5">compensation costs</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:373"><nobr><span class="ft5">25</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:134"><nobr><span class="ft5">earnings &amp; wages, metro. areas</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:373"><nobr><span class="ft5">18</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:134"><nobr><span class="ft5">benefits, compensation costs</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:373"><nobr><span class="ft5">18</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:134"><nobr><span class="ft5">earnings &amp; wages, occupations</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:373"><nobr><span class="ft5">17</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:134"><nobr><span class="ft5">employment, occupations</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:373"><nobr><span class="ft5">14</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:134"><nobr><span class="ft5">benefits</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:373"><nobr><span class="ft5">14</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:134"><nobr><span class="ft5">earnings &amp; wage, regions</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:373"><nobr><span class="ft5">13</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:134"><nobr><span class="ft5">work stoppages</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:373"><nobr><span class="ft5">12</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:134"><nobr><span class="ft5">earnings &amp; wages, industries</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:373"><nobr><span class="ft5">11</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:134"><nobr><span class="ft5">Total</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:366"><nobr><span class="ft5">609</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:81"><nobr><span class="ft5">Table 2:</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:155"><nobr><span class="ft5">Contingecy Table for Three Document</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:81"><nobr><span class="ft5">Representations</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:126"><nobr><span class="ft5">Representation</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:232"><nobr><span class="ft5">Right</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:282"><nobr><span class="ft5">Wrong</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:339"><nobr><span class="ft5">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:126"><nobr><span class="ft5">Full-text</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:238"><nobr><span class="ft5">392</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:292"><nobr><span class="ft5">217</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:370"><nobr><span class="ft5">0.64</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:126"><nobr><span class="ft5">Title</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:238"><nobr><span class="ft5">441</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:292"><nobr><span class="ft5">168</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:370"><nobr><span class="ft5">0.72</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:126"><nobr><span class="ft5">Keyword</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:238"><nobr><span class="ft5">601</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:299"><nobr><span class="ft5">8</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:370"><nobr><span class="ft5">0.98</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:81"><nobr><span class="ft10">there were 19 such subject headings, which altogether cov-<br>ered 609 (54%) of the documents with subjects assigned.<br>These document-subject pairings formed the basis of our<br>analysis. Limiting analysis to subjects with N &gt; 10 kept<br>the resultant </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:171"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:182"><nobr><span class="ft5">tests suitably robust.</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:94"><nobr><span class="ft5">The clustering derived by each document representation</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:81"><nobr><span class="ft10">was tested by its ability to collocate documents with the<br>same subjects. Thus for each of the 19 subject headings<br>in Table 1, S</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:162"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:166"><nobr><span class="ft5">, we calculated the proportion of documents</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:81"><nobr><span class="ft5">assigned to S</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:164"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:175"><nobr><span class="ft5">that each clustering co-classified. Further,</span></nobr></DIV>
<DIV style="position:absolute;top:813;left:81"><nobr><span class="ft10">we assumed that whichever cluster captured the majority of<br>documents for a given class constituted the "right answer"<br>for that class. For instance, There were 92 documents whose<br>subject heading was prices. Taking the BLS editors' classi-<br>fications as ground truth, all 92 of these documents should<br>have ended up in the same cluster. Under the full-text repre-<br>sentation 52 of these documents were clustered into category<br>5, while 35 were in category 3, and 5 documents were in cat-<br>egory 6. Taking the majority cluster as the putative right<br>home for these documents, we consider the accuracy of this<br>clustering on this subject to be 52/92 = 0.56. Repeating<br>this process for each topic across all three representations<br>led to the contingency table shown in Table 2.</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:94"><nobr><span class="ft5">The obvious superiority of the keyword-based clustering</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:81"><nobr><span class="ft5">evidenced by Table 2 was borne out by a </span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:358"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:370"><nobr><span class="ft5">test on the</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:81"><nobr><span class="ft5">accuracy proportions. Comparing the proportion right and</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:537"><nobr><span class="ft5">Table 3: Keyword-Based Clusters</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:483"><nobr><span class="ft5">benefits</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:569"><nobr><span class="ft5">costs</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:668"><nobr><span class="ft5">international</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:773"><nobr><span class="ft5">jobs</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:483"><nobr><span class="ft5">plans</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:569"><nobr><span class="ft5">compensation</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:668"><nobr><span class="ft5">import</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:773"><nobr><span class="ft5">employment</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:483"><nobr><span class="ft5">benefits</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:569"><nobr><span class="ft5">costs</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:668"><nobr><span class="ft5">prices</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:773"><nobr><span class="ft5">jobs</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:483"><nobr><span class="ft5">employees</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:569"><nobr><span class="ft5">benefits</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:668"><nobr><span class="ft5">petroleum</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:773"><nobr><span class="ft5">youth</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:483"><nobr><span class="ft5">occupations</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:569"><nobr><span class="ft5">prices</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:668"><nobr><span class="ft5">productivity</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:773"><nobr><span class="ft5">safety</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:483"><nobr><span class="ft5">workers</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:569"><nobr><span class="ft5">prices</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:668"><nobr><span class="ft5">productivity</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:773"><nobr><span class="ft5">safety</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:483"><nobr><span class="ft5">earnings</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:569"><nobr><span class="ft5">index</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:668"><nobr><span class="ft5">output</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:773"><nobr><span class="ft5">health</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:483"><nobr><span class="ft5">operators</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:569"><nobr><span class="ft5">inflation</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:668"><nobr><span class="ft5">nonfarm</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:773"><nobr><span class="ft5">occupational</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:569"><nobr><span class="ft5">spending</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:668"><nobr><span class="ft5">unemployment</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:569"><nobr><span class="ft5">expenditures</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:668"><nobr><span class="ft5">unemployment</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:569"><nobr><span class="ft5">consumer</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:668"><nobr><span class="ft5">mass</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:569"><nobr><span class="ft5">spending</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:668"><nobr><span class="ft5">jobless</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:475"><nobr><span class="ft10">wrong achieved by keyword and title-based clustering led to<br>p</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:504"><nobr><span class="ft5">0.001. Due to this result, in the remainder of this paper,</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:475"><nobr><span class="ft10">we focus our attention on the clusters derived by analysis of<br>the Editor's Desk keywords. The ten keyword-based clusters<br>are shown in Table 3, represented by the three terms most<br>highly associated with each cluster, in terms of the log-odds<br>ratio. Additionally, each cluster has been given a label by<br>the researchers.</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:489"><nobr><span class="ft5">Evaluating the results of clustering is notoriously difficult.</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:475"><nobr><span class="ft10">In order to lend our analysis suitable rigor and utility, we<br>made several simplifying assumptions. Most problematic is<br>the fact that we have assumed that each document belongs<br>in only a single category. This assumption is certainly false.<br>However, by taking an extremely rigid view of what con-<br>stitutes a subject--that is, by taking a fully qualified and<br>often multipart subject heading as our unit of analysis--we<br>mitigate this problem. Analogically, this is akin to consid-<br>ering the location of books on a library shelf. Although a<br>given book may cover many subjects, a classification system<br>should be able to collocate books that are extremely similar,<br>say books about occupational safety and health. The most<br>serious liability with this evaluation, then, is the fact that<br>we have compressed multiple subject headings, say prices :<br>international into single subjects. This flattening obscures<br>the multivalence of documents. We turn to a more realistic<br>assessment of document-class relations in Section 6.2.</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:475"><nobr><span class="ft4"><b>6.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:807;left:516"><nobr><span class="ft4"><b>Accuracy of the Document Classifiers</b></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:489"><nobr><span class="ft5">Although the keyword-based clusters appear to classify</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:475"><nobr><span class="ft10">the Editor's Desk documents very well, their discovery only<br>solved half of the problem required for the successful im-<br>plementation of a dynamic user interface such as the re-<br>lation browser. The matter of roughly fourteen thousand<br>unclassified documents remained to be addressed. To solve<br>this problem, we trained the statistical classifiers described<br>above in Section 5.</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:608"><nobr><span class="ft5">For each document in the collection</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:475"><nobr><span class="ft5">d</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:482"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:487"><nobr><span class="ft5">, these classifiers give p</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:627"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:631"><nobr><span class="ft5">, a k-vector of probabilities or dis-</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:475"><nobr><span class="ft10">tances (depending on the classification method used), where<br>p</span></nobr></DIV>
<DIV style="position:absolute;top:991;left:482"><nobr><span class="ft1">ik</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:500"><nobr><span class="ft5">quantifies the strength of association between the i</span></nobr></DIV>
<DIV style="position:absolute;top:983;left:822"><nobr><span class="ft1">th</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:475"><nobr><span class="ft5">document and the k</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:599"><nobr><span class="ft1">th</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:615"><nobr><span class="ft5">class. All classifiers were trained on</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:475"><nobr><span class="ft10">the full text of each document, regardless of the represen-<br>tation used to discover the initial clusters. The different<br>training sets were thus constructed simply by changing the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">156</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="132007.png" alt="background image">
<DIV style="position:absolute;top:132;left:86"><nobr><span class="ft5">Table 4: Cross Validation Results for 3 Classifiers</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:129"><nobr><span class="ft5">Method</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:218"><nobr><span class="ft5">Av. Percent Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:373"><nobr><span class="ft5">SE</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:129"><nobr><span class="ft5">Prind</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:319"><nobr><span class="ft5">59.07</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:367"><nobr><span class="ft5">1.07</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:129"><nobr><span class="ft5">Naive Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:319"><nobr><span class="ft5">75.57</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:374"><nobr><span class="ft5">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:129"><nobr><span class="ft5">SVM</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:319"><nobr><span class="ft5">75.08</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:367"><nobr><span class="ft5">0.68</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:81"><nobr><span class="ft10">class variable for each instance (document) to reflect its as-<br>signed cluster under a given model.</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:94"><nobr><span class="ft5">To test the ability of each classifier to locate documents</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:81"><nobr><span class="ft10">correctly, we first performed a 10-fold cross validation on<br>the Editor's Desk documents. During cross-validation the<br>data are split randomly into n subsets (in this case n = 10).<br>The process proceeds by iteratively holding out each of the<br>n subsets as a test collection for a model trained on the<br>remaining n - 1 subsets. Cross validation is described in<br>[15]. Using this methodology, we compared the performance<br>of the three classification models described above. Table 4<br>gives the results from cross validation.</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:94"><nobr><span class="ft5">Although naive Bayes is not significantly more accurate</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:81"><nobr><span class="ft10">for these data than the SVM classifier, we limit the remain-<br>der of our attention to analysis of its performance.</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:416"><nobr><span class="ft5">Our</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:81"><nobr><span class="ft10">selection of naive Bayes is due to the fact that it appears to<br>work comparably to the SVM approach for these data, while<br>being much simpler, both in theory and implementation.</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:94"><nobr><span class="ft5">Because we have only 1279 documents and 10 classes, the</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:81"><nobr><span class="ft10">number of training documents per class is relatively small.<br>In addition to models fitted to the Editor's Desk data, then,<br>we constructed a fourth model, supplementing the training<br>sets of each class by querying the Google search engine</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:407"><nobr><span class="ft1">7</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:417"><nobr><span class="ft5">and</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft10">applying naive Bayes to the augmented test set. For each<br>class, we created a query by submitting the three terms<br>with the highest log-odds ratio with that class. Further,<br>each query was limited to the domain www.bls.gov.</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:419"><nobr><span class="ft5">For</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:81"><nobr><span class="ft10">each class we retrieved up to 400 documents from Google<br>(the actual number varied depending on the size of the re-<br>sult set returned by Google).</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:277"><nobr><span class="ft5">This led to a training set</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:81"><nobr><span class="ft10">of 4113 documents in the "augmented model," as we call<br>it below</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:130"><nobr><span class="ft1">8</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:136"><nobr><span class="ft5">. Cross validation suggested that the augmented</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:81"><nobr><span class="ft10">model decreased classification accuracy (accuracy= 58.16%,<br>with standard error= 0.32). As we discuss below, however,<br>augmenting the training set appeared to help generalization<br>during our second experiment.</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:94"><nobr><span class="ft5">The results of our cross validation experiment are encour-</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:81"><nobr><span class="ft10">aging. However, the success of our classifiers on the Editor's<br>Desk documents that informed the cross validation study<br>may not be good predictors of the models' performance on<br>the remainder to the BLS website. To test the generality<br>of the naive Bayes classifier, we solicited input from 11 hu-<br>man judges who were familiar with the BLS website. The<br>sample was chosen by convenience, and consisted of faculty<br>and graduate students who work on the GovStat project.<br>However, none of the reviewers had prior knowledge of the<br>outcome of the classification before their participation. For<br>the experiment, a random sample of 100 documents was<br>drawn from the entire BLS collection. On average each re-</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:81"><nobr><span class="ft1">7</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:88"><nobr><span class="ft5">http://www.google.com</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:81"><nobr><span class="ft1">8</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:88"><nobr><span class="ft5">A more formal treatment of the combination of labeled and</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:81"><nobr><span class="ft5">unlabeled data is available in [4].</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:475"><nobr><span class="ft10">Table 5: Human-Model Agreement on 100 Sample<br>Docs.</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:486"><nobr><span class="ft5">Human Judge 1</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:582"><nobr><span class="ft1">st</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:596"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:486"><nobr><span class="ft5">Model</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:596"><nobr><span class="ft5">Model 1</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:644"><nobr><span class="ft1">st</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:659"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:716"><nobr><span class="ft5">Model 2</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:764"><nobr><span class="ft1">nd</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:782"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:486"><nobr><span class="ft5">N. Bayes (aug.)</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:686"><nobr><span class="ft5">14</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:810"><nobr><span class="ft5">24</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:486"><nobr><span class="ft5">N. Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:686"><nobr><span class="ft5">24</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:816"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:486"><nobr><span class="ft5">Human Judge 2</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:582"><nobr><span class="ft1">nd</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:600"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:486"><nobr><span class="ft5">Model</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:596"><nobr><span class="ft5">Model 1</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:644"><nobr><span class="ft1">st</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:659"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:716"><nobr><span class="ft5">Model 2</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:764"><nobr><span class="ft1">nd</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:782"><nobr><span class="ft5">Choice</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:486"><nobr><span class="ft5">N. Bayes (aug.)</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:686"><nobr><span class="ft5">14</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:810"><nobr><span class="ft5">21</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:486"><nobr><span class="ft5">N. Bayes</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:686"><nobr><span class="ft5">21</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:816"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:475"><nobr><span class="ft10">viewer classified 83 documents, placing each document into<br>as many of the categories shown in Table 3 as he or she saw<br>fit.</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:489"><nobr><span class="ft5">Results from this experiment suggest that room for im-</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:475"><nobr><span class="ft10">provement remains with respect to generalizing to the whole<br>collection from the class models fitted to the Editor's Desk<br>documents. In Table 5, we see, for each classifier, the num-<br>ber of documents for which it's first or second most probable<br>class was voted best or second best by the 11 human judges.</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:489"><nobr><span class="ft5">In the context of this experiment, we consider a first- or</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:475"><nobr><span class="ft10">second-place classification by the machine to be accurate<br>because the relation browser interface operates on a multi-<br>way classification, where each document is classified into<br>multiple categories. Thus a document with the "correct"<br>class as its second choice would still be easily available to<br>a user. Likewise, a correct classification on either the most<br>popular or second most popular category among the human<br>judges is considered correct in cases where a given document<br>was classified into multiple classes. There were 72 multi-<br>class documents in our sample, as seen in Figure 4. The<br>remaining 28 documents were assigned to 1 or 0 classes.</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:489"><nobr><span class="ft5">Under this rationale, The augmented naive Bayes classi-</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:475"><nobr><span class="ft10">fier correctly grouped 73 documents, while the smaller model<br>(not augmented by a Google search) correctly classified 50.<br>The resultant </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:573"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:585"><nobr><span class="ft5">test gave p = 0.001, suggesting that in-</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:475"><nobr><span class="ft10">creasing the training set improved the ability of the naive<br>Bayes model to generalize from the Editor's Desk documents<br>to the collection as a whole. However, the improvement af-<br>forded by the augmented model comes at some cost. In par-<br>ticular, the augmented model is significantly inferior to the<br>model trained solely on Editor's Desk documents if we con-<br>cern ourselves only with documents selected by the majority<br>of human reviewers--i.e. only first-choice classes. Limiting<br>the right answers to the left column of Table 5 gives p = 0.02<br>in favor of the non-augmented model. For the purposes of<br>applying the relation browser to complex digital library con-<br>tent (where documents will be classified along multiple cat-<br>egories), the augmented model is preferable. But this is not<br>necessarily the case in general.</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:489"><nobr><span class="ft5">It must also be said that 73% accuracy under a fairly</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:475"><nobr><span class="ft10">liberal test condition leaves room for improvement in our<br>assignment of topics to categories. We may begin to un-<br>derstand the shortcomings of the described techniques by<br>consulting Figure 5, which shows the distribution of cate-<br>gories across documents given by humans and by the aug-<br>mented naive Bayes model. The majority of reviewers put</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">157</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft15{font-size:10px;font-family:Times;color:#ffffff;}
	.ft16{font-size:10px;font-family:Times;color:#bdbdbd;}
	.ft17{font-size:12px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="132008.png" alt="background image">
<DIV style="position:absolute;top:470;left:179"><nobr><span class="ft13">Number of Human-Assigned Classes</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:93"><nobr><span class="ft13">Frequency</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:140"><nobr><span class="ft13">0</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:180"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:219"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:259"><nobr><span class="ft13">3</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:299"><nobr><span class="ft13">4</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:338"><nobr><span class="ft13">5</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:378"><nobr><span class="ft13">6</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:418"><nobr><span class="ft13">7</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:118"><nobr><span class="ft13">0</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:118"><nobr><span class="ft13">5</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:118"><nobr><span class="ft13">10</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:118"><nobr><span class="ft13">15</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:118"><nobr><span class="ft13">20</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:118"><nobr><span class="ft13">25</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:118"><nobr><span class="ft13">30</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:118"><nobr><span class="ft13">35</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:81"><nobr><span class="ft5">Figure 4:</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:160"><nobr><span class="ft5">Number of Classes Assigned to Docu-</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:81"><nobr><span class="ft5">ments by Judges</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:81"><nobr><span class="ft10">documents into only three categories, jobs, benefits, and oc-<br>cupations. On the other hand, the naive Bayes classifier dis-<br>tributed classes more evenly across the topics. This behavior<br>suggests areas for future improvement. Most importantly,<br>we observed a strong correlation among the three most fre-<br>quent classes among the human judges (for instance, there<br>was 68% correlation between benefits and occupations). This<br>suggests that improving the clustering to produce topics<br>that were more nearly orthogonal might improve perfor-<br>mance.</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:81"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:776;left:112"><nobr><span class="ft4"><b>CONCLUSIONS AND FUTURE WORK</b></span></nobr></DIV>
<DIV style="position:absolute;top:798;left:94"><nobr><span class="ft5">Many developers and maintainers of digital libraries share</span></nobr></DIV>
<DIV style="position:absolute;top:813;left:81"><nobr><span class="ft10">the basic problem pursued here. Given increasingly large,<br>complex bodies of data, how may we improve access to col-<br>lections without incurring extraordinary cost, and while also<br>keeping systems receptive to changes in content over time?<br>Data mining and machine learning methods hold a great deal<br>of promise with respect to this problem. Empirical meth-<br>ods of knowledge discovery can aid in the organization and<br>retrieval of information. As we have argued in this paper,<br>these methods may also be brought to bear on the design<br>and implementation of advanced user interfaces.</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:94"><nobr><span class="ft5">This study explored a hybrid technique for aiding informa-</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:81"><nobr><span class="ft10">tion architects as they implement dynamic interfaces such<br>as the relation browser. Our approach combines unsuper-<br>vised learning techniques, applied to a focused subset of the<br>BLS website. The goal of this initial stage is to discover the<br>most basic and far-reaching topics in the collection. Based</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:538"><nobr><span class="ft13">jobs</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:618"><nobr><span class="ft13">prices</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:694"><nobr><span class="ft13">spending</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:761"><nobr><span class="ft13">costs</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:604"><nobr><span class="ft17">Human Classifications</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:513"><nobr><span class="ft13">0.00</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:513"><nobr><span class="ft13">0.15</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:538"><nobr><span class="ft13">jobs</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:618"><nobr><span class="ft13">prices</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:694"><nobr><span class="ft13">spending</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:761"><nobr><span class="ft13">costs</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:600"><nobr><span class="ft17">Machine Classifications</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:513"><nobr><span class="ft13">0.00</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:513"><nobr><span class="ft13">0.10</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:475"><nobr><span class="ft5">Figure 5: Distribution of Classes Across Documents</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:475"><nobr><span class="ft10">on a statistical model of these topics, the second phase of<br>our approach uses supervised learning (in particular, a naive<br>Bayes classifier, trained on individual words), to assign top-<br>ical relations to the remaining documents in the collection.</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:489"><nobr><span class="ft5">In the study reported here, this approach has demon-</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:475"><nobr><span class="ft10">strated promise. In its favor, our approach is highly scalable.<br>It also appears to give fairly good results. Comparing three<br>modes of document representation--full-text, title only, and<br>keyword--we found 98% accuracy as measured by colloca-<br>tion of documents with identical subject headings. While it<br>is not surprising that editor-generated keywords should give<br>strong evidence for such learning, their superiority over full-<br>text and titles was dramatic, suggesting that even a small<br>amount of metadata can be very useful for data mining.</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:489"><nobr><span class="ft5">However, we also found evidence that learning topics from</span></nobr></DIV>
<DIV style="position:absolute;top:813;left:475"><nobr><span class="ft10">a subset of the collection may lead to overfitted models.<br>After clustering 1279 Editor's Desk documents into 10 cate-<br>gories, we fitted a 10-way naive Bayes classifier to categorize<br>the remaining 14,000 documents in the collection. While we<br>saw fairly good results (classification accuracy of 75% with<br>respect to a small sample of human judges), this experiment<br>forced us to reconsider the quality of the topics learned by<br>clustering. The high correlation among human judgments<br>in our sample suggests that the topics discovered by analy-<br>sis of the Editor's Desk were not independent. While we do<br>not desire mutually exclusive categories in our setting, we<br>do desire independence among the topics we model.</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:489"><nobr><span class="ft5">Overall, then, the techniques described here provide an</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:475"><nobr><span class="ft10">encouraging start to our work on acquiring subject meta-<br>data for dynamic interfaces automatically. It also suggests<br>that a more sophisticated modeling approach might yield</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">158</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="132009.png" alt="background image">
<DIV style="position:absolute;top:122;left:81"><nobr><span class="ft10">better results in the future. In upcoming work we will exper-<br>iment with streamlining the two-phase technique described<br>here.</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:122"><nobr><span class="ft5">Instead of clustering documents to find topics and</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:81"><nobr><span class="ft10">then fitting a model to the learned clusters, our goal is to<br>expand the unsupervised portion of our analysis beyond a<br>narrow subset of the collection, such as The Editor's Desk.<br>In current work we have defined algorithms to identify docu-<br>ments likely to help the topic discovery task. Supplied with<br>a more comprehensive training set, we hope to experiment<br>with model-based clustering, which combines the clustering<br>and classification processes into a single modeling procedure.</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:94"><nobr><span class="ft5">Topic discovery and document classification have long been</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:81"><nobr><span class="ft10">recognized as fundamental problems in information retrieval<br>and other forms of text mining. What is increasingly clear,<br>however, as digital libraries grow in scope and complexity,<br>is the applicability of these techniques to problems at the<br>front-end of systems such as information architecture and<br>interface design. Finally, then, in future work we will build<br>on the user studies undertaken by Marchionini and Brunk<br>in efforts to evaluate the utility of automatically populated<br>dynamic interfaces for the users of digital libraries.</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft4"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:469;left:112"><nobr><span class="ft4"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:88"><nobr><span class="ft5">[1] A. Agresti. An Introduction to Categorical Data</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:109"><nobr><span class="ft5">Analysis. Wiley, New York, 1996.</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:88"><nobr><span class="ft5">[2] C. Ahlberg, C. Williamson, and B. Shneiderman.</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:109"><nobr><span class="ft10">Dynamic queries for information exploration: an<br>implementation and evaluation. In Proceedings of the<br>SIGCHI conference on Human factors in computing<br>systems, pages 619­626, 1992.</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:88"><nobr><span class="ft5">[3] R. Baeza-Yates and B. Ribeiro-Neto. Modern</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:109"><nobr><span class="ft5">Information Retrieval. ACM Press, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:88"><nobr><span class="ft5">[4] A. Blum and T. Mitchell. Combining labeled and</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:109"><nobr><span class="ft10">unlabeled data with co-training. In Proceedings of the<br>eleventh annual conference on Computational learning<br>theory, pages 92­100. ACM Press, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:88"><nobr><span class="ft5">[5] H. Chen and S. Dumais. Hierarchical classification of</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:109"><nobr><span class="ft10">web content. In Proceedings of the 23rd annual<br>international ACM SIGIR conference on Research and<br>development in information retrieval, pages 256­263,<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:88"><nobr><span class="ft5">[6] M. Efron, G. Marchionini, and J. Zhang. Implications</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:109"><nobr><span class="ft10">of the recursive representation problem for automatic<br>concept identification in on-line governmental<br>information. In Proceedings of the ASIST Special<br>Interest Group on Classification Research (ASIST<br>SIG-CR), 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:482"><nobr><span class="ft5">[7] C. Fraley and A. E. Raftery. How many clusters?</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:503"><nobr><span class="ft10">which clustering method? answers via model-based<br>cluster analysis. The Computer Journal,<br>41(8):578­588, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:482"><nobr><span class="ft5">[8] A. K. Jain, M. N. Murty, and P. J. Flynn. Data</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:503"><nobr><span class="ft10">clustering: a review. ACM Computing Surveys,<br>31(3):264­323, September 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:482"><nobr><span class="ft5">[9] T. Joachims. A probabilistic analysis of the Rocchio</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:503"><nobr><span class="ft10">algorithm with TFIDF for text categorization. In<br>D. H. Fisher, editor, Proceedings of ICML-97, 14th<br>International Conference on Machine Learning, pages<br>143­151, Nashville, US, 1997. Morgan Kaufmann<br>Publishers, San Francisco, US.</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:475"><nobr><span class="ft5">[10] T. Joachims. Text categorization with support vector</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:503"><nobr><span class="ft10">machines: learning with many relevant features. In<br>C. N´</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:532"><nobr><span class="ft5">edellec and C. Rouveirol, editors, Proceedings of</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:503"><nobr><span class="ft10">ECML-98, 10th European Conference on Machine<br>Learning, pages 137­142, Chemnitz, DE, 1998.<br>Springer Verlag, Heidelberg, DE.</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:475"><nobr><span class="ft5">[11] I. T. Jolliffe. Principal Component Analysis. Springer,</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:503"><nobr><span class="ft5">2nd edition, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:475"><nobr><span class="ft5">[12] L. Kaufman and P. J. Rosseeuw. Finding Groups in</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:503"><nobr><span class="ft10">Data: an Introduction to Cluster Analysis. Wiley,<br>1990.</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:475"><nobr><span class="ft5">[13] G. Marchionini and B. Brunk. Toward a general</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:503"><nobr><span class="ft10">relation browser: a GUI for information architects.<br>Journal of Digital Information, 4(1), 2003.<br>http://jodi.ecs.soton.ac.uk/Articles/v04/i01/Marchionini/.</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:475"><nobr><span class="ft5">[14] A. K. McCallum. Bow: A toolkit for statistical</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:503"><nobr><span class="ft10">language modeling, text retrieval, classification and<br>clustering. http://www.cs.cmu.edu/~mccallum/bow,<br>1996.</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:475"><nobr><span class="ft18">[15] T. Mitchell. Machine Learning. McGraw Hill, 1997.<br>[16] E. Rasmussen. Clustering algorithms. In W. B. Frakes</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:503"><nobr><span class="ft10">and R. Baeza-Yates, editors, Information Retrieval:<br>Data Structures and Algorithms, pages 419­442.<br>Prentice Hall, 1992.</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:475"><nobr><span class="ft5">[17] R. Tibshirani, G. Walther, and T. Hastie. Estimating</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:503"><nobr><span class="ft10">the number of clusters in a dataset via the gap<br>statistic, 2000.<br>http://citeseer.nj.nec.com/tibshirani00estimating.html.</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:475"><nobr><span class="ft5">[18] V. N. Vapnik. The Nature of Statistical Learning</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:503"><nobr><span class="ft5">Theory. Springer, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">159</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
