<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\89</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-07-11T17:44:11+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:6px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Courier;color:#000000;}
	.ft9{font-size:11px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft11{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="89001.png" alt="background image">
<DIV style="position:absolute;top:108;left:228"><nobr><span class="ft0"><b>Evolutionary Learning with Kernels:</b></span></nobr></DIV>
<DIV style="position:absolute;top:138;left:162"><nobr><span class="ft0"><b>A Generic Solution for Large Margin Problems</b></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:404"><nobr><span class="ft1">Ingo Mierswa</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:376"><nobr><span class="ft2">Artificial Intelligence Unit</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:346"><nobr><span class="ft2">Department of Computer Science</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:381"><nobr><span class="ft2">University of Dortmund</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:331"><nobr><span class="ft1">ingo.mierswa@uni-dortmund.de</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:81"><nobr><span class="ft3"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:343;left:81"><nobr><span class="ft10">In this paper we embed evolutionary computation into sta-<br>tistical learning theory. First, we outline the connection be-<br>tween large margin optimization and statistical learning and<br>see why this paradigm is successful for many pattern recog-<br>nition problems. We then embed evolutionary computation<br>into the most prominent representative of this class of learn-<br>ing methods, namely into Support Vector Machines (SVM).<br>In contrast to former applications of evolutionary algorithms<br>to SVMs we do not only optimize the method or kernel pa-<br>rameters. We rather use both evolution strategies and par-<br>ticle swarm optimization in order to directly solve the posed<br>constrained optimization problem. Transforming the prob-<br>lem into the Wolfe dual reduces the total runtime and al-<br>lows the usage of kernel functions. Exploiting the knowledge<br>about this optimization problem leads to a hybrid mutation<br>which further decreases convergence time while classifica-<br>tion accuracy is preserved. We will show that evolutionary<br>SVMs are at least as accurate as their quadratic program-<br>ming counterparts on six real-world benchmark data sets.<br>The evolutionary SVM variants frequently outperform their<br>quadratic programming competitors. Additionally, the pro-<br>posed algorithm is more generic than existing traditional<br>solutions since it will also work for non-positive semidefinite<br>kernel functions and for several, possibly competing, perfor-<br>mance criteria.</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:81"><nobr><span class="ft10">Track: Learning Classifier Systems and other Genetics-<br>Based Machine Learning</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:81"><nobr><span class="ft3">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:81"><nobr><span class="ft10">Learning</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:81"><nobr><span class="ft4">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:81"><nobr><span class="ft10">Algorithms, Theory, Experimentation</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:81"><nobr><span class="ft10">Keywords: Support vector machines, machine learning,<br>kernel methods, evolution strategies, particle swarms</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:81"><nobr><span class="ft3"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:879;left:112"><nobr><span class="ft3"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:901;left:94"><nobr><span class="ft4">In this paper we will discuss how evolutionary algorithms</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:81"><nobr><span class="ft4">can be used to solve large margin optimization problems.</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft11">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>GECCO'06, </i>July 8­12, 2006, Seattle, Washington, USA.<br>Copyright 2006 ACM 1-59593-186-4/06/0007 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:316"><nobr><span class="ft4">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:323"><nobr><span class="ft5">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:475"><nobr><span class="ft10">We explore the intersection of three highly active research<br>areas, namely machine learning, statistical learning theory,<br>and evolutionary algorithms. While the connection between<br>statistical learning and machine learning was analyzed be-<br>fore, embedding evolutionary algorithms into this connec-<br>tion will lead to a more generic algorithm which can deal<br>with problems today's learning schemes cannot cope with.</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:489"><nobr><span class="ft4">Supervised machine learning is often about classification</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:475"><nobr><span class="ft10">problems. A set of data points is divided into several classes<br>and the machine learning method should learn a decision<br>function in order to decide into which class an unseen data<br>point should be classified.</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:489"><nobr><span class="ft4">The maximization of a margin between data points of dif-</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:475"><nobr><span class="ft10">ferent classes, i. e. the distance between a decision hyper-<br>plane and the nearest data points, interferes with the ideas<br>of statistical learning theory. This allows the definition of an<br>error bound for the generalization error. Furthermore, the<br>usage of kernel functions allows the learning of non-linear<br>decision functions. We focus on Support Vector Machines<br>(SVM) as they are the most prominent representatives for<br>large margin problems. Since SVMs guarantee an optimal<br>solution for the given data set they are currently one of the<br>mostly used learning methods. Furthermore, many other<br>optimization problems can also be formulated as large mar-<br>gin problem [26]. The relevance of large margin methods<br>can be measured by the number of submissions to the main<br>machine learning conferences over the past years</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:769"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:776"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:489"><nobr><span class="ft4">Usually, the optimization problem posed by SVMs is solved</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:475"><nobr><span class="ft10">with quadratic programming. However, there are some draw-<br>backs.</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:525"><nobr><span class="ft4">First, for kernel functions which are not positive</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:475"><nobr><span class="ft10">semidefinite no unique global optimum exists. In these cases<br>quadratic programming is not able to find satisfying solu-<br>tions at all. Moreover, most implementations do not even<br>terminate [8]. There exist several useful non-positive ker-<br>nels [15], among them the sigmoid kernel which simulates a<br>neural network [3, 23]. A more generic optimization scheme<br>should allow such non-positive kernels without the need for<br>omitting the more efficient dual optimization problem [17].</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:489"><nobr><span class="ft4">Second, SVMs should be able to optimize several perfor-</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:475"><nobr><span class="ft10">mance measures at the same time. Traditional SVMs try<br>to maximize the prediction accuracy alone. However, de-<br>pending on the application area other specific performance<br>criteria should be optimized instead of or additionally to<br>prediction accuracy. Although first attempts were made to<br>incorporate multivariate performance measures into SVMs<br>[13], the problem is not generally solved and no solution exist</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:476"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:482"><nobr><span class="ft4">More than 30% of all accepted papers for ICML 2005 dealt</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">with SVMs and other large margin methods.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1553</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft12{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="89002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">for competing criteria. This problem as well as the general<br>trade-off between training error and capacity could be eas-<br>ily solved by an (multi-objective) evolutionary optimization<br>approach.</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:94"><nobr><span class="ft4">Former applications of evolutionary algorithms to SVMs</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft10">include the optimization of method and kernel parameters<br>[6, 19], the selection of optimal feature subsets [7], and the<br>creation of new kernel functions by means of genetic pro-<br>gramming [10]. The latter is particularly interesting since<br>it cannot be guaranteed that the resulting kernel functions<br>are again positive semi-definite.</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:94"><nobr><span class="ft4">Replacing the traditional optimization techniques by evo-</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:81"><nobr><span class="ft10">lution strategies or particle swarm optimization can tackle<br>both problems mentioned above. We will extract as much<br>information as possible from the optimization problem at<br>hand and develop and compare different search point op-<br>erations. We will show that the proposed implementation<br>leads to as good results as traditional SVMs on all real-<br>world benchmark data sets. Additionally, the optimization<br>is more generic since it also allows non-positive semi-definite<br>kernel functions and the simultaneous optimization of differ-<br>ent, maybe competing, criteria.</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:81"><nobr><span class="ft3"><b>1.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:440;left:121"><nobr><span class="ft3"><b>Outline</b></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:94"><nobr><span class="ft4">In Section 2 we give a short introduction into the concept</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:81"><nobr><span class="ft10">of structural risk minimization and the ideas of statistical<br>learning theory. We will also discuss an upper bound for<br>the generalization error. This allows us to formalize the op-<br>timization problem of large margin methods in Section 3.<br>We will introduce SVMs for the classification of given data<br>points in Section 3.1 and extend the separation problem to<br>non-separable datasets (see Section 3.2) with non-linear hy-<br>perplanes (see Section 3.3). This leads to a constrained op-<br>timization problem for which we utilize evolution strategies<br>and particle swarm optimization in Section 4. We discuss<br>several enhancements and a new type of mutation before<br>we evaluate the proposed methods on real-world benchmark<br>datasets in Section 5.</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:81"><nobr><span class="ft3"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:699;left:112"><nobr><span class="ft3"><b>STRUCTURAL RISK MINIMIZATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:722;left:94"><nobr><span class="ft4">In this section we discuss the idea of structural risk mini-</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:81"><nobr><span class="ft10">mization. Machine learning methods following this paradigm<br>have a solid theoretical foundation and it is possible to define<br>bounds for prediction errors.</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:94"><nobr><span class="ft4">Let X  IR</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:161"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:176"><nobr><span class="ft4">be a real-valued vector of random variables.</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft10">Let Y  IR be another random variable. X and Y obey a<br>fixed but unknown probability distribution P (X, Y ). Ma-<br>chine Learning tries to find a function f(x, ) which predict<br>the value of Y for a given input x  X. The function class<br>f depends on a vector of parameters , e. g. if f is the<br>class of all polynomials,  might be the degree. We de-<br>fine a loss function L(Y, f(X, )) in order to penalize errors<br>during prediction [9]. Every convex function with arity 2,<br>positive range, and L(x, x) = 0 can be used as loss function<br>[22]. This leads to a possible criterion for the selection of a<br>function f, the expected risk:</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:164"><nobr><span class="ft4">R() =</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:212"><nobr><span class="ft4">Z</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:228"><nobr><span class="ft4">L(y, f(x, ))dP (x, y).</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:422"><nobr><span class="ft4">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft10">Since the underlying distribution is not known we are not<br>able to calculate the expected risk.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:316"><nobr><span class="ft4">However, instead of</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft10">estimating the probability distribution in order to allow this<br>calculation, we directly estimate the expected risk by using</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft4">a set of known data points T = {(x</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:700"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:706"><nobr><span class="ft4">, y</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:719"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:725"><nobr><span class="ft4">) , . . . , (x</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:777"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:784"><nobr><span class="ft4">, y</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:798"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:805"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:811"><nobr><span class="ft4">} </span></nobr></DIV>
<DIV style="position:absolute;top:102;left:475"><nobr><span class="ft10">X × Y . T is usually called training data. Using this set of<br>data points we can calculate the empirical risk :</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:555"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:565"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:587"><nobr><span class="ft4">() = 1</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:626"><nobr><span class="ft4">n</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:645"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:638"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:639"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:643"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:660"><nobr><span class="ft4">L (y</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:684"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:689"><nobr><span class="ft4">, f (x</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:719"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:723"><nobr><span class="ft4">, )) .</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:816"><nobr><span class="ft4">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:475"><nobr><span class="ft10">If training data is sampled according to P (X, Y ), the em-<br>pirical risk approximates the expected risk if the number of<br>samples grows:</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:593"><nobr><span class="ft4">lim</span></nobr></DIV>
<DIV style="position:absolute;top:258;left:588"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:595"><nobr><span class="ft7"></span></nobr></DIV>
<DIV style="position:absolute;top:247;left:619"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:629"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:651"><nobr><span class="ft4">() = R().</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:816"><nobr><span class="ft4">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:475"><nobr><span class="ft10">It is, however, a well known problem that for a finite num-<br>ber of samples the minimization of R</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:701"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:723"><nobr><span class="ft4">() alone does not</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:475"><nobr><span class="ft10">lead to a good prediction model [27]. For each loss func-<br>tion L, each candidate , and each set of tuples T  X ×<br>Y with T  T</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:580"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:599"><nobr><span class="ft4"> exists another parameter vector </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:475"><nobr><span class="ft10">so that L(y, f(x, )) = L(y, f(x,  )) for all x  T and<br>L(y, f(x, )) &gt; L(y, f(x,  )) for all x  T . Therefore, the<br>minimization of R</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:586"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:608"><nobr><span class="ft4">() alone does not guarantee the op-</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:475"><nobr><span class="ft10">timal selection of a parameter vector  for other samples<br>according to the distribution P (X, Y ). This problem is of-<br>ten referred to as overfitting.</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:489"><nobr><span class="ft4">At this point we use one of the main ideas of statistical</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:475"><nobr><span class="ft10">learning theory. Think of two different functions perfectly<br>approximating a given set of training points. The first func-<br>tion is a linear function, i. e. a simple hyperplane in the con-<br>sidered space IR</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:570"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:581"><nobr><span class="ft4">. The second function also hits all training</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:475"><nobr><span class="ft10">points but is strongly wriggling in between. Naturally, if we<br>had to choose between these two approximation functions,<br>we tend to select the more simple one, i. e. the linear hy-<br>perplane in this example. This derives from the observation<br>that more simple functions behave better on unseen exam-<br>ples than very complicated functions. Since the mere mini-<br>mization of the empirical risk according to the training data<br>is not appropriate to find a good generalization, we incorpo-<br>rate the capacity</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:575"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:585"><nobr><span class="ft4">of the used function into the optimization</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:475"><nobr><span class="ft10">problem (see Figure 1). This leads to the minimization of<br>the structural risk</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:561"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:571"><nobr><span class="ft7">struct</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:603"><nobr><span class="ft4">() = R</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:651"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:672"><nobr><span class="ft4">() + ().</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:816"><nobr><span class="ft4">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:475"><nobr><span class="ft10"> is a function which measures the capacity of the function<br>class f depending on the parameter vector . Since the<br>empirical risk is usually a monotonically decreasing function<br>of , we use  to manage the trade-off between training error<br>and capacity. Methods minimizing this type of risk function<br>are known as shrinkage estimators [11].</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:475"><nobr><span class="ft3"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:837;left:515"><nobr><span class="ft3"><b>Bound on the generalization performance</b></span></nobr></DIV>
<DIV style="position:absolute;top:859;left:489"><nobr><span class="ft4">For certain functions  the structural risk is an upper</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:475"><nobr><span class="ft4">bound for the empirical risk.</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:671"><nobr><span class="ft4">The capacity of the func-</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:475"><nobr><span class="ft10">tion f for a given  can for example be measured with help<br>of the Vapnik-Chervonenkis dimension (VC dimension) [27,<br>28]. The VC dimension is defined as the cardinality of the<br>biggest set of tuples which can separated with help of f in all<br>possible ways. For example, the VC dimension of linear hy-<br>perplanes in an m-dimensional space is m+1. Using the VC<br>dimension as a measure for capacity leads to a probabilistic<br>bound for the structural risk [27]. Let f be a function class<br>with finite VC dimension h and f() the best solution for the</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:476"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:482"><nobr><span class="ft4">Although not the same, the capacity of a function resembles</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:475"><nobr><span class="ft12">a measurement of the function complexity. In our example<br>we measure the ability to "wriggle". More details in [27].</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1554</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:13px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;font-family:Times;color:#000000;}
	.ft15{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
	.ft16{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft17{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="89003.png" alt="background image">
<DIV style="position:absolute;top:251;left:384"><nobr><span class="ft13"><i>X</i></span></nobr></DIV>
<DIV style="position:absolute;top:98;left:138"><nobr><span class="ft13"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:304;left:81"><nobr><span class="ft10">Figure 1: The simultaneous minimization of empir-<br>ical risk and model complexity gives a hint which<br>function should be used in order to generalize the<br>given data points.</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:81"><nobr><span class="ft10">empirical risk minimization for T with |T | = n. Now choose<br>some  such that 0    1. Then for losses smaller than<br>some number B, the following bound holds with probability<br>1</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:91"><nobr><span class="ft4">- :</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:123"><nobr><span class="ft4">R()  R</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:181"><nobr><span class="ft7">emp</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:203"><nobr><span class="ft4">() + B</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:249"><nobr><span class="ft4">s</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:265"><nobr><span class="ft4">h `log</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:304"><nobr><span class="ft7">2l</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:305"><nobr><span class="ft7">h</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:318"><nobr><span class="ft4">+ 1´</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:348"><nobr><span class="ft4">- log</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:383"><nobr><span class="ft15"><br>4</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:326"><nobr><span class="ft4">l</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:393"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:422"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:81"><nobr><span class="ft10">Surprisingly, this bound is independent of P (X, Y ). It only<br>assumes that both the seen and the unseen data points are<br>independently sampled according to some P (X, Y ). Please<br>note that this bound also no longer contains a weighting<br>factor  or any other trade-off at all. The existence of a<br>guaranteed error bound is the reason for the great success of<br>structural risk minimization in a wide range of applications.</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:81"><nobr><span class="ft3"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:654;left:112"><nobr><span class="ft3"><b>LARGE MARGIN METHODS</b></span></nobr></DIV>
<DIV style="position:absolute;top:677;left:94"><nobr><span class="ft4">As discussed in the previous section we need to use a class</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:81"><nobr><span class="ft10">of functions whose capacity can be controlled. In this sec-<br>tion we will discuss a special form of structural risk mini-<br>mization, namely large margin approaches. All large margin<br>methods have one thing in common: they embed structural<br>risk minimization by maximizing a margin between a linear<br>function and the nearest data points. The most prominent<br>large margin method for classification tasks is the Support<br>Vector Machine (SVM).</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:81"><nobr><span class="ft3"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:121"><nobr><span class="ft3"><b>Support Vector Machines</b></span></nobr></DIV>
<DIV style="position:absolute;top:852;left:94"><nobr><span class="ft4">We constrain the number of possible values of Y to 2,</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:81"><nobr><span class="ft4">without loss of generality these values should be</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:393"><nobr><span class="ft4">-1 and</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:81"><nobr><span class="ft10">+1. In this case, finding a function f in order to decide<br>which of both predictions is correct for an unseen data point<br>is referred to as classification learning for the classes</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:422"><nobr><span class="ft4">-1</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:81"><nobr><span class="ft10">and +1. We start with the simplest case: learning a linear<br>function from perfectly separable data. As we shall see in<br>Section 3.2 and 3.3, the general case - non-linear functions<br>derived from non-separable data - leads to a very similar<br>problem.</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:94"><nobr><span class="ft4">If the data points are linearly separable, a linear hyper-</span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:81"><nobr><span class="ft4">plane must exist in the input space IR</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:325"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:342"><nobr><span class="ft4">which separates</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:81"><nobr><span class="ft4">both classes. This hyperplane is defined as</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:187"><nobr><span class="ft4">H = {x| w, x + b = 0} ,</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:422"><nobr><span class="ft4">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:531"><nobr><span class="ft14"><i>H</i></span></nobr></DIV>
<DIV style="position:absolute;top:147;left:672"><nobr><span class="ft14"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:756"><nobr><span class="ft14"><i>Margin</i></span></nobr></DIV>
<DIV style="position:absolute;top:275;left:549"><nobr><span class="ft14"><i>Origin</i></span></nobr></DIV>
<DIV style="position:absolute;top:241;left:570"><nobr><span class="ft16"><i>-b<br>|w|</i></span></nobr></DIV>
<DIV style="position:absolute;top:100;left:690"><nobr><span class="ft14"><i>+1</i></span></nobr></DIV>
<DIV style="position:absolute;top:202;left:539"><nobr><span class="ft14"><i>-1</i></span></nobr></DIV>
<DIV style="position:absolute;top:312;left:475"><nobr><span class="ft10">Figure 2: A simple binary classification problem for<br>two classes</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:552"><nobr><span class="ft4">-1 (empty bullets) and +1 (filled bullets).</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:475"><nobr><span class="ft10">The separating hyperplane is defined by the vector<br>w and the offset b. The distance between the nearest<br>data point(s) and the hyperplane is called</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:773"><nobr><span class="ft4">margin.</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:475"><nobr><span class="ft10">where w is normal to the hyperplane, |b|/||w|| is the per-<br>pendicular distance of the hyperplane to the origin (offset<br>or bias), and</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:555"><nobr><span class="ft4">||w|| is the Euclidean norm of w. The vector w</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:475"><nobr><span class="ft10">and the offset b define the position and orientation of the hy-<br>perplane in the input space. These parameters correspond<br>to the function parameters . After the optimal parameters<br>w and b were found, the prediction of new data points can<br>be calculated as</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:569"><nobr><span class="ft4">f(x, w, b) = sgn ( w, x + b) ,</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:816"><nobr><span class="ft4">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:475"><nobr><span class="ft10">which is one of the reasons why we constrained the classes<br>to</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:492"><nobr><span class="ft4">-1 and +1.</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:489"><nobr><span class="ft4">Figure 2 shows some data points and a separating hyper-</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:475"><nobr><span class="ft10">plane. If all given data points are correctly classified by the<br>hyperplane at hand the following must hold:</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:584"><nobr><span class="ft4">i : y</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:615"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:622"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:657"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:670"><nobr><span class="ft4">+ b)  0.</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:816"><nobr><span class="ft4">(8)</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:475"><nobr><span class="ft10">Of course, an infinite number of different hyperplanes exist<br>which perfectly separate the given data points. However,<br>one would intuitively choose the hyperplane which has the<br>biggest amount of safety margin to both sides of the data<br>points.</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:528"><nobr><span class="ft4">Normalizing w and b in a way that the point(s)</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:475"><nobr><span class="ft4">closest to the hyperplane satisfy</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:682"><nobr><span class="ft4">| w, x</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:716"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:730"><nobr><span class="ft4">+ b| = 1 we can</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:475"><nobr><span class="ft4">transform equation 8 into</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:584"><nobr><span class="ft4">i : y</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:615"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:622"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:657"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:670"><nobr><span class="ft4">+ b)  1.</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:816"><nobr><span class="ft4">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:475"><nobr><span class="ft10">We can now define the margin as the perpendicular distance<br>of the nearest point(s) to the hyperplane.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:752"><nobr><span class="ft4">Consider two</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:475"><nobr><span class="ft4">points x</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:525"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:537"><nobr><span class="ft4">and x</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:573"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:584"><nobr><span class="ft4">on opposite sides of the margin. That is</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:481"><nobr><span class="ft4">w, x</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:505"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:517"><nobr><span class="ft4">+b = +1 and w, x</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:630"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:643"><nobr><span class="ft4">+b = -1 and w, (x</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:761"><nobr><span class="ft7">1</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:768"><nobr><span class="ft4">-x</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:788"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:794"><nobr><span class="ft4">) = 2.</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:475"><nobr><span class="ft4">The margin is then given by 1/||w||.</span></nobr></DIV>
<DIV style="position:absolute;top:926;left:489"><nobr><span class="ft4">It can be shown, that the capacity of the class of sep-</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:475"><nobr><span class="ft10">arating hyperplanes decreases with increasing margin [21].<br>Maximizing the margin of a hyperplane therefore formalizes<br>the structural risk minimization discussed in the previous<br>section. Instead of maximizing 1/||w|| we could also min-<br>imize</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:514"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:522"><nobr><span class="ft4">||w||</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:547"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:559"><nobr><span class="ft4">which will result into more simple equations</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft4">later. This leads to the optimization problem</span></nobr></DIV>
<DIV style="position:absolute;top:1047;left:604"><nobr><span class="ft4">minimize</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:665"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:673"><nobr><span class="ft4">||w||</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:698"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:1047;left:809"><nobr><span class="ft4">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:552"><nobr><span class="ft4">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:617"><nobr><span class="ft4">i : y</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:648"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:655"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:690"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:703"><nobr><span class="ft4">+ b)  1.</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:809"><nobr><span class="ft4">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1555</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:5px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="89004.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">Function 10 is the objective function and the constraints<br>from equation 11 are called inequality constraints.</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:408"><nobr><span class="ft4">They</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:81"><nobr><span class="ft10">form a constrained optimization problem. We will use a La-<br>grangian formulation of the problem. This allows us to re-<br>place the inequality constraints by constraints on the La-<br>grange multipliers which are easier to handle. The second<br>reason is that after the transformation of the optimization<br>problem, the training data will only appear in dot products.<br>This will allow us to generalize the optimization to the non-<br>linear case (see Section 3.3). We will now introduce positive<br>Lagrange multipliers </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:222"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:227"><nobr><span class="ft4">, i = 1, . . . , n, one for each of the</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:81"><nobr><span class="ft4">inequality constraints. The Lagrangian has the form</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:111"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:120"><nobr><span class="ft7">P</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:130"><nobr><span class="ft4">(w, b, ) = 12||w||</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:232"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:242"><nobr><span class="ft4">-</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:262"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:255"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:256"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:260"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:278"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:301;left:286"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:291"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:298"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:305"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:340"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:353"><nobr><span class="ft4">+ b) .</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:415"><nobr><span class="ft4">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:81"><nobr><span class="ft10">Finding a minimum of this function requires that the deriva-<br>tives</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:179"><nobr><span class="ft7">L</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:193"><nobr><span class="ft18">P</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:202"><nobr><span class="ft7">(w,b,)</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:201"><nobr><span class="ft7">w</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:243"><nobr><span class="ft4">= w -</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:290"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:287"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:285"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:289"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:305"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:387;left:314"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:319"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:325"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:330"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:338"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:415"><nobr><span class="ft4">(13)</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:199"><nobr><span class="ft7">L</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:213"><nobr><span class="ft18">P</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:222"><nobr><span class="ft7">(w,b,)</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:222"><nobr><span class="ft7">b</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:263"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:283"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:279"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:278"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:282"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:298"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:424;left:307"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:311"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:318"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:415"><nobr><span class="ft4">(14)</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:81"><nobr><span class="ft4">are zero, i. e.</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:217"><nobr><span class="ft4">w =</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:251"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:247"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:245"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:249"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:266"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:275"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:279"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:286"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:291"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:299"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:415"><nobr><span class="ft4">(15)</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:223"><nobr><span class="ft4">0 =</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:254"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:250"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:248"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:252"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:268"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:530;left:277"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:282"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:289"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:293"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:415"><nobr><span class="ft4">(16)</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:81"><nobr><span class="ft10">The Wolfe dual, which has to be maximized, results from<br>the Lagrangian by substituting 15 and 16 into 12, thus</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:98"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:107"><nobr><span class="ft7">D</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:117"><nobr><span class="ft4">(w, b, ) =</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:190"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:184"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:185"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:189"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:206"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:215"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:222"><nobr><span class="ft4">- 12</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:256"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:249"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:250"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:254"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:278"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:271"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:272"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:277"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:294"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:301"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:305"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:312"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:318"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:327"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:332"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:340"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:354"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:362"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:366"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:381"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:394"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:415"><nobr><span class="ft4">(17)</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:81"><nobr><span class="ft10">This leads to the dual optimization problem which must<br>be solved in order to find a separating maximum margin<br>hyperplane for given set of data points:</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:126"><nobr><span class="ft4">maximize</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:196"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:192"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:190"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:194"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:210"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:724;left:219"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:227"><nobr><span class="ft4">-</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:243"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:258"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:254"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:252"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:256"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:278"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:275"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:272"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:277"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:294"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:301"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:305"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:312"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:318"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:724;left:327"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:332"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:724;left:340"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:354"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:362"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:366"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:381"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:415"><nobr><span class="ft4">(18)</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:151"><nobr><span class="ft4">subject to </span></nobr></DIV>
<DIV style="position:absolute;top:755;left:225"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:233"><nobr><span class="ft4"> 0 for all i = 1, . . . , n</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:415"><nobr><span class="ft4">(19)</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:210"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:245"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:241"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:240"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:244"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:260"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:782;left:269"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:273"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:280"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:289"><nobr><span class="ft4">= 0.</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:415"><nobr><span class="ft4">(20)</span></nobr></DIV>
<DIV style="position:absolute;top:813;left:81"><nobr><span class="ft4">From an optimal vector </span></nobr></DIV>
<DIV style="position:absolute;top:808;left:245"><nobr><span class="ft7"></span></nobr></DIV>
<DIV style="position:absolute;top:813;left:258"><nobr><span class="ft4">we can calculate the optimal</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:81"><nobr><span class="ft4">normal vector w</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:178"><nobr><span class="ft7"></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:189"><nobr><span class="ft4">using equation 15. The optimal offset can</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft10">be calculated with help of equation 11. Please note, that w<br>is a linear combination of those data points x</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:356"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:365"><nobr><span class="ft4">with </span></nobr></DIV>
<DIV style="position:absolute;top:865;left:405"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:414"><nobr><span class="ft4">= 0.</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:81"><nobr><span class="ft10">These data points are called support vectors, hence the name<br>support vector machine. Only support vectors determine the<br>position and orientation of the separating hyperplane, other<br>data points might as well be omitted during learning. In<br>Figure 2 the support vectors are marked with circles. The<br>number of support vectors is usually much smaller than the<br>total number of data points.</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:81"><nobr><span class="ft3"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:997;left:121"><nobr><span class="ft3"><b>Non-separable data</b></span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:94"><nobr><span class="ft4">We now consider the case that the given set of data points</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft4">is not linearly separable.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:248"><nobr><span class="ft4">The optimization problem dis-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft10">cussed in the previous section would not have a solution<br>since in this case constraint 11 could not be fulfilled for all</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft10">i. We relax this constraint by introducing positive slack<br>variables </span></nobr></DIV>
<DIV style="position:absolute;top:107;left:539"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:543"><nobr><span class="ft4">, i = 1, . . . , n. Constraint 11 becomes</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:571"><nobr><span class="ft4">i : y</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:601"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:608"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:644"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:656"><nobr><span class="ft4">+ b)  1 - </span></nobr></DIV>
<DIV style="position:absolute;top:131;left:730"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:735"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:809"><nobr><span class="ft4">(21)</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:475"><nobr><span class="ft10">In order to minimize the number of wrong classifications<br>we introduce a correction term C P</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:692"><nobr><span class="ft17">n<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:696"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:713"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:171;left:719"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:728"><nobr><span class="ft4">into the objective</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:475"><nobr><span class="ft4">function. The optimization problems then becomes</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:574"><nobr><span class="ft4">minimize</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:635"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:642"><nobr><span class="ft4">||w||</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:668"><nobr><span class="ft7">2</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:677"><nobr><span class="ft4">+ C</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:710"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:706"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:704"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:708"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:724"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:217;left:730"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:809"><nobr><span class="ft4">(22)</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:538"><nobr><span class="ft4">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:603"><nobr><span class="ft4">i : y</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:634"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:641"><nobr><span class="ft4">( w, x</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:676"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:689"><nobr><span class="ft4">+ b)  1 - </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:762"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:767"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:809"><nobr><span class="ft4">(23)</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:475"><nobr><span class="ft10">The factor C determines the weight of wrong predictions as<br>part of the objective function. As in the previous section<br>we create the dual form of the Lagrangian. The slacking<br>variables </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:539"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:548"><nobr><span class="ft4">vanish and we get the optimization problem</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:521"><nobr><span class="ft4">maximize</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:590"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:586"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:585"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:589"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:605"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:348;left:614"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:622"><nobr><span class="ft4">-</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:637"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:652"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:648"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:647"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:651"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:673"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:669"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:667"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:672"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:688"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:695"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:700"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:707"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:712"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:348;left:721"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:726"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:348;left:735"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:748"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:756"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:761"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:775"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:809"><nobr><span class="ft4">(24)</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:531"><nobr><span class="ft4">subject to 0</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:607"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:379;left:630"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:639"><nobr><span class="ft4"> C for all i = 1, . . . , n</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:809"><nobr><span class="ft4">(25)</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:605"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:640"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:636"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:634"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:638"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:654"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:406;left:663"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:668"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:675"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:683"><nobr><span class="ft4">= 0.</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:809"><nobr><span class="ft4">(26)</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:475"><nobr><span class="ft10">It can easily be seen that the only difference to the separable<br>case is the additional upper bound C for all </span></nobr></DIV>
<DIV style="position:absolute;top:454;left:754"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:759"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:475"><nobr><span class="ft3"><b>3.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:515"><nobr><span class="ft3"><b>Non-linear learning with kernels</b></span></nobr></DIV>
<DIV style="position:absolute;top:496;left:489"><nobr><span class="ft4">The optimization problem described with equations 24,</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:475"><nobr><span class="ft10">25, and 26 will deliver a linear separating hyperplane for<br>arbitrary datasets. The result is optimal in a sense that no<br>other linear function is expected to provide a better classifi-<br>cation function on unseen data according to P (X, Y ). How-<br>ever, if the data is not linearly separable at all the question<br>arises how the described optimization problem can be gener-<br>alized to non-linear decision functions. Please note that the<br>data points only appear in the form of dot products x</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:800"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:805"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:819"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:830"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:475"><nobr><span class="ft10">A possible interpretation of this dot product is the similarity<br>of these data points in the input space IR</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:722"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:733"><nobr><span class="ft4">. Now consider a</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:475"><nobr><span class="ft4">mapping  : IR</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:568"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:584"><nobr><span class="ft4"> H into some other Euclidean space H</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:475"><nobr><span class="ft10">(called feature space) which might be performed before the<br>dot product is calculated. The optimization would depend<br>on dot products in this new space H, i. e. on functions of<br>the form  (x</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:564"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:569"><nobr><span class="ft4">) ,  (x</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:609"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:614"><nobr><span class="ft4">) . A function k : IR</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:744"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:759"><nobr><span class="ft4">× IR</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:786"><nobr><span class="ft7">m</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:802"><nobr><span class="ft4"> IR</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:475"><nobr><span class="ft4">with the characteristic</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:573"><nobr><span class="ft4">k (x</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:596"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:601"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:615"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:621"><nobr><span class="ft4">) =  (x</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:676"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:680"><nobr><span class="ft4">) ,  (x</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:719"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:725"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:809"><nobr><span class="ft4">(27)</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:475"><nobr><span class="ft10">is called kernel function or kernel. Figure 3 gives a rough<br>idea how transforming the data points can help to solve<br>non-linear problems with the optimization in a (higher di-<br>mensional) space where the points can be linearly separated.</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:489"><nobr><span class="ft4">A fascinating property of kernels is that for some map-</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:475"><nobr><span class="ft10">pings  a kernel k exists which can be calculated without<br>actually performing . Since often the dimension of H is<br>greater than the dimension m of the input space and H<br>sometimes is even infinite dimensional, the usage of such<br>kernels is a very efficient way to introduce non-linear deci-<br>sion functions into large margin approaches. Prominent ex-<br>amples for such efficient non-linear kernels are polynomial<br>kernels with degree d</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:570"><nobr><span class="ft4">k (x</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:593"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:598"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:612"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:617"><nobr><span class="ft4">) = ( x</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:668"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:672"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:687"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:701"><nobr><span class="ft4">+ )</span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:727"><nobr><span class="ft7">d</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:735"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:809"><nobr><span class="ft4">(28)</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:475"><nobr><span class="ft4">radial basis function kernels (RBF kernels)</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:584"><nobr><span class="ft4">k (x</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:607"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:612"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:626"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:632"><nobr><span class="ft4">) = e</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:662"><nobr><span class="ft7">-</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:672"><nobr><span class="ft18">||xi-xj||2</span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:689"><nobr><span class="ft18">22</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:809"><nobr><span class="ft4">(29)</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1556</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft19{font-size:7px;font-family:Times;color:#000000;}
	.ft20{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
	.ft21{font-size:15px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="89005.png" alt="background image">
<DIV style="position:absolute;top:83;left:362"><nobr><span class="ft13"><i>H</i></span></nobr></DIV>
<DIV style="position:absolute;top:83;left:163"><nobr><span class="ft13"><i>R</i></span></nobr></DIV>
<DIV style="position:absolute;top:79;left:174"><nobr><span class="ft19"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:278;left:81"><nobr><span class="ft10">Figure 3: After the transformation of all data points<br>into the feature space H the non-linear separation<br>problem can be solved with a linear separation al-<br>gorithm. In this case a transformation in the space<br>of polynomials with degree 2 was chosen.</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:81"><nobr><span class="ft4">for a  &gt; 0, and the sigmoid kernel</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:165"><nobr><span class="ft4">k (x</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:189"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:193"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:207"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:213"><nobr><span class="ft4">) = tanh ( x</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:296"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:300"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:314"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:329"><nobr><span class="ft4">- )</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:415"><nobr><span class="ft4">(30)</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:81"><nobr><span class="ft10">which can be used to simulate a neural network.  and <br>are scaling and shifting parameters. Since the RBF kernel<br>is easy interpretable and often yields good prediction per-<br>formance, it is used in a wide range of applications. We will<br>also use the RBF kernel for our experiments described in<br>section 5 in order to demonstrate the learning ability of the<br>proposed SVM.</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:94"><nobr><span class="ft4">We replace the dot product in the objective function by</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:81"><nobr><span class="ft10">kernel functions and achieve the final optimization problem<br>for finding a non-linear separation for non-separable data<br>points</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:122"><nobr><span class="ft4">maximize</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:192"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:188"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:186"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:190"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:207"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:215"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:223"><nobr><span class="ft4">-</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:239"><nobr><span class="ft17">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:254"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:250"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:248"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:252"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:275"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:271"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:269"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:274"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:290"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:297"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:302"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:308"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:314"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:323"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:328"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:337"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:342"><nobr><span class="ft4">k (x</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:366"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:370"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:384"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:390"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:415"><nobr><span class="ft4">(31)</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:136"><nobr><span class="ft4">subject to 0</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:212"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:236"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:244"><nobr><span class="ft4"> C for all i = 1, . . . , n</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:415"><nobr><span class="ft4">(32)</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:210"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:245"><nobr><span class="ft7">n</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:241"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:240"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:244"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:260"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:700;left:269"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:273"><nobr><span class="ft4">y</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:280"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:289"><nobr><span class="ft4">= 0.</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:415"><nobr><span class="ft4">(33)</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:94"><nobr><span class="ft4">It can be shown that if the kernel k, i. e. it's kernel ma-</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:81"><nobr><span class="ft10">trix, is positive definite, the objective function is concave<br>[2]. The optimization problem therefore has a global unique<br>maximum. However, in some cases a specialized kernel func-<br>tion must be used to measure the similarity between data<br>points which is not positive definite, sometimes not even<br>positive semidefinite [21]. In these cases the usual quadratic<br>programming approaches might not be able to find a global<br>maximum in feasible time.</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:81"><nobr><span class="ft3"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:898;left:112"><nobr><span class="ft20"><b>EVOLUTIONARY COMPUTATION FOR<br>LARGE MARGIN OPTIMIZATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:941;left:94"><nobr><span class="ft4">Since traditional SVMs are not able to optimize for non-</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:81"><nobr><span class="ft10">positive semidefinite kernel function, it is a very appealing<br>idea to replace the usual quadratic programming approaches<br>by an evolution strategies (ES) approach [1] or by particle<br>swarm optimization (PSO) [14]. In this section we will de-<br>scribe both a straightforward application of these techniques<br>and how we can exploit some information about our opti-<br>mization problem and incorporate that information into our<br>search operators.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:475"><nobr><span class="ft3"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:83;left:515"><nobr><span class="ft21"><b>Solving the dual problem and other sim-<br>plifications</b></span></nobr></DIV>
<DIV style="position:absolute;top:123;left:489"><nobr><span class="ft4">The used optimization problem is the dual problem for</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:475"><nobr><span class="ft10">non-linear separation of non-separable data developed in the<br>last sections (equations 31, 32, and 33). Of course it would<br>also be possible to directly optimize the original form of<br>our optimization problem depicted in equations 22 and 23.<br>That is, we could directly optimize the weight vectors and<br>the offset. As mentioned before, there are two drawbacks:<br>first, the costs of calculating the fitness function would be<br>much higher for the original optimization problem since the<br>fulfillment of all n constraints must be recalculated for each<br>new hyperplane. It is a lot easier to check if all 0</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:788"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:285;left:813"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:823"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:296;left:475"><nobr><span class="ft10">C apply. Second, it would not be possible to allow non-<br>linear learning with efficient kernel functions in the original<br>formulation of the problem. Furthermore, the kernel matrix<br>K with K</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:537"><nobr><span class="ft7">ij</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:553"><nobr><span class="ft4">= k (x</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:592"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:597"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:611"><nobr><span class="ft7">j</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:616"><nobr><span class="ft4">) can be calculated beforehand and</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:475"><nobr><span class="ft10">the training data is never used during optimization again.<br>This further reduces the needed runtime for optimization<br>since the kernel matrix calculation is done only once.</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:489"><nobr><span class="ft4">This is a nice example for a case, where transforming the</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:475"><nobr><span class="ft10">objective function beforehand is both more efficient and al-<br>lows enhancements which would not have been possible be-<br>fore. Transformations of the fitness functions became a very<br>interesting topic recently [25].</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:489"><nobr><span class="ft4">Another efficiency improvement can be achieved by for-</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:475"><nobr><span class="ft10">mulating the problem with b = 0. All solution hyperplanes<br>must then contain the origin and the constraint 33 will van-<br>ish. This is a mild restriction for high-dimensional spaces<br>since the number of degrees of freedom is only decreased by<br>one. However, during optimization we do not have to cope<br>with this equality constraint which would take an additional<br>runtime of O(n).</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:475"><nobr><span class="ft3"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:620;left:515"><nobr><span class="ft3"><b>EvoSVM and PsoSVM</b></span></nobr></DIV>
<DIV style="position:absolute;top:643;left:489"><nobr><span class="ft4">We developed a support vector machine based on evolu-</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:475"><nobr><span class="ft10">tion strategies optimization (EvoSVM). We utilized three<br>different types of mutation which will be described in this<br>section.</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:533"><nobr><span class="ft4">Furthermore, we developed another SVM based</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:475"><nobr><span class="ft10">on particle swarm optimization (PsoSVM) which is also de-<br>scribed.</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:489"><nobr><span class="ft4">The first approach (EvoSVM-G, G for Gaussian muta-</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft10">tion) merely utilizes a standard ES optimization. Individu-<br>als are the real-valued vectors  and mutation is performed<br>by adding a Gaussian distributed random variable with stan-<br>dard deviation C/10. In addition, a variance adaptation is<br>conducted during optimization (1/5 rule [18]). Crossover<br>probability is high (0.9). We use tournament selection with<br>a tournament size of 0.25 multiplied by the population size.<br>The initial individuals are random vectors with 0</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:773"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:867;left:796"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:805"><nobr><span class="ft4"> C.</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:475"><nobr><span class="ft10">The maximum number of generations is 1000 and the opti-<br>mization is terminated if no improvement occurred during<br>the last 5 generations. The population size is 10.</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:489"><nobr><span class="ft4">The second version is called EvoSVM-S (S for switching</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:475"><nobr><span class="ft10">mutation). Here we utilize the fact that only a small amount<br>of input data points will become support vectors (sparsity ).<br>On the other hand, one can often observe that non-zero<br>alpha values are equal to the upper bound C and only a very<br>small amount of support vectors exists with 0 &lt; </span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:792"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:803"><nobr><span class="ft4">&lt; C.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft10">Therefore, we just use the well known mutation of genetic<br>algorithms and switch between 0 and C with probability<br>1/n for each </span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:566"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:571"><nobr><span class="ft4">. The other parameters are equal to those</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">described for the EvoSVM-G.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1557</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="89006.png" alt="background image">
<DIV style="position:absolute;top:82;left:81"><nobr><span class="ft4">for i = 1 to n do {</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:109"><nobr><span class="ft4">if (random(0, 1) &lt; 1/n) do {</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:137"><nobr><span class="ft4">if (alpha_i &gt; 0) do {</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:165"><nobr><span class="ft4">alpha_i = 0;</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:137"><nobr><span class="ft4">} else do {</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:165"><nobr><span class="ft4">alpha_i = random(0, C);</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:137"><nobr><span class="ft4">}</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:109"><nobr><span class="ft4">}</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:81"><nobr><span class="ft4">}</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:81"><nobr><span class="ft10">Figure 4: A simple hybrid mutation which should<br>speed-up the search for sparser solutions.</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:391"><nobr><span class="ft4">It con-</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:81"><nobr><span class="ft10">tains elements from standard mutations from both<br>genetic algorithms and evolution strategies.</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:94"><nobr><span class="ft4">Using this switching mutation inspired by genetic algo-</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:81"><nobr><span class="ft4">rithms only allow </span></nobr></DIV>
<DIV style="position:absolute;top:351;left:200"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:210"><nobr><span class="ft4">= 0 or </span></nobr></DIV>
<DIV style="position:absolute;top:351;left:263"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:272"><nobr><span class="ft4">= C. Instead of a complete</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:81"><nobr><span class="ft10">switch between 0 and C or a smooth change of all values<br></span></nobr></DIV>
<DIV style="position:absolute;top:383;left:89"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:99"><nobr><span class="ft4">like the Gaussian mutation does, we developed a hybrid</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:81"><nobr><span class="ft10">mutation combining both elements. That means that we<br>check for each </span></nobr></DIV>
<DIV style="position:absolute;top:414;left:177"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:185"><nobr><span class="ft4">with probability 1/n if the value should be</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:81"><nobr><span class="ft4">mutated at all. If the current value </span></nobr></DIV>
<DIV style="position:absolute;top:430;left:300"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:309"><nobr><span class="ft4">is greater than 0, </span></nobr></DIV>
<DIV style="position:absolute;top:430;left:422"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:430"><nobr><span class="ft4">is</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:81"><nobr><span class="ft4">set to 0. If </span></nobr></DIV>
<DIV style="position:absolute;top:446;left:157"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:166"><nobr><span class="ft4">is equal to 0, </span></nobr></DIV>
<DIV style="position:absolute;top:446;left:256"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:265"><nobr><span class="ft4">is set to a random value with</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:81"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:93"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:461;left:119"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:129"><nobr><span class="ft4"> C. Figure 4 gives an overview over this hybrid</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:81"><nobr><span class="ft10">mutation. The function random(a, b) returns an uniformly<br>distributed random number between a and b. The other pa-<br>rameters are the same as described for the EvoSVM-G. We<br>call this version EvoSVM-H (H for hybrid).</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:94"><nobr><span class="ft4">As was mentioned before, the optimization problem usu-</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:81"><nobr><span class="ft10">ally is concave and the risk for local extrema is small. There-<br>fore, we also applied a PSO technique. It should be inves-<br>tigated if PSO, which is similar to the usual quadratic pro-<br>gramming approaches for SVMs in a sense that the gradient<br>information is exploited, is able to find a global optimum in<br>shorter time. We call this last version PsoSVM and use a<br>standard PSO with inertia weight 0.1, local best weight 1.0,<br>and global best weight 1.0. The inertia weight is dynami-<br>cally adapted during optimization [14].</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft3"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:715;left:112"><nobr><span class="ft3"><b>EXPERIMENTS AND RESULTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:94"><nobr><span class="ft4">In this section we try to evaluate the proposed evolution-</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:81"><nobr><span class="ft10">ary optimization SVMs. We compare our implementation to<br>the quadratic programming approaches usually applied to<br>large margin problems. The experiments demonstrate the<br>competitiveness in terms of classification error minimization,<br>runtime, and robustness.</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:94"><nobr><span class="ft4">We apply the discussed EvoSVM variants as well as the</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft10">PsoSVM on six real-world benchmark datasets. We selected<br>these datasets from the UCI machine learning repository<br>[16] and the StatLib dataset library [24], because they al-<br>ready define a binary classification task, consist of real-<br>valued numbers only and do not contain missing values.<br>Therefore, we did not need to perform additional prepro-<br>cessing steps which might introduce some bias. The proper-<br>ties of all datasets are summarized in Table 1. The default<br>error corresponds to the error a lazy default classifier would<br>make by always predicting the major class. Classifiers must<br>produce lower error rates in order to learn at all instead of<br>just guessing.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:94"><nobr><span class="ft4">In order to compare the evolutionary SVMs described</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft10">in this paper with standard implementations we also ap-<br>plied two other SVMs on all datasets. Both SVMs use a</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:498"><nobr><span class="ft4">Dataset</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:586"><nobr><span class="ft4">n</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:618"><nobr><span class="ft4">m</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:648"><nobr><span class="ft4">Source</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:723"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:88;left:759"><nobr><span class="ft4">Default</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:498"><nobr><span class="ft4">Liver</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:580"><nobr><span class="ft4">346</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:621"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:658"><nobr><span class="ft4">UCI</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:711"><nobr><span class="ft4">0.010</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:770"><nobr><span class="ft4">42.03</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:498"><nobr><span class="ft4">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:580"><nobr><span class="ft4">351</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:617"><nobr><span class="ft4">34</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:658"><nobr><span class="ft4">UCI</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:711"><nobr><span class="ft4">1.000</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:770"><nobr><span class="ft4">35.90</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:498"><nobr><span class="ft4">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:580"><nobr><span class="ft4">208</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:617"><nobr><span class="ft4">60</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:658"><nobr><span class="ft4">UCI</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:711"><nobr><span class="ft4">1.000</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:770"><nobr><span class="ft4">46.62</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:498"><nobr><span class="ft4">Lawsuit</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:580"><nobr><span class="ft4">264</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:621"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:648"><nobr><span class="ft4">StatLib</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:711"><nobr><span class="ft4">0.010</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:773"><nobr><span class="ft4">7.17</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:498"><nobr><span class="ft4">Lupus</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:583"><nobr><span class="ft4">87</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:621"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:648"><nobr><span class="ft4">StatLib</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:711"><nobr><span class="ft4">0.001</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:770"><nobr><span class="ft4">40.00</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:498"><nobr><span class="ft4">Crabs</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:580"><nobr><span class="ft4">200</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:621"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:648"><nobr><span class="ft4">StatLib</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:711"><nobr><span class="ft4">0.100</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:770"><nobr><span class="ft4">50.00</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:475"><nobr><span class="ft4">Table 1: The evaluation datasets.</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:734"><nobr><span class="ft4">n is the num-</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:475"><nobr><span class="ft10">ber of data points, m is the dimension of the input<br>space.</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:529"><nobr><span class="ft4">The kernel parameter  was optimized for</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:475"><nobr><span class="ft4">the comparison SVM learner</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:682"><nobr><span class="ft4">mySVM. The last col-</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:475"><nobr><span class="ft10">umn contains the default error, i. e. the error for<br>always predicting the major class.</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:475"><nobr><span class="ft10">slightly different optimization technique based on quadratic<br>programming. The used implementations were mySVM [20]<br>and LibSVM [4]. The latter is an adaptation of the widely<br>used SV M</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:542"><nobr><span class="ft7">light</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:572"><nobr><span class="ft4">[12].</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:489"><nobr><span class="ft4">We use a RBF kernel for all SVMs and determine the</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:475"><nobr><span class="ft10">best parameter value for  with a grid search parameter op-<br>timization for mySVM. This ensures a fair comparison since<br>the parameter is not optimized for one of the evolutionary<br>SVMs. Possible parameters were 0.001, 0.01, 0.1, 1 and 10.<br>The optimal value for each dataset is also given in Table 1.</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:489"><nobr><span class="ft4">In order to determine the performance of all methods we</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:475"><nobr><span class="ft10">perform a k-fold cross validation. That means that the<br>dataset T is divided into k disjoint subsets T</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:761"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:765"><nobr><span class="ft4">. For each</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:475"><nobr><span class="ft4">i  {1, . . . , k} we use T \T</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:627"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:636"><nobr><span class="ft4">as training set and the remaining</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:475"><nobr><span class="ft4">subset T</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:526"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:536"><nobr><span class="ft4">as test set. If F</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:634"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:644"><nobr><span class="ft4">is the number of wrong predic-</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:475"><nobr><span class="ft4">tions on test set T</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:590"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:601"><nobr><span class="ft4">we calculate the average classification</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:475"><nobr><span class="ft4">error</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:610"><nobr><span class="ft4">E = 1</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:641"><nobr><span class="ft4">k</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:660"><nobr><span class="ft7">k</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:653"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:654"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:658"><nobr><span class="ft7">=1</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:680"><nobr><span class="ft4">F</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:689"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:677"><nobr><span class="ft4">|T</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:689"><nobr><span class="ft7">i</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:694"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:809"><nobr><span class="ft4">(34)</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:475"><nobr><span class="ft10">over all test sets in order to measure the classification per-<br>formance. In our experiments we choose k = 20, i. e. for<br>each evolutionary method the average and standard devia-<br>tion of 20 runs is reported. All experiments were performed<br>with the machine learning environment</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:717"><nobr><span class="ft4">Yale [5].</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:489"><nobr><span class="ft4">Table 2 summarizes the results for different values of C.</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:475"><nobr><span class="ft10">It can be seen that the EvoSVM variants frequently yield<br>smaller classification errors than the quadratic programming<br>counterparts (mySVM and LibSVM). For C = 1, a statisti-<br>cal significant better result was achieved by using LibSVM<br>only for the Liver data set. For all other datasets the evo-<br>lutionary optimization outperforms the quadratic program-<br>ming approaches. The same applies for C = 0.1. For rather<br>small values of C most learning schemes were not able to<br>produce better predictions than the default classifier. For<br>C = 0.01, however, PsoSVM at least provides a similar<br>accuracy to LibSVM. The reason for higher errors of the<br>quadratic programming approaches is probably a too ag-<br>gressive termination criterion. Although this termination<br>behavior further reduces runtime for mySVM and LibSVM,<br>the classification error is often increased.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:489"><nobr><span class="ft4">It turns out that the standard ES approach EvoSVM-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:475"><nobr><span class="ft10">G using a mutation adding a Gaussian distributed random<br>variable often outperforms the other SVMs. However, the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1558</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="89007.png" alt="background image">
<DIV style="position:absolute;top:85;left:444"><nobr><span class="ft18">C = 1</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:234"><nobr><span class="ft18">Liver</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:318"><nobr><span class="ft18">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:427"><nobr><span class="ft18">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:517"><nobr><span class="ft18">Lawsuit</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:620"><nobr><span class="ft18">Lupus</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:724"><nobr><span class="ft18">Crabs</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:221"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:281"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:319"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:374"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:415"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:473"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:510"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:565"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:608"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:670"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:712"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:770"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:136"><nobr><span class="ft18">EvoSVM-G</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:208"><nobr><span class="ft18">34.71±8.60</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:279"><nobr><span class="ft18">68</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:306"><nobr><span class="ft18">10.81±5.71</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:373"><nobr><span class="ft18">80</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:400"><nobr><span class="ft18">14.03±4.52</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:471"><nobr><span class="ft18">26</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:500"><nobr><span class="ft18">2.05±1.87</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:563"><nobr><span class="ft18">52</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:593"><nobr><span class="ft18">25.20±11.77</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:671"><nobr><span class="ft18">8</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:700"><nobr><span class="ft18">2.25±3.72</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:768"><nobr><span class="ft18">25</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:136"><nobr><span class="ft18">EvoSVM-S</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:208"><nobr><span class="ft18">35.37±6.39</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:282"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:309"><nobr><span class="ft18">8.49±3.80</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:375"><nobr><span class="ft18">9</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:402"><nobr><span class="ft18">17.45±6.64</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:474"><nobr><span class="ft18">6</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:500"><nobr><span class="ft18">2.40±1.91</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:563"><nobr><span class="ft18">10</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:593"><nobr><span class="ft18">30.92±12.42</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:667"><nobr><span class="ft18">&lt;1</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:702"><nobr><span class="ft18">4.05±4.63</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:771"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:136"><nobr><span class="ft18">EvoSVM-H</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:208"><nobr><span class="ft18">34.97±7.32</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:282"><nobr><span class="ft18">7</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:307"><nobr><span class="ft18">6.83±3.87</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:373"><nobr><span class="ft18">22</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:402"><nobr><span class="ft18">15.41±6.39</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:471"><nobr><span class="ft18">10</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:500"><nobr><span class="ft18">2.01±1.87</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:563"><nobr><span class="ft18">14</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:590"><nobr><span class="ft18">24.03±13.68</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:702"><nobr><span class="ft18">3.95±4.31</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:771"><nobr><span class="ft18">7</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:136"><nobr><span class="ft18">PsoSVM</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:208"><nobr><span class="ft18">34.78±4.95</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:282"><nobr><span class="ft18">8</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:309"><nobr><span class="ft18">9.90±4.38</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:375"><nobr><span class="ft18">9</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:402"><nobr><span class="ft18">16.94±5.61</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:474"><nobr><span class="ft18">7</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:500"><nobr><span class="ft18">3.02±2.83</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:566"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:594"><nobr><span class="ft18">25.22± 7.67</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:667"><nobr><span class="ft18">&lt;1</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:702"><nobr><span class="ft18">3.40±3.70</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:771"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:136"><nobr><span class="ft18">mySVM</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:208"><nobr><span class="ft18">33.62±4.31</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:309"><nobr><span class="ft18">8.56±4.25</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:375"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:402"><nobr><span class="ft18">15.81±5.59</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:474"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:498"><nobr><span class="ft18">1.89±2.51</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:594"><nobr><span class="ft18">25.28± 8.58</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:702"><nobr><span class="ft18">3.00±3.32</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:136"><nobr><span class="ft18">LibSVM</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:206"><nobr><span class="ft18">32.72±5.41</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:309"><nobr><span class="ft18">7.70±3.63</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:375"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:402"><nobr><span class="ft18">14.60±4.96</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:474"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:500"><nobr><span class="ft18">2.41±2.64</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:593"><nobr><span class="ft18">24.14±12.33</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:702"><nobr><span class="ft18">3.00±4.58</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:136"><nobr><span class="ft18">F Test</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:220"><nobr><span class="ft18">3.20 (0.01)</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:318"><nobr><span class="ft18">9.78 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:414"><nobr><span class="ft18">6.19 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:512"><nobr><span class="ft18">1.51 (0.19)</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:606"><nobr><span class="ft18">11.94 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:711"><nobr><span class="ft18">2.25 (0.05)</span></nobr></DIV>
<DIV style="position:absolute;top:226;left:440"><nobr><span class="ft18">C = 0.1</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:234"><nobr><span class="ft18">Liver</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:318"><nobr><span class="ft18">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:427"><nobr><span class="ft18">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:517"><nobr><span class="ft18">Lawsuit</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:620"><nobr><span class="ft18">Lupus</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:724"><nobr><span class="ft18">Crabs</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:221"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:281"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:319"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:374"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:415"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:473"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:510"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:565"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:608"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:670"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:712"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:770"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:136"><nobr><span class="ft18">EvoSVM-G</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:208"><nobr><span class="ft18">33.90±4.19</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:279"><nobr><span class="ft18">74</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:309"><nobr><span class="ft18">9.40±6.14</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:373"><nobr><span class="ft18">89</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:400"><nobr><span class="ft18">21.72±6.63</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:471"><nobr><span class="ft18">35</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:498"><nobr><span class="ft18">2.35±1.92</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:563"><nobr><span class="ft18">50</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:590"><nobr><span class="ft18">24.90±10.51</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:671"><nobr><span class="ft18">7</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:702"><nobr><span class="ft18">7.20±4.36</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:768"><nobr><span class="ft18">27</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:136"><nobr><span class="ft18">EvoSVM-S</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:208"><nobr><span class="ft18">35.57±3.55</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:282"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:309"><nobr><span class="ft18">7.12±3.54</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:373"><nobr><span class="ft18">18</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:402"><nobr><span class="ft18">24.90±6.62</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:474"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:500"><nobr><span class="ft18">4.47±2.31</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:563"><nobr><span class="ft18">13</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:593"><nobr><span class="ft18">25.98±12.56</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:667"><nobr><span class="ft18">&lt;1</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:702"><nobr><span class="ft18">7.95±5.68</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:771"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:136"><nobr><span class="ft18">EvoSVM-H</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:208"><nobr><span class="ft18">34.76±4.70</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:282"><nobr><span class="ft18">5</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:307"><nobr><span class="ft18">6.55±4.61</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:373"><nobr><span class="ft18">23</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:402"><nobr><span class="ft18">24.40±6.09</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:471"><nobr><span class="ft18">11</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:500"><nobr><span class="ft18">4.16±3.14</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:563"><nobr><span class="ft18">19</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:593"><nobr><span class="ft18">26.51±13.03</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:702"><nobr><span class="ft18">6.50±5.02</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:771"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:136"><nobr><span class="ft18">PsoSVM</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:208"><nobr><span class="ft18">36.81±5.04</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:282"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:306"><nobr><span class="ft18">13.96±7.56</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:373"><nobr><span class="ft18">10</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:402"><nobr><span class="ft18">24.18±6.11</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:474"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:500"><nobr><span class="ft18">3.03±2.83</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:566"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:593"><nobr><span class="ft18">29.86±12.84</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:702"><nobr><span class="ft18">8.15±6.02</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:136"><nobr><span class="ft18">mySVM</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:375"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:402"><nobr><span class="ft18">46.62±1.62</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:474"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:595"><nobr><span class="ft18">41.25±6.92</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:700"><nobr><span class="ft18">6.50±4.50</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:136"><nobr><span class="ft18">LibSVM</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:203"><nobr><span class="ft18">33.08±10.63</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:306"><nobr><span class="ft18">11.40±6.52</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:375"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:402"><nobr><span class="ft18">22.40±6.45</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:474"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:500"><nobr><span class="ft18">4.55±3.25</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:593"><nobr><span class="ft18">25.29±16.95</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:697"><nobr><span class="ft18">21.00±12.41</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:136"><nobr><span class="ft18">F Test</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:217"><nobr><span class="ft18">34.46 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:312"><nobr><span class="ft18">492.88 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:408"><nobr><span class="ft18">323.83 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:507"><nobr><span class="ft18">20.64 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:606"><nobr><span class="ft18">64.83 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:706"><nobr><span class="ft18">100.92 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:437"><nobr><span class="ft18">C = 0.01</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:234"><nobr><span class="ft18">Liver</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:318"><nobr><span class="ft18">Ionosphere</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:427"><nobr><span class="ft18">Sonar</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:517"><nobr><span class="ft18">Lawsuit</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:620"><nobr><span class="ft18">Lupus</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:724"><nobr><span class="ft18">Crabs</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:221"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:281"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:319"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:374"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:415"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:473"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:510"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:565"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:608"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:670"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:712"><nobr><span class="ft18">Error</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:770"><nobr><span class="ft18">T</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:136"><nobr><span class="ft18">EvoSVM-G</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:279"><nobr><span class="ft18">75</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:373"><nobr><span class="ft18">86</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:402"><nobr><span class="ft18">45.33±2.20</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:471"><nobr><span class="ft18">39</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:563"><nobr><span class="ft18">55</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:595"><nobr><span class="ft18">40.00±6.33</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:671"><nobr><span class="ft18">7</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:697"><nobr><span class="ft18">26.20±12.66</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:768"><nobr><span class="ft18">27</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:136"><nobr><span class="ft18">EvoSVM-S</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:282"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:375"><nobr><span class="ft18">9</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:402"><nobr><span class="ft18">46.62±1.62</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:474"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:566"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:595"><nobr><span class="ft18">40.00±6.33</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:667"><nobr><span class="ft18">&lt;1</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:702"><nobr><span class="ft18">8.58±4.35</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:136"><nobr><span class="ft18">EvoSVM-H</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:282"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:373"><nobr><span class="ft18">20</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:402"><nobr><span class="ft18">46.27±1.42</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:471"><nobr><span class="ft18">12</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:566"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:595"><nobr><span class="ft18">40.00±6.33</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:702"><nobr><span class="ft18">7.00±4.00</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:771"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:136"><nobr><span class="ft18">PsoSVM</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:206"><nobr><span class="ft18">41.39±8.59</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:282"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:375"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:400"><nobr><span class="ft18">27.90±6.28</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:474"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:566"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:593"><nobr><span class="ft18">31.94±12.70</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:699"><nobr><span class="ft18">10.05±7.26</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:136"><nobr><span class="ft18">mySVM</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:375"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:402"><nobr><span class="ft18">46.62±1.62</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:474"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:595"><nobr><span class="ft18">40.00±6.33</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:700"><nobr><span class="ft18">6.50±4.50</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:136"><nobr><span class="ft18">LibSVM</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:208"><nobr><span class="ft18">42.03±1.46</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:282"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:306"><nobr><span class="ft18">35.90±1.35</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:375"><nobr><span class="ft18">3</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:400"><nobr><span class="ft18">28.46±10.44</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:474"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:500"><nobr><span class="ft18">7.17±2.55</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:566"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:590"><nobr><span class="ft18">26.11±16.44</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:671"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:699"><nobr><span class="ft18">50.00±0.00</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:771"><nobr><span class="ft18">1</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:136"><nobr><span class="ft18">F Test</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:222"><nobr><span class="ft18">0.52 (0.77)</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:321"><nobr><span class="ft18">0.00 (1.00)</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:408"><nobr><span class="ft18">442.46 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:512"><nobr><span class="ft18">0.00 (1.00)</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:606"><nobr><span class="ft18">78.27 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:703"><nobr><span class="ft18">1095.94 (0.00)</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:81"><nobr><span class="ft10">Table 2: Classification error, standard deviation, and runtime of all SVMs on the evaluation datasets for<br>parameters C = 1, C = 0.1, and C = 0.01. The runtime T is given in seconds. The last line for each table<br>depicts the F test value and the probability that the results are not statistical significant.</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:81"><nobr><span class="ft10">runtime for this approach is far to big to be feasible in<br>practical situations. The mere GA based selection muta-<br>tion switching between 0 and C converges much faster but<br>is often less accurate. The remaining runtime differences<br>between EvoSVM-S and the quadratic programming coun-<br>terparts can surely be reduced by code optimization. The<br>used SVM implementations are matured and have been op-<br>timized over the years whereas the implementations of the<br>evolutionary approaches follow standard recipes without any<br>code optimization.</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:94"><nobr><span class="ft4">The hybrid version EvoSVM-H combines the best ele-</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:81"><nobr><span class="ft10">ments of both worlds. It converges nearly as fast as the<br>EvoSVM-S and is often nearly as accurate as the EvoSVM-<br>G. In some cases (Ionosphere, Lupus) it even outperforms<br>all other SVMs.</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:94"><nobr><span class="ft4">PsoSVM on the other hand does not provide the best</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:81"><nobr><span class="ft10">performance in terms of classification error. Compared to<br>the other evolutionary approaches, however, it converged<br>much earlier than the other competitors.</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:94"><nobr><span class="ft4">Please note that the standard deviations of the errors</span></nobr></DIV>
<DIV style="position:absolute;top:920;left:81"><nobr><span class="ft10">achieved with the evolutionary SVMs are similar to the stan-<br>dard deviations achieved with mySVM or LibSVM. We can<br>therefore conclude that the evolutionary optimization is as<br>robust as the quadratic programming approaches and differ-<br>ences mainly derives from different subsets for training and<br>testing due to cross validation instead of the used random-<br>ized heuristics.</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:94"><nobr><span class="ft4">Therefore, evolutionary SVMs provide an interesting al-</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:81"><nobr><span class="ft10">ternative to more traditional SVM implementations. Beside<br>the similar results EvoSVM is also able to cope with non-</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:475"><nobr><span class="ft10">positive definite kernel functions and multivariate optimiza-<br>tion.</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:475"><nobr><span class="ft3"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:652;left:506"><nobr><span class="ft3"><b>CONCLUSION</b></span></nobr></DIV>
<DIV style="position:absolute;top:674;left:489"><nobr><span class="ft4">In this paper we connected evolutionary computation with</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:475"><nobr><span class="ft10">statistical learning theory. The idea of large margin meth-<br>ods was very successful in many applications from machine<br>learning and data mining.</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:651"><nobr><span class="ft4">We used the most prominent</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:475"><nobr><span class="ft10">representative of this paradigm, namely Support Vector Ma-<br>chines, and employed evolution strategies and particle swarm<br>optimization in order to solve the constrained optimization<br>problem at hand. We developed a hybrid mutation which<br>decreases convergence time while the classification accuracy<br>is preserved.</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:489"><nobr><span class="ft4">An interesting property of large margin methods is that</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:475"><nobr><span class="ft10">the runtime for fitness evaluation is reduced by transforming<br>the problem into the dual problem. In our case, the algo-<br>rithm is both faster and provides space for other improve-<br>ments like incorporating a kernel function for non-linear<br>classification tasks. This is a nice example how a transfor-<br>mation into the dual optimization problem can be exploited<br>by evolutionary algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:489"><nobr><span class="ft4">We have seen that evolutionary SVMs are at least as ac-</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft10">curate as their quadratic programming counterparts. For<br>practical values of C the evolutionary SVM variants fre-<br>quently outperformed their competitors. We can conclude<br>that evolutionary algorithms proved as reliable as other op-<br>timization schemes for this type of problems. In addition,<br>beside the inherent advantages of evolutionary algorithms<br>(e. g. parallelization, multi-objective optimization of train-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1559</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="89008.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">ing error and capacity) it is now also possible to employ<br>non positive semidefinite kernel functions which would lead<br>to unsolvable problems for other optimization techniques.<br>In our future work we plan to make experiments with such<br>non positive semidefinite kernel functions. This also applies<br>for multi-objective optimization of both the margin and the<br>training error.</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:94"><nobr><span class="ft4">It turns out that the hybrid mutation delivers results</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft10">nearly as accurate as the Gaussian mutation and has a sim-<br>ilar convergence behavior compared to the switching mu-<br>tation known from GAs. Future improvements could start<br>with a switching mutation and can post-optimize with a<br>Gaussian mutation after a first convergence. Values always<br>remaining 0 or C during the first run could be omitted in<br>the post-optimization step. It is possible that this mutation<br>is even faster and more accurate then EvoSVM-H.</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:81"><nobr><span class="ft3"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:354;left:112"><nobr><span class="ft3"><b>ACKNOWLEDGMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:377;left:94"><nobr><span class="ft4">This work was supported by the Deutsche Forschungsge-</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:81"><nobr><span class="ft10">meinschaft (DFG) within the Collaborative Research Center<br>"Reduction of Complexity for Multivariate Data Structures".</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:81"><nobr><span class="ft3"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:441;left:112"><nobr><span class="ft3"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:464;left:88"><nobr><span class="ft4">[1] H.-G. Beyer and H.-P. Schwefel. Evolution strategies:</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:109"><nobr><span class="ft10">A comprehensive introduction. Journal Natural<br>Computing, 1(1):2­52, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:88"><nobr><span class="ft4">[2] C. Burges. A tutorial on support vector machines for</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:109"><nobr><span class="ft10">pattern recognition. Data Mining and Knowledge<br>Discovery, 2(2):121­167, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:88"><nobr><span class="ft4">[3] G. Camps-Valls, J. Martin-Guerrero, J. Rojo-Alvarez,</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:109"><nobr><span class="ft10">and E. Soria-Olivas. Fuzzy sigmoid kernel for support<br>vector classifiers. Neurocomputing, 62:501­506, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:88"><nobr><span class="ft4">[4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:109"><nobr><span class="ft4">support vector machines, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:88"><nobr><span class="ft4">[5] S. Fischer, R. Klinkenberg, I. Mierswa, and</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:109"><nobr><span class="ft10">O. Ritthoff. Yale: Yet Another Learning Environment<br>­ Tutorial. Technical Report CI-136/02, Collaborative<br>Research Center 531, University of Dortmund,<br>Dortmund, Germany, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:88"><nobr><span class="ft4">[6] F. Friedrichs and C. Igel. Evolutionary tuning of</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:109"><nobr><span class="ft10">multiple svm parameters. In Proc. of the 12th<br>European Symposium on Artificial Neural Networks<br>(ESANN 2004), pages 519­524, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:88"><nobr><span class="ft4">[7] H. Fr¨</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:141"><nobr><span class="ft4">phlich, O. Chapelle, and B. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:333"><nobr><span class="ft4">olkopf. Feature</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:109"><nobr><span class="ft10">selection for support vector machines using genetic<br>algorithms. International Journal on Artificial<br>Intelligence Tools, 13(4):791­800, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:88"><nobr><span class="ft4">[8] B. Haasdonk. Feature space interpretation of svms</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:109"><nobr><span class="ft10">with indefinite kernels. IEEE Transactions on Pattern<br>Analysis and Machine Intelligence, 27(4):482­492,<br>2005.</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:88"><nobr><span class="ft4">[9] T. Hastie, R. Tibshirani, and J. Friedman. The</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:109"><nobr><span class="ft10">Elements of Statistical Learning: Data Mining,<br>Inference, and Prediction. Springer Series in Statistics.<br>Springer, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:81"><nobr><span class="ft4">[10] T. Howley and M. Madden. The genetic kernel</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:109"><nobr><span class="ft10">support vector machine: Description and evaluation.<br>Artificial Intelligence Review, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:81"><nobr><span class="ft4">[11] W. James and C. Stein. Estimation with quadratic</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:109"><nobr><span class="ft10">loss. In Proceedings of the Fourth Berkeley Symposium<br>on Mathematics, Statistics and Probability,</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:107"><nobr><span class="ft4">pages 361­380, 1960.</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:475"><nobr><span class="ft4">[12] T. Joachims. Making large-scale SVM learning</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:503"><nobr><span class="ft4">practical. In B. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:621"><nobr><span class="ft4">olkopf, C. Burges, and A. Smola,</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:503"><nobr><span class="ft10">editors, Advances in Kernel Methods - Support Vector<br>Learning, chapter 11. MIT Press, Cambridge, MA,<br>1999.</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:475"><nobr><span class="ft4">[13] T. Joachims. A support vector method for</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:503"><nobr><span class="ft10">multivariate performance measures. In Proc. of the<br>International Conference on Machine Learning<br>(ICML), pages 377­384, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:475"><nobr><span class="ft4">[14] J. Kennedy and R. C. Eberhart. Particle swarm</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:503"><nobr><span class="ft10">optimization. In Proc. of the International Conference<br>on Neural Networks, pages 1942­1948, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:475"><nobr><span class="ft4">[15] H.-T. Lin and C.-J. Lin. A study on sigmoid kernels</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:503"><nobr><span class="ft10">for svm and the training of non-psd kernels by<br>smo-type methods, March 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:475"><nobr><span class="ft4">[16] D. Newman, S. Hettich, C. Blake, and C. Merz. UCI</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:503"><nobr><span class="ft10">repository of machine learning databases, 1998.<br>http://www.ics.uci.edu/</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:649"><nobr><span class="ft4">mlearn/MLRepository.html.</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:475"><nobr><span class="ft4">[17] C. Ong, X. Mary, S. Canu, and A. J. Smola. Learning</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:503"><nobr><span class="ft10">with non-positive kernels. In Proc. of the 21st<br>International Conference on Machine Learning<br>(ICML), pages 639­646, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:475"><nobr><span class="ft4">[18] I. Rechenberg. Evolutionsstrategie: Optimierung</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:503"><nobr><span class="ft10">technischer Systeme nach Prinzipien der biologischen<br>Evolution. Frommann-Holzboog, 1973.</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:475"><nobr><span class="ft4">[19] T. Runarsson and S. Sigurdsson. Asynchronous</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:503"><nobr><span class="ft10">parallel evolutionary model selection for support<br>vector machines. Neural Information Processing,<br>3(3):59­67, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:475"><nobr><span class="ft4">[20] S. R¨</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:530"><nobr><span class="ft4">uping. mySVM Manual. Universit¨</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:734"><nobr><span class="ft4">at Dortmund,</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:503"><nobr><span class="ft10">Lehrstuhl Informatik VIII, 2000. http://www-<br>ai.cs.uni-dortmund.de/SOFTWARE/MYSVM/.</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:475"><nobr><span class="ft4">[21] B. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:543"><nobr><span class="ft4">olkopf and A. J. Smola. Learning with Kernels ­</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:503"><nobr><span class="ft10">Support Vector Machines, Regularization,<br>Optimization, and Beyond. MIT Press, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:475"><nobr><span class="ft4">[22] A. Smola, B. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:607"><nobr><span class="ft4">olkopf, and K.-R. M¨</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:729"><nobr><span class="ft4">uller. General</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:503"><nobr><span class="ft10">cost functions for support vector regression. In<br>Proceedings of the 8th International Conference on<br>Artificial Neural Networks, pages 79­83, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:475"><nobr><span class="ft4">[23] A. J. Smola, Z. L. Ovari, and R. C. Williamson.</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:503"><nobr><span class="ft10">Regularization with dot-product kernels. In Proc. of<br>the Neural Information Processing Systems (NIPS),<br>pages 308­314, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:475"><nobr><span class="ft4">[24] Statlib ­ datasets archive.</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:503"><nobr><span class="ft4">http://lib.stat.cmu.edu/datasets/.</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:475"><nobr><span class="ft4">[25] T. Storch. On the impact of objective function</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:503"><nobr><span class="ft10">transformations on evolutionary and black-box<br>algorithms. In Proc. of the Genetic and Evolutionary<br>Computation Conference (GECCO), pages 833­840,<br>2005.</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:475"><nobr><span class="ft4">[26] B. Taskar, V. Chatalbashev, D. Koller, and</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:503"><nobr><span class="ft10">C. Guestrin. Learning structured prediction models: A<br>large margin approach. In Proc. of the International<br>Conference on Machine Learning (ICML), 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft4">[27] V. Vapnik. Statistical Learning Theory. Wiley, New</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:503"><nobr><span class="ft4">York, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:475"><nobr><span class="ft4">[28] V. Vapnik and A. Chervonenkis. The necessary and</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:503"><nobr><span class="ft10">sufficient conditions for consistency in the empirical<br>risk minimization method. Pattern Recognition and<br>Image Analysis, 1(3):283­305, 1991.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:446"><nobr><span class="ft9">1560</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
