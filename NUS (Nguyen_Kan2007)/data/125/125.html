<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\125</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-08-05T21:37:27+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft8{font-size:11px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft10{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="941" height="1235" src="125001.png" alt="background image">
<DIV style="position:absolute;top:115;left:97"><nobr><span class="ft0"><b>Learning Spatially Variant Dissimilarity (SVaD) Measures</b></span></nobr></DIV>
<DIV style="position:absolute;top:183;left:154"><nobr><span class="ft1">Krishna Kummamuru</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:158"><nobr><span class="ft2">IBM India Research Lab</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:161"><nobr><span class="ft2">Block 1, IIT, Hauz Khas</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:155"><nobr><span class="ft2">New Delhi 110016 INDIA</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:139"><nobr><span class="ft1">kkummamu@in.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:373"><nobr><span class="ft1">Raghu Krishnapuram</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:377"><nobr><span class="ft2">IBM India Research Lab</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:380"><nobr><span class="ft2">Block 1, IIT, Hauz Khas</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:374"><nobr><span class="ft2">New Delhi 110016 INDIA</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:370"><nobr><span class="ft1">kraghura@in.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:612"><nobr><span class="ft1">Rakesh Agrawal</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:598"><nobr><span class="ft2">IBM Almaden Research</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:654"><nobr><span class="ft2">Center</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:589"><nobr><span class="ft2">San Jose, CA 95120, USA</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:575"><nobr><span class="ft1">ragrawal@almaden.ibm.com</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:81"><nobr><span class="ft3"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:343;left:81"><nobr><span class="ft9">Clustering algorithms typically operate on a feature vector<br>representation of the data and find clusters that are compact<br>with respect to an assumed (dis)similarity measure between<br>the data points in feature space. This makes the type of clus-<br>ters identified highly dependent on the assumed similarity<br>measure. Building on recent work in this area, we formally<br>define a class of spatially varying dissimilarity measures and<br>propose algorithms to learn the dissimilarity measure auto-<br>matically from the data. The idea is to identify clusters that<br>are compact with respect to the unknown spatially vary-<br>ing dissimilarity measure. Our experiments show that the<br>proposed algorithms are more stable and achieve better ac-<br>curacy on various textual data sets when compared with<br>similar algorithms proposed in the literature.</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:81"><nobr><span class="ft3"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:603;left:81"><nobr><span class="ft9">H.2.8 [Database Management]: Database Applications--<br>Data Mining</span></nobr></DIV>
<DIV style="position:absolute;top:650;left:81"><nobr><span class="ft3"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:675;left:81"><nobr><span class="ft4">Algorithms</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:81"><nobr><span class="ft3"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:731;left:81"><nobr><span class="ft4">Clustering, Learning Dissimilarity Measures</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:81"><nobr><span class="ft3"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:762;left:112"><nobr><span class="ft3"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:784;left:94"><nobr><span class="ft4">Clustering plays a major role in data mining as a tool</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft9">to discover structure in data. Object clustering algorithms<br>operate on a feature vector representation of the data and<br>find clusters that are compact with respect to an assumed<br>(dis)similarity measure between the data points in feature<br>space. As a consequence, the nature of clusters identified by<br>a clustering algorithm is highly dependent on the assumed<br>similarity measure. The most commonly used dissimilarity<br>measure, namely the Euclidean metric, assumes that the dis-<br>similarity measure is isotropic and spatially invariant, and</span></nobr></DIV>
<DIV style="position:absolute;top:984;left:81"><nobr><span class="ft10">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>KDD'04, </i>August 22­25, 2004, Seattle, Washington, USA.<br>Copyright 2004 ACM 1-58113-888-1/04/0008 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1077;left:315"><nobr><span class="ft4">$</span></nobr></DIV>
<DIV style="position:absolute;top:1078;left:322"><nobr><span class="ft5">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:475"><nobr><span class="ft9">it is effective only when the clusters are roughly spherical<br>and all of them have approximately the same size, which is<br>rarely the case in practice [8]. The problem of finding non-<br>spherical clusters is often addressed by utilizing a feature<br>weighting technique. These techniques discover a single set<br>of weights such that relevant features are given more impor-<br>tance than irrelevant features. However, in practice, each<br>cluster may have a different set of relevant features. We<br>consider Spatially Varying Dissimilarity (SVaD) measures<br>to address this problem.</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:489"><nobr><span class="ft4">Diday et. al. [4] proposed the adaptive distance dynamic</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:475"><nobr><span class="ft9">clusters (ADDC) algorithm in this vain. A fuzzified version<br>of ADDC, popularly known as the Gustafson-Kessel (GK)<br>algorithm [7] uses a dynamically updated covariance matrix<br>so that each cluster can have its own norm matrix. These al-<br>gorithms can deal with hyperelliposoidal clusters of various<br>sizes and orientations. The EM algorithm [2] with Gaussian<br>probability distributions can also be used to achieve similar<br>results. However, the above algorithms are computationally<br>expensive for high-dimensional data since they invert covari-<br>ance matrices in every iteration. Moreover, matrix inversion<br>can be unstable when the data is sparse in relation to the<br>dimensionality.</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:489"><nobr><span class="ft4">One possible solution to the problems of high computa-</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:475"><nobr><span class="ft9">tion and instability arising out of using covariance matrices<br>is to force the matrices to be diagonal, which amounts to<br>weighting each feature differently in different clusters. While<br>this restricts the dissimilarity measures to have axis parallel<br>isometry, the weights also provide a simple interpretation of<br>the clusters in terms of relevant features, which is important<br>in knowledge discovery. Examples of such algorithms are<br>SCAD and Fuzzy-SKWIC [5, 6], which perform fuzzy clus-<br>tering of data while simultaneously finding feature weights<br>in individual clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:489"><nobr><span class="ft4">In this paper, we generalize the idea of the feature weight-</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:475"><nobr><span class="ft9">ing approach to define a class of spatially varying dissimi-<br>larity measures and propose algorithms that learn the dis-<br>similarity measure automatically from the given data while<br>performing the clustering. The idea is to identify clusters<br>inherent in the data that are compact with respect to the<br>unknown spatially varying dissimilarity measure. We com-<br>pare the proposed algorithms with a diagonal version of GK<br>(DGK) and a crisp version of SCAD (CSCAD) on a variety<br>of data sets. Our algorithms perform better than DGK and<br>CSCAD, and use more stable update equations for weights<br>than CSCAD.</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:489"><nobr><span class="ft4">The rest of the paper is organized as follows. In the next</span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:475"><nobr><span class="ft4">section, we define a general class of dissimilarity measures</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">611</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:678"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
	.ft11{font-size:6px;font-family:Times;color:#000000;}
	.ft12{font-size:5px;font-family:Times;color:#000000;}
	.ft13{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
	.ft14{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
	.ft15{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
	.ft16{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="941" height="1235" src="125002.png" alt="background image">
<DIV style="position:absolute;top:93;left:81"><nobr><span class="ft9">and formulate two objective functions based on them. In<br>Section 3, we derive learning algorithms that optimize the<br>objective functions. We present an experimental study of<br>the proposed algorithms in Section 4. We compare the per-<br>formance of the proposed algorithms with that of DGK and<br>CSCAD. These two algorithms are explained in Appendix A.<br>Finally, we summarize our contributions and conclude with<br>some future directions in Section 5.</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:81"><nobr><span class="ft3"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:112"><nobr><span class="ft13"><b>SPATIALLY VARIANT DISSIMILARITY<br>(SVAD) MEASURES</b></span></nobr></DIV>
<DIV style="position:absolute;top:280;left:94"><nobr><span class="ft4">We first define a general class of dissimilarity measures</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:81"><nobr><span class="ft9">and formulate a few objective functions in terms of the given<br>data set. Optimization of the objective functions would re-<br>sult in learning the underlying dissimilarity measure.</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:81"><nobr><span class="ft3"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:352;left:121"><nobr><span class="ft3"><b>SVaD Measures</b></span></nobr></DIV>
<DIV style="position:absolute;top:374;left:94"><nobr><span class="ft4">In the following definition, we generalize the concept of</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:81"><nobr><span class="ft9">dissimilarity measures in which the weights associated with<br>features change over feature space.</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:94"><nobr><span class="ft4">Definition 2.1 We define the measure of dissimilarity of</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:81"><nobr><span class="ft4">x from y</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:139"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:152"><nobr><span class="ft4">to be a weighted sum of M dissimilarity mea-</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:81"><nobr><span class="ft9">sures between x and y where the values of the weights de-<br>pend on the region from which the dissimilarity is being mea-<br>sured. Let P = {R</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:199"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:205"><nobr><span class="ft4">, . . . , R</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:246"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:257"><nobr><span class="ft4">} be a collection of K regions</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:81"><nobr><span class="ft4">that partition the feature space, and w</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:312"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:318"><nobr><span class="ft4">, w</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:336"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:342"><nobr><span class="ft4">, . . ., and w</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:412"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:427"><nobr><span class="ft4">be</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:81"><nobr><span class="ft4">the weights associated with R</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:253"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:259"><nobr><span class="ft4">, R</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:276"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:282"><nobr><span class="ft4">, . . ., and R</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:349"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:360"><nobr><span class="ft4">, respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:81"><nobr><span class="ft4">Let g</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:112"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:118"><nobr><span class="ft4">, g</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:131"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:137"><nobr><span class="ft4">, . . ., and g</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:203"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:220"><nobr><span class="ft4">be M dissimilarity measures. Then,</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:81"><nobr><span class="ft4">each w</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:123"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:129"><nobr><span class="ft4">, j = 1, . . . , K, is an M -dimensional vector where its</span></nobr></DIV>
<DIV style="position:absolute;top:563;left:81"><nobr><span class="ft4">l-th component, w</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:189"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:563;left:204"><nobr><span class="ft4">is associated with g</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:319"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:563;left:323"><nobr><span class="ft4">. Let W denote the</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:81"><nobr><span class="ft4">K-tuple (w</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:149"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:155"><nobr><span class="ft4">, . . . , w</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:198"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:208"><nobr><span class="ft4">) and let r be a real number. Then, the</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:81"><nobr><span class="ft4">dissimilarity of x from y is given by:</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:145"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:152"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:164"><nobr><span class="ft4">(x, y)</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:204"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:632;left:203"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:222"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:219"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:240"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:250"><nobr><span class="ft14">r<br>jl</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:259"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:266"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:270"><nobr><span class="ft4">(x, y), if y  R</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:365"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:371"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:422"><nobr><span class="ft4">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:81"><nobr><span class="ft4">We refer to f</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:160"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:177"><nobr><span class="ft4">as a Spatially Variant Dissimilarity (SVaD)</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:81"><nobr><span class="ft4">measure.</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:94"><nobr><span class="ft4">Note that f</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:165"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:182"><nobr><span class="ft4">need not be symmetric even if g</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:378"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:387"><nobr><span class="ft4">are sym-</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:81"><nobr><span class="ft4">metric. Hence, f</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:182"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:199"><nobr><span class="ft4">is not a metric. Moreover, the behavior</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:81"><nobr><span class="ft4">of f</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:103"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:120"><nobr><span class="ft4">depends on the behavior of g</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:295"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:300"><nobr><span class="ft4">. There are many ways</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:81"><nobr><span class="ft4">to define g</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:144"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:149"><nobr><span class="ft4">. We list two instances of f</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:314"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:326"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:94"><nobr><span class="ft4">Example 2.1 (Minkowski) Let</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:290"><nobr><span class="ft11">d</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:301"><nobr><span class="ft4">be the feature space and</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:81"><nobr><span class="ft4">M = d. Let a point x </span></nobr></DIV>
<DIV style="position:absolute;top:777;left:245"><nobr><span class="ft11">d</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:257"><nobr><span class="ft4">be represented as (x</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:378"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:385"><nobr><span class="ft4">, . . . , x</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:423"><nobr><span class="ft11">d</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:430"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft4">Then, when g</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:162"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:167"><nobr><span class="ft4">(x, y) = |x</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:232"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:239"><nobr><span class="ft4">- y</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:258"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:263"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:267"><nobr><span class="ft11">p</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:278"><nobr><span class="ft4">for i = 1, . . . , d, and p  1,</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:81"><nobr><span class="ft4">the resulting SVaD measure, f</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:266"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:264"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:283"><nobr><span class="ft4">is called Minkowski SVaD</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:81"><nobr><span class="ft4">(MSVaD) measure. That is,</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:140"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:148"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:147"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:160"><nobr><span class="ft4">(x, y)</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:200"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:866;left:199"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:221"><nobr><span class="ft11">d</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:215"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:236"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:246"><nobr><span class="ft14">r<br>jl</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:255"><nobr><span class="ft4">|x</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:267"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:274"><nobr><span class="ft4">- y</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:295"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:299"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:303"><nobr><span class="ft11">p</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:310"><nobr><span class="ft4">, if y  R</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:370"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:376"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:422"><nobr><span class="ft4">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:94"><nobr><span class="ft4">One may note that when w</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:263"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:273"><nobr><span class="ft4">= · · · = w</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:335"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:350"><nobr><span class="ft4">and p = 2, f</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:427"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:426"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:926;left:81"><nobr><span class="ft4">is the weighted Euclidean distance. When p = 2, we call f</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:427"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:426"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:81"><nobr><span class="ft4">a Euclidean SVaD (ESVaD) measure and denote it by f</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:421"><nobr><span class="ft11">E</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:419"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:432"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:81"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:88"><nobr><span class="ft4">We use the phrase "dissimilarity of x from y" rather than</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:81"><nobr><span class="ft15">"dissimilarity between x and y" because we consider a gen-<br>eral situation where the dissimilarity measure depends on<br>the location of y. As an example of this situation in text<br>mining, when the dissimilarity is measured from a document<br>on `terrorism' to a document x, a particular set of keywords<br>may be weighted heavily whereas when the dissimilarity is<br>measured from a document on `football' to x, a different set<br>of keywords may be weighted heavily.</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:489"><nobr><span class="ft4">Example 2.2 (Cosine) Let the feature space be the set</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:475"><nobr><span class="ft4">of points with l</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:572"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:585"><nobr><span class="ft4">norm equal to one. That is,</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:780"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:796"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:809"><nobr><span class="ft4">= 1</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:475"><nobr><span class="ft4">for all points x in feature space. Then, when g</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:777"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:781"><nobr><span class="ft4">(x, y) =</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:475"><nobr><span class="ft4">(1/d - x</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:527"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:535"><nobr><span class="ft4">· y</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:549"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:553"><nobr><span class="ft4">) for l = 1, . . . , d, the resulting SVaD measure</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:475"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:483"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:482"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:499"><nobr><span class="ft4">is called a Cosine SVaD (CSVaD) measure:</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:521"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:530"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:528"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:541"><nobr><span class="ft4">(x, y)</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:580"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:198;left:579"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:601"><nobr><span class="ft11">d</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:595"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:616"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:626"><nobr><span class="ft14">r<br>jl</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:635"><nobr><span class="ft4">(1/d - x</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:687"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:694"><nobr><span class="ft4">· y</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:708"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:712"><nobr><span class="ft4">), if y  R</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:778"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:784"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:816"><nobr><span class="ft4">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:489"><nobr><span class="ft4">In the formulation of the objective function below, we use</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:475"><nobr><span class="ft4">a set of parameters to represent the regions R</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:752"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:758"><nobr><span class="ft4">, R</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:775"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:781"><nobr><span class="ft4">, . . ., and</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:475"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:486"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:496"><nobr><span class="ft4">. Let c</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:543"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:549"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:562"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:568"><nobr><span class="ft4">, . . ., and c</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:636"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:652"><nobr><span class="ft4">be K points in feature space.</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:475"><nobr><span class="ft4">Then y  R</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:547"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:558"><nobr><span class="ft4">iff</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:559"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:566"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:578"><nobr><span class="ft4">(y, c</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:605"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:611"><nobr><span class="ft4">) &lt; f</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:642"><nobr><span class="ft11">W</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:654"><nobr><span class="ft4">(y, c</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:682"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:686"><nobr><span class="ft4">) for i = j.</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:816"><nobr><span class="ft4">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:475"><nobr><span class="ft9">In the case of ties, y is assigned to the region with the lowest<br>index. Thus, the K-tuple of points C = (c</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:731"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:738"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:751"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:757"><nobr><span class="ft4">, . . . , c</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:795"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:806"><nobr><span class="ft4">) de-</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:475"><nobr><span class="ft9">fines a partition in feature space. The partition induced by<br>the points in C is similar in nature to a Voronoi tessellation.<br>We use the notation f</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:616"><nobr><span class="ft11">W,C</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:645"><nobr><span class="ft4">whenever we use the set C to</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:475"><nobr><span class="ft4">parameterize the regions used in the dissimilarity measure.</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:475"><nobr><span class="ft3"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:457;left:516"><nobr><span class="ft3"><b>Objective Function for Clustering</b></span></nobr></DIV>
<DIV style="position:absolute;top:479;left:489"><nobr><span class="ft4">The goal of the present work is to identify the spatially</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:475"><nobr><span class="ft9">varying dissimilarity measure and the associated compact<br>clusters simultaneously. It is worth mentioning here that,<br>as in the case of any clustering algorithm, the underlying<br>assumption in this paper is the existence of such a dissimi-<br>larity measure and clusters for a given data set.</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:489"><nobr><span class="ft4">Let x</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:523"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:529"><nobr><span class="ft4">, x</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:544"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:550"><nobr><span class="ft4">, . . ., and x</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:618"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:630"><nobr><span class="ft4">be n given data points. Let K be</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:475"><nobr><span class="ft9">a given positive integer. Assuming that C represents the<br>cluster centers, let us assign each data point x</span></nobr></DIV>
<DIV style="position:absolute;top:609;left:756"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:766"><nobr><span class="ft4">to a cluster</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:475"><nobr><span class="ft4">R</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:486"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:496"><nobr><span class="ft4">with the closest c</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:602"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:613"><nobr><span class="ft4">as the cluster center</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:735"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:741"><nobr><span class="ft4">, i.e.,</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:581"><nobr><span class="ft4">j = arg min</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:637"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:653"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:659"><nobr><span class="ft11">W,C</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:682"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:697"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:701"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:715"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:719"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:816"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:475"><nobr><span class="ft4">Then, the within-cluster dissimilarity is given by</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:541"><nobr><span class="ft4">J (W, C) =</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:614"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:609"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:631"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:640"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:645"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:660"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:673"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:669"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:690"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:701"><nobr><span class="ft14">r<br>jl</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:710"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:716"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:721"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:735"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:740"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:753"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:759"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:816"><nobr><span class="ft4">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:475"><nobr><span class="ft9">J (W, C) represents the sum of the dissimilarity measures of<br>all the data points from their closest centroids. The objec-<br>tive is to find W and C that minimize J (W, C). To avoid<br>the trivial solution to J (W, C), we consider a normalization<br>condition on w</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:567"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:573"><nobr><span class="ft4">, viz.,</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:623"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:620"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:641"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:651"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:665"><nobr><span class="ft4">= 1.</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:816"><nobr><span class="ft4">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:475"><nobr><span class="ft9">Note that even with this condition, J (W, C) has a trivial<br>solution: w</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:547"><nobr><span class="ft11">jp</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:566"><nobr><span class="ft4">= 1 where p = arg min</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:716"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:737"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:746"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:751"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:767"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:775"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:782"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:786"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:801"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:806"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:819"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:825"><nobr><span class="ft4">),</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:475"><nobr><span class="ft9">and the remaining weights are zero. One way to avoid con-<br>vergence of w</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:563"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:575"><nobr><span class="ft4">to unit vectors is to impose a regulariza-</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:475"><nobr><span class="ft4">tion condition on w</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:599"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:605"><nobr><span class="ft4">. We consider the following two reg-</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:475"><nobr><span class="ft4">ularization measures in this paper: (1) Entropy measure:</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:490"><nobr><span class="ft16">M<br>l=1</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:510"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:520"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:532"><nobr><span class="ft4">log(w</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:565"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:575"><nobr><span class="ft4">) and (2) Gini measure:</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:739"><nobr><span class="ft16">M<br>l=1</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:760"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:770"><nobr><span class="ft14">2<br>jl</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:779"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:476"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:482"><nobr><span class="ft4">We use P = {R</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:575"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:581"><nobr><span class="ft4">, R</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:597"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:604"><nobr><span class="ft4">, . . . , R</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:645"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:655"><nobr><span class="ft4">} to represent the correspond-</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:475"><nobr><span class="ft15">ing partition of the data set as well. The intended interpre-<br>tation (cluster or region) would be evident from the context.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">612</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:81"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="941" height="1235" src="125003.png" alt="background image">
<DIV style="position:absolute;top:90;left:81"><nobr><span class="ft3"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:90;left:112"><nobr><span class="ft13"><b>ALGORITHMS TO LEARN SVAD<br>MEASURES</b></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:94"><nobr><span class="ft4">The problem of determining the optimal W and C is sim-</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:81"><nobr><span class="ft9">ilar to the traditional clustering problem that is solved by<br>the K-Means Algorithm (KMA) except for the additional W<br>matrix. We propose a class of iterative algorithms similar to<br>KMA. These algorithms start with a random partition of the<br>data set and iteratively update C, W and P so that J (W, C)<br>is minimized. These iterative algorithms are instances of Al-<br>ternating Optimization (AO) algorithms. In [1], it is shown<br>that AO algorithms converge to a local optimum under some<br>conditions. We outline the algorithm below before actually<br>describing how to update C, W and P in every iteration.</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:94"><nobr><span class="ft4">Randomly assign the data points to K clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft4">REPEAT</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:90"><nobr><span class="ft4">Update C: Compute the centroid of each cluster c</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:396"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:401"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:90"><nobr><span class="ft4">Update W : Compute the w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:258"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:267"><nobr><span class="ft4">j, l.</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:90"><nobr><span class="ft4">Update P: Reassign the data points to the clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:81"><nobr><span class="ft4">UNTIL (termination condition is reached).</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:94"><nobr><span class="ft4">In the above algorithm, the update of C depends on the</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:81"><nobr><span class="ft4">definition of g</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:165"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:170"><nobr><span class="ft4">, and the update of W on the regularization</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:81"><nobr><span class="ft9">terms. The update of P is done by reassigning the data<br>points according to (5). Before explaining the computation<br>of C in every iteration for various g</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:295"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:300"><nobr><span class="ft4">, we first derive update</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:81"><nobr><span class="ft4">equations for W for various regularization measures.</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:81"><nobr><span class="ft3"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:121"><nobr><span class="ft3"><b>Update of Weights</b></span></nobr></DIV>
<DIV style="position:absolute;top:557;left:94"><nobr><span class="ft4">While updating weights, we need to find the values of</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:81"><nobr><span class="ft9">weights that minimize the objective function for a given C<br>and P. As mentioned above, we consider the two regular-<br>ization measures for w</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:216"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:229"><nobr><span class="ft4">and derive update equations. If we</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:81"><nobr><span class="ft9">consider the entropy regularization with r = 1, the objective<br>function becomes:</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:122"><nobr><span class="ft4">J</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:130"><nobr><span class="ft11">EN T</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:157"><nobr><span class="ft4">(W, C)</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:211"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:240"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:235"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:257"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:266"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:271"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:287"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:299"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:296"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:317"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:327"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:336"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:343"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:347"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:362"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:366"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:380"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:386"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:107"><nobr><span class="ft4">+</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:125"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:121"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:143"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:725;left:149"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:161"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:158"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:179"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:189"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:201"><nobr><span class="ft4">log(w</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:234"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:243"><nobr><span class="ft4">) +</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:271"><nobr><span class="ft11">K</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:266"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:288"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:725;left:296"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:319"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:316"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:337"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:347"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:360"><nobr><span class="ft4">- 1</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:394"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:422"><nobr><span class="ft4">(8)</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:81"><nobr><span class="ft4">Note that </span></nobr></DIV>
<DIV style="position:absolute;top:763;left:157"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:170"><nobr><span class="ft4">are the Lagrange multipliers corresponding</span></nobr></DIV>
<DIV style="position:absolute;top:774;left:81"><nobr><span class="ft4">to the normalization constraints in (7), and </span></nobr></DIV>
<DIV style="position:absolute;top:779;left:372"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:774;left:384"><nobr><span class="ft4">represent</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:81"><nobr><span class="ft9">the relative importance given to the regularization term<br>relative to the within-cluster dissimilarity. Differentiating<br>J</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:88"><nobr><span class="ft11">EN T</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:115"><nobr><span class="ft4">(W, C) with respect to w</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:264"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:278"><nobr><span class="ft4">and equating it to zero, we</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:81"><nobr><span class="ft4">obtain w</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:133"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:146"><nobr><span class="ft4">= exp</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:196"><nobr><span class="ft11">-(</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:215"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:221"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:240"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:249"><nobr><span class="ft12">i Rj</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:277"><nobr><span class="ft11">g</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:282"><nobr><span class="ft12">l</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:287"><nobr><span class="ft11">(</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:291"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:300"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:305"><nobr><span class="ft11">,</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:308"><nobr><span class="ft4">c</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:315"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:321"><nobr><span class="ft11">))</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:257"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:856;left:262"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:335"><nobr><span class="ft4">- 1 . Solving for</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:81"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:875;left:89"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:99"><nobr><span class="ft4">by substituting the above value of w</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:320"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:334"><nobr><span class="ft4">in (7) and substi-</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:81"><nobr><span class="ft4">tuting the value of </span></nobr></DIV>
<DIV style="position:absolute;top:890;left:203"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:213"><nobr><span class="ft4">back in the above equation, we obtain</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:127"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:137"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:150"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:187"><nobr><span class="ft4">exp -</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:247"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:256"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:260"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:276"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:285"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:291"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:296"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:310"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:315"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:328"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:334"><nobr><span class="ft4">)/</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:353"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:181"><nobr><span class="ft16">M<br>n=1</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:205"><nobr><span class="ft4">exp -</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:264"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:273"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:278"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:294"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:302"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:309"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:317"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:331"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:336"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:349"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:355"><nobr><span class="ft4">)/</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:373"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:389"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:422"><nobr><span class="ft4">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:94"><nobr><span class="ft4">If we consider the Gini measure for regularization with</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:81"><nobr><span class="ft4">r = 2, the corresponding w</span></nobr></DIV>
<DIV style="position:absolute;top:991;left:249"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:264"><nobr><span class="ft4">that minimizes the objective</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:81"><nobr><span class="ft4">function can be shown to be</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:132"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:142"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:155"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:197"><nobr><span class="ft4">1/(</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:222"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:231"><nobr><span class="ft4">+</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:259"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:268"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:273"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:289"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:297"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:304"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:308"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:323"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:328"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:341"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:347"><nobr><span class="ft4">))</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:186"><nobr><span class="ft16">M<br>n=1</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:208"><nobr><span class="ft4">(1/(</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:238"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:247"><nobr><span class="ft4">+</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:276"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:285"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:289"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:305"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:314"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:320"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:328"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:343"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:347"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:361"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:366"><nobr><span class="ft4">))) .</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:415"><nobr><span class="ft4">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:1073;left:94"><nobr><span class="ft4">In both cases, the updated value of w</span></nobr></DIV>
<DIV style="position:absolute;top:1078;left:316"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1073;left:329"><nobr><span class="ft4">is inversely related</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:561"><nobr><span class="ft4">Algorithm</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:639"><nobr><span class="ft4">Update Equations</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:561"><nobr><span class="ft4">Acronyms</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:642"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:679"><nobr><span class="ft4">C</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:723"><nobr><span class="ft4">W</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:561"><nobr><span class="ft4">EEnt</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:639"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:672"><nobr><span class="ft4">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:722"><nobr><span class="ft4">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:561"><nobr><span class="ft4">EsGini</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:639"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:672"><nobr><span class="ft4">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:719"><nobr><span class="ft4">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:561"><nobr><span class="ft4">CEnt</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:639"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:672"><nobr><span class="ft4">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:722"><nobr><span class="ft4">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:561"><nobr><span class="ft4">CsGini</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:639"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:672"><nobr><span class="ft4">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:719"><nobr><span class="ft4">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:539"><nobr><span class="ft4">Table 1: Summary of algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:475"><nobr><span class="ft4">to</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:505"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:515"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:519"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:535"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:544"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:550"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:555"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:569"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:574"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:587"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:593"><nobr><span class="ft4">). This has various interpretations based</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:475"><nobr><span class="ft4">on the nature of g</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:583"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:587"><nobr><span class="ft4">. For example, when we consider the ES-</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:475"><nobr><span class="ft4">VaD measure, w</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:573"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:587"><nobr><span class="ft4">is inversely related to the variance of l-th</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:475"><nobr><span class="ft9">element of the data vectors in the j-th cluster. In other<br>words, when the variance along a particular dimension is<br>high in a cluster, then the dimension is less important to<br>the cluster. This popular heuristic has been used in various<br>contexts (such as relevance feedback) in the literature [9].<br>Similarly, when we consider the CSVaD measure, w</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:789"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:804"><nobr><span class="ft4">is di-</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:475"><nobr><span class="ft9">rectly proportional to the correlation of the j-th dimension<br>in the l-th cluster.</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:475"><nobr><span class="ft3"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:516"><nobr><span class="ft3"><b>Update of Centroids</b></span></nobr></DIV>
<DIV style="position:absolute;top:468;left:489"><nobr><span class="ft4">Learning ESVaD Measures: Substituting the ESVaD mea-</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:475"><nobr><span class="ft9">sure in the objective function and solving the first order<br>necessary conditions, we observe that</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:596"><nobr><span class="ft4">c</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:602"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:615"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:640"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:632"><nobr><span class="ft4">|R</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:646"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:652"><nobr><span class="ft4">| x</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:669"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:673"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:689"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:697"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:705"><nobr><span class="ft11">il</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:809"><nobr><span class="ft4">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:475"><nobr><span class="ft4">minimizes J</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:547"><nobr><span class="ft11">ESV AD</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:590"><nobr><span class="ft4">(W, C).</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:489"><nobr><span class="ft4">Learning CSVaD Measures: Let x</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:697"><nobr><span class="ft11">il</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:710"><nobr><span class="ft4">= w</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:735"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:745"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:753"><nobr><span class="ft11">il</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:761"><nobr><span class="ft4">, then using</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:475"><nobr><span class="ft4">the Cauchy-Swartz inequality, it can be shown that</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:596"><nobr><span class="ft4">c</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:602"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:615"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:640"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:632"><nobr><span class="ft4">|R</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:646"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:652"><nobr><span class="ft4">| x</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:669"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:673"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:689"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:697"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:705"><nobr><span class="ft11">il</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:809"><nobr><span class="ft4">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:475"><nobr><span class="ft4">maximizes</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:559"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:568"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:573"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:588"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:612"><nobr><span class="ft16">d<br>l=1</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:632"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:642"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:652"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:659"><nobr><span class="ft11">il</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:668"><nobr><span class="ft4">c</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:674"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:683"><nobr><span class="ft4">. Hence, (12) also min-</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:475"><nobr><span class="ft9">imizes the objective function when CSVaD is used as the<br>dissimilarity measure.</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:489"><nobr><span class="ft4">Table 1 summarizes the update equations used in vari-</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:475"><nobr><span class="ft9">ous algorithms. We refer to this set of algorithms as SVaD<br>learning algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:475"><nobr><span class="ft3"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:800;left:507"><nobr><span class="ft3"><b>EXPERIMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:489"><nobr><span class="ft4">In this section, we present an experimental study of the al-</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:475"><nobr><span class="ft9">gorithms described in the previous sections. We applied the<br>proposed algorithms on various text data sets and compared<br>the performance of EEnt and EsGini with that of K-Means,<br>CSCAD and DGK algorithms. The reason for choosing the<br>K-Means algorithm (KMA) apart from CSCAD and DGK<br>is that it provides a baseline for assessing the advantages of<br>feature weighting. KMA is also a popular algorithm for text<br>clustering. We have included a brief description of CSCAD<br>and DGK algorithms in Appendix A.</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:489"><nobr><span class="ft4">Text data sets are sparse and high dimensional. We con-</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:475"><nobr><span class="ft9">sider standard labeled document collections and test the<br>proposed algorithms for their ability to discover dissimilar-<br>ity measures that distinguish one class from another without<br>actually considering the class labels of the documents. We<br>measure the success of the algorithms by the purity of the<br>regions that they discover.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">613</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:678"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
	.ft17{font-size:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="941" height="1235" src="125004.png" alt="background image">
<DIV style="position:absolute;top:90;left:81"><nobr><span class="ft3"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:90;left:121"><nobr><span class="ft3"><b>Data Sets</b></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:94"><nobr><span class="ft4">We performed our experiments on three standard data</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:81"><nobr><span class="ft9">sets: 20 News Group, Yahoo K1, and Classic 3. These data<br>sets are described below.</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:94"><nobr><span class="ft4">20 News Group</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:202"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:208"><nobr><span class="ft4">: We considered different subsets of 20</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:81"><nobr><span class="ft9">News Group data that are known to contain clusters of vary-<br>ing degrees of separation [10]. As in [10], we considered three<br>random samples of three subsets of the 20 News Group data.<br>The subsets denoted by Binary has 250 documents each<br>from talk.politics.mideast and talk.politics.misc. Multi5 has<br>100 documents each from comp.graphics, rec.motorcycles,<br>rec.sport.baseball, sci.space, and talk.politics.mideast. Fi-<br>nally, Multi10 has 50 documents each from alt.atheism, comp.<br>sys.mac.hardware, misc.forsale, rec.autos, rec.sport.hockey,<br>sci.crypt, sci.electronics, sci.med, sci.space, and talk.politics.<br>gun. It may be noted that Binary data sets have two highly<br>overlapping classes. Each of Multi5 data sets has samples<br>from 5 distinct classes, whereas Multi10 data sets have only<br>a few samples from 10 different classes. The size of the vo-<br>cabulary used to represent the documents in Binary data set<br>is about 4000, Multi5 about 3200 and Multi10 about 2800.<br>We observed that the relative performance of the algorithms<br>on various samples of Binary, Multi5 and Multi10 data sets<br>was similar. Hence, we report results on only one of them.</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:94"><nobr><span class="ft4">Yahoo K1</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:164"><nobr><span class="ft11">4</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:171"><nobr><span class="ft4">: This data set contains 2340 Reuters news</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:81"><nobr><span class="ft4">articles downloaded from Yahoo in 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:351"><nobr><span class="ft4">There are 494</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:81"><nobr><span class="ft9">from Health, 1389 from Entertainment, 141 from Sports, 114<br>from Politics, 60 from Technology and 142 from Business.<br>After preprocessing, the documents from this data set are<br>represented using 12015 words. Note that this data set has<br>samples from 6 different classes. Here, the distribution of<br>data points across the class is uneven, ranging from 60 to<br>1389.</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:94"><nobr><span class="ft4">Classic 3</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:157"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:163"><nobr><span class="ft4">: Classic 3 data set contains 1400 aerospace</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:81"><nobr><span class="ft9">systems abstracts from the Cranfield collection, 1033 medi-<br>cal abstracts from the Medline collection and 1460 informa-<br>tion retrieval abstracts from the Cisi collection, making up<br>3893 documents in all. After preprocessing, this data set<br>has 4301 words. The points are almost equally distributed<br>among the three distinct classes.</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:94"><nobr><span class="ft4">The data sets were preprocessed using two major steps.</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:81"><nobr><span class="ft9">First, a set of words (vocabulary) is extracted and then each<br>document is represented with respect to this vocabulary.<br>Finding the vocabulary includes: (1) elimination of the stan-<br>dard list of stop words from the documents, (2) application<br>of Porter stemming</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:195"><nobr><span class="ft11">6</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:205"><nobr><span class="ft4">for term normalization, and (3) keeping</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:81"><nobr><span class="ft9">only the words which appear in at least 3 documents. We<br>represent each document by the unitized frequency vector.</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:81"><nobr><span class="ft3"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:864;left:121"><nobr><span class="ft3"><b>Evaluation of Algorithms</b></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:94"><nobr><span class="ft4">We use the accuracy measure to compare the performance</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:81"><nobr><span class="ft4">of various algorithms. Let a</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:249"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:264"><nobr><span class="ft4">represent the number of data</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:81"><nobr><span class="ft9">points from class i that are in cluster j. Then the accuracy<br>of the partition is given by</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:270"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:278"><nobr><span class="ft4">max</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:304"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:311"><nobr><span class="ft4">a</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:318"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:328"><nobr><span class="ft4">/n where n is the</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:81"><nobr><span class="ft4">total number of data points.</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:94"><nobr><span class="ft4">It is to be noted that points coming from a single class</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:81"><nobr><span class="ft4">need not form a single cluster.</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:288"><nobr><span class="ft4">There could be multiple</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:81"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:88"><nobr><span class="ft4">http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:81"><nobr><span class="ft4">20/www/data/news20.tar.gz</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:81"><nobr><span class="ft11">4</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:88"><nobr><span class="ft4">ftp://ftp.cs.umn.edu/dept/users/boley/PDDPdata/doc-K</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:81"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:88"><nobr><span class="ft4">ftp://ftp.cs.cornell.edu/pub/smart</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:81"><nobr><span class="ft11">6</span></nobr></DIV>
<DIV style="position:absolute;top:1073;left:88"><nobr><span class="ft4">http://www.tartarus.org/~martin/PorterStemmer/</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:487"><nobr><span class="ft4">Iteration</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:568"><nobr><span class="ft4">0</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:612"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:660"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:708"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:755"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:803"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:487"><nobr><span class="ft4">J (W, C)</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:600"><nobr><span class="ft4">334.7</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:648"><nobr><span class="ft4">329.5</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:695"><nobr><span class="ft4">328.3</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:743"><nobr><span class="ft4">328.1</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:791"><nobr><span class="ft4">327.8</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:487"><nobr><span class="ft4">Accuracy</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:559"><nobr><span class="ft4">73.8</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:603"><nobr><span class="ft4">80.2</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:651"><nobr><span class="ft4">81.4</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:699"><nobr><span class="ft4">81.6</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:752"><nobr><span class="ft4">82</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:799"><nobr><span class="ft4">82</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:475"><nobr><span class="ft9">Table 2: Evolution of J (W, C) and Accuracies with<br>iterations when EEnt applied on a Multi5 data.</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:475"><nobr><span class="ft9">clusters in a class that represent sub-classes. We study the<br>performance of SVaD learning algorithms for various values<br>of K, i.e., the number of clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:475"><nobr><span class="ft3"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:284;left:516"><nobr><span class="ft3"><b>Experimental Setup</b></span></nobr></DIV>
<DIV style="position:absolute;top:306;left:489"><nobr><span class="ft4">In our implementations, we have observed that the pro-</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:475"><nobr><span class="ft9">posed algorithms, if applied on randomly initialized cen-<br>troids, show unstable behavior. One reason for this behav-<br>ior is that the number of parameters that are estimated in<br>feature-weighting clustering algorithms is twice as large as<br>that estimated by the traditional KMA. We, therefore, first<br>estimate the cluster centers giving equal weights to all the<br>dimensions using KMA and then fine-tune the cluster cen-<br>ters and the weights using the feature-weighting clustering<br>algorithms. In every iteration, the new sets of weights are<br>updated as follows. Let w</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:628"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:636"><nobr><span class="ft4">(t+1) represent the weights com-</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:475"><nobr><span class="ft9">puted using one of (9), (10), (14) or (15) in iteration (t + 1)<br>and w(t) the weights in iteration t. Then, the weights in<br>iteration (t + 1) are</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:527"><nobr><span class="ft4">w(t + 1) = (1 - (t))w(t) + (t)w</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:731"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:738"><nobr><span class="ft4">(t + 1),</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:809"><nobr><span class="ft4">(13)</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:475"><nobr><span class="ft9">where (t)  [0, 1] decreases with t. That is, (t) = (t -<br>1), for a given constant   [0, 1]. In our experiments, we<br>observed that the variance of purity values for different ini-<br>tial values of (0) and  above 0.5 is very small. Hence, we<br>report the results for (0) = 0.5 and  = 0.5. We set the<br>value of </span></nobr></DIV>
<DIV style="position:absolute;top:643;left:533"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:543"><nobr><span class="ft4">= 1.</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:489"><nobr><span class="ft4">It may be noted that when the documents are represented</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:475"><nobr><span class="ft9">as unit vectors, KMA with the cosine dissimilarity measure<br>and Euclidean distance measure would yield the same clus-<br>ters. This is essentially the same as Spherical K-Means al-<br>gorithms described in [3]. Therefore, we consider only the<br>weighted Euclidean measure and restrict our comparisons to<br>EEnt and EsGini in the experiments.</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:489"><nobr><span class="ft4">Since the clusters obtained by KMA are used to initialize</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:475"><nobr><span class="ft9">all other algorithms considered here, and since the results<br>of KMA are sensitive to initialization, the accuracy num-<br>bers reported in this section are averages over 10 random<br>initializations of KMA.</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:475"><nobr><span class="ft3"><b>4.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:516"><nobr><span class="ft3"><b>Results and Observations</b></span></nobr></DIV>
<DIV style="position:absolute;top:883;left:479"><nobr><span class="ft17"><i>4.4.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:883;left:529"><nobr><span class="ft17"><i>Effect of SVaD Measures on Accuracies</i></span></nobr></DIV>
<DIV style="position:absolute;top:905;left:489"><nobr><span class="ft4">In Table 2, we show a sample run of EEnt algorithm on</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:475"><nobr><span class="ft9">one of the Multi5 data sets. This table shows the evolution<br>of J (W, C) and the corresponding accuracies of the clusters<br>with the iterations. The accuracy, shown at iteration 0, is<br>that of the clusters obtained by KMA. The purity of clusters<br>increases with decrease in the value of the objective function<br>defined using SVaD measures. We have observed a similar<br>behavior of EEnt and EsGini on other data sets also. This<br>validates our hypothesis that SVaD measures capture the<br>underlying structure in the data sets more accurately.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">614</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:81"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="941" height="1235" src="125005.png" alt="background image">
<DIV style="position:absolute;top:91;left:85"><nobr><span class="ft17"><i>4.4.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:91;left:134"><nobr><span class="ft17"><i>Comparison with Other Algorithms</i></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:94"><nobr><span class="ft4">Figure 1 to Figure 5 show average accuracies of various</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:81"><nobr><span class="ft9">algorithms on the 5 data sets for various number of clus-<br>ters. The accuracies of KMA and DGK are very close to<br>each other and hence, in the figures, the lines corresponding<br>to these algorithms are indistinguishable. The lines corre-<br>sponding to CSCAD are also close to that of KMA in all the<br>cases except Class 3.</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:94"><nobr><span class="ft4">General observations: The accuracies of SVaD algo-</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:81"><nobr><span class="ft9">rithms follow the trend of the accuracies of other algorithms.<br>In all our experiments, both SVaD learning algorithms im-<br>prove the accuracies of clusters obtained by KMA. It is ob-<br>served in our experiments that the improvement could be<br>as large as 8% in some instances. EEnt and EsGini consis-<br>tently perform better than DGK on all data sets and for all<br>values of K. EEnt and EsGini perform better than CSCAD<br>on all data sets excepts in the case of Classic 3 and for a few<br>values of K.</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:94"><nobr><span class="ft4">Note that the weight update equation of CSCAD (15)</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:81"><nobr><span class="ft4">may result in negative values of w</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:292"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:302"><nobr><span class="ft4">. Our experience with</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:81"><nobr><span class="ft9">CSCAD shows that it is quite sensitive to initialization and<br>it may have convergence problems. In contrast, it may be<br>observed that w</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:176"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:190"><nobr><span class="ft4">in (9) and (10) are always positive. More-</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:81"><nobr><span class="ft9">over, in our experience, these two versions are much less<br>sensitive to the choice of </span></nobr></DIV>
<DIV style="position:absolute;top:478;left:240"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:246"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:94"><nobr><span class="ft4">Data specific observations: When K = 2, EEnt and</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:81"><nobr><span class="ft9">EsGini could not further improve the results of KMA on the<br>Binary data set. The reason is that the data set contains<br>two highly overlapping classes. However, for other values of<br>K, they marginally improve the accuracies.</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:94"><nobr><span class="ft4">In the case of Multi5, the accuracies of the algorithms are</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:81"><nobr><span class="ft9">non-monotonic with K. The improvement of accuracies is<br>large for intermediate values of K and small for extreme<br>values of K. When K = 5, KMA finds relatively stable<br>clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:142"><nobr><span class="ft4">Hence, SVaD algorithms are unable to improve</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:81"><nobr><span class="ft9">the accuracies as much as they did for intermediate values<br>of K. For larger values of K, the clusters are closely spaced<br>and hence there is little scope for improvement by the SVaD<br>algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:94"><nobr><span class="ft4">Multi10 data sets are the toughest to cluster because of</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:81"><nobr><span class="ft9">the large number of classes present in the data. In this case,<br>the accuracies of the algorithms are monotonically increas-<br>ing with the number of clusters. The extent of improvement<br>of accuracies of SVaD algorithms over KMA is almost con-<br>stant over the entire range of K. This reflects the fact that<br>the documents in Multi10 data set are uniformly distributed<br>over feature space.</span></nobr></DIV>
<DIV style="position:absolute;top:834;left:94"><nobr><span class="ft4">The distribution of documents in Yahoo K1 data set is</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:81"><nobr><span class="ft9">highly skewed. The extent of improvements that the SVaD<br>algorithms could achieve decrease with K. For higher values<br>of K, KMA is able to find almost pure sub-clusters, result-<br>ing in accuracies of about 90%. This leaves little scope for<br>improvement.</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:94"><nobr><span class="ft4">The performance of CSCAD differs noticeably in the case</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:81"><nobr><span class="ft9">of Classic 3. It performs better than the SVaD algorithms<br>for K = 3 and better than EEnt for K = 9. However, for<br>larger values of K, the SVaD algorithms perform better than<br>the rest. As in the case of Multi5, the improvements of SVaD<br>algorithms over others are significant and consistent. One<br>may recall that Multi5 and Classic 3 consist of documents<br>from distinct classes.</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:223"><nobr><span class="ft4">Therefore, this observation implies</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:81"><nobr><span class="ft9">that when there are distinct clusters in the data set, KMA<br>yields confusing clusters when the number of clusters is over-</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:504"><nobr><span class="ft4">Figure 1: Accuracy results on Binary data.</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:505"><nobr><span class="ft4">Figure 2: Accuracy results on Multi5 data.</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:475"><nobr><span class="ft9">specified. In this scenario, EEnt and EsGini can fine-tune<br>the clusters to improve their purity.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:475"><nobr><span class="ft3"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:507"><nobr><span class="ft3"><b>SUMMARY AND CONCLUSIONS</b></span></nobr></DIV>
<DIV style="position:absolute;top:630;left:489"><nobr><span class="ft4">We have defined a general class of spatially variant dissim-</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:475"><nobr><span class="ft9">ilarity measures and proposed algorithms to learn the mea-<br>sure underlying a given data set in an unsupervised learning<br>framework.</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:555"><nobr><span class="ft4">Through our experiments on various textual</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:475"><nobr><span class="ft9">data sets, we have shown that such a formulation of dis-<br>similarity measure can more accurately capture the hidden<br>structure in the data than a standard Euclidean measure<br>that does not vary over feature space. We have also shown<br>that the proposed learning algorithms perform better than<br>other similar algorithms in the literature, and have better<br>stability properties.</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:489"><nobr><span class="ft4">Even though we have applied these algorithms only to</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:475"><nobr><span class="ft9">text data sets, the algorithms derived here do not assume<br>any specific characteristics of textual data sets. Hence, they</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:501"><nobr><span class="ft4">Figure 3: Accuracy results on Multi10 data.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">615</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:678"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:941;height:1235;">
<STYLE type="text/css">
<!--
	.ft18{font-size:15px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="941" height="1235" src="125006.png" alt="background image">
<DIV style="position:absolute;top:288;left:95"><nobr><span class="ft4">Figure 4: Accuracy results on Yahoo K1 data.</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:99"><nobr><span class="ft4">Figure 5: Accuracy results on Classic 3 data.</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:81"><nobr><span class="ft9">are applicable to general data sets. Since the algorithms<br>perform better for larger K, it would be interesting to in-<br>vestigate whether they can be used to find subtopics of a<br>topic. Finally, it will be interesting to learn SVaD measures<br>for labeled data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:81"><nobr><span class="ft3"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:663;left:112"><nobr><span class="ft3"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:688;left:88"><nobr><span class="ft4">[1] J. C. Bezdek and R. J. Hathaway. Some notes on</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:109"><nobr><span class="ft9">alternating optimization. In Proceedings of the 2002<br>AFSS International Conference on Fuzzy Systems.<br>Calcutta, pages 288­300. Springer-Verlag, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:88"><nobr><span class="ft4">[2] A. P. Dempster, N. M. Laird, and Rubin. Maximum</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:109"><nobr><span class="ft9">likelihood from incomplete data via the EM algorithm.<br>Journal Royal Statistical Society B, 39(2):1­38, 1977.</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:88"><nobr><span class="ft4">[3] I. S. Dhillon and D. S. Modha. Concept</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:109"><nobr><span class="ft9">decompositions for large sparse text data using<br>clustering. Machine Learning, 42(1):143­175, January<br>2001.</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:88"><nobr><span class="ft4">[4] E. Diday and J. C. Simon. Cluster analysis. In K. S.</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:109"><nobr><span class="ft9">Fu, editor, Pattern Recognition, pages 47­94.<br>Springer-Verlag, 1976.</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:88"><nobr><span class="ft4">[5] H. Frigui and O. Nasraoui. Simultaneous clustering</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:109"><nobr><span class="ft9">and attribute discrimination. In Proceedings of<br>FUZZIEEE, pages 158­163, San Antonio, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:88"><nobr><span class="ft4">[6] H. Frigui and O. Nasraoui. Simultaneous</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:109"><nobr><span class="ft9">categorization of text documents and identification of<br>cluster-dependent keywords. In Proceedings of<br>FUZZIEEE, pages 158­163, Honolulu, Hawaii, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:88"><nobr><span class="ft4">[7] D. E. Gustafson and W. C. Kessel. Fuzzy clustering</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:109"><nobr><span class="ft9">with the fuzzy covariance matrix. In Proccedings of<br>IEEE CDC, pages 761­766, San Diego, California,<br>1979.</span></nobr></DIV>
<DIV style="position:absolute;top:93;left:482"><nobr><span class="ft4">[8] R. Krishnapuram and J. Kim. A note on fuzzy</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:503"><nobr><span class="ft9">clustering algorithms for Gaussian clusters. IEEE<br>Transactions on Fuzzy Systems, 7(4):453­461, Aug<br>1999.</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:482"><nobr><span class="ft4">[9] Y. Rui, T. S. Huang, and S. Mehrotra. Relevance</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:503"><nobr><span class="ft9">feedback techniques in interactive content-based image<br>retrieval. In Storage and Retrieval for Image and<br>Video Databases (SPIE), pages 25­36, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:475"><nobr><span class="ft4">[10] N. Slonim and N. Tishby. Document clustering using</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:503"><nobr><span class="ft9">word clusters via the information bottleneck method.<br>In Proceedings of SIGIR, pages 208­215, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:475"><nobr><span class="ft3"><b>APPENDIX</b></span></nobr></DIV>
<DIV style="position:absolute;top:313;left:475"><nobr><span class="ft3"><b>A.</b></span></nobr></DIV>
<DIV style="position:absolute;top:313;left:511"><nobr><span class="ft13"><b>OTHER FEATURE WEIGHTING<br>CLUSTERING TECHNIQUES</b></span></nobr></DIV>
<DIV style="position:absolute;top:365;left:475"><nobr><span class="ft3"><b>A.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:365;left:520"><nobr><span class="ft3"><b>Diagonal Gustafson-Kessel (DGK)</b></span></nobr></DIV>
<DIV style="position:absolute;top:387;left:489"><nobr><span class="ft4">Gustafson and Kessel [7] associate each cluster with a dif-</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:475"><nobr><span class="ft4">ferent norm matrix. Let A = (A</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:678"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:684"><nobr><span class="ft4">, . . . , A</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:726"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:732"><nobr><span class="ft4">) be the set of k</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:475"><nobr><span class="ft4">norm matrices associated with k clusters. Let u</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:755"><nobr><span class="ft11">ji</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:768"><nobr><span class="ft4">is the fuzzy</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:475"><nobr><span class="ft4">membership of x</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:578"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:587"><nobr><span class="ft4">in cluster j and U = [u</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:727"><nobr><span class="ft11">ji</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:737"><nobr><span class="ft4">]. By restricting</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:475"><nobr><span class="ft4">A</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:486"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:491"><nobr><span class="ft4">s to be diagonal and u</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:627"><nobr><span class="ft11">ji</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:640"><nobr><span class="ft4"> {0, 1}, we can reformulate the</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:475"><nobr><span class="ft9">original optimization problem in terms of SVaD measures as<br>follows:</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:514"><nobr><span class="ft4">min</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:514"><nobr><span class="ft11">C,W</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:539"><nobr><span class="ft4">J</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:547"><nobr><span class="ft11">DGK</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:575"><nobr><span class="ft4">(C, W ) =</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:642"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:636"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:658"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:667"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:672"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:687"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:700"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:696"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:718"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:727"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:737"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:743"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:748"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:762"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:767"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:780"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:786"><nobr><span class="ft4">),</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:475"><nobr><span class="ft4">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:550"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:557"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:567"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:580"><nobr><span class="ft4">= </span></nobr></DIV>
<DIV style="position:absolute;top:562;left:602"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:608"><nobr><span class="ft4">. Note that this problem can be solved</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:475"><nobr><span class="ft9">using the same AO algorithms described in Section 3. Here,<br>the update for C and P would remain the same as that<br>discussed in Section 3. It can be easily shown that when<br></span></nobr></DIV>
<DIV style="position:absolute;top:625;left:482"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:492"><nobr><span class="ft4">= 1, j,</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:537"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:547"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:561"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:598"><nobr><span class="ft16">M<br>m=1</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:640"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:649"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:654"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:669"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:678"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:685"><nobr><span class="ft11">m</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:695"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:710"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:714"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:728"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:733"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:747"><nobr><span class="ft11">1/M</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:634"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:643"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:648"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:664"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:673"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:679"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:683"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:698"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:703"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:716"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:722"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:809"><nobr><span class="ft4">(14)</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:475"><nobr><span class="ft4">minimize J</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:542"><nobr><span class="ft11">DGK</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:575"><nobr><span class="ft4">for a given C.</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:475"><nobr><span class="ft3"><b>A.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:728;left:520"><nobr><span class="ft18"><b>Crisp Simultaneous Clustering and<br>Attribute Discrimination (CSCAD)</b></span></nobr></DIV>
<DIV style="position:absolute;top:768;left:489"><nobr><span class="ft4">Frigui et.</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:560"><nobr><span class="ft4">al.</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:588"><nobr><span class="ft4">in [5, 6], considered a fuzzy version of</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:475"><nobr><span class="ft9">the feature-weighting based clustering problem (SCAD). To<br>make a fair comparison of our algorithms with SCAD, we de-<br>rive its crisp version and refer to it as Crisp SCAD (CSCAD).<br>In [5, 6], the Gini measure is used for regularization. If the<br>Gini measure is considered with r = 1, the weights w</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:795"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:809"><nobr><span class="ft4">that</span></nobr></DIV>
<DIV style="position:absolute;top:862;left:475"><nobr><span class="ft9">minimize the corresponding objective function for a given C<br>and P, are given by</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:475"><nobr><span class="ft4">w</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:485"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:498"><nobr><span class="ft4">= 1</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:515"><nobr><span class="ft4">M +</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:550"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:544"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:557"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:567"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:924;left:567"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:910;left:582"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:578"><nobr><span class="ft4">M</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:602"><nobr><span class="ft11">M</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:597"><nobr><span class="ft11">n=1</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:620"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:629"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:634"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:650"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:657"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:664"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:672"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:686"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:691"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:704"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:710"><nobr><span class="ft4">) -</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:732"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:741"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:746"><nobr><span class="ft11">R</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:762"><nobr><span class="ft12">j</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:770"><nobr><span class="ft4">g</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:776"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:781"><nobr><span class="ft4">(x</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:795"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:800"><nobr><span class="ft4">, c</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:813"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:819"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:824"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:924;left:824"><nobr><span class="ft4"> .</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:809"><nobr><span class="ft4">(15)</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:475"><nobr><span class="ft9">Since SCAD uses the weighted Euclidean measure, the up-<br>date equations of centroids in CSCAD remain the same as in<br>(11). The update equation for w</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:672"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:685"><nobr><span class="ft4">in SCAD is quite similar</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:475"><nobr><span class="ft4">to (15). One may note that, in (15), the value of w</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:798"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:813"><nobr><span class="ft4">can</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:475"><nobr><span class="ft9">become negative. In [5], a heuristic is used to estimate the<br>value </span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:517"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:527"><nobr><span class="ft4">in every iteration and set the negative values of w</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:824"><nobr><span class="ft11">jl</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:475"><nobr><span class="ft4">to zero before normalizing the weights.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:450"><nobr><span class="ft8">616</span></nobr></DIV>
<DIV style="position:absolute;top:53;left:81"><nobr><span class="ft7"><b>Research Track Poster</b></span></nobr></DIV>
</DIV>
</BODY>
</HTML>
