<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\153</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-08-19T10:40:50+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:7px;font-family:Times;color:#000000;}
	.ft1{font-size:19px;font-family:Times;color:#000000;}
	.ft2{font-size:12px;font-family:Times;color:#000000;}
	.ft3{font-size:9px;font-family:Times;color:#000000;}
	.ft4{font-size:15px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Times;color:#000000;}
	.ft7{font-size:7px;line-height:10px;font-family:Times;color:#000000;}
	.ft8{font-size:11px;line-height:14px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="153001.png" alt="background image">
<DIV style="position:absolute;top:974;left:81"><nobr><span class="ft7">Permission to make digital or hard copies of part or all of this work for personal or<br>classroom use is granted without fee provided that copies are not made or distributed for<br>profit or direct commercial advantage and that copies show this notice on the first page or<br>initial screen of a display along with the full citation. Copyrights for components of this<br>work owned by others than ACM must be honored. Abstracting with credit is permitted. To<br>copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any<br>component of this work in other works requires prior specific permission and/or a fee.<br>Permissions may be requested from Publications Dept., ACM, Inc., 1515 Broadway, New<br>York, NY 10036 USA, fax +1 (212) 869-0481, or permissions@acm.org.</span></nobr></DIV>
<DIV style="position:absolute;top:81;left:174"><nobr><span class="ft1">Protected Interactive 3D Graphics Via Remote Rendering</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:80"><nobr><span class="ft2">David Koller</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:158"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:179"><nobr><span class="ft2">Michael Turitzin</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:279"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:301"><nobr><span class="ft2">Marc Levoy</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:374"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:395"><nobr><span class="ft2">Marco Tarini</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:473"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:495"><nobr><span class="ft2">Giuseppe Croccia</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:601"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:623"><nobr><span class="ft2">Paolo Cignoni</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:709"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:730"><nobr><span class="ft2">Roberto Scopigno</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:838"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:269"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:155;left:275"><nobr><span class="ft2">Stanford University</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:562"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:155;left:568"><nobr><span class="ft2">ISTI-CNR, Italy</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:81"><nobr><span class="ft4">Abstract</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:81"><nobr><span class="ft8">Valuable 3D graphical models, such as high-resolution digital scans<br>of cultural heritage objects, may require protection to prevent piracy<br>or misuse, while still allowing for interactive display and manipu-<br>lation by a widespread audience. We have investigated techniques<br>for protecting 3D graphics content, and we have developed a re-<br>mote rendering system suitable for sharing archives of 3D mod-<br>els while protecting the 3D geometry from unauthorized extrac-<br>tion. The system consists of a 3D viewer client that includes low-<br>resolution versions of the 3D models, and a rendering server that<br>renders and returns images of high-resolution models according to<br>client requests. The server implements a number of defenses to<br>guard against 3D reconstruction attacks, such as monitoring and<br>limiting request streams, and slightly perturbing and distorting the<br>rendered images. We consider several possible types of reconstruc-<br>tion attacks on such a rendering server, and we examine how these<br>attacks can be defended against without excessively compromising<br>the interactive experience for non-malicious users.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft9">CR Categories: I.3.2 [Computer Graphics]: Graphics Systems--<br>Remote systems</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:81"><nobr><span class="ft9">Keywords: security, 3D models, remote rendering, digital rights<br>management</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:81"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:111"><nobr><span class="ft4">Introduction</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:81"><nobr><span class="ft8">Protecting digital information from theft and misuse, a subset of the<br>digital rights management problem, has been the subject of much<br>research and many attempted practical solutions. Efforts to protect<br>software, databases, digital images, digital music files, and other<br>content are ubiquitous, and data security is a primary concern in<br>the design of modern computing systems and processes. However,<br>there have been few technological solutions to specifically protect<br>interactive 3D graphics content.</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft8">The demand for protecting 3D graphical models is significant. Con-<br>temporary 3D digitization technologies allow for the reliable and<br>efficient creation of accurate 3D models of many physical objects,<br>and a number of sizable archives of such objects have been created.<br>The Stanford Digital Michelangelo Project [Levoy et al. 2000], for<br>example, has created a high-resolution digital archive of 10 large<br>statues of Michelangelo, including the David. These statues rep-<br>resent the artistic patrimony of Italy's cultural institutions, and the<br>contract with the Italian authorities permits the distribution of the<br>3D models only to established scholars for non-commercial use.<br>Though all parties involved would like the models to be widely</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:477"><nobr><span class="ft8">available for constructive purposes, were the digital 3D model of<br>the David to be distributed in an unprotected fashion, it would soon<br>be pirated, and simulated marble replicas would be manufactured<br>outside the provisions of the parties authorizing the creation of the<br>model.</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:477"><nobr><span class="ft8">Digital 3D archives of archaeological artifacts are another example<br>of 3D models often requiring piracy protection. Curators of such<br>artifact collections are increasingly turning to 3D digitization as a<br>way to preserve and widen scholarly usage of their holdings, by al-<br>lowing virtual display and object examination over the Internet, for<br>example. However, the owners and maintainers of the artifacts of-<br>ten desire to maintain strict control over the use of the 3D data and<br>to guard against theft. An example of such a collection is [Stan-<br>ford Digital Forma Urbis Project 2004], in which over one thousand<br>fragments of an ancient Roman map were digitized and are being<br>made available through a web-based database, providing that the<br>3D models can be adequately protected.</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:477"><nobr><span class="ft8">Other application areas such as entertainment and online commerce<br>may also require protection for 3D graphics content. 3D character<br>models developed for use in motion pictures are often repurposed<br>for widespread use in video games and promotional materials. Such<br>models represent valuable intellectual property, and solutions for<br>preventing their piracy from these interactive applications would be<br>very useful. In some cases, such as 3D body scans of high pro-<br>file actors, content developers may be reluctant to distribute the 3D<br>models without sufficient control over reuse. In the area of online<br>commerce, a number of Internet content developers have reported<br>an unwillingness of clients to pursue 3D graphics projects specif-<br>ically due to the lack of ability to prevent theft of the 3D content<br>[Ressler 2001].</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:477"><nobr><span class="ft8">Prior technical research in the area of intellectual property protec-<br>tions for 3D data has primarily concentrated on 3D digital water-<br>marking techniques. Over 30 papers in the last 7 years describe<br>steganographic approaches to embedding hidden information into<br>3D graphical models, with varying degrees of robustness to attacks<br>that seek to disable watermarks through alterations to the 3D shape<br>or data representation. Many of the most successful 3D water-<br>marking schemes are based on spread-spectrum frequency domain<br>transformations, which embed watermarks at multiple scales by in-<br>troducing controlled perturbations into the coordinates of the 3D<br>model vertices [Praun et al. 1999; Ohbuchi et al. 2002]. Comple-<br>mentary technologies search collections of 3D models and examine<br>them for the presence of digital watermarks, in an effort to detect<br>piracy.</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:477"><nobr><span class="ft8">We believe that for the digital representations of highly valuable<br>3D objects such as cultural heritage artifacts, it is not sufficient to<br>detect piracy after the fact; we must instead prevent it. The com-<br>puter industry has experimented with a number of techniques for<br>preventing unauthorized use and copying of computer software and<br>digital data. These techniques have included physical dongles, soft-<br>ware access keys, node-locked licensing schemes, copy prevention<br>software, program and data obfuscation, and encryption with em-<br>bedded keys. Most such schemes are either broken or bypassed by<br>determined attackers, and cause undue inconvenience and expense<br>for non-malicious users. High-profile data and software is particu-<br>larly susceptible to being quickly targeted by attackers.</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">695</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:81"><nobr><span class="ft0"> 2004 ACM 0730-0301/04/0800-0695 $5.00</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153002.png" alt="background image">
<DIV style="position:absolute;top:87;left:81"><nobr><span class="ft8">Fortunately, 3D graphics data differs from most other forms of dig-<br>ital media in that the presentation format, 2D images, is fundamen-<br>tally different from the underlying representation (3D geometry).<br>Usually, 3D graphics data is displayed as a projection onto a 2D<br>display device, resulting in tremendous information loss for single<br>views. This property supports an optimistic view that 3D graphics<br>systems can be designed that maintain usability and utility, while<br>not being as vulnerable to piracy as other types of digital content.</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:81"><nobr><span class="ft8">In this paper, we address the problem of preventing the piracy of 3D<br>models, while still allowing for their interactive display and manip-<br>ulation. Specifically, we attempt to provide a solution for maintain-<br>ers of large collections of high-resolution static 3D models, such as<br>the digitized cultural heritage artifacts described above. The meth-<br>ods we develop aim to protect both the geometric shape of the 3D<br>models, as well as their particular geometric representation, such<br>as the 3D mesh vertex coordinates, surface normals, and connectiv-<br>ity information. We accept that the coarse shape of visible objects<br>can be easily reproduced regardless of our protection efforts, so we<br>concentrate on defending the high-resolution geometric details of<br>3D models, which may have been most expensive to model or mea-<br>sure (perhaps requiring special access and advanced 3D digitizing<br>technology), and which are most valuable in exhibiting fidelity to<br>the original object.</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:81"><nobr><span class="ft8">In the following paper sections, we first examine the graphics<br>pipeline to identify its possible points of attack, and then propose<br>several possible techniques for protecting 3D graphics data from<br>such attacks. Our experimentation with these techniques led us to<br>conclude that remote rendering provides the best solution for pro-<br>tecting 3D graphical models, and we describe the design and imple-<br>mentation of a prototype system in Section 4. Section 5 describes<br>some types of reconstruction attacks against such a remote render-<br>ing system and the initial results of our efforts to guard against<br>them.</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:81"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:111"><nobr><span class="ft4">Possible Attacks in the Graphics Pipeline</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:81"><nobr><span class="ft8">Figure 1 shows a simple abstraction of the graphics pipeline for<br>purposes of identifying possible attacks to recover 3D geometry.<br>We note several places in the pipeline where attacks may occur:</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:81"><nobr><span class="ft8">3D model file reverse-engineering. Fig. 1(a). 3D graphics models<br>are typically distributed to users in data streams such as files in<br>common file formats. One approach to protecting the data is to<br>obfuscate or encrypt the data file. If the user has full access to the<br>data file, such encryptions can be reverse-engineered and broken,<br>and the 3D geometry data is then completely unprotected.</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:81"><nobr><span class="ft8">Tampering with the viewing application. Fig. 1(b). A 3D viewer<br>application is typically used to display the 3D model and allow for<br>its manipulation. Techniques such program tracing, memory dump-<br>ing, and code replacement are practiced by attackers to obtain ac-<br>cess to data in use by application programs.</span></nobr></DIV>
<DIV style="position:absolute;top:912;left:81"><nobr><span class="ft8">Graphics driver tampering. Fig. 1(c). Because the 3D geometry<br>usually passes through the graphics driver software on its way to<br>the GPU, the driver is vulnerable to tampering. Attackers can re-<br>place graphics drivers with malicious or instrumented versions to<br>capture streams of 3D vertex data, for example. Such replacement<br>drivers are widely distributed for purposes of tracing and debugging<br>graphics programs.</span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:81"><nobr><span class="ft8">Reconstruction from the framebuffer. Fig. 1(d). Because the<br>framebuffer holds the result of the rendered scene, its contents can<br>be used by sophisticated attackers to reconstruct the model ge-<br>ometry, using computer vision 3D reconstruction techniques. The</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:477"><nobr><span class="ft8">Figure 1: Abstracted graphics pipeline showing possible attack lo-<br>cations (a-e). These attacks are described in the text.</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:477"><nobr><span class="ft8">framebuffer contents may even include depth values for each pixel,<br>and attackers may have precise control over the rendering param-<br>eters used to create the scene (viewing and projection transforma-<br>tions, lighting, etc.). This potentially creates a perfect opportunity<br>for computer vision reconstruction, as the synthetic model data and<br>controlled parameters do not suffer from the noise, calibration, and<br>imprecision problems that make robust real world vision with real<br>sensors very difficult.</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:477"><nobr><span class="ft8">Reconstruction from the final image display. Fig. 1(e). Re-<br>gardless of whatever protections a graphics system can guarantee<br>throughout the pipeline, the rendered images finally displayed to<br>the user are accessible to attackers. Just as audio signals may be<br>recorded by external devices when sound is played through speak-<br>ers, the video signals or images displayed on a computer monitor<br>may be recorded with a variety of video devices. The images so<br>gathered may be used as input to computer vision reconstruction<br>attacks such as those possible when the attacker has access to the<br>framebuffer itself, though the images may be of degraded quality,<br>unless a perfect digital video signal (such as DVI) is available.</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:477"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:507"><nobr><span class="ft4">Techniques for Protecting 3D Graphics</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:477"><nobr><span class="ft8">In light of the possible attacks in the graphics pipeline as described<br>in the previous section, we have considered a number of approaches<br>for sharing and rendering protected 3D graphics.</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:477"><nobr><span class="ft8">Software-only rendering. A 3D graphics viewing system that does<br>not make use of hardware acceleration may be easier to protect from<br>the application programmer's point of view. Displaying graphics<br>with a GPU can require transferring the graphics data in precisely<br>known and open formats, through a graphics driver and hardware<br>path that is often out of the programmer's control. A custom 3D<br>viewing application with software rendering allows the 3D content<br>distributor to encrypt or obfuscate the data in a specific manner, all<br>the way through the graphics pipeline until display.</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:477"><nobr><span class="ft8">Hybrid hardware/software rendering. Hybrid hardware and soft-<br>ware rendering schemes can be used to take at least some advantage<br>of hardware accelerated rendering, while benefiting from software<br>rendering's protections as described above. In one such scheme, a<br>small but critically important portion of a protected model's geom-<br>etry (such as the nose of a face) is rendered in software, while the<br>rest of the model is rendered normally with the accelerated GPU<br>hardware. This technique serves as a deterrent to attackers tamper-<br>ing with the graphics drivers or hardware path, but the two-phase<br>drawing with readback of the color and depth buffers can incur a</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">696</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153003.png" alt="background image">
<DIV style="position:absolute;top:87;left:81"><nobr><span class="ft8">performance hit, and may require special treatment to avoid arti-<br>facts on the border of the composition of the two images.</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:81"><nobr><span class="ft8">In another hybrid rendering scheme, the 3D geometry is trans-<br>formed and per-vertex lighting computations are performed in soft-<br>ware. The depth values computed for each vertex are distorted in<br>a manner that still preserves the correct relative depth ordering,<br>while concealing the actual model geometry as much as possible.<br>The GPU is then used to complete rendering, performing rasteri-<br>zation, texturing, etc. Such a technique potentially keeps the 3D<br>vertex stream hidden from attackers, but the distortions of the depth<br>buffer values may impair certain graphics operations (fog compu-<br>tation, some shadow techniques), and the geometry may need to be<br>coarsely depth sorted so that Z-interpolation can still be performed<br>in a linear space.</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:81"><nobr><span class="ft8">Deformations of the geometry. Small deformations in large 2D<br>images displayed on the Internet are sometimes used as a defense<br>against image theft; zoomed higher resolution sub-images with<br>varying deformations cannot be captured and easily reassembled<br>into a whole. A similar idea can be used with 3D data: subtle 3D<br>deformations are applied to geometry before the vertices are passed<br>to the graphics driver. The deformations are chosen so as to vary<br>smoothly as the view of the model changes, and to prohibit recov-<br>ery of the original coordinates by averaging the deformations over<br>time. Even if an attacker is able to access the stream of 3D data af-<br>ter it is deformed, they will encounter great difficulty reconstructing<br>a high-resolution version of the whole model due to the distortions<br>that have been introduced.</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:81"><nobr><span class="ft8">Hardware decryption in the GPU. One sound approach to provid-<br>ing for protected 3D graphics is to encrypt the 3D model data with<br>public-key encryption at creation time, and then implement custom<br>GPUs that accept encrypted data, and perform on-chip decryption<br>and rendering. Additional system-level protections would need to<br>be implemented to prevent readback of framebuffer and other video<br>memory, and to place potential restrictions on the command stream<br>sent to the GPU, in order to prevent recovery of the 3D data.</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:81"><nobr><span class="ft8">Image-based rendering. Since our goal is to protect the 3D ge-<br>ometry of graphic models, one technique is to distribute the mod-<br>els using image-based representations, which do not explicitly in-<br>clude the complete geometry data. Examples of such represen-<br>tations include light fields and Lumigraphs [Levoy and Hanrahan<br>1996; Gortler et al. 1996], both of which are highly amenable to<br>interactive display.</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:81"><nobr><span class="ft8">Remote rendering. A final approach to secure 3D graphics is to<br>retain the 3D model data on a secure server, under the control of<br>the content owner, and pass only 2D rendered images of the models<br>back to client requests. Very low-resolution versions of the models,<br>for which piracy is not a concern, can be distributed with special<br>client programs to allow for interactive performance during ma-<br>nipulation of the 3D model. This method relies on good network<br>bandwidth between the client and server, and may require signifi-<br>cant server resources to do the rendering for all client requests, but<br>it is vulnerable primarily only to reconstruction attacks.</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:81"><nobr><span class="ft8">Discussion. We have experimented with several of the 3D model<br>protection approaches described above. For example, our first pro-<br>tected 3D model viewer was an encrypted version of the "QS-<br>plat" [Rusinkiewicz and Levoy 2000] point-based rendering sys-<br>tem, which omits geometric connectivity information.</span></nobr></DIV>
<DIV style="position:absolute;top:996;left:398"><nobr><span class="ft5">The 3D</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:81"><nobr><span class="ft8">model files were encrypted using a strong symmetric block cipher<br>scheme, and the decryption key was hidden in a heavily obfus-<br>cated 3D model viewer program, using modern program obfusca-<br>tion techniques [Collberg and Thomborson 2000]. Vertex data was<br>decrypted on demand during rendering, so that only a very small</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:477"><nobr><span class="ft8">portion of the decrypted model was ever in memory, and only soft-<br>ware rendering modes were used.</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:477"><nobr><span class="ft8">Unfortunately, systems such as this ultimately rely on "security<br>through obfuscation," which is theoretically unsound from a com-<br>puter security point of view. Given enough time and resources, an<br>attacker will be able to discover the embedded encryption key or<br>otherwise reverse-engineer the protections on the 3D data. For this<br>reason, any of the 3D graphics protection techniques that make the<br>actual 3D data available to potential attackers in software can be<br>broken [Schneier 2000]. It is possible that future "trusted comput-<br>ing" platforms for general purpose computers will be available that<br>make software tampering difficult or impossible, but such systems<br>are not widely deployed today. Similarly, the idea of a GPU with<br>decryption capability has theoretical merit, but it will be some years<br>before such hardware is widely available for standard PC comput-<br>ing environments, if ever.</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:477"><nobr><span class="ft8">Thus, for providing practical, robust, anti-piracy protections for 3D<br>data, we gave strongest consideration to purely image-based rep-<br>resentations and to remote rendering. Distributing light fields at<br>the high resolutions necessary would involve huge, unwieldy file<br>sizes, would not allow for any geometric operations on the data<br>(such as surface measurements performed by archaeologists), and<br>would still give attackers unlimited access to the light field for pur-<br>poses of performing 3D reconstruction attacks using computer vi-<br>sion algorithms. For these reasons, we finally concluded that the<br>last technique, remote rendering, offers the best solution for pro-<br>tecting interactive 3D graphics content.</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:477"><nobr><span class="ft8">Remote rendering has been used before in networked environments<br>for 3D visualization, although we are not aware of a system specif-<br>ically designed to use remote rendering for purposes of security<br>and 3D content protection. Remote rendering systems have been<br>previously implemented to take advantage of off-site specialized<br>rendering capabilities not available in client systems, such as in-<br>tensive volume rendering [Engel et al. 2000], and researchers have<br>developed special algorithmic approaches to support efficient dis-<br>tribution of rendering loads and data transmission between render-<br>ing servers and clients [Levoy 1995; Yoon and Neumann 2000].<br>Remote rendering of 2D graphical content is common for Internet<br>services such as online map sites; only small portions of the whole<br>database are viewed by users at one time, and protection of the en-<br>tire 2D data corpus from theft via image harvesting may be a factor<br>in the design of these systems.</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:477"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:507"><nobr><span class="ft4">Remote Rendering System</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:477"><nobr><span class="ft8">To test our ideas for providing controlled, protected interactive ac-<br>cess to collections of 3D graphics models, we have implemented<br>a remote rendering system with a client-server architecture, as de-<br>scribed below.</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:477"><nobr><span class="ft2">4.1</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:514"><nobr><span class="ft2">Client Description</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:477"><nobr><span class="ft8">Users of our protected graphics system employ a specially-designed<br>3D viewing program to interactively view protected 3D con-<br>tent.</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:515"><nobr><span class="ft5">This client program is implemented as an OpenGL and</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:477"><nobr><span class="ft8">wxWindows-based 3D viewer, with menus and GUI dialogs to con-<br>trol various viewing and networking parameters (Figure 2). The<br>client program includes very low-resolution, decimated versions of<br>the 3D models, which can be interactively rotated, zoomed, and re-<br>lit by the user in real-time. When the user stops manipulating the<br>low-resolution model, detected via a "mouse up" event, the client<br>program queries the remote rendering server via the network for a</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">697</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153004.png" alt="background image">
<DIV style="position:absolute;top:381;left:145"><nobr><span class="ft5">Figure 2: Screenshot of the client program.</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:81"><nobr><span class="ft8">high-resolution rendered image corresponding to the selected ren-<br>dering parameters. These parameters include the 3D model name,<br>viewpoint position and orientation, and lighting conditions. When<br>the server passes the rendered image back to the client program, it<br>replaces the low-resolution rendering seen by the user (Figure 3).</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:81"><nobr><span class="ft8">On computer networks with reasonably low latencies, the user thus<br>has the impression of manipulating a high-resolution version of<br>the model. In typical usage for cultural heritage artifacts, we use<br>models with approximately 10,000 polygons for the low resolution<br>version, whereas the server-side models often contain tens of mil-<br>lions polygons. Such low-resolution model complexities are of lit-<br>tle value to potential thieves, yet still provide enough clues for the<br>user to navigate. The client viewer could be further extended to<br>cache the most recent images returned from the server and projec-<br>tively texture map them onto the low-resolution model as long as<br>they remain valid during subsequent rotation and zooming actions.</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:81"><nobr><span class="ft2">4.2</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:118"><nobr><span class="ft2">Server Description</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:81"><nobr><span class="ft8">The remote rendering server receives rendering requests from<br>users' client programs, renders corresponding images, and passes<br>them back to the clients. The rendering server is implemented as<br>a module running under the Apache 2.0 HTTP Server; as such,<br>the module communicates with client programs using the standard<br>HTTP protocol, and takes advantage of the wide variety of access<br>protection and monitoring tools built into Apache. The rendering<br>server module is based upon the FastCGI Apache module, and al-<br>lows for multiple rendering processes to be spread across any num-<br>ber of server hardware nodes.</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft8">As render requests are received from clients, the rendering server<br>checks their validity and dispatches the valid requests to a GPU for<br>OpenGL hardware-accelerated rendering. The rendered images are<br>read back from the framebuffer, compressed using JPEG compres-<br>sion, and returned to the client. If multiple requests from the same<br>client are pending (such as if the user rapidly changes views while<br>on a slow network), earlier requests are discarded, and only the<br>most recent is rendered. The server uses level-of-detail techniques<br>to speed the rendering of highly complex models, and lower level-<br>of-detail renderings can be used during times of high server load<br>to maintain high throughput rates. In practice, an individual server<br>node with a Pentium 4 CPU and an NVIDIA GeForce4 video card<br>can handle a maximum of 8 typical client requests per second; the</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:477"><nobr><span class="ft8">Figure 3: Client-side low resolution (left) and server-side high res-<br>olution (right) model renderings.</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:477"><nobr><span class="ft8">bottlenecks are in the rendering and readback (about 100 millisec-<br>onds), and in the JPEG compression (approximately 25 millisec-<br>onds). Incoming request sizes are about 700 bytes each, and the<br>images returned from our deployed servers average 30 kB per re-<br>quest.</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:477"><nobr><span class="ft2">4.3</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:514"><nobr><span class="ft2">Server Defenses</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:477"><nobr><span class="ft8">In Section 2, we enumerated several possible places in the graphics<br>pipeline that an attacker could steal 3D graphics data. The benefit of<br>using remote rendering is that it leaves only 3D reconstruction from<br>2D images in the framebuffer or display device as possible attacks.<br>General 3D reconstruction from images constitutes a very difficult<br>computer vision problem, as evidenced by the great amount of re-<br>search effort being expended to design and build robust computer<br>vision systems. However, synthetic 3D graphics renderings can be<br>particularly susceptible to reconstruction because the attacker may<br>be able to exactly specify the parameters used to create the images,<br>there is a low human cost to harvest a large number of images, and<br>synthetic images are potentially perfect, with no sensor noise or<br>miscalibration errors. Thus, it is still necessary to defend the remote<br>rendering system from reconstruction attacks; below, we describe a<br>number of such defenses that we have implemented in combination<br>for our server.</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:477"><nobr><span class="ft8">Session-based defenses. Client programs that access the remote<br>rendering system are uniquely identified during the course of a us-<br>age session. This allows the server to monitor and track the specific<br>sequence of rendering requests made by each client. Automatic<br>analysis of the server logs allows suspicious request streams to be<br>classified, such as an unusually high number of requests per unit<br>time, or a particular pattern of requests that is indicative of an im-<br>age harvesting program. High quality computer vision reconstruc-<br>tions often require a large number of images that densely sample<br>the space of possible views, so we are able to effectively identify<br>such access patterns and terminate service to those clients. We can<br>optionally require recurrent user authentication in order to further<br>deter some image harvesting attacks, although a coalition of users<br>mounting a low-rate distributed attack from multiple IP addresses<br>could still defeat such session-based defenses.</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:477"><nobr><span class="ft8">Obfuscation. Although we do not rely on obfuscation to protect the<br>3D model data, we do use obfuscation techniques on the client side<br>of the system to discourage and slow down certain attacks. The<br>low-resolution models that are distributed with the client viewer<br>program are encrypted using an RC4-variant stream cipher, and the<br>keys are embedded in the viewer and heavily obfuscated. The ren-<br>dering request messages sent from the client to the server are also<br>encrypted with heavily obfuscated keys. These encryptions simply<br>serve as another line of defense; even if they were broken, attackers<br>would still not be able to gain access to the high resolution 3D data<br>except through reconstruction from 2D images.</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">698</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft10{font-size:8px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="153005.png" alt="background image">
<DIV style="position:absolute;top:87;left:81"><nobr><span class="ft8">Limitations on valid rendering requests. As a further defense,<br>we provide the capability in our client and remote server to con-<br>strain the viewing conditions. Some models may have particular<br>"stayout" regions defined that disallow certain viewing and light-<br>ing angles, thus keeping attackers from being able to reconstruct a<br>complete model. For the particular purpose of defending against the<br>enumeration attacks described in Section 5.1, we put restrictions on<br>the class of projection transformations allowed to be requested by<br>users (requiring a perspective projection with particular fixed field<br>of view and near and far planes), and we prevent viewpoints within<br>a small offset of the model surface.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:81"><nobr><span class="ft8">Perturbations and distortions. Passive 3D computer vision recon-<br>structions of real-world objects from real-world images are usually<br>of relatively poor quality compared to the original object. This fail-<br>ure inspires the belief that we can protect our synthetically rendered<br>models from reconstruction by introducing into the images the same<br>types of obstacles known to plague vision algorithms. The partic-<br>ular perturbations and distortions that we use are described below;<br>we apply these defenses to the images only to the degree that they<br>do not distract the user viewing the models. Additionally, these de-<br>fenses are applied in a pseudorandomly generated manner for each<br>different rendering request, so that attackers cannot systematically<br>determine and reverse their effects, even if the specific form of the<br>defenses applied is known (such as if the source code for the ren-<br>dering server is available). Rendering requests with identical pa-<br>rameters are mapped to the same set of perturbations, in order to<br>deter attacks which attempt to defeat these defenses by averaging<br>multiple images obtained under the same viewing conditions.</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:97"><nobr><span class="ft5"> Perturbed viewing parameters We pseudorandomly intro-</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:111"><nobr><span class="ft9">duce subtle perturbations into the view transformation ma-<br>trix for the images rendered by the server; these perturbations<br>have the effect of slightly rotating, translating, scaling, and<br>shearing the model. The range of these distortions is bounded<br>such that no point in the rendered image is further than either<br>m</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:124"><nobr><span class="ft5">object space units or n pixels from its corresponding point</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:111"><nobr><span class="ft8">in an unperturbed view. In practice, we generally set m pro-<br>portional to the size of the model's geometry being protected,<br>and use values of n = 15 pixels, as experience has shown that<br>users can be distracted by larger shifts between consecutively<br>displayed images.</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:97"><nobr><span class="ft5"> Perturbed lighting parameters We pseudorandomly intro-</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:111"><nobr><span class="ft8">duce subtle perturbations into the lighting parameters used<br>to render the images; these perturbations include modifying<br>the lighting direction specified in the client request, as well<br>as addition of randomly changing secondary lighting to illu-<br>minate the model. Users are somewhat sensitive to shifts in<br>the overall scene intensity and shading, so the primary light<br>direction perturbations used are generally fairly small (maxi-<br>mum of 10</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:170"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:839;left:180"><nobr><span class="ft5">for typical models, which are rendered using the</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:111"><nobr><span class="ft5">OpenGL local lighting model).</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:97"><nobr><span class="ft5"> High-frequency noise added to the images We introduce</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:111"><nobr><span class="ft8">two types of high-frequency noise artifacts into the rendered<br>images. The first, JPEG artifacts, are a convenient result of<br>the compression scheme applied to the images returned from<br>the server. At high compression levels (we use a maximum<br>libjpeg quality factor of 50), the quantization of DCT coeffi-<br>cients used in JPEG compression creates "blocking" disconti-<br>nuities in the images, and adds noise in areas of sharp contrast.<br>These artifacts create problems for low-level computer vision<br>image processing algorithms, while the design of JPEG com-<br>pression specifically seeks to minimize the overall perceptual<br>loss of image quality for human users.</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:111"><nobr><span class="ft5">Additionally, we add pseudorandomly generated monochro-</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:507"><nobr><span class="ft8">matic Gaussian noise to the images, implemented efficiently<br>by blending noise textures during hardware rendering on the<br>server. The added noise defends against computer vision at-<br>tacks by making background segmentation more difficult, and<br>by breaking up the highly regular shading patterns of the syn-<br>thetic renderings. Interestingly, users are not generally dis-<br>tracted by the added noise, but have even commented that the<br>rendered models often appear "more realistic" with the high-<br>frequency variations caused by the noise. One drawback of<br>the added noise is that the increased entropy of the images can<br>result in significantly larger compressed file sizes; we address<br>this in part by primarily limiting the application of noise to the<br>non-background regions of the image via stenciled rendering.</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:492"><nobr><span class="ft5"> Low-frequency image distortions Just as real computer vi-</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:507"><nobr><span class="ft8">sion lens and sensor systems sometimes suffer from image<br>distortions due to miscalibration, we can effectively simulate<br>and extend these distortions in the rendering server. Sub-<br>tle non-linear radial distortions, pinching, and low-frequency<br>waves can be efficiently implemented with vertex shaders, or<br>with two-pass rendering of the image as a texture onto a non-<br>uniform mesh, accelerated with the "render to texture" capa-<br>bilities of modern graphics hardware.</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:477"><nobr><span class="ft8">Due to the variety of random perturbations and distortions that are<br>applied to the images returned from the rendering server, there is<br>a risk of distracting the user, as the rendered 3D model exhibits<br>changes from frame to frame, even when the user makes very mi-<br>nor adjustments to the view. However, we have found that the<br>brief switch to the lower resolution model in between display of the<br>high resolution perturbed images, inherent to our remote render-<br>ing scheme, very effectively masks these changes. This masking of<br>changes is attributed to the visual perception phenomenon known<br>as change blindness [Simons and Levin 1997], in which significant<br>changes occurring in full view are not noticed due to a brief dis-<br>ruption in visual continuity, such as a "flicker" introduced between<br>successive images.</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:477"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:507"><nobr><span class="ft4">Reconstruction Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:477"><nobr><span class="ft8">In this section we consider several classes of attacks, in which sets<br>of images may be gathered from our remote rendering server to<br>make 3D reconstructions of the model, and we analyze their effi-<br>cacy against the countermeasures we have implemented.</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:477"><nobr><span class="ft2">5.1</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:514"><nobr><span class="ft2">Enumeration Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:477"><nobr><span class="ft8">The rendering server responds to rendering requests from users<br>specifying the viewing conditions for the rendered images. This<br>ability for precise specification can be exploited by attackers, as<br>they can potentially explore the entire 3D model space, using the re-<br>turned images to discover the location of the 3D model to any arbi-<br>trary precision. In practice, these attacks involve enumerating many<br>small cells in a voxel grid, and testing each such voxel to determine<br>intersection with the remote high-resolution model's surface; thus<br>we term them enumeration attacks. Once this enumeration process<br>is complete, occupied cells of the voxel grid are exported as a point<br>cloud and then input to a surface reconstruction algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:996;left:477"><nobr><span class="ft8">In the plane sweep enumeration attack, the view frustum is speci-<br>fied as a rectangular, one-voxel-thick "plane," and is swept over the<br>model (Figure 4(a)). Each requested image represents one slice of<br>the model's surface, and each pixel of each image corresponds to a<br>single voxel. A simple comparison of each image pixel against the<br>expected background color is performed to determine whether that</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">699</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153006.png" alt="background image">
<DIV style="position:absolute;top:220;left:160"><nobr><span class="ft3">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:324"><nobr><span class="ft3">(b)</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:81"><nobr><span class="ft8">Figure 4: Enumeration Attacks: (a) the plane sweep enumeration<br>attack sweeps a one-voxel thick orthographic view frustum over<br>the model, (b) the near plane sweep enumeration attack sweeps the<br>viewpoint over the model, marking voxels where the model surface<br>is clipped by the near plane.</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:81"><nobr><span class="ft8">pixel is a model surface or background pixel. Sweeps from multiple<br>view angles (such as the six faces of the voxels) are done to catch<br>backfacing polygons that may not be visible from a particular angle.<br>These redundant multiple sweeps also allow the attacker to be lib-<br>eral about ignoring questionable background pixels that may occur,<br>such as if low-amplitude background noise or JPEG compression is<br>being used as a defense on the server.</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft8">Our experiments demonstrate that the remote model can be effi-<br>ciently reconstructed against a defenseless server using this attack<br>(Figure 5(b)). Perturbing viewing parameters can be an effective<br>defense against this attack; the maximum reconstruction resolution<br>will be limited by the maximum relative displacement that an in-<br>dividual model surface point undergoes. Figure 5(c) shows the re-<br>sults of a reconstruction attempt against a server pseudorandomly<br>perturbing the viewing direction by up to 0.3</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:328"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:573;left:338"><nobr><span class="ft5">in the returned im-</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:81"><nobr><span class="ft8">ages. Since plane sweep enumeration relies on the correspondence<br>between image pixels and voxels, image warps can also be effec-<br>tive as a defense. The large number of remote image requests re-<br>quired for plane sweep enumeration (O(n) requests for an n  n  n<br>voxel grid) and the unusual request parameters may look suspicious<br>and trigger the rendering server log analysis monitors. Plane sweep<br>enumeration attacks can be completely nullified by limiting user<br>control of the view frustum parameters, which we implement in our<br>system and use for valuable models.</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:81"><nobr><span class="ft8">Another enumeration attack, near plane sweep enumeration, in-<br>volves sweeping the viewpoint (and thus the near plane) over the<br>model, checking when the model surface is clipped by the near<br>plane and marking voxels when this happens (Figure 4(b)). The<br>attacker knows that the near plane has clipped the model when a<br>pixel previously containing the model surface begins to be classi-<br>fied as the background. In order to determine which voxel each<br>image pixel corresponds to, the attacker must know two related pa-<br>rameters: the distance between the viewpoint position and the near<br>plane, and the field of view.</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft8">These parameters can be easily discovered. The near plane dis-<br>tance can be determined by first obtaining the exact location of one<br>feature point on the model surface through triangulation of multi-<br>ple rendering requests and then moving the viewpoint slowly to-<br>ward that point on the model. When the near plane clips the feature<br>point, the distance between that point and the view position equals<br>the near plane distance. The horizontal and vertical field of view<br>angles can be obtained by moving the viewpoint slowly toward the<br>model surface, stopping when any surface point becomes clipped by<br>the near plane. The viewpoint is then moved a small amount per-<br>pendicular to its original direction of motion such that the clipped<br>point moves slightly relative to the view but stays on the new im-<br>age (near plane). Since the near plane distance has already been</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:561"><nobr><span class="ft3">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:735"><nobr><span class="ft3">(b)</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:561"><nobr><span class="ft3">(c)</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:735"><nobr><span class="ft3">(d)</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:477"><nobr><span class="ft8">Figure 5: 3D reconstruction results from enumeration attacks:<br>(a) original 3D model, (b) plane sweep attack against defenseless<br>server (6 passes, 3,168 total rendered images), (c) plane sweep at-<br>tack against 0.3</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:562"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:405;left:572"><nobr><span class="ft5">viewing direction perturbation defense (6 passes,</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:477"><nobr><span class="ft8">3,168 total rendered images), (d) near plane sweep attack against<br>defenseless server (6 passes, 7,952 total rendered images).</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:477"><nobr><span class="ft8">obtained, the field of view angle (horizontal or vertical depending<br>on direction of motion) can be obtained from the relative motion of<br>the clipped point across the image.</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:477"><nobr><span class="ft8">Because the near plane is usually small compared to the dimensions<br>of the model, many sweeps must be tiled in order to attain full cov-<br>erage. Sweeps must also be made in several directions to ensure<br>that all model faces are seen. Because this attack relies on seeing<br>the background to determine when the near plane has clipped a sur-<br>face, concave model geometries will present a problem for surface<br>detection. Although sweeps from multiple directions will help, this<br>problem is not completely avoidable. Figure 5(d) illustrates this<br>problem, showing a case in which six sweeps have not fully cap-<br>tured all the surface geometry.</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:477"><nobr><span class="ft8">Viewing parameter perturbations and image warps will nearly de-<br>stroy the effectiveness of near plane sweep enumeration attacks, as<br>they can make it very difficult to determine where the surface lies<br>and where it does not near silhouette edges (pixels near these edges<br>will change erratically between surface and background). The most<br>solid defense against this attack is to prevent views within a cer-<br>tain small offset of the model surface. This defense, which we use<br>in our system to protect valuable models, prevents the near plane<br>from ever clipping the model surface and thereby completely nulli-<br>fies this attack.</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:477"><nobr><span class="ft2">5.2</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:514"><nobr><span class="ft2">Shape-from-silhouette Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:477"><nobr><span class="ft8">Shape-from-silhouette [Slabaugh et al. 2001] is one well studied,<br>robust technique for extracting a 3D model from a set of images.<br>The method consists of segmenting the object pixels from the back-<br>ground in each image, then intersecting in space their resulting ex-<br>tended truncated silhouettes, and finally computing the surface of<br>the resulting shape. The main limitation of this technique is that<br>only a visual hull [Laurentini 1994] of the 3D shape can be recov-<br>ered; the line-concave parts of the model are beyond the capabilities<br>of the reconstruction. Thus, the effectiveness of this attack depends<br>on the specific geometric characteristics of the object; the high-<br>resolution 3D models that we target often have many concavities<br>that are difficult or impossible to fully recover using shape-from-<br>silhouette. However, this attack may also be of use to attackers</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">700</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153007.png" alt="background image">
<DIV style="position:absolute;top:284;left:81"><nobr><span class="ft8">Figure 6: The 160 viewpoints used to reconstruct the model with a<br>shape-from-silhouette attack; results are shown in Figure 7.</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:81"><nobr><span class="ft8">to obtain a coarse, low-resolution version of the model, if they are<br>unable to break through the obfuscation protections we use for the<br>low-resolution models distributed with the client.</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:81"><nobr><span class="ft8">To measure the potential of a shape-from-silhouette attack against<br>our protected graphics system, we have conducted reconstruction<br>experiments on a 3D model of the David as served via the render-<br>ing server, using a shape-from-silhouette implementation described<br>in [Tarini et al. 2002]. With all server defenses disabled, 160 im-<br>ages were harvested from a variety of viewpoints around the model<br>(Figure 6); these viewpoints were selected incrementally, with later<br>viewpoints chosen to refine the reconstruction accuracy as mea-<br>sured during the process. The resulting 3D reconstruction is shown<br>in Figure 7(b).</span></nobr></DIV>
<DIV style="position:absolute;top:563;left:81"><nobr><span class="ft8">Several of the perturbation and distortion defenses implemented in<br>our server are effective against the shape-from-silhouette attack.<br>Results from experiments showing the reconstructed model qual-<br>ity with server defenses independently enabled are shown in Fig-<br>ures 7(c-g). Small perturbations in the viewing parameters were<br>particularly effective at decreasing the quality of the reconstructed<br>model, as would be expected; Niem [1997] performed an error anal-<br>ysis of silhouette-based modeling techniques and showed the linear<br>relationship between error in the estimation of the view position<br>and error in the resulting reconstruction. Perturbations in the im-<br>ages returned from the server, such as radial distortion and small<br>random shifts, were also effective. Combining the different pertur-<br>bation defenses, as they are implemented in our remote rendering<br>system, makes for further deterioration of the reconstructed model<br>quality (Figure 7(h)).</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:81"><nobr><span class="ft8">High frequency noise and JPEG defenses in the server images can<br>increase the difficulty of segmenting the object from the back-<br>ground.</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:137"><nobr><span class="ft5">However, shape-from-silhouette software implementa-</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:81"><nobr><span class="ft8">tions with specially tuned image processing operations can take the<br>noise characteristics into account to help classify pixels accurately.<br>The intersection stage of shape-from-silhouette reconstruction al-<br>gorithms makes them innately robust with respect to background<br>pixels misclassified as foreground.</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:81"><nobr><span class="ft2">5.3</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:118"><nobr><span class="ft2">Stereo Correspondence-based Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:81"><nobr><span class="ft8">Stereo reconstruction is another well known 3D computer vision<br>technique. Stereo pairs of similarly neighborhooded pixels are de-<br>tected, and the position of the corresponding point on the 3D sur-<br>face is found via the intersection of epipolar lines. Of particular<br>relevance to our remote rendering system, Debevec et al. [1996]<br>showed that the reconstruction task can be made easier and more<br>accurate if an approximate low resolution model is available, by<br>warping the images over it before performing the stereo matching.</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:502"><nobr><span class="ft3">(a) E = 0</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:585"><nobr><span class="ft3">(b) E = 4.5</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:665"><nobr><span class="ft3">(c) E = 13.5</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:757"><nobr><span class="ft3">(d) E = 45.5</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:494"><nobr><span class="ft3">(e) E = 11.6</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:586"><nobr><span class="ft3">(f) E = 9.3</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:669"><nobr><span class="ft3">(g) E = 16.2</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:757"><nobr><span class="ft3">(h) E = 26.6</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:477"><nobr><span class="ft8">Figure 7: Performance of shape-from-silhouette reconstructions<br>against various server defenses. Error values (E) measure the mean<br>surface distance (mm) from the 5m tall original model. Top row:<br>(a) original model, (b) reconstruction from defenseless server, re-<br>construction with (c) 0.5</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:614"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:625"><nobr><span class="ft5">and (d) 2.0</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:687"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:698"><nobr><span class="ft5">perturbations of the view</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:477"><nobr><span class="ft8">direction. Bottom row: (e) reconstruction with a random image off-<br>set of 4 pixels, with (f) 1.2% and (g) 2.5% radial image distortion,<br>and (h) reconstruction against combined defenses (1.0</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:775"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:695;left:785"><nobr><span class="ft5">view per-</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:477"><nobr><span class="ft5">turbation, 2 pixel random offset, and 1.2% radial image distortion).</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:477"><nobr><span class="ft8">Ultimately, however, stereo correspondence techniques usually rely<br>on matching detailed, high-frequency features in order to yield<br>high-resolution reconstruction results. The smoothly shaded 3D<br>computer models generated by laser scanning that we share via our<br>remote rendering system thus present significant problems to basic<br>two-frame stereo matching algorithms. When we add in the server<br>defenses such as image-space high frequency noise, and slight per-<br>turbations in the viewing and lighting parameters, the stereo match-<br>ing task becomes even more ill-posed. Other stereo research such as<br>[Scharstein and Szeliski 2002] also reports great difficulty in stereo<br>reconstruction of noise-contaminated, low-texture synthetic scenes.<br>Were we to distribute 3D models with high resolution textures ap-<br>plied to their surfaces, stereo correspondence methods may be a<br>more effective attack.</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:477"><nobr><span class="ft2">5.4</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:514"><nobr><span class="ft2">Shape-from-shading Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:477"><nobr><span class="ft8">Shape-from-shading attacks represent another family of computer<br>vision techniques for reconstructing the shape of a 3D object (see<br>[Zhang et al. 1999] for a survey). The primary attack on our re-<br>mote rendering system that we consider in this class involves first</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">701</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="153008.png" alt="background image">
<DIV style="position:absolute;top:214;left:117"><nobr><span class="ft3">(a) E = 0</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:233"><nobr><span class="ft3">(b) E = 1.9</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:353"><nobr><span class="ft3">(c) E = 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:112"><nobr><span class="ft3">(d) E = 1.1</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:233"><nobr><span class="ft3">(e) E = 1.7</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:354"><nobr><span class="ft3">(f) E = 2.0</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:81"><nobr><span class="ft8">Figure 8: Performance of shape-from-shading reconstruction at-<br>tacks. Error values (E) measure the mean surface distance (mm)<br>from the original model. Top row: (a) original model, (b) low-<br>resolution base mesh, (c) reconstruction from defenseless server.<br>Bottom row: reconstruction results against (d) high-frequency im-<br>age noise, (e) complicated lighting model (3 lights), and (f) viewing<br>angle perturbation (up to 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:235"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:490;left:241"><nobr><span class="ft5">) defenses.</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:81"><nobr><span class="ft8">obtaining several images from the same viewpoint under varying,<br>known lighting conditions. Then, using photometric stereo meth-<br>ods, a normal is computed for each pixel by solving a system of<br>rendering equations. The resulting normal map can be registered<br>and applied to an available approximate 3D geometry, such as the<br>low-resolution model used by the client, or one obtained from an-<br>other reconstruction technique such as shape-from-silhouette.</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:81"><nobr><span class="ft8">This coarse normal-mapped model itself may be of value to some<br>attackers: when rendered it will show convincing 3D high fre-<br>quency details that can be shaded under new lighting conditions,<br>though with artifacts at silhouettes. However, the primary purpose<br>of our system is to protect the high-resolution 3D geometry, which<br>if stolen could be used maliciously for shape analysis or to create<br>replicas. Thus, a greater risk is posed if the normal map is integrated<br>by the attacker to compute a displacement map, and the results are<br>used to displace a refined version of the low-resolution model mesh.</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:81"><nobr><span class="ft8">Following this procedure with images harvested from a defenseless<br>remote rendering server and using a low-resolution client model,<br>we were able to successfully reconstruct a high-resolution 3D<br>model. The results shown in Figure 8(c) depict a reconstruction<br>of the David's head produced from 200 1600x1114 pixel images<br>taken from 10 viewpoints, with 20 lighting positions used at each<br>viewpoint, assuming a known, single-illuminant OpenGL lighting<br>model and using a 10,000 polygon low-resolution model (Fig. 8(b))<br>of the whole statue.</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:81"><nobr><span class="ft8">Some of the rendering server defenses, such as adding high-<br>frequency noise to the images, can be compensated for by attack-<br>ers by simply adding enough input images to increase the robust-<br>ness of the photometric stereo solution step (although harvesting<br>too many images will eventually trigger the rendering server log<br>analysis monitors). Figure 8(d) shows the high quality reconstruc-<br>tion result possible when only random Gaussian noise is used as<br>a defense. More effective defenses against shape-from-shading at-<br>tacks include viewing and lighting perturbations and low-frequency</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:477"><nobr><span class="ft8">image distortions, which can make it difficult to precisely register<br>images onto the low-resolution model, and can disrupt the photo-<br>metric stereo solution step without a large number of aligned in-<br>put images. Figure 8(e) shows a diminished quality reconstruction<br>when the rendering server complicates the lighting model by us-<br>ing 3 perturbed light sources with a Phong component unknown to<br>the attacker, and Figure 8(f) shows the significant loss of geometric<br>detail in the reconstruction when the server randomly perturbs the<br>viewing direction by up to 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:643"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:653"><nobr><span class="ft5">(note that the reconstruction error</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:477"><nobr><span class="ft5">exceeds that of the starting base mesh).</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:477"><nobr><span class="ft8">The quality of the base mesh is an important determinant in the suc-<br>cess of this particular attack. For example, repeating the experiment<br>of Figure 8 with a more accurate base mesh of 30,000 polygons<br>yields results of E = 0.8, E = 0.6, and E = 0.7 for the conditions<br>of Figures 8(b), 8(c), and 8(e), respectively. This reliance on an<br>accurate low-resolution base mesh for the 3D model reconstruction<br>is a potential weak point of the attack; attackers may be deterred<br>by the effort required to reverse-engineer the protections guarding<br>the low-resolution model or to reconstruct an acceptable base mesh<br>from harvested images using another technique.</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:477"><nobr><span class="ft2">5.5</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:514"><nobr><span class="ft2">Discussion</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:477"><nobr><span class="ft8">Because we know of no single mechanism for guaranteeing the se-<br>curity of 3D content delivered through a rendering server, we have<br>instead taken a systems-based approach, implementing multiple de-<br>fenses and using them in combination. Moreover, we know of no<br>formalism for rigorously analyzing the security provided by our de-<br>fenses; the reconstruction attacks that we have empirically consid-<br>ered here are merely representative of the possible threats.</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:477"><nobr><span class="ft8">Of the reconstruction attacks we have experimented with so far, the<br>shape-from-shading approach has yielded the best results against<br>our defended rendering server.</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:663"><nobr><span class="ft5">Enumeration attacks are easily</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:477"><nobr><span class="ft8">foiled when the user's control over the viewpoint and view frus-<br>tum is constrained, pure shape-from-silhouette methods are limited<br>to reconstructing a visual hull, and two-frame stereo algorithms rely<br>on determining accurate correspondences which is difficult with the<br>synthetic, untextured models we are attempting to protect. Attack-<br>ers could improve the results of the shape-from-shading algorithm<br>against our perturbation defenses by explicitly modeling the distor-<br>tions and trying to take them into account in the optimization step,<br>or alternatively by attempting to align the images by interactively<br>establishing point to point correspondences or using an automatic<br>technique such as [Lensch et al. 2001].</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:477"><nobr><span class="ft8">Such procedures for explicitly modeling the server defenses, or cor-<br>recting for them via manual specification of correspondences, are<br>applicable to any style of reconstruction attempt. To combat these<br>attacks, we must rely on the combined discouraging effect of multi-<br>ple defenses running simultaneously, which increases the number of<br>degrees of freedom of perturbation to a level that would be difficult<br>and time-consuming to overcome. Some of our rendering server<br>defenses, such as the lighting model and non-linear image distor-<br>tions, can be increased arbitrarily in their complexity. Likewise, the<br>magnitude of server defense perturbations can be increased with a<br>corresponding decrease in the fidelity of the rendered images.</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:477"><nobr><span class="ft8">Ultimately, no fixed set of defenses is bulletproof against a so-<br>phisticated, malicious attacker with enough resources at their dis-<br>posal, and one is inevitably led to an "arms race" between attacks<br>and countermeasures such as we have implemented. As the ex-<br>pense required to overcome our remote rendering server defenses<br>becomes greater, determined attackers may instead turn to reaching<br>their piracy goals via non-reconstruction-based methods beyond the<br>scope of this paper, such as computer network intrusion or exploita-<br>tion of non-technical human factors.</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">702</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="153009.png" alt="background image">
<DIV style="position:absolute;top:83;left:81"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:111"><nobr><span class="ft4">Results and Future Work</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft8">A prototype of our remote rendering system (ScanView, avail-<br>able at http://graphics.stanford.edu/software/scanview/ ) has been<br>deployed to share 3D models from a major cultural heritage archive,<br>the Digital Michelangelo Project [Levoy et al. 2000], as well as<br>other collections of archaeological artifacts that require protected<br>usage. In the several months since becoming publically available,<br>more than 4,000 users have installed the client program on their per-<br>sonal computers and accessed the remote servers to view the pro-<br>tected 3D models. The users have included art students, art schol-<br>ars, art enthusiasts, and sculptors examining high-resolution art-<br>works, as well as archaeologists examining particular artifacts. Few<br>of these individuals would have qualified under the strict guidelines<br>required to obtain completely unrestricted access to the models, so<br>the protected remote rendering system has enabled large, entirely<br>new groups of users access to 3D graphical models for professional<br>study and enjoyment.</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:81"><nobr><span class="ft8">Reports from users of the system have been uniformly positive<br>and enthusiastic. Fetching high-resolution renderings over inter-<br>continental broadband Internet connections takes less than 2 sec-<br>onds of latency, while fast continental connections generally experi-<br>ence latencies dominated by the rendering server's processing time<br>(around 150 ms). The rendering server architecture can scale up to<br>support an arbitrary number of requests per second by adding addi-<br>tional CPU and GPU nodes, and rendering servers can be installed<br>at distributed locations around the world to reduce intercontinental<br>latencies if desired.</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft8">Our log analysis defenses have detected multiple episodes of sys-<br>tem users attempting to harvest large sets of images from the server<br>for purposes of later 3D reconstruction attempts, though these inci-<br>dents were determined to be non-malicious. In general, the moni-<br>toring capabilities of a remote rendering server are useful for rea-<br>sons beyond just security, as the server logs provide complete ac-<br>counts of all usage of the 3D models in the archive, which can be<br>valuable information for archive managers to gauge popularity of<br>individual models and understand user interaction patterns.</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:81"><nobr><span class="ft8">Our plans for future work include further investigation of computer<br>vision techniques that address 3D reconstruction of synthetic data<br>under antagonistic conditions, and analysis of their efficacy against<br>the various rendering server defenses. More sophisticated exten-<br>sions to the basic vision approaches described above, such as multi-<br>view stereo algorithms, and robust hybrid vision algorithms which<br>combine the strengths of different reconstruction techniques, can<br>present difficult challenges to protecting the models. Another direc-<br>tion of research is to consider how to allow users a greater degree<br>of geometric analysis of the protected 3D models without further<br>exposing the data to theft; scholarly and professional users have<br>expressed interest in measuring distances and plotting profiles of<br>3D objects for analytical purposes beyond the simple 3D viewing<br>supported in the current system. Finally, we are continuing to in-<br>vestigate alternative approaches to protecting 3D graphics, design-<br>ing specialized systems which make data security a priority while<br>potentially sacrificing some general purpose computing platform<br>capabilities. The GPU decryption scheme described herein, for ex-<br>ample, is one such idea that may be appropriate for console devices<br>and other custom graphics systems.</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:81"><nobr><span class="ft2">Acknowledgements</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:225"><nobr><span class="ft5">We thank Kurt Akeley, Sean Anderson,</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:81"><nobr><span class="ft8">Jonathan Berger, Dan Boneh, Ian Buck, James Davis, Pat Han-<br>rahan, Hughes Hoppe, David Kirk, Matthew Papakipos, Nick<br>Triantos, and the anonymous reviewers for their useful feedback,<br>and Szymon Rusinkiewicz for sharing code. This work has been<br>supported in part by NSF contract IIS0113427, the Max Planck<br>Center for Visual Computing and Communication, and the EU IST-<br>2001-32641 ViHAP3D Project.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:477"><nobr><span class="ft4">References</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:477"><nobr><span class="ft3">C</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:486"><nobr><span class="ft0">OLLBERG</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:534"><nobr><span class="ft3">, C.,</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:562"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:588"><nobr><span class="ft3">T</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:596"><nobr><span class="ft0">HOMBORSON</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:663"><nobr><span class="ft3">, C. 2000. Watermarking, tamper-</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:492"><nobr><span class="ft11">proofing, and obfuscation: Tools for software protection. Tech. Rep.<br>170, Dept. of Computer Science, The University of Auckland.</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:477"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:486"><nobr><span class="ft0">EBEVEC</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:527"><nobr><span class="ft3">, P., T</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:557"><nobr><span class="ft0">AYLOR</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:592"><nobr><span class="ft3">, C.,</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:618"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:644"><nobr><span class="ft3">M</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:655"><nobr><span class="ft0">ALIK</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:680"><nobr><span class="ft3">, J. 1996. Modeling and render-</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:492"><nobr><span class="ft11">ing architecture from photographs: A hybrid geometry- and image-based<br>approach. In Proc. of ACM SIGGRAPH 96, 1120.</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:477"><nobr><span class="ft3">E</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:485"><nobr><span class="ft0">NGEL</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:513"><nobr><span class="ft3">, K., H</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:553"><nobr><span class="ft0">ASTREITER</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:610"><nobr><span class="ft3">, P., T</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:645"><nobr><span class="ft0">OMANDL</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:691"><nobr><span class="ft3">, B., E</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:729"><nobr><span class="ft0">BERHARDT</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:784"><nobr><span class="ft3">, K.,</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:815"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:492"><nobr><span class="ft3">E</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:500"><nobr><span class="ft0">RTL</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:519"><nobr><span class="ft3">, T. 2000. Combining local and remote visualization techniques for</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:492"><nobr><span class="ft11">interactive volume rendering in medical applications. In Proc. of IEEE<br>Visualization 2000</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:581"><nobr><span class="ft3">, 449452.</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:477"><nobr><span class="ft3">G</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:486"><nobr><span class="ft0">ORTLER</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:527"><nobr><span class="ft3">, S., G</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:565"><nobr><span class="ft0">RZESZCZUK</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:625"><nobr><span class="ft3">, R., S</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:663"><nobr><span class="ft0">ZELISKI</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:703"><nobr><span class="ft3">, R.,</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:734"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:761"><nobr><span class="ft3">C</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:770"><nobr><span class="ft0">OHEN</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:799"><nobr><span class="ft3">, M. F.</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:492"><nobr><span class="ft3">1996. The lumigraph. In Proc. of ACM SIGGRAPH 96, 4354.</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:477"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:485"><nobr><span class="ft0">AURENTINI</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:542"><nobr><span class="ft3">, A. 1994. The visual hull concept for silhouette-based image</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:492"><nobr><span class="ft11">understanding. IEEE Trans. on Pattern Analysis and Machine Intelli-<br>gence 16</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:535"><nobr><span class="ft3">, 2, 150162.</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:477"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:485"><nobr><span class="ft0">ENSCH</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:519"><nobr><span class="ft3">, H. P., H</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:568"><nobr><span class="ft0">EIDRICH</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:611"><nobr><span class="ft3">, W.,</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:640"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:665"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:673"><nobr><span class="ft0">EIDEL</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:703"><nobr><span class="ft3">, H.-P. 2001. A silhouette-</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:492"><nobr><span class="ft11">based algorithm for texture registration and stitching. Graphical Models<br>63</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:504"><nobr><span class="ft3">, 245262.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:477"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:485"><nobr><span class="ft0">EVOY</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:512"><nobr><span class="ft3">, M.,</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:540"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:566"><nobr><span class="ft3">H</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:575"><nobr><span class="ft0">ANRAHAN</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:627"><nobr><span class="ft3">, P. 1996. Light field rendering. In Proc. of</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:492"><nobr><span class="ft3">ACM SIGGRAPH 96</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:592"><nobr><span class="ft3">, 3142.</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:477"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:485"><nobr><span class="ft0">EVOY</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:512"><nobr><span class="ft3">, M., P</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:547"><nobr><span class="ft0">ULLI</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:571"><nobr><span class="ft3">, K., C</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:606"><nobr><span class="ft0">URLESS</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:645"><nobr><span class="ft3">, B., R</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:678"><nobr><span class="ft0">USINKIEWICZ</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:748"><nobr><span class="ft3">, S., K</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:781"><nobr><span class="ft0">OLLER</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:814"><nobr><span class="ft3">, D.,</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:492"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:499"><nobr><span class="ft0">EREIRA</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:537"><nobr><span class="ft3">, L., G</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:572"><nobr><span class="ft0">INZTON</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:611"><nobr><span class="ft3">, M., A</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:650"><nobr><span class="ft0">NDERSON</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:699"><nobr><span class="ft3">, S., D</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:733"><nobr><span class="ft0">AVIS</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:756"><nobr><span class="ft3">, J., G</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:789"><nobr><span class="ft0">INSBERG</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:834"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:492"><nobr><span class="ft3">J., S</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:515"><nobr><span class="ft0">HADE</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:544"><nobr><span class="ft3">, J.,</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:567"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:592"><nobr><span class="ft3">F</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:600"><nobr><span class="ft0">ULK</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:621"><nobr><span class="ft3">, D. 2000. The digital michelangelo project.</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:492"><nobr><span class="ft3">In Proc. of ACM SIGGRAPH 2000, 131144.</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:477"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:485"><nobr><span class="ft0">EVOY</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:512"><nobr><span class="ft3">, M. 1995. Polygon-assisted jpeg and mpeg compression of synthetic</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:492"><nobr><span class="ft3">images. In Proc. of ACM SIGGRAPH 95, 2128.</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:477"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:486"><nobr><span class="ft0">IEM</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:506"><nobr><span class="ft3">, W. 1997. Error analysis for silhouette-based 3d shape estimation</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:492"><nobr><span class="ft11">from multiple views. In International Workshop on Synthetic-Natural<br>Hybrid Coding and 3D Imaging</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:645"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:477"><nobr><span class="ft3">O</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:486"><nobr><span class="ft0">HBUCHI</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:527"><nobr><span class="ft3">, R., M</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:571"><nobr><span class="ft0">UKAIYAMA</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:628"><nobr><span class="ft3">, A.,</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:661"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:690"><nobr><span class="ft3">T</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:697"><nobr><span class="ft0">AKAHASHI</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:752"><nobr><span class="ft3">, S.</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:787"><nobr><span class="ft3">2002.</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:828"><nobr><span class="ft3">A</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:492"><nobr><span class="ft3">frequency-domain approach to watermarking 3d shapes.</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:789"><nobr><span class="ft3">Computer</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:492"><nobr><span class="ft3">Graphics Forum 21</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:585"><nobr><span class="ft3">, 3.</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:477"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:484"><nobr><span class="ft0">RAUN</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:513"><nobr><span class="ft3">, E., H</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:548"><nobr><span class="ft0">OPPE</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:574"><nobr><span class="ft3">, H.,</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:601"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:627"><nobr><span class="ft3">F</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:634"><nobr><span class="ft0">INKELSTEIN</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:696"><nobr><span class="ft3">, A. 1999. Robust mesh wa-</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:492"><nobr><span class="ft3">termarking. In Proc. of ACM SIGGRAPH 99, 4956.</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:477"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:486"><nobr><span class="ft0">ESSLER</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:524"><nobr><span class="ft3">, S., 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:605"><nobr><span class="ft3">Web3d security discussion.</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:764"><nobr><span class="ft3">Online article:</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:492"><nobr><span class="ft3">http://web3d.about.com/library/weekly/aa013101a.htm</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:756"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:477"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:485"><nobr><span class="ft0">USINKIEWICZ</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:555"><nobr><span class="ft3">, S.,</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:582"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:608"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:616"><nobr><span class="ft0">EVOY</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:643"><nobr><span class="ft3">, M. 2000. QSplat: A multiresolution</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:492"><nobr><span class="ft11">point rendering system for large meshes. In Proc. of ACM SIGGRAPH<br>2000</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:516"><nobr><span class="ft3">, 343352.</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:477"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:484"><nobr><span class="ft0">CHARSTEIN</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:543"><nobr><span class="ft3">, D.,</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:569"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:594"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:601"><nobr><span class="ft0">ZELISKI</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:641"><nobr><span class="ft3">, R. 2002. A taxonomy and evaluation of</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:492"><nobr><span class="ft11">dense two-frame stereo correspondence algorithms. International Jour-<br>nal of Computer Vision 47</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:618"><nobr><span class="ft3">, 13, 742.</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:477"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:484"><nobr><span class="ft0">CHNEIER</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:530"><nobr><span class="ft3">, B. 2000. The fallacy of trusted client software. Information</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:492"><nobr><span class="ft3">Security</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:534"><nobr><span class="ft3">(August).</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:477"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:484"><nobr><span class="ft0">IMONS</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:518"><nobr><span class="ft3">, D.,</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:543"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:568"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:576"><nobr><span class="ft0">EVIN</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:602"><nobr><span class="ft3">, D. 1997. Change blindness. Trends in Cognitive</span></nobr></DIV>
<DIV style="position:absolute;top:830;left:492"><nobr><span class="ft3">Sciences 1</span></nobr></DIV>
<DIV style="position:absolute;top:830;left:542"><nobr><span class="ft3">, 7, 261267.</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:477"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:484"><nobr><span class="ft0">LABAUGH</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:535"><nobr><span class="ft3">, G., C</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:571"><nobr><span class="ft0">ULBERTSON</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:632"><nobr><span class="ft3">, B., M</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:671"><nobr><span class="ft0">ALZBENDER</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:733"><nobr><span class="ft3">, T.,</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:759"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:785"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:792"><nobr><span class="ft0">CHAFER</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:834"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:492"><nobr><span class="ft11">R. 2001. A survey of methods for volumetric scene reconstruction from<br>photographs. In Proc. of the Joint IEEE TCVG and Eurographics Work-<br>shop (VolumeGraphics-01)</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:621"><nobr><span class="ft3">, Springer-Verlag, 81100.</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:477"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:484"><nobr><span class="ft0">TANFORD</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:555"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:564"><nobr><span class="ft0">IGITAL</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:620"><nobr><span class="ft3">F</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:628"><nobr><span class="ft0">ORMA</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:681"><nobr><span class="ft3">U</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:690"><nobr><span class="ft0">RBIS</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:735"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:743"><nobr><span class="ft0">ROJECT</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:781"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:810"><nobr><span class="ft3">2004.</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:492"><nobr><span class="ft3">http://formaurbis.stanford.edu.</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:477"><nobr><span class="ft3">T</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:484"><nobr><span class="ft0">ARINI</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:514"><nobr><span class="ft3">, M., C</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:550"><nobr><span class="ft0">ALLIERI</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:591"><nobr><span class="ft3">, M., M</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:630"><nobr><span class="ft0">ONTANI</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:670"><nobr><span class="ft3">, C., R</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:703"><nobr><span class="ft0">OCCHINI</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:747"><nobr><span class="ft3">, C., O</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:781"><nobr><span class="ft0">LSSON</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:814"><nobr><span class="ft3">, K.,</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:492"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:518"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:525"><nobr><span class="ft0">ERSSON</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:565"><nobr><span class="ft3">, T. 2002. Marching intersections: An efficient approach</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:492"><nobr><span class="ft11">to shape-from-silhouette. In Proceedings of the Conference on Vision,<br>Modeling, and Visualization (VMV 2002)</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:689"><nobr><span class="ft3">, 255262.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:477"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:486"><nobr><span class="ft0">OON</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:509"><nobr><span class="ft3">, I.,</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:532"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:558"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:567"><nobr><span class="ft0">EUMANN</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:613"><nobr><span class="ft3">, U. 2000. Web-based remote rendering with</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:492"><nobr><span class="ft3">IBRAC. Computer Graphics Forum 19, 3.</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:477"><nobr><span class="ft3">Z</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:485"><nobr><span class="ft0">HANG</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:515"><nobr><span class="ft3">, R., T</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:548"><nobr><span class="ft0">SAI</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:565"><nobr><span class="ft3">, P.-S., C</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:611"><nobr><span class="ft0">RYER</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:638"><nobr><span class="ft3">, J. E.,</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:673"><nobr><span class="ft0">AND</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:698"><nobr><span class="ft3">S</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:705"><nobr><span class="ft0">HAH</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:728"><nobr><span class="ft3">, M. 1999. Shape from</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:492"><nobr><span class="ft11">shading: A survey. IEEE Transactions on Pattern Analysis and Machine<br>Intelligence 21</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:563"><nobr><span class="ft3">, 8, 690706.</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:445"><nobr><span class="ft6">703</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
