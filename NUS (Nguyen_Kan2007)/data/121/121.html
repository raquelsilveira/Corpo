<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - p236-larkey.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-07-08T13:47:32+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:16px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft8{font-size:12px;font-family:Times;color:#000000;}
	.ft9{font-size:8px;font-family:Times;color:#000000;}
	.ft10{font-size:10px;font-family:Times;color:#000000;}
	.ft11{font-size:10px;font-family:Times;color:#000000;}
	.ft12{font-size:16px;font-family:Courier;color:#000000;}
	.ft13{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft15{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft16{font-size:10px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121001.png" alt="background image">
<DIV style="position:absolute;top:114;left:95"><nobr><span class="ft0"><b>Language-specific Models in Multilingual Topic Tracking</b></span></nobr></DIV>
<DIV style="position:absolute;top:153;left:196"><nobr><span class="ft1">Leah S. Larkey, Fangfang Feng, Margaret Connell, Victor Lavrenko </span></nobr></DIV>
<DIV style="position:absolute;top:173;left:321"><nobr><span class="ft2">Center for Intelligent Information Retrieval </span></nobr></DIV>
<DIV style="position:absolute;top:190;left:348"><nobr><span class="ft2">Department of Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:366"><nobr><span class="ft2">University of Massachusetts                        </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:393"><nobr><span class="ft2">Amherst, MA 01003  </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:269"><nobr><span class="ft1">{larkey, feng, connell, lavrenko}@cs.umass.edu </span></nobr></DIV>
<DIV style="position:absolute;top:273;left:459"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:296;left:81"><nobr><span class="ft4"><b>ABSTRACT </b></span></nobr></DIV>
<DIV style="position:absolute;top:316;left:81"><nobr><span class="ft13">Topic tracking is complicated when the stories in the stream occur <br>in multiple languages. Typically, researchers have trained only <br>English topic models because the training stories have been pro-<br>vided in English. In tracking, non-English test stories are then <br>machine translated into English to compare them with the topic <br>models. We propose a <i>native language hypothesis</i> stating that <br>comparisons would be more effective in the original language of <br>the story. We first test and support the hypothesis for story link <br>detection. For topic tracking the hypothesis implies that it should <br>be preferable to build separate language-specific topic models for <br>each language in the stream. We compare different methods of <br>incrementally building such native language topic models. <b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:523;left:81"><nobr><span class="ft4"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:352"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:542;left:81"><nobr><span class="ft13">H.3.1 [<b>Information Storage and Retrieval</b>]: Content Analysis <br>and Indexing ­ <i>Indexing methods, Linguistic processing</i>.<i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:594;left:81"><nobr><span class="ft4"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:597;left:197"><nobr><span class="ft3">: Algorithms, Experimentation. </span></nobr></DIV>
<DIV style="position:absolute;top:624;left:81"><nobr><span class="ft4"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:627;left:159"><nobr><span class="ft3">: classification, crosslingual, Arabic, TDT, topic </span></nobr></DIV>
<DIV style="position:absolute;top:643;left:81"><nobr><span class="ft3">tracking, multilingual </span></nobr></DIV>
<DIV style="position:absolute;top:670;left:81"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:669;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:670;left:105"><nobr><span class="ft4"><b>INTRODUCTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:690;left:81"><nobr><span class="ft13">Topic detection and tracking (TDT) is a research area concerned <br>with organizing a multilingual stream of news broadcasts as it ar-<br>rives over time. TDT investigations sponsored by the U.S. gov-<br>ernment include five different tasks: story link detection, cluster-<br>ing (topic detection), topic tracking, new event (first story) detec-<br>tion, and story segmentation. The present research focuses on <br>topic tracking, which is similar to filtering in information re-<br>trieval. Topics are defined by a small number of (training) stories, <br>typically one to four, and the task is to find all the stories on those <br>topics in the incoming stream.  </span></nobr></DIV>
<DIV style="position:absolute;top:854;left:81"><nobr><span class="ft13">TDT evaluations have included stories in multiple languages since <br>1999. TDT2 contained stories in English and Mandarin. TDT3 <br>and TDT4 included English, Mandarin, and Arabic. Machine-<br>translations into English for all non-English stories were pro-<br>vided, allowing participants to ignore issues of story translation.  </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:477"><nobr><span class="ft13">All TDT tasks have at their core a comparison of two text models. <br>In story link detection, the simplest case, the comparison is be-<br>tween pairs of stories, to decide whether given pairs of stories are <br>on the same topic or not. In topic tracking, the comparison is be-<br>tween a story and a topic, which is often represented as a centroid <br>of story vectors, or as a language model covering several stories.  </span></nobr></DIV>
<DIV style="position:absolute;top:395;left:477"><nobr><span class="ft14">Our focus in this research was to explore the best ways to com-<br>pare stories and topics when stories are in multiple languages. We <br>began with the hypothesis that if two stories originated in the <br>same language, it would be best to compare them in that language, <br>rather than translating them both into another language for com-<br>parison. This simple assertion, which we call the <i>native language <br>hypothesis</i>, is easily tested in the TDT story link detection task.  </span></nobr></DIV>
<DIV style="position:absolute;top:512;left:477"><nobr><span class="ft13">The picture gets more complex in a task like topic tracking, which <br>begins with a small number of training stories (in English) to de-<br>fine each topic. New stories from a stream must be placed into <br>these topics. The streamed stories originate in different languages, <br>but are also available in English translation. The translations have <br>been performed automatically by machine translation algorithms, <br>and are inferior to manual translations. At the beginning of the <br>stream, native language comparisons cannot be performed be-<br>cause there are no native language topic models (other than Eng-<br>lish). However, later in the stream, once non-English documents <br>have been seen, one can base subsequent tracking on native-lan-<br>guage comparisons, by adaptively training models for additional <br>languages. There are many ways this adaptation could be per-<br>formed, and we suspect that it is crucial for the first few non-Eng-<br>lish stories to be placed into topics correctly, to avoid building <br>non-English models from off-topic stories. </span></nobr></DIV>
<DIV style="position:absolute;top:772;left:477"><nobr><span class="ft15">Previous research in multilingual TDT has not attempted to com-<br>pare the building of multiple language-specific models with sin-<br>gle-language topic models, or to obtain native-language models <br>through adaptation. The focus of most multilingual work in TDT <br>for example </span></nobr></DIV>
<DIV style="position:absolute;top:832;left:547"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:836;left:547"><nobr><span class="ft3">[2]</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:563"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:836;left:563"><nobr><span class="ft3">[12]</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:586"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:836;left:586"><nobr><span class="ft3">[13], has been to compare the efficacy of ma-</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:477"><nobr><span class="ft15">chine translation of test stories into a base language, with other <br>means of translation. Although these researchers normalize scores <br>for the source language, all story comparisons are done within the <br>base language. This is also true in multilingual filtering, which is <br>a similar task </span></nobr></DIV>
<DIV style="position:absolute;top:913;left:552"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:917;left:552"><nobr><span class="ft3">[14]. </span></nobr></DIV>
<DIV style="position:absolute;top:940;left:477"><nobr><span class="ft13">The present research is an exploration of the native language hy-<br>pothesis for multilingual topic tracking. We first present results on <br>story link detection, to support the native language hypothesis in a <br>simple, understandable task. Then we present experiments that <br>test the hypothesis in the topic tracking task. Finally we consider <br>several different ways to adapt topic models to allow native lan-<br>guage comparisons downstream.  </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:86"><nobr><span class="ft9"> </span></nobr></DIV>
<DIV style="position:absolute;top:970;left:86"><nobr><span class="ft16">Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are <br>not made or distributed for profit or commercial advantage and that <br>copies bear this notice and the full citation on the first page. To copy <br>otherwise, or republish, to post on servers or to redistribute to lists, <br>requires prior specific permission and/or a fee. <br><i>SIGIR '04</i>, July 25-29, 2003, Sheffield, South Yorkshire, UK. <br>Copyright 2004 ACM 1-58113-881-4/04/0007...$5.00. </span></nobr></DIV>
<DIV style="position:absolute;top:1084;left:86"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">402</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft17{font-size:7px;font-family:Times;color:#000000;}
	.ft18{font-size:10px;font-family:Times;color:#000000;}
	.ft19{font-size:11px;font-family:Times;color:#000000;}
	.ft20{font-size:7px;font-family:Times;color:#000000;}
	.ft21{font-size:14px;font-family:Times;color:#000000;}
	.ft22{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft23{font-size:11px;font-family:Times;color:#000000;}
	.ft24{font-size:15px;font-family:Symbol;color:#000000;}
	.ft25{font-size:11px;font-family:Symbol;color:#000000;}
	.ft26{font-size:5px;font-family:Times;color:#000000;}
	.ft27{font-size:5px;font-family:Times;color:#000000;}
	.ft28{font-size:20px;font-family:Symbol;color:#000000;}
	.ft29{font-size:22px;font-family:Symbol;color:#000000;}
	.ft30{font-size:18px;font-family:Times;color:#000000;}
	.ft31{font-size:7px;font-family:Times;color:#000000;}
	.ft32{font-size:5px;font-family:Symbol;color:#000000;}
	.ft33{font-size:18px;font-family:Symbol;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121002.png" alt="background image">
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft13">Although these experiments were carried out in service of TDT, <br>the results should equally apply to other domains which require <br>the comparison of documents in different languages, particularly <br>filtering, text classification and clustering. </span></nobr></DIV>
<DIV style="position:absolute;top:192;left:81"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:192;left:105"><nobr><span class="ft4"><b>EXPERIMENTAL SETUP </b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:81"><nobr><span class="ft13">Experiments are replicated with two different data sets, TDT3 and <br>TDT4, and two very different similarity functions - cosine simi-<br>larity, and another based on relevance modeling, described in the <br>following two sections. Cosine similarity can be seen as a basic <br>default approach, which performs adequately, and relevance mod-<br>eling is a state of the art approach which yields top-rated perform-<br>ance. Confirming the native-language hypothesis in both systems <br>would show its generality. </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:81"><nobr><span class="ft13">In the rest of this section, we describe the TDT data sets, then we <br>describe how story link detection and topic tracking are carried <br>out in cosine similarity and relevance modeling systems. Next, we <br>describe the multilingual aspects of the systems.  </span></nobr></DIV>
<DIV style="position:absolute;top:424;left:81"><nobr><span class="ft4"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:104"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:424;left:114"><nobr><span class="ft4"><b>TDT3 Data </b></span></nobr></DIV>
<DIV style="position:absolute;top:443;left:81"><nobr><span class="ft13">TDT data consist of a stream of news in multiple languages and <br>from different media - audio from television, radio, and web news <br>broadcasts, and text from newswires. Two forms of transcription <br>are available for the audio stream. The first form comes from <br>automatic speech recognition and includes transcription errors <br>made by such systems. The second form is a manual transcription, <br>which has few if any errors. The audio stream can also be divided <br>into stories automatically or manually (so-called <i>reference <br>boundaries</i>). For all the research reported here, we used manual <br>transcriptions and reference boundaries. </span></nobr></DIV>
<DIV style="position:absolute;top:607;left:81"><nobr><span class="ft13">The characteristics of the TDT3 data sets for story link detection <br>and topic tracking are summarized in Tables 1-3. </span></nobr></DIV>
<DIV style="position:absolute;top:645;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:667;left:135"><nobr><span class="ft6"><b>Table 1: Number of stories in TDT3 Corpus </b></span></nobr></DIV>
<DIV style="position:absolute;top:689;left:132"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:690;left:175"><nobr><span class="ft6"><b>English  Arabic  Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:690;left:351"><nobr><span class="ft6"><b>Total </b></span></nobr></DIV>
<DIV style="position:absolute;top:706;left:132"><nobr><span class="ft6"><b>TDT3  </b>37,526 15,928  13,657 67,111 </span></nobr></DIV>
<DIV style="position:absolute;top:721;left:81"><nobr><span class="ft17"> </span></nobr></DIV>
<DIV style="position:absolute;top:740;left:83"><nobr><span class="ft6"><b>Table 2: Characteristics of TDT3 story link detection data sets </b></span></nobr></DIV>
<DIV style="position:absolute;top:762;left:109"><nobr><span class="ft18"><b>Number of topics </b></span></nobr></DIV>
<DIV style="position:absolute;top:762;left:251"><nobr><span class="ft10">8 </span></nobr></DIV>
<DIV style="position:absolute;top:778;left:109"><nobr><span class="ft18"><b>Number of link pairs </b></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:251"><nobr><span class="ft18"><b>Same topic </b></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:333"><nobr><span class="ft18"><b>Different topic  </b></span></nobr></DIV>
<DIV style="position:absolute;top:794;left:153"><nobr><span class="ft18"><b>English-English </b></span></nobr></DIV>
<DIV style="position:absolute;top:793;left:300"><nobr><span class="ft10">605 </span></nobr></DIV>
<DIV style="position:absolute;top:793;left:388"><nobr><span class="ft10">3999 </span></nobr></DIV>
<DIV style="position:absolute;top:809;left:160"><nobr><span class="ft18"><b>Arabic-Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:300"><nobr><span class="ft10">669 </span></nobr></DIV>
<DIV style="position:absolute;top:809;left:388"><nobr><span class="ft10">3998 </span></nobr></DIV>
<DIV style="position:absolute;top:825;left:125"><nobr><span class="ft18"><b>Mandarin-Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:825;left:300"><nobr><span class="ft10">440 </span></nobr></DIV>
<DIV style="position:absolute;top:825;left:388"><nobr><span class="ft10">4000 </span></nobr></DIV>
<DIV style="position:absolute;top:841;left:157"><nobr><span class="ft18"><b>English-Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:841;left:300"><nobr><span class="ft10">676 </span></nobr></DIV>
<DIV style="position:absolute;top:841;left:388"><nobr><span class="ft10">4000 </span></nobr></DIV>
<DIV style="position:absolute;top:857;left:139"><nobr><span class="ft18"><b>English-Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:300"><nobr><span class="ft10">569 </span></nobr></DIV>
<DIV style="position:absolute;top:857;left:388"><nobr><span class="ft10">4000 </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:142"><nobr><span class="ft18"><b>Arabic-Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:872;left:300"><nobr><span class="ft10">583 </span></nobr></DIV>
<DIV style="position:absolute;top:872;left:388"><nobr><span class="ft10">3998 </span></nobr></DIV>
<DIV style="position:absolute;top:889;left:211"><nobr><span class="ft18"><b>Total </b></span></nobr></DIV>
<DIV style="position:absolute;top:888;left:293"><nobr><span class="ft10">3542 </span></nobr></DIV>
<DIV style="position:absolute;top:888;left:378"><nobr><span class="ft10">23,995 </span></nobr></DIV>
<DIV style="position:absolute;top:905;left:261"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:926;left:98"><nobr><span class="ft6"><b>Table 3: Characteristics of TDT3 topic tracking data sets </b></span></nobr></DIV>
<DIV style="position:absolute;top:948;left:87"><nobr><span class="ft18"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:237"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:954;left:247"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:249"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:362"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:954;left:372"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:375"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:965;left:87"><nobr><span class="ft18"><b>Number of topics </b></span></nobr></DIV>
<DIV style="position:absolute;top:965;left:244"><nobr><span class="ft10">36 30 </span></nobr></DIV>
<DIV style="position:absolute;top:981;left:87"><nobr><span class="ft18"><b>Num. test stories </b></span></nobr></DIV>
<DIV style="position:absolute;top:981;left:195"><nobr><span class="ft18"><b>On-topic </b></span></nobr></DIV>
<DIV style="position:absolute;top:981;left:273"><nobr><span class="ft18"><b>All On-topic </b></span></nobr></DIV>
<DIV style="position:absolute;top:981;left:398"><nobr><span class="ft18"><b>All </b></span></nobr></DIV>
<DIV style="position:absolute;top:997;left:144"><nobr><span class="ft18"><b>English </b></span></nobr></DIV>
<DIV style="position:absolute;top:997;left:219"><nobr><span class="ft10">2042 </span></nobr></DIV>
<DIV style="position:absolute;top:997;left:266"><nobr><span class="ft10">883,887 </span></nobr></DIV>
<DIV style="position:absolute;top:997;left:343"><nobr><span class="ft10">2042 </span></nobr></DIV>
<DIV style="position:absolute;top:997;left:393"><nobr><span class="ft10">796,373 </span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:147"><nobr><span class="ft18"><b>Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:226"><nobr><span class="ft10">572 </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:266"><nobr><span class="ft10">372,889 </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:349"><nobr><span class="ft10">572 </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:393"><nobr><span class="ft10">336,563 </span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:129"><nobr><span class="ft18"><b>Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:226"><nobr><span class="ft10">405 </span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:266"><nobr><span class="ft10">329,481 </span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:349"><nobr><span class="ft10">369 </span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:393"><nobr><span class="ft10">301,568 </span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:156"><nobr><span class="ft18"><b>Total </b></span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:219"><nobr><span class="ft10">3019  1,593,782 </span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:343"><nobr><span class="ft10">2983  1,434,504 </span></nobr></DIV>
<DIV style="position:absolute;top:113;left:477"><nobr><span class="ft4"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:113;left:510"><nobr><span class="ft4"><b>Story Representation and Similarity </b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:477"><nobr><span class="ft21"><i>2.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:510"><nobr><span class="ft22"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:521"><nobr><span class="ft21"><i>Cosine similarity </i></span></nobr></DIV>
<DIV style="position:absolute;top:161;left:477"><nobr><span class="ft13">To compare two stories for link detection, or a story with a topic <br>model for tracking, each story is represented as a vector of terms <br>with <i>tf·idf</i> term weights: </span></nobr></DIV>
<DIV style="position:absolute;top:218;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:637"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:681"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:633"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:704"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:653"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:684"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:678"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:635"><nobr><span class="ft3">log</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:674"><nobr><span class="ft3">5</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:671"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:664"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:616"><nobr><span class="ft3">log</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:670"><nobr><span class="ft25">+</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:654"><nobr><span class="ft25">+</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:605"><nobr><span class="ft25">×</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:583"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:657"><nobr><span class="ft5"><i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:215;left:690"><nobr><span class="ft5"><i>df</i></span></nobr></DIV>
<DIV style="position:absolute;top:215;left:642"><nobr><span class="ft5"><i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:593"><nobr><span class="ft5"><i>tf</i></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:569"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:230;left:576"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:218;left:711"><nobr><span class="ft3"> (1) </span></nobr></DIV>
<DIV style="position:absolute;top:257;left:477"><nobr><span class="ft13">where <i>tf</i> is the number of occurrences of the term in the story, <i>N</i> is <br>the total number of documents in the collection, and <i>df</i> is the <br>number of documents containing the term. Collection statistics <i>N</i> <br>and  <i>df</i> are computed incrementally, based on the documents al-<br>ready in the stream within a deferral period after the test story <br>arrives. The deferral period was 10 for link detection and 1 for <br>topic tracking. For link detection, story vectors were pruned to the <br>1000 terms with the highest term weights. </span></nobr></DIV>
<DIV style="position:absolute;top:390;left:477"><nobr><span class="ft3">The similarity of two (weighted, pruned) vectors </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:810"><nobr><span class="ft26"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:803"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:774"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:752"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:785"><nobr><span class="ft3">,...,</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:780"><nobr><span class="ft27">1</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:763"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:754"><nobr><span class="ft23"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:390;left:817"><nobr><span class="ft3">and </span></nobr></DIV>
<DIV style="position:absolute;top:422;left:535"><nobr><span class="ft26"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:528"><nobr><span class="ft5"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:500"><nobr><span class="ft5"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:479"><nobr><span class="ft5"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:510"><nobr><span class="ft3">,...,</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:505"><nobr><span class="ft27">1</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:489"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:480"><nobr><span class="ft23"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:412;left:543"><nobr><span class="ft3">is the inner product between the two vectors: </span></nobr></DIV>
<DIV style="position:absolute;top:443;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:433;left:589"><nobr><span class="ft28">(</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:632"><nobr><span class="ft28">)</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:653"><nobr><span class="ft29">(</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:691"><nobr><span class="ft29">)(</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:732"><nobr><span class="ft29">)</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:697"><nobr><span class="ft30"></span></nobr></DIV>
<DIV style="position:absolute;top:439;left:656"><nobr><span class="ft30"></span></nobr></DIV>
<DIV style="position:absolute;top:439;left:592"><nobr><span class="ft30"></span></nobr></DIV>
<DIV style="position:absolute;top:442;left:578"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:712"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:722"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:456;left:670"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:682"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:456;left:606"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:627"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:618"><nobr><span class="ft26"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:447;left:716"><nobr><span class="ft5"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:447;left:675"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:447;left:621"><nobr><span class="ft5"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:447;left:611"><nobr><span class="ft5"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:447;left:542"><nobr><span class="ft5"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:443;left:726"><nobr><span class="ft27">2</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:685"><nobr><span class="ft27">2</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:563"><nobr><span class="ft27">cos</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:738"><nobr><span class="ft3"> (2) </span></nobr></DIV>
<DIV style="position:absolute;top:475;left:477"><nobr><span class="ft13">If the similarity of two stories exceeds a yes/no threshold, the <br>stories are considered to be about the same topic. </span></nobr></DIV>
<DIV style="position:absolute;top:513;left:477"><nobr><span class="ft13">For topic tracking, a topic model is a centroid, an average of the <br>vectors for the <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:568"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:529;left:571"><nobr><span class="ft3"> training stories. Topic models are pruned to 100 </span></nobr></DIV>
<DIV style="position:absolute;top:545;left:477"><nobr><span class="ft13">terms based on the term weights. Story vectors pruned to 100 <br>terms are compared to centroids using equation (2). If the similar-<br>ity exceeds a yes/no threshold, the story is considered on-topic.  </span></nobr></DIV>
<DIV style="position:absolute;top:608;left:477"><nobr><span class="ft21"><i>2.2.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:510"><nobr><span class="ft22"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:608;left:521"><nobr><span class="ft21"><i>Relevance modeling </i></span></nobr></DIV>
<DIV style="position:absolute;top:627;left:477"><nobr><span class="ft15">Relevance modeling is a statistical technique for estimating lan-<br>guage models from extremely small samples, such as queries, </span></nobr></DIV>
<DIV style="position:absolute;top:640;left:817"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:644;left:817"><nobr><span class="ft3">[9]. </span></nobr></DIV>
<DIV style="position:absolute;top:660;left:477"><nobr><span class="ft13">If  <i>Q</i> is small sample of text, and <i>C</i> is a large collection of docu-<br>ments, the language model for <i>Q</i> is estimated as: </span></nobr></DIV>
<DIV style="position:absolute;top:698;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:703;left:726"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:711"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:686"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:672"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:648"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:632"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:588"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:573"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:556"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:716"><nobr><span class="ft5"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:690"><nobr><span class="ft5"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:677"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:653"><nobr><span class="ft5"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:637"><nobr><span class="ft5"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:623"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:578"><nobr><span class="ft5"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:561"><nobr><span class="ft5"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:703;left:548"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:710;left:703"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:720;left:616"><nobr><span class="ft26"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:720;left:606"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:710;left:666"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:695;left:607"><nobr><span class="ft30"></span></nobr></DIV>
<DIV style="position:absolute;top:717;left:611"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:699;left:595"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:732"><nobr><span class="ft3"> (3) </span></nobr></DIV>
<DIV style="position:absolute;top:737;left:477"><nobr><span class="ft3">A relevance model, then, is a mixture of language models <i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:743;left:817"><nobr><span class="ft31"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:821"><nobr><span class="ft3"> of </span></nobr></DIV>
<DIV style="position:absolute;top:753;left:477"><nobr><span class="ft13">every document <i>d</i> in the collection, where the document models <br>are weighted by the posterior probability of producing the query <br><i>P(M</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:501"><nobr><span class="ft31"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:785;left:505"><nobr><span class="ft5"><i>|Q). </i>The posterior probability is computed as: </span></nobr></DIV>
<DIV style="position:absolute;top:820;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:832;left:615"><nobr><span class="ft30"></span></nobr></DIV>
<DIV style="position:absolute;top:832;left:661"><nobr><span class="ft33"></span></nobr></DIV>
<DIV style="position:absolute;top:801;left:651"><nobr><span class="ft33"></span></nobr></DIV>
<DIV style="position:absolute;top:854;left:620"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:853;left:618"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:854;left:666"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:726"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:656"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:834;left:653"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:825;left:602"><nobr><span class="ft25">=</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:624"><nobr><span class="ft26"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:614"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:671"><nobr><span class="ft26"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:662"><nobr><span class="ft26"><i>q</i></span></nobr></DIV>
<DIV style="position:absolute;top:847;left:721"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:826;left:661"><nobr><span class="ft26"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:826;left:653"><nobr><span class="ft26"><i>q</i></span></nobr></DIV>
<DIV style="position:absolute;top:816;left:712"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:836;left:570"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:708"><nobr><span class="ft5"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:693"><nobr><span class="ft5"><i>q</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:680"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:645"><nobr><span class="ft5"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:631"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:699"><nobr><span class="ft5"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:684"><nobr><span class="ft5"><i>q</i></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:670"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:639"><nobr><span class="ft5"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:809;left:625"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:584"><nobr><span class="ft5"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:557"><nobr><span class="ft5"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:543"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:730"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:702"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:688"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:656"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:640"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:718"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:693"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:679"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:647"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:634"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:594"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:579"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:552"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:737"><nobr><span class="ft3"> (4) </span></nobr></DIV>
<DIV style="position:absolute;top:876;left:477"><nobr><span class="ft13">Equation (4) assigns the highest weights to documents that are <br>most likely to have generated <i>Q</i>, and can be interpreted as nearest-<br>neighbor smoothing, or a massive query expansion technique. </span></nobr></DIV>
<DIV style="position:absolute;top:929;left:477"><nobr><span class="ft13">To apply relevance modeling to story link detection, we estimate <br>the similarity between two stories <i>A</i> and <i>B</i> by pruning the stories <br>to short queries, estimating relevance models for the queries, and <br>measuring the similarity between the two relevance models. Each <br>story is replaced by a query consisting of the ten words in the <br>query with the lowest probability of occurring by chance in ran-<br>domly drawing |<i>A</i>| words from the collection <i>C</i>: </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">403</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft34{font-size:10px;font-family:Times;color:#000000;}
	.ft35{font-size:10px;font-family:Symbol;color:#000000;}
	.ft36{font-size:16px;font-family:Times;color:#000000;}
	.ft37{font-size:3px;font-family:Times;color:#000000;}
	.ft38{font-size:10px;line-height:-7px;font-family:Times;color:#000000;}
	.ft39{font-size:10px;line-height:15px;font-family:Times;color:#000000;}
	.ft40{font-size:10px;line-height:-5px;font-family:Times;color:#000000;}
	.ft41{font-size:10px;line-height:13px;font-family:Times;color:#000000;}
	.ft42{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121003.png" alt="background image">
<DIV style="position:absolute;top:127;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:283"><nobr><span class="ft39"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:283"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:264"><nobr><span class="ft39"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:264"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:311"><nobr><span class="ft39"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:311"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:264"><nobr><span class="ft39"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:264"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:129;left:285"><nobr><span class="ft35">-</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:285"><nobr><span class="ft35">-</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:259"><nobr><span class="ft41"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:259"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:124;left:237"><nobr><span class="ft41"><br><br></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:237"><nobr><span class="ft34"></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:226"><nobr><span class="ft35">=</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:273"><nobr><span class="ft11"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:154;left:272"><nobr><span class="ft11"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:295"><nobr><span class="ft11"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:273"><nobr><span class="ft11"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:294"><nobr><span class="ft11"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:272"><nobr><span class="ft11"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:244"><nobr><span class="ft11"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:243"><nobr><span class="ft11"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:205"><nobr><span class="ft11"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:170"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:303"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:121;left:303"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:252"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:121;left:252"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:212"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:176"><nobr><span class="ft26"><i>chance</i></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:219"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:199"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:318"><nobr><span class="ft3"> (5) </span></nobr></DIV>
<DIV style="position:absolute;top:200;left:81"><nobr><span class="ft3">where |<i>A</i>| is the length of the story <i>A</i>,  <i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:205;left:304"><nobr><span class="ft31"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:310"><nobr><span class="ft3"> is the number of times </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:81"><nobr><span class="ft3">word <i>w</i> occurs in <i>A</i>, |<i>C</i>| is the size of the collection, and <i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:221;left:402"><nobr><span class="ft31"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:408"><nobr><span class="ft3"> is the </span></nobr></DIV>
<DIV style="position:absolute;top:231;left:81"><nobr><span class="ft3">number of times word <i>w</i> occurs in <i>C</i>.  </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:81"><nobr><span class="ft15">Story relevance models are estimated using equation (4). Similar-<br>ity between relevance models is measured using the symmetrized <br>clarity-adjusted divergence </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:231"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:286;left:231"><nobr><span class="ft3">[11]: </span></nobr></DIV>
<DIV style="position:absolute;top:310;left:277"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:310;left:129"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:313;left:267"><nobr><span class="ft35">+</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:119"><nobr><span class="ft35">=</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:281"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:316;left:400"><nobr><span class="ft26"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:329"><nobr><span class="ft26"><i>B</i></span></nobr></DIV>
<DIV style="position:absolute;top:333;left:133"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:316;left:252"><nobr><span class="ft26"><i>B</i></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:182"><nobr><span class="ft26"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:103"><nobr><span class="ft26"><i>RM</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:389"><nobr><span class="ft11"><i>GE</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:374"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:362"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:390"><nobr><span class="ft11"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:375"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:362"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:320"><nobr><span class="ft11"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:305"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:292"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:241"><nobr><span class="ft11"><i>GE</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:227"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:214"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:242"><nobr><span class="ft11"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:227"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:214"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:172"><nobr><span class="ft11"><i>Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:157"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:145"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:83"><nobr><span class="ft11"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:407"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:385"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:370"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:406"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:385"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:370"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:342"><nobr><span class="ft10">log</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:336"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:315"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:300"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:259"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:237"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:222"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:258"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:237"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:223"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:194"><nobr><span class="ft10">log</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:188"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:167"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:153"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:414"><nobr><span class="ft3"> (6) </span></nobr></DIV>
<DIV style="position:absolute;top:351;left:81"><nobr><span class="ft3">where  <i>P(w|Q</i></span></nobr></DIV>
<DIV style="position:absolute;top:357;left:154"><nobr><span class="ft31"><i>A</i></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:159"><nobr><span class="ft5"><i>)</i> is the relevance model estimated for story <i>A</i>, and </span></nobr></DIV>
<DIV style="position:absolute;top:367;left:81"><nobr><span class="ft13"><i>P(w|GE)</i> is the background (General English, Arabic, or Manda-<br>rin) probability of <i>w</i>, computed from the entire collection of sto-<br>ries in the language within the same deferral period used for co-<br>sine similarity.  </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:81"><nobr><span class="ft13">To apply relevance modeling to topic tracking, the asymmetric <br>clarity adjusted divergence is used: </span></nobr></DIV>
<DIV style="position:absolute;top:475;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:474;left:218"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:478;left:209"><nobr><span class="ft35">=</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:223"><nobr><span class="ft26"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:489;left:159"><nobr><span class="ft26"><i>track</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:323"><nobr><span class="ft11"><i>GE</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:308"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:295"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:328"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:312"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:299"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:261"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:246"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:234"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:194"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:181"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:141"><nobr><span class="ft11"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:340"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:318"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:303"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:335"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:323"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:308"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:275"><nobr><span class="ft10">log</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:270"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:257"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:242"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:201"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:189"><nobr><span class="ft10">,</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:177"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:348"><nobr><span class="ft3"> (7) </span></nobr></DIV>
<DIV style="position:absolute;top:515;left:81"><nobr><span class="ft13">where  <i>P(w|T)</i> is a relevance model of the topic <i>T</i>. Because of <br>computational constraints, smoothed maximum likelihood esti-<br>mates rather than relevance models are used for the story model <br><i>P(w|S)</i>. The topic model, based on Equation (3), is: </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:246"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:605;left:249"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:588;left:217"><nobr><span class="ft35">=</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:258"><nobr><span class="ft37"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:254"><nobr><span class="ft26"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:245"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:598;left:302"><nobr><span class="ft26"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:608;left:237"><nobr><span class="ft26"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:290"><nobr><span class="ft11"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:274"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:262"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:602;left:230"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:201"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:186"><nobr><span class="ft11"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:173"><nobr><span class="ft11"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:309"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:285"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:270"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:232"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:209"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:196"><nobr><span class="ft10">|</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:181"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:315"><nobr><span class="ft3"> (8) </span></nobr></DIV>
<DIV style="position:absolute;top:627;left:81"><nobr><span class="ft3">where <i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:632;left:124"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:627;left:127"><nobr><span class="ft3"> is the set of training stories. The topic model is pruned to </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:81"><nobr><span class="ft15">100 terms.<i>  </i>More detail about applying relevance models to TDT <br>can be found in </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:169"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:660;left:169"><nobr><span class="ft3">[2]. </span></nobr></DIV>
<DIV style="position:absolute;top:693;left:81"><nobr><span class="ft4"><b>2.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:692;left:104"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:693;left:114"><nobr><span class="ft4"><b>Evaluation </b></span></nobr></DIV>
<DIV style="position:absolute;top:712;left:81"><nobr><span class="ft13">TDT tasks are evaluated as detection tasks. For each test trial, the <br>system attempts to make a yes/no decision. In story link detection, <br>the decision is whether the two members of a story pair belong to <br>the same topic. In topic tracking, the decision is whether a story in <br>the stream belongs to a particular topic. In all tasks, performance <br>is summarized in two ways: a detection cost function (<i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:797;left:390"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:791;left:403"><nobr><span class="ft3">) and a </span></nobr></DIV>
<DIV style="position:absolute;top:807;left:81"><nobr><span class="ft13">decision error tradeoff (DET) curve. Both are based on the rates <br>of two kinds of errors a detection system can make: <i>misses</i>, in <br>which the system gives a <i>no</i> answer where the correct answer is <br><i>yes</i>, and <i>false alarms</i>, in which the system gives a <i>yes</i> answer <br>where the correct answer is <i>no</i>.  </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft3">The DET curve plots the miss rate (<i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:898;left:295"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:892;left:312"><nobr><span class="ft3">) as a function of false </span></nobr></DIV>
<DIV style="position:absolute;top:908;left:81"><nobr><span class="ft3">alarm rate (<i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:913;left:152"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:908;left:162"><nobr><span class="ft3">), as the yes/no decision threshold is swept through </span></nobr></DIV>
<DIV style="position:absolute;top:924;left:81"><nobr><span class="ft3">its range. <i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:148"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:924;left:165"><nobr><span class="ft3"> and <i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:205"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:924;left:215"><nobr><span class="ft3"> are computed for each topic, and then </span></nobr></DIV>
<DIV style="position:absolute;top:940;left:81"><nobr><span class="ft13">averaged across topics to yield <i>topic-weighted</i> curves. An example <br>can be seen in Figure 1 below. Better performance is indicated by <br>curves more to the lower left of the graph.  </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:81"><nobr><span class="ft13">The detection cost function is computed for a particular threshold <br>as follows:  </span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:81"><nobr><span class="ft5"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:98"><nobr><span class="ft5"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:107"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:120"><nobr><span class="ft5"><i> = (C</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:149"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:166"><nobr><span class="ft5"><i> * P</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:188"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:205"><nobr><span class="ft5"><i> * P</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:227"><nobr><span class="ft31"><i>Target</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:251"><nobr><span class="ft5"><i> + C</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:276"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:286"><nobr><span class="ft5"><i> * P</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:308"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:318"><nobr><span class="ft5"><i> * (1-P</i></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:355"><nobr><span class="ft31"><i>Target</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:379"><nobr><span class="ft5"><i>)) </i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:422"><nobr><span class="ft3">(9)<i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:81"><nobr><span class="ft3">where:   <i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:143"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:160"><nobr><span class="ft5"><i> = #Misses / #Targets </i></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:531"><nobr><span class="ft5"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:539"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:549"><nobr><span class="ft5"><i> = #False Alarms / #NonTargets </i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:477"><nobr><span class="ft5"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:139;left:486"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:503"><nobr><span class="ft5"><i> </i>and<i> C</i></span></nobr></DIV>
<DIV style="position:absolute;top:139;left:540"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:550"><nobr><span class="ft5"><i> </i>are the costs of a missed detection and false alarm, </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:477"><nobr><span class="ft13">respectively, and are specified for the application, usually at 10 <br>and 1, penalizing misses more than false alarms. <i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:765"><nobr><span class="ft31"><i>Target</i></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:789"><nobr><span class="ft5"><i>  </i>is the a </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:477"><nobr><span class="ft13">priori probability of finding a <i>target</i>, an item where the answer <br>should be yes, set by convention to 0.02.  </span></nobr></DIV>
<DIV style="position:absolute;top:218;left:477"><nobr><span class="ft3">The cost function is normalized: </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:477"><nobr><span class="ft3">  <i>(C</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:504"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:516"><nobr><span class="ft5"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:521"><nobr><span class="ft31"><i>Norm</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:541"><nobr><span class="ft5"><i> = C</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:566"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:579"><nobr><span class="ft5"><i> / MIN(C</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:628"><nobr><span class="ft31"><i>Miss</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:645"><nobr><span class="ft5"><i> * C</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:668"><nobr><span class="ft31"><i>Target</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:691"><nobr><span class="ft5"><i>, C</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:707"><nobr><span class="ft31"><i>Fa</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:717"><nobr><span class="ft5"><i> * (1-P</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:755"><nobr><span class="ft31"><i>Target</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:779"><nobr><span class="ft5"><i>))</i> (10) </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:477"><nobr><span class="ft13">and averaged over topics. Each point along the detection error <br>tradeoff curve has a value of (<i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:283;left:648"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:278;left:661"><nobr><span class="ft5"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:283;left:666"><nobr><span class="ft31"><i>Norm</i></span></nobr></DIV>
<DIV style="position:absolute;top:278;left:686"><nobr><span class="ft5"><i>. </i>The minimum value found </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:477"><nobr><span class="ft3">on the curve is known as the <i>min(C</i></span></nobr></DIV>
<DIV style="position:absolute;top:299;left:669"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:293;left:682"><nobr><span class="ft5"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:299;left:687"><nobr><span class="ft31"><i>Norm</i></span></nobr></DIV>
<DIV style="position:absolute;top:293;left:707"><nobr><span class="ft5"><i>.</i> It can be interpreted as </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:477"><nobr><span class="ft3">the value of <i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:555"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:568"><nobr><span class="ft5"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:573"><nobr><span class="ft31"><i>Norm</i></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:593"><nobr><span class="ft5"><i> </i>at the best possible threshold. This measure </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:477"><nobr><span class="ft15">allows us to separate performance on the task from the choice of <br>yes/no threshold. Lower cost scores indicate better performance. <br>More information about these measures can be found in </span></nobr></DIV>
<DIV style="position:absolute;top:354;left:782"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:782"><nobr><span class="ft3">[5]. </span></nobr></DIV>
<DIV style="position:absolute;top:391;left:477"><nobr><span class="ft4"><b>2.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:390;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:391;left:510"><nobr><span class="ft4"><b>Language-specific Comparisons </b></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:477"><nobr><span class="ft15">English stories were lower-cased and stemmed using the <i>kstem</i> <br>stemmer </span></nobr></DIV>
<DIV style="position:absolute;top:424;left:532"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:428;left:532"><nobr><span class="ft3">[6]. Stop words were removed. For native Arabic </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:477"><nobr><span class="ft15">comparisons, stories were converted from Unicode UTF-8 to win-<br>dows (CP1256) encoding, then normalized and stemmed with a <br>light stemmer </span></nobr></DIV>
<DIV style="position:absolute;top:473;left:555"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:477;left:555"><nobr><span class="ft3">[7]. Stop words were removed. For native Mandarin </span></nobr></DIV>
<DIV style="position:absolute;top:493;left:477"><nobr><span class="ft3">comparisons, overlapping character bigrams were compared.  </span></nobr></DIV>
<DIV style="position:absolute;top:526;left:477"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:526;left:501"><nobr><span class="ft4"><b>STORY LINK DETECTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:546;left:477"><nobr><span class="ft13">In this section we present experimental results for story link de-<br>tection, comparing a <i>native</i> condition with an <i>English </i>baseline. In <br>the English baseline, all comparisons are in English, using ma-<br>chine translation (MT) for Arabic and Mandarin stories. Corpus <br>statistics are computed incrementally for all the English and <br>translated-into-English stories. In the Native condition, two sto-<br>ries originating in the same language are compared in that lan-<br>guage. Corpus statistics are computed incrementally for the stories <br>in the language of the comparison. Cross language pairs in the <br>native condition are compared in English using MT, as in the <br>baseline.  </span></nobr></DIV>
<DIV style="position:absolute;top:987;left:840"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:477"><nobr><span class="ft42"><b>Figure 1: DET curve for TDT3 link detection based on English <br>versions of stories, or native language versions, for cosine and <br>relevance model similarity </b></span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">404</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft43{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
	.ft44{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121004.png" alt="background image">
<DIV style="position:absolute;top:112;left:113"><nobr><span class="ft6"><b>Table 4: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:200"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:211"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:216"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:237"><nobr><span class="ft6"><b> for TDT3 story link detection </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:143"><nobr><span class="ft6"><b>Similarity English </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:342"><nobr><span class="ft6"><b>Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:143"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:281"><nobr><span class="ft3">.3440 .2586 </span></nobr></DIV>
<DIV style="position:absolute;top:168;left:143"><nobr><span class="ft6"><b>Relevance Model </b></span></nobr></DIV>
<DIV style="position:absolute;top:167;left:281"><nobr><span class="ft3">.2625 .1900 </span></nobr></DIV>
<DIV style="position:absolute;top:184;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:81"><nobr><span class="ft13">Figure 1 shows the DET curves for the TDT3 story link detection <br>task, and Table 4 shows the minimum cost. The figure and table <br>show that native language comparisons (dotted) consistently out-<br>perform comparisons based on machine-translated English (solid). <br>This difference holds both for the basic cosine similarity system <br>(first row) (black curves), and for the relevance modeling system <br>(second row) (gray curves). These results support the general <br>conclusion that when two stories originate in the same language, it <br>is better to carry out similarity comparisons in that language, <br>rather than translating them into a different language. </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:81"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:379;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:381;left:105"><nobr><span class="ft4"><b>TOPIC TRACKING </b></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:81"><nobr><span class="ft13">In tracking, the system decides whether stories in a stream belong <br>to predefined topics. Similarity is measured between a topic <br>model and a story, rather than between two stories. The native <br>language hypothesis for tracking predicts better performance if <br>incoming stories are compared in their original language with <br>topic models in that language, and worse performance if translated <br>stories are compared with English topic models.  </span></nobr></DIV>
<DIV style="position:absolute;top:517;left:81"><nobr><span class="ft13">The hypothesis can only be tested indirectly, because Arabic and <br>Mandarin training stories were not available for all tracking top-<br>ics. In this first set of experiments, we chose to obtain native lan-<br>guage training stories from the stream of test stories using topic <br>adaptation, that is, gradual modification of topic models to incor-<br>porate test stories that fit the topic particularly well.  </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:81"><nobr><span class="ft15">Adaptation begins with the topic tracking scenario described <br>above in section </span></nobr></DIV>
<DIV style="position:absolute;top:631;left:178"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:635;left:178"><nobr><span class="ft3">2.2, using a single model per topic based on a </span></nobr></DIV>
<DIV style="position:absolute;top:651;left:81"><nobr><span class="ft43">small set of training stories in English. Each time a story is com-<br>pared to a topic model to determine whether it should be classed <br>as on-topic, it is also compared to a fixed adaptation threshold <br><i></i></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:88"><nobr><span class="ft31"><i>ad</i></span></nobr></DIV>
<DIV style="position:absolute;top:699;left:97"><nobr><span class="ft3">= 0.5 (not to be confused with the yes/no threshold mentioned </span></nobr></DIV>
<DIV style="position:absolute;top:716;left:81"><nobr><span class="ft3">in section </span></nobr></DIV>
<DIV style="position:absolute;top:712;left:142"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:716;left:142"><nobr><span class="ft3">2.2.1). If the similarity score is greater than <i></i></span></nobr></DIV>
<DIV style="position:absolute;top:721;left:406"><nobr><span class="ft31"><i>ad</i></span></nobr></DIV>
<DIV style="position:absolute;top:716;left:415"><nobr><span class="ft3">, the </span></nobr></DIV>
<DIV style="position:absolute;top:732;left:81"><nobr><span class="ft13">story is added to the topic set, and the topic model recomputed. <br>For clarity, we use the phrase <i>topic set</i> to refer to the set of stories <br>from which the topic model is built, which grows under adapta-<br>tion. The <i>training set</i> includes only the original <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:785;left:354"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:780;left:356"><nobr><span class="ft3"> training stories </span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft13">for each topic. For cosine similarity, adaptation consists of <br>computing a new centroid for the topic set and pruning to 100 <br>terms. For relevance modeling, a new topic model is computed <br>according to Equation (8). At most 100 stories are placed in each <br>topic set.  </span></nobr></DIV>
<DIV style="position:absolute;top:881;left:81"><nobr><span class="ft13">We have just described <i>global adaptation</i>, in which stories are <br>added to global topic models in English. Stories that originated in <br>Arabic or Mandarin are compared and added in their machine-<br>translated version. </span></nobr></DIV>
<DIV style="position:absolute;top:950;left:81"><nobr><span class="ft13"><i>Native adaptation </i>differs from global adaptation in making sepa-<br>rate topic models for each source language. To decide whether a <br>test story should be added to a native topic set, the test story is <br>compared in its native language with the native model, and added <br>to the native topic set for that language if its similarity score ex-<br>ceeds <i></i></span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:121"><nobr><span class="ft31"><i>ad</i></span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:130"><nobr><span class="ft3">. The English version of the story is also compared to the </span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:81"><nobr><span class="ft3">global topic model, and if its similarity score exceeds <i></i></span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:401"><nobr><span class="ft31"><i>ad</i></span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:410"><nobr><span class="ft3">, it is </span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:81"><nobr><span class="ft3">added to the global topic set.  (Global models continue to adapt </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft13">for other languages which may not yet have a native model, or for <br>smoothing, discussed later.)  </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:477"><nobr><span class="ft13">At the start there are global topic models and native English topic <br>models based on the training stories, but no native Arabic or <br>Mandarin topic models. When there is not yet a native topic <br>model in the story's original language, the translated story is <br>compared to the global topic model. If the similarity exceeds <i></i></span></nobr></DIV>
<DIV style="position:absolute;top:218;left:825"><nobr><span class="ft31"><i>ad</i></span></nobr></DIV>
<DIV style="position:absolute;top:212;left:834"><nobr><span class="ft3">, </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:477"><nobr><span class="ft3">the native topic model is initialized with the untranslated story.  </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:477"><nobr><span class="ft13">Yes/no decisions for topic tracking can then be based on the un-<br>translated story's similarity to the native topic model if one exists. <br>If there is no native topic model yet for that language and topic, <br>the translated story is compared to the global topic model.  </span></nobr></DIV>
<DIV style="position:absolute;top:319;left:477"><nobr><span class="ft13">We have described three experimental conditions: <i>global adapted</i>, <br><i>native adapted,</i> and a baseline. The baseline, described in Section </span></nobr></DIV>
<DIV style="position:absolute;top:348;left:477"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:352;left:477"><nobr><span class="ft13">2.2, can also be called <i>global unadapted</i>. The baseline uses a <br>single English model per topic based on the small set of training <br>stories. A fourth possible condition, <i>native unadapted</i> is problem-<br>atic and not included here. There is no straightforward way to <br>initialize native language topic models without adaptation when <br>training stories are provided only in English.  </span></nobr></DIV>
<DIV style="position:absolute;top:983;left:477"><nobr><span class="ft42"><b>Figure 2: DET curves for TDT3 tracking, cosine similarity <br>(above) and relevance models (below), <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:721"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:999;left:724"><nobr><span class="ft6"><b>=4 training stories, </b></span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:477"><nobr><span class="ft6"><b>global unadapted baseline, global adapted, and native adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">405</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="121005.png" alt="background image">
<DIV style="position:absolute;top:112;left:126"><nobr><span class="ft6"><b>Table 5: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:214"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:225"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:229"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:251"><nobr><span class="ft6"><b> for TDT3 topic tracking.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:85"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:203"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:213"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:215"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:357"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:366"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:369"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:217"><nobr><span class="ft6"><b>Adapted Adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:85"><nobr><span class="ft6"><b> Base-</b></span></nobr></DIV>
<DIV style="position:absolute;top:167;left:140"><nobr><span class="ft6"><b>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:191"><nobr><span class="ft6"><b>Global Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:302"><nobr><span class="ft42"><b>Base-<br>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:344"><nobr><span class="ft6"><b>Global  Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:140"><nobr><span class="ft3">.1501 .1197  .1340  .1238  .1074 </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:394"><nobr><span class="ft3">.1028 </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:85"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:205;left:140"><nobr><span class="ft3">.1283 .0892  .0966  .1060  .0818 .0934 </span></nobr></DIV>
<DIV style="position:absolute;top:222;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:81"><nobr><span class="ft13">The TDT3 tracking results on three conditions, replicated with the <br>two different similarity measures (cosine similarity and relevance <br>modeling) and two different training set sizes (<i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:347"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:275;left:349"><nobr><span class="ft3">=2 and 4) can be </span></nobr></DIV>
<DIV style="position:absolute;top:291;left:81"><nobr><span class="ft3">seen in Table 5. DET curves for <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:297;left:274"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:276"><nobr><span class="ft3">=4 are shown in Figure 2, for </span></nobr></DIV>
<DIV style="position:absolute;top:307;left:81"><nobr><span class="ft3">cosine similarity (above) and relevance modeling (RM) (below). </span></nobr></DIV>
<DIV style="position:absolute;top:329;left:81"><nobr><span class="ft13">Table 5 shows a robust adaptation effect for cosine and relevance <br>model experiments, and for 2 or 4 training stories. Native and <br>global adaptation are always better (lower cost) than baseline <br>unadapted tracking. In addition, relevance modeling produces <br>better results than cosine similarity. However, results do not show <br>the predicted advantage for native adapted topic models over <br>global adapted topic models. Only cosine similarity, <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:429;left:384"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:424;left:386"><nobr><span class="ft3">=4, seems </span></nobr></DIV>
<DIV style="position:absolute;top:440;left:81"><nobr><span class="ft13">to show the expected difference (shaded cells), but the difference <br>is very small. The DET curve in Figure 2 shows no sign of a na-<br>tive language effect. </span></nobr></DIV>
<DIV style="position:absolute;top:493;left:81"><nobr><span class="ft13">Table 6 shows minimum cost figures computed separately for <br>English, Mandarin, and Arabic test sets.  Only English shows a <br>pattern similar to the composite results of Table 5 (see the shaded <br>cells). For cosine similarity, there is not much difference between <br>global and native English topic models. For relevance modeling, <br>Native English topic models are slightly worse than global mod-<br>els. Arabic and Mandarin appear to show a native language ad-<br>vantage for all cosine similarity conditions and most relevance <br>model conditions. However, DET curves comparing global and <br>native adapted models separately for English, Arabic, and Manda-<br>rin, (Figure 3) show no real native language advantage.  </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:696;left:83"><nobr><span class="ft6"><b>Table 6: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:701;left:171"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:695;left:182"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:701;left:186"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:208"><nobr><span class="ft6"><b> for TDT3 topic tracking; breakdown by </b></span></nobr></DIV>
<DIV style="position:absolute;top:711;left:195"><nobr><span class="ft6"><b>original story language </b></span></nobr></DIV>
<DIV style="position:absolute;top:734;left:240"><nobr><span class="ft6"><b>English </b></span></nobr></DIV>
<DIV style="position:absolute;top:751;left:85"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:750;left:203"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:756;left:213"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:751;left:215"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:750;left:357"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:756;left:366"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:751;left:369"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:217"><nobr><span class="ft6"><b>Adapted Adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:85"><nobr><span class="ft6"><b> Base-</b></span></nobr></DIV>
<DIV style="position:absolute;top:783;left:140"><nobr><span class="ft6"><b>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:784;left:191"><nobr><span class="ft6"><b>Global Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:302"><nobr><span class="ft42"><b>Base-<br>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:784;left:344"><nobr><span class="ft6"><b>Global  Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:800;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:800;left:140"><nobr><span class="ft3">.1177 .0930  .0977  .0903  .0736 </span></nobr></DIV>
<DIV style="position:absolute;top:800;left:394"><nobr><span class="ft3">.0713 </span></nobr></DIV>
<DIV style="position:absolute;top:817;left:85"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:816;left:140"><nobr><span class="ft3">.1006 .0681  .0754  .0737  .0573 .0628 </span></nobr></DIV>
<DIV style="position:absolute;top:833;left:242"><nobr><span class="ft6"><b>Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:850;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:849;left:140"><nobr><span class="ft3">.2023 </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:191"><nobr><span class="ft3">.1654 </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:248"><nobr><span class="ft3">.1486 .1794  .1558 </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:394"><nobr><span class="ft3">.1348 </span></nobr></DIV>
<DIV style="position:absolute;top:866;left:85"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:866;left:140"><nobr><span class="ft3">.1884 .1356  .1404  .1581  .1206 .1377 </span></nobr></DIV>
<DIV style="position:absolute;top:883;left:232"><nobr><span class="ft6"><b>Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:900;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:899;left:140"><nobr><span class="ft3">.2156 </span></nobr></DIV>
<DIV style="position:absolute;top:899;left:191"><nobr><span class="ft3">.1794 </span></nobr></DIV>
<DIV style="position:absolute;top:899;left:248"><nobr><span class="ft3">.1714 .1657  .1557 </span></nobr></DIV>
<DIV style="position:absolute;top:899;left:394"><nobr><span class="ft3">.1422 </span></nobr></DIV>
<DIV style="position:absolute;top:916;left:85"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:916;left:140"><nobr><span class="ft3">.1829 </span></nobr></DIV>
<DIV style="position:absolute;top:916;left:191"><nobr><span class="ft3">.1272 </span></nobr></DIV>
<DIV style="position:absolute;top:916;left:248"><nobr><span class="ft3">.0991 .1286  .0935 </span></nobr></DIV>
<DIV style="position:absolute;top:916;left:394"><nobr><span class="ft3">.0847 </span></nobr></DIV>
<DIV style="position:absolute;top:932;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:954;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:842"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:374;left:477"><nobr><span class="ft42"><b>Figure 3: DET curves for TDT3 tracking, cosine similarity, <br><i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:395;left:487"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:390;left:489"><nobr><span class="ft6"><b>=4 training stories, global adapted vs. native adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:406;left:477"><nobr><span class="ft6"><b>breakdown for English, Arabic, and Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:427;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:449;left:477"><nobr><span class="ft13">In trying to account for the discrepancy between the findings on <br>link detection and tracking, we suspected that the root of the <br>problem was the quality of native models for Arabic and Manda-<br>rin. For English, adaptation began with 2 or 4 on-topic models. <br>However, Mandarin and Arabic models did not begin with on-<br>topic stories; they could begin with off-topic models, which <br>should hurt tracking performance. A related issue is data sparse-<br>ness. When a native topic model is first formed, it is based on one <br>story, which is a poorer basis for tracking than <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:581;left:753"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:756"><nobr><span class="ft3"> stories. In the </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:477"><nobr><span class="ft15">next three sections we pursue different aspects of these suspicions. <br>In section </span></nobr></DIV>
<DIV style="position:absolute;top:604;left:534"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:609;left:534"><nobr><span class="ft3">5 we perform a best-case experiment, initializing native </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:477"><nobr><span class="ft15">topic sets with on-topic stories, and smoothing native scores with <br>global scores to address the sparseness problem. If these condi-<br>tions do not show a native language advantage, we would reject <br>the native language hypothesis. In section </span></nobr></DIV>
<DIV style="position:absolute;top:670;left:707"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:674;left:707"><nobr><span class="ft3">6 we explore the role of </span></nobr></DIV>
<DIV style="position:absolute;top:691;left:477"><nobr><span class="ft3">the adaptation threshold. In section </span></nobr></DIV>
<DIV style="position:absolute;top:687;left:672"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:691;left:672"><nobr><span class="ft3">7 we compare some additional </span></nobr></DIV>
<DIV style="position:absolute;top:708;left:477"><nobr><span class="ft3">methods of initializing native language topic models. </span></nobr></DIV>
<DIV style="position:absolute;top:740;left:477"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:739;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:501"><nobr><span class="ft4"><b>ON-TOPIC NATIVE CENTROIDS </b></span></nobr></DIV>
<DIV style="position:absolute;top:760;left:477"><nobr><span class="ft13">In this section, we consider a best-case scenario, where we take <br>the first <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:781;left:533"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:775;left:536"><nobr><span class="ft3"> stories in each language relevant to each topic, to ini-</span></nobr></DIV>
<DIV style="position:absolute;top:791;left:477"><nobr><span class="ft13">tialize adaptation of native topic models. While this is cheating, <br>and not a way to obtain native training documents in a realistic <br>tracking scenario, it demonstrates what performance can be at-<br>tained if native training documents are available. More realistic <br>approaches to adapting native topic models are considered in <br>subsequent sections. </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:477"><nobr><span class="ft15">The baseline and global adapted conditions were carried out as in <br>Section </span></nobr></DIV>
<DIV style="position:absolute;top:905;left:522"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:909;left:522"><nobr><span class="ft3">4, and the native adapted condition was similar except in </span></nobr></DIV>
<DIV style="position:absolute;top:926;left:477"><nobr><span class="ft3">the way adaptation of native topics began. If there were not yet <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:931;left:835"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:926;left:837"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:942;left:477"><nobr><span class="ft13">native stories in the topic set for the current test story in its native <br>language, the story was added to the topic set if it was relevant. <br>Once a native topic model had <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:979;left:655"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:973;left:658"><nobr><span class="ft3"> stories, we switched to the usual </span></nobr></DIV>
<DIV style="position:absolute;top:989;left:477"><nobr><span class="ft13">non-cheating mode of adaptation, based on similarity score and <br>adaptation threshold.  </span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:477"><nobr><span class="ft13">To address the data sparseness problem, we also smoothed the <br>native similarity scores with the global similarity scores:  </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">406</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft45{font-size:5px;font-family:Helvetica;color:#000000;}
	.ft46{font-size:6px;font-family:Helvetica;color:#000000;}
	.ft47{font-size:6px;font-family:Helvetica;color:#000000;}
	.ft48{font-size:6px;line-height:-5px;font-family:Helvetica;color:#000000;}
	.ft49{font-size:6px;line-height:-4px;font-family:Helvetica;color:#000000;}
	.ft50{font-size:6px;line-height:10px;font-family:Helvetica;color:#000000;}
	.ft51{font-size:6px;line-height:11px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121006.png" alt="background image">
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:115;left:377"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:365"><nobr><span class="ft10">,</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:352"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:307"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:281"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:278"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:261"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:249"><nobr><span class="ft10">,</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:236"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:173"><nobr><span class="ft10">)</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:160"><nobr><span class="ft10">,</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:147"><nobr><span class="ft10">(</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:370"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:356"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:311"><nobr><span class="ft11"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:254"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:240"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:197"><nobr><span class="ft11"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:165"><nobr><span class="ft11"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:151"><nobr><span class="ft11"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:115;left:105"><nobr><span class="ft11"><i>Sim</i></span></nobr></DIV>
<DIV style="position:absolute;top:122;left:331"><nobr><span class="ft26"><i>global</i></span></nobr></DIV>
<DIV style="position:absolute;top:122;left:216"><nobr><span class="ft26"><i>native</i></span></nobr></DIV>
<DIV style="position:absolute;top:122;left:124"><nobr><span class="ft26"><i>smooth</i></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:299"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:190"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:289"><nobr><span class="ft35">-</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:268"><nobr><span class="ft35">+</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:180"><nobr><span class="ft35">=</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:383"><nobr><span class="ft3">  </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:415"><nobr><span class="ft3">(11)  </span></nobr></DIV>
<DIV style="position:absolute;top:140;left:81"><nobr><span class="ft3">The parameter <i></i> was not tuned, but set to a fixed value of 0.5.   </span></nobr></DIV>
<DIV style="position:absolute;top:162;left:81"><nobr><span class="ft13">The results can be seen in Table 7. Shaded cell pairs indicate con-<br>firmation of the native language hypothesis, where language-spe-<br>cific topic models outperform global models.  </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:90"><nobr><span class="ft6"><b>Table 7: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:221;left:178"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:189"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:221;left:194"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:215"><nobr><span class="ft6"><b> for TDT3 topic tracking, using <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:221;left:408"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:410"><nobr><span class="ft6"><b> on-</b></span></nobr></DIV>
<DIV style="position:absolute;top:232;left:101"><nobr><span class="ft6"><b>topic native training stories and smoothing native scores </b></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:85"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:203"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:260;left:213"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:215"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:357"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:260;left:366"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:369"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:271;left:217"><nobr><span class="ft6"><b>Adapted Adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:271;left:85"><nobr><span class="ft6"><b> Base-</b></span></nobr></DIV>
<DIV style="position:absolute;top:287;left:140"><nobr><span class="ft6"><b>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:191"><nobr><span class="ft6"><b>Global Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:271;left:302"><nobr><span class="ft42"><b>Base-<br>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:344"><nobr><span class="ft6"><b>Global  Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:309;left:140"><nobr><span class="ft3">.1501 </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:191"><nobr><span class="ft3">.1197 </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:248"><nobr><span class="ft3">.0932 .1238  .1074 </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:394"><nobr><span class="ft3">.0758 </span></nobr></DIV>
<DIV style="position:absolute;top:326;left:85"><nobr><span class="ft6"><b>Rel. </b></span></nobr></DIV>
<DIV style="position:absolute;top:325;left:140"><nobr><span class="ft3">.1283 </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:191"><nobr><span class="ft3">.0892 </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:248"><nobr><span class="ft3">.0702 .1060  .0818 </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:394"><nobr><span class="ft3">.0611 </span></nobr></DIV>
<DIV style="position:absolute;top:342;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:609;left:437"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:628;left:81"><nobr><span class="ft42"><b>Figure 4: DET curve for TDT3 tracking, initializing native <br>adaptation with relevant training stories during adaptation, <br>cosine similarity, <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:665;left:191"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:660;left:194"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:81"><nobr><span class="ft3">Figure 4 shows the DET curves for cosine, <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:687;left:335"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:338"><nobr><span class="ft3">=4 case. When the </span></nobr></DIV>
<DIV style="position:absolute;top:697;left:81"><nobr><span class="ft13">native models are initialized with on-topic stories, the advantage <br>to native models is clearly seen in the tracking performance.  </span></nobr></DIV>
<DIV style="position:absolute;top:981;left:437"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:81"><nobr><span class="ft42"><b>Figure 5: DET curve for TDT3 tracking initializing native <br>adaptation with relevant training stories during adaptation <br>and smoothing, vs. global adaptation, cosine similarity, <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:421"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:423"><nobr><span class="ft6"><b>=4, </b></span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:81"><nobr><span class="ft6"><b>separate analyses for English, Arabic, and Mandarin. </b></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft13">DET curves showing results computed separately for the three <br>languages can be seen in Figure 5, for the cosine, <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:756"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:127;left:759"><nobr><span class="ft3">=4 case. It can </span></nobr></DIV>
<DIV style="position:absolute;top:143;left:477"><nobr><span class="ft13">be clearly seen that English tracking remains about the same but <br>the Arabic and Mandarin native tracking show a large native lan-<br>guage advantage. </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:477"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:501"><nobr><span class="ft4"><b>ADAPTATION THRESHOLD </b></span></nobr></DIV>
<DIV style="position:absolute;top:227;left:477"><nobr><span class="ft13">The adaptation threshold was set to 0.5 in the experiments de-<br>scribed above without any tuning. The increase in global tracking <br>performance after adaptation shows that the value is at least ac-<br>ceptable. However, an analysis of the details of native adaptation <br>showed that many Arabic and Mandarin topics were not adapting. <br>A summary of some of this analysis can be seen in Table 8.  </span></nobr></DIV>
<DIV style="position:absolute;top:328;left:480"><nobr><span class="ft6"><b>Table 8: Number of topics receiving new stories during native </b></span></nobr></DIV>
<DIV style="position:absolute;top:344;left:554"><nobr><span class="ft6"><b>adaptation, breakdown by language </b></span></nobr></DIV>
<DIV style="position:absolute;top:367;left:496"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:366;left:565"><nobr><span class="ft19"><i><b> </b></i></span></nobr></DIV>
<DIV style="position:absolute;top:367;left:588"><nobr><span class="ft6"><b>Total </b></span></nobr></DIV>
<DIV style="position:absolute;top:367;left:646"><nobr><span class="ft6"><b>Topics receiving more stories </b></span></nobr></DIV>
<DIV style="position:absolute;top:383;left:496"><nobr><span class="ft6"><b>Similarity  <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:388;left:575"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:383;left:578"><nobr><span class="ft6"><b>  Topics English </b></span></nobr></DIV>
<DIV style="position:absolute;top:383;left:703"><nobr><span class="ft6"><b>Arabic Mandarin </b></span></nobr></DIV>
<DIV style="position:absolute;top:399;left:565"><nobr><span class="ft3">2 36  24 </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:722"><nobr><span class="ft3">8 </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:782"><nobr><span class="ft3">11 </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:496"><nobr><span class="ft3">Cosine </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:565"><nobr><span class="ft3">4 30  26 </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:722"><nobr><span class="ft3">7 </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:785"><nobr><span class="ft3">9 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:565"><nobr><span class="ft3">2 36  36 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:722"><nobr><span class="ft3">8 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:785"><nobr><span class="ft3">7 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:496"><nobr><span class="ft13">Relevance <br>Model </span></nobr></DIV>
<DIV style="position:absolute;top:449;left:565"><nobr><span class="ft3">4 30  30 </span></nobr></DIV>
<DIV style="position:absolute;top:449;left:722"><nobr><span class="ft3">8 </span></nobr></DIV>
<DIV style="position:absolute;top:449;left:785"><nobr><span class="ft3">5 </span></nobr></DIV>
<DIV style="position:absolute;top:465;left:477"><nobr><span class="ft13"> <br>Fewer than a third of the topics received adapted stories. This <br>means that for most topics, native tracking was based on the <br>global models. In order to determine whether this was due to the <br>adaptation threshold, we performed an experiment varying the <br>adaptation threshold from .3 to .65 in steps of .05. The results can <br>be seen in Figure 6, which shows the minimum cost, <br><i>min(C</i></span></nobr></DIV>
<DIV style="position:absolute;top:582;left:511"><nobr><span class="ft31"><i>Det</i></span></nobr></DIV>
<DIV style="position:absolute;top:576;left:524"><nobr><span class="ft5"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:582;left:528"><nobr><span class="ft31"><i>Norm</i></span></nobr></DIV>
<DIV style="position:absolute;top:576;left:548"><nobr><span class="ft3">, across the range of adaptation threshold values. </span></nobr></DIV>
<DIV style="position:absolute;top:592;left:477"><nobr><span class="ft13">Although we see that the original threshold, .5, was not always the <br>optimal value, it is also clear that the pattern we saw at .5 (and in <br>Figure 6) does not change as the threshold is varied, that is track-<br>ing with native topic models is not better than tracking with <br>global models. An improperly tuned adaptation threshold was <br>therefore not the reason that the native language hypothesis was <br>not confirmed for tracking. We suspect that different adaptation <br>thresholds may be needed for the different languages, but it would <br>be better to handle this problem by language-specific normaliza-<br>tion of similarity scores.  </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:678"><nobr><span class="ft45">0.06</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:678"><nobr><span class="ft45">0.07</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:678"><nobr><span class="ft45">0.08</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:678"><nobr><span class="ft45">0.09</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:682"><nobr><span class="ft45">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:678"><nobr><span class="ft45">0.11</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:678"><nobr><span class="ft45">0.12</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:678"><nobr><span class="ft45">0.13</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:678"><nobr><span class="ft45">0.14</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:678"><nobr><span class="ft45">0.15</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:693"><nobr><span class="ft45">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:719"><nobr><span class="ft45">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:746"><nobr><span class="ft45">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:772"><nobr><span class="ft45">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:798"><nobr><span class="ft45">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:824"><nobr><span class="ft45">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:743"><nobr><span class="ft46"><b>T hreshold</b></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:673"><nobr><span class="ft46"><b>Mi</b></span></nobr></DIV>
<DIV style="position:absolute;top:835;left:673"><nobr><span class="ft48"><b>n<br> C</b></span></nobr></DIV>
<DIV style="position:absolute;top:821;left:673"><nobr><span class="ft49"><b>o<br>s<br>t</b></span></nobr></DIV>
<DIV style="position:absolute;top:776;left:758"><nobr><span class="ft50">Global Nt=2<br>Global Nt=4<br>Native Nt=2<br>Native Nt=4</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:726"><nobr><span class="ft46"><b>Relevance Model</b></span></nobr></DIV>
<DIV style="position:absolute;top:892;left:496"><nobr><span class="ft45">0.06</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:496"><nobr><span class="ft45">0.07</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:496"><nobr><span class="ft45">0.08</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:496"><nobr><span class="ft45">0.09</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:500"><nobr><span class="ft45">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:496"><nobr><span class="ft45">0.11</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:496"><nobr><span class="ft45">0.12</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:496"><nobr><span class="ft45">0.13</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:496"><nobr><span class="ft45">0.14</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:496"><nobr><span class="ft45">0.15</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:510"><nobr><span class="ft45">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:536"><nobr><span class="ft45">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:563"><nobr><span class="ft45">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:589"><nobr><span class="ft45">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:615"><nobr><span class="ft45">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:642"><nobr><span class="ft45">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:561"><nobr><span class="ft46"><b>T hreshold</b></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:490"><nobr><span class="ft46"><b>Mi</b></span></nobr></DIV>
<DIV style="position:absolute;top:835;left:490"><nobr><span class="ft48"><b>n<br> C</b></span></nobr></DIV>
<DIV style="position:absolute;top:821;left:490"><nobr><span class="ft49"><b>o<br>s<br>t</b></span></nobr></DIV>
<DIV style="position:absolute;top:850;left:596"><nobr><span class="ft51">Global Nt=2<br>Global Nt=4<br>Native Nt=2<br>Native Nt=4</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:548"><nobr><span class="ft46"><b>Cosine Similarity</b></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:842"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:942;left:477"><nobr><span class="ft6"><b>Figure 6: Effect of adaptation threshold on <i>min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:778"><nobr><span class="ft20"><i><b>Det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:941;left:791"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:796"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:942;left:817"><nobr><span class="ft6"><b> on </b></span></nobr></DIV>
<DIV style="position:absolute;top:957;left:477"><nobr><span class="ft6"><b>TDT3 tracking with adaptation.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:989;left:477"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:988;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:989;left:501"><nobr><span class="ft4"><b>IMPROVING NATIVE TOPIC MODELS </b></span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:477"><nobr><span class="ft13">In the previous two sections we showed that when native topic <br>models are initialized with language specific training stories that <br>are truly on-topic, then topic tracking is indeed better with native <br>models than with global models. However, in context of the TDT </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">407</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="121007.png" alt="background image">
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft13">test situation, the way we obtained our language-specific training <br>stories was cheating.  </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:81"><nobr><span class="ft13">In this section we experiment with 2 different "legal" ways to ini-<br>tialize better native language models: (1) Use both global and <br>native models, and smooth native similarity scores with global <br>similarity scores. (2) Initialize native models with dictionary or <br>other translations of the English training stories into the other <br>language. </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:81"><nobr><span class="ft13">Smoothing was carried out in the native adapted condition ac-<br>cording to Equation (11), setting <i></i>=0.5, without tuning. The com-<br>parison with unadapted and globally adapted tracking can be seen <br>in Table 9. The smoothing improves the native topic model per-<br>formance relative to unsmoothed native topic models (cf. Table <br>5), and brings the native model performance to roughly the same <br>level as the global. In other words, smoothing improves perform-<br>ance, but we still do not have strong support for the native lan-<br>guage hypothesis. This is apparent in Figure 7. Native adapted <br>tracking is not better than global adapted tracking. </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:95"><nobr><span class="ft6"><b>Table 9: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:183"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:194"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:198"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:415;left:220"><nobr><span class="ft6"><b> for TDT3 topic tracking, smoothing </b></span></nobr></DIV>
<DIV style="position:absolute;top:431;left:171"><nobr><span class="ft6"><b>native scores with global scores </b></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:85"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:203"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:212"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:215"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:357"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:367"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:369"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:470;left:217"><nobr><span class="ft6"><b>Adapted Adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:470;left:85"><nobr><span class="ft6"><b> Base-</b></span></nobr></DIV>
<DIV style="position:absolute;top:486;left:140"><nobr><span class="ft6"><b>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:486;left:190"><nobr><span class="ft6"><b>Global Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:502;left:248"><nobr><span class="ft6"><b>Smooth </b></span></nobr></DIV>
<DIV style="position:absolute;top:470;left:302"><nobr><span class="ft42"><b>Base-<br>line </b></span></nobr></DIV>
<DIV style="position:absolute;top:486;left:344"><nobr><span class="ft6"><b>Global  Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:502;left:394"><nobr><span class="ft6"><b>Smooth </b></span></nobr></DIV>
<DIV style="position:absolute;top:519;left:85"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:518;left:140"><nobr><span class="ft3">.1501 </span></nobr></DIV>
<DIV style="position:absolute;top:518;left:190"><nobr><span class="ft3">.1197 </span></nobr></DIV>
<DIV style="position:absolute;top:518;left:248"><nobr><span class="ft3">.1125 .1238  .1074 </span></nobr></DIV>
<DIV style="position:absolute;top:518;left:394"><nobr><span class="ft3">.1010 </span></nobr></DIV>
<DIV style="position:absolute;top:535;left:85"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:535;left:140"><nobr><span class="ft3">.1283 </span></nobr></DIV>
<DIV style="position:absolute;top:535;left:190"><nobr><span class="ft3">.0892 </span></nobr></DIV>
<DIV style="position:absolute;top:535;left:248"><nobr><span class="ft3">.0872 .1060  .0818 .0840 </span></nobr></DIV>
<DIV style="position:absolute;top:551;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:820;left:446"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:839;left:81"><nobr><span class="ft42"><b>Figure 7: DET curve for TDT3 tracking with smoothing, <br>cosine similarity, <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:860;left:191"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:855;left:194"><nobr><span class="ft6"><b>=4 training stories </b></span></nobr></DIV>
<DIV style="position:absolute;top:876;left:81"><nobr><span class="ft15">The final method of initializing topic models for different lan-<br>guages would be to translate the English training stories into the <br>other languages required. We did not have machine translation <br>from English into Arabic or Mandarin available for these experi-<br>ments. However, we have had success with dictionary translations <br>for Arabic. In </span></nobr></DIV>
<DIV style="position:absolute;top:953;left:161"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:957;left:161"><nobr><span class="ft3">[2] we found that dictionary translations from Ara-</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:81"><nobr><span class="ft13">bic into English resulted in comparable performance to the ma-<br>chine translations on tracking, and better performance on link <br>detection. Such translated stories would not be "native language" <br>training stories, but might be a better starting point for language-<br>specific adaptation anyway.  </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft15">Training story translations into Arabic used an English/Arabic <br>probabilistic<i> </i>dictionary derived from the Linguistic Data Consor-<br>tium's UN Arabic/English parallel corpus, developed for our <br>cross-language information retrieval work </span></nobr></DIV>
<DIV style="position:absolute;top:156;left:710"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:160;left:710"><nobr><span class="ft3">[7]. Each English word </span></nobr></DIV>
<DIV style="position:absolute;top:177;left:477"><nobr><span class="ft13">has many different Arabic translations, each with a translation <br>probability  <i>p(a|e)</i>. The Arabic words, but not the English words, <br>have been stemmed according to a light stemming algorithm. To <br>translate an English story, English stop words were removed, and <br>each English word occurrence was replaced by all of its dictionary <br>translations, weighted by their translation probabilities. Weights <br>were summed across all the occurrences of each Arabic word, and <br>the resulting Arabic term vector was truncated to retain only terms <br>above a threshold weight. We translated training stories only into <br>Arabic, because we did not have a method to produce good qual-<br>ity English to Mandarin translation. </span></nobr></DIV>
<DIV style="position:absolute;top:357;left:477"><nobr><span class="ft13">The results for Arabic can be seen in Table 10. For translation, it <br>makes sense to include an <i>unadapted native</i> condition, labeled <br><i>translated</i> in the table.  </span></nobr></DIV>
<DIV style="position:absolute;top:411;left:497"><nobr><span class="ft6"><b>Table 10: <i>Min(C</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:416;left:592"><nobr><span class="ft20"><i><b>det</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:603"><nobr><span class="ft19"><i><b>)</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:416;left:607"><nobr><span class="ft20"><i><b>Norm</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:629"><nobr><span class="ft6"><b> for Arabic TDT3 topic tracking, </b></span></nobr></DIV>
<DIV style="position:absolute;top:427;left:493"><nobr><span class="ft6"><b>initializing native topic models with dictionary-translated </b></span></nobr></DIV>
<DIV style="position:absolute;top:442;left:614"><nobr><span class="ft6"><b>training stories  </b></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:510"><nobr><span class="ft6"><b> Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:693"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:470;left:703"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:705"><nobr><span class="ft6"><b>=2 </b></span></nobr></DIV>
<DIV style="position:absolute;top:481;left:598"><nobr><span class="ft6"><b>Unadapted Adapted </b></span></nobr></DIV>
<DIV style="position:absolute;top:481;left:510"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:566"><nobr><span class="ft6"><b>Baseline  Translated Global  Native </b></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:801"><nobr><span class="ft6"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:515;left:510"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:566"><nobr><span class="ft3">.2023 .2219 </span></nobr></DIV>
<DIV style="position:absolute;top:514;left:703"><nobr><span class="ft3">.1694 .2209 </span></nobr></DIV>
<DIV style="position:absolute;top:531;left:510"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:531;left:566"><nobr><span class="ft3">.1884 </span></nobr></DIV>
<DIV style="position:absolute;top:531;left:626"><nobr><span class="ft3">.1625 .1356 </span></nobr></DIV>
<DIV style="position:absolute;top:531;left:760"><nobr><span class="ft3">.1613 </span></nobr></DIV>
<DIV style="position:absolute;top:548;left:510"><nobr><span class="ft6"><b> Arabic </b></span></nobr></DIV>
<DIV style="position:absolute;top:548;left:693"><nobr><span class="ft19"><i><b>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:553;left:703"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:548;left:705"><nobr><span class="ft6"><b>=4 </b></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:510"><nobr><span class="ft6"><b>Cosine </b></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:566"><nobr><span class="ft3">.1794 </span></nobr></DIV>
<DIV style="position:absolute;top:564;left:626"><nobr><span class="ft3">.1640 .1558 </span></nobr></DIV>
<DIV style="position:absolute;top:564;left:760"><nobr><span class="ft3">1655 </span></nobr></DIV>
<DIV style="position:absolute;top:581;left:510"><nobr><span class="ft6"><b>RM </b></span></nobr></DIV>
<DIV style="position:absolute;top:580;left:566"><nobr><span class="ft3">.1581 </span></nobr></DIV>
<DIV style="position:absolute;top:580;left:626"><nobr><span class="ft3">.1316 .1206 </span></nobr></DIV>
<DIV style="position:absolute;top:580;left:760"><nobr><span class="ft3">.1325 </span></nobr></DIV>
<DIV style="position:absolute;top:597;left:477"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:875;left:842"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:477"><nobr><span class="ft42"><b>Figure 8: DET curve for TDT3 tracking initializing native <br>topics with dictionary-translated training stories, cosine <br>similarity, <i>N</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:931;left:549"><nobr><span class="ft20"><i><b>t</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:926;left:551"><nobr><span class="ft6"><b>=4, Arabic only </b></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:477"><nobr><span class="ft13">The results are mixed. First of all, this case is unusual in that <br>adaptation does not improve translated models. Further analysis <br>revealed that very little adaptation was taking place. Because of <br>this lack of native adaptation, global adaptation consistently out-<br>performed native adaptation here. However, in the unadapted <br>conditions, translated training stories outperformed the global <br>models for Arabic in three of the four cases - cosine <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:795"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:797"><nobr><span class="ft3">=4 and </span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:477"><nobr><span class="ft3">relevance models for <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:609"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:611"><nobr><span class="ft3">=2 and <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:665"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:668"><nobr><span class="ft3">=4 (the shaded baseline-trans-</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">408</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft52{font-size:12px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="121008.png" alt="background image">
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft3">lated pairs in Table 10). The DET curve for the cosine <i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:397"><nobr><span class="ft31"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:111;left:399"><nobr><span class="ft3">=4 case </span></nobr></DIV>
<DIV style="position:absolute;top:127;left:81"><nobr><span class="ft13">can be seen in Figure 8. The native unadapted curve is better <br>(lower) than the global unadapted curve. </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft13">The translated stories were very different from the test stories, so <br>their similarity scores almost always fell below the adaptation <br>threshold. We believe the need to normalize scores between native <br>stories and dictionary translations is part of the problem, but we <br>also need to investigate the compatibility of the dictionary trans-<br>lations with the native Arabic stories. </span></nobr></DIV>
<DIV style="position:absolute;top:277;left:81"><nobr><span class="ft4"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:276;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:277;left:105"><nobr><span class="ft4"><b>CONCLUSIONS </b></span></nobr></DIV>
<DIV style="position:absolute;top:296;left:81"><nobr><span class="ft13">We have confirmed the native language hypothesis for story link <br>detection. For topic tracking, the picture is more complicated. <br>When native language training stories are available, good native <br>language topic models can be built for tracking stories in their <br>original language. Smoothing the native models with global mod-<br>els improves performance slightly. However, if training stories are <br>not available in the different languages, it is difficult to form na-<br>tive models by adaptation or by translation of training stories, <br>which perform better than the adapted global models. </span></nobr></DIV>
<DIV style="position:absolute;top:445;left:81"><nobr><span class="ft15">Why should language specific comparisons be more accurate than <br>comparisons based on machine translation? Machine translations <br>are not always good translations. If the translation distorts the <br>meaning of the original story, it is unlikely to be similar to the <br>topic model, particularly if proper names are incorrect, or spelled <br>differently in the machine translations than they are in the English <br>training stories, a common problem in English translations from <br>Mandarin or Arabic. Secondly, even if the translations are correct, <br>the choice of words, and hence the language models, are likely to <br>be different across languages. The second problem could be han-<br>dled by normalizing for source language, as in </span></nobr></DIV>
<DIV style="position:absolute;top:600;left:385"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:604;left:385"><nobr><span class="ft3">[12]. But </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:81"><nobr><span class="ft3">normalization cannot compensate for poor translation.  </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:81"><nobr><span class="ft13">We were surprised that translating the training stories into Arabic <br>to make Arabic topic models did not improve tracking, but again, <br>our dictionary based translations of the topic models were differ-<br>ent from native Arabic stories. We intend to try the same experi-<br>ment with manual translations of the training stories into Arabic <br>and Mandarin.  We are also planning to investigate the best way <br>to normalize scores for different languages. When TDT4 rele-<br>vance judgments are available we intend to replicate some of <br>these experiments on TDT4 data. </span></nobr></DIV>
<DIV style="position:absolute;top:802;left:81"><nobr><span class="ft4"><b>9.</b></span></nobr></DIV>
<DIV style="position:absolute;top:801;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:802;left:105"><nobr><span class="ft4"><b>ACKNOWLEDGMENTS </b></span></nobr></DIV>
<DIV style="position:absolute;top:821;left:81"><nobr><span class="ft13">This work was supported in part by the Center for Intelligent In-<br>formation Retrieval and in part by SPAWARSYSCEN-SD grant <br>number N66001-02-1-8903. Any opinions, findings and conclu-<br>sions or recommendations expressed in this material are the au-<br>thor(s) and do not necessarily reflect those of the sponsor. </span></nobr></DIV>
<DIV style="position:absolute;top:917;left:81"><nobr><span class="ft4"><b>10.</b></span></nobr></DIV>
<DIV style="position:absolute;top:916;left:104"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:917;left:114"><nobr><span class="ft4"><b>REFERENCES </b></span></nobr></DIV>
<DIV style="position:absolute;top:937;left:88"><nobr><span class="ft52">[1]</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:106"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:938;left:115"><nobr><span class="ft13">Allan, J. Introduction to topic detection and tracking. In <br><i>Topic detection and tracking: Event-based information or-<br>ganization</i>, J. Allan (ed.): Kluwer Academic Publishers, 1-<br>16, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:112;left:484"><nobr><span class="ft52">[2]</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:113;left:511"><nobr><span class="ft13">Allan, J. Bolivar, A., Connell, M., Cronen-Townsend, S., <br>Feng, A, Feng, F., Kumaran, G., Larkey, L., Lavrenko, V., <br>Raghavan, H. UMass TDT 2003 Research Summary.  In <br><i>Proceedings of TDT 2003 evaluation</i>, unpublished, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:183;left:484"><nobr><span class="ft52">[3]</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:184;left:511"><nobr><span class="ft13">Chen, H.-H. and Ku, L. W. An NLP &amp; IR approach to topic <br>detection. In <i>Topic detection and tracking: Event-based <br>information organization</i>, J. Allan (ed.). Boston, MA: Klu-<br>wer, 243-264, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:484"><nobr><span class="ft52">[4]</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:511"><nobr><span class="ft13">Chen, Y.-J. and Chen, H.-H. Nlp and IR approaches to <br>monolingual and multilingual link detection. Presented at <br>Proceedings of 19th International Conference on Computa-<br>tional Linguistics, Taipei, Taiwan, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:484"><nobr><span class="ft52">[5]</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:511"><nobr><span class="ft13">Fiscus, J. G. and Doddington, G. R. Topic detection and <br>tracking evaluation overview. In <i>Topic detection and <br>tracking: Event-based information organization</i>, J. Allan <br>(ed.). Boston, MA: Kluwer, 17-32, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:395;left:484"><nobr><span class="ft52">[6]</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:396;left:511"><nobr><span class="ft13">Krovetz, R. Viewing morphology as an inference process. <br>In <i>Proceedings of SIGIR '93</i>, 191-203, 1993. </span></nobr></DIV>
<DIV style="position:absolute;top:434;left:484"><nobr><span class="ft52">[7]</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:435;left:511"><nobr><span class="ft14">Larkey, Leah S. and Connell, Margaret E. (2003) Structured <br>Queries, Language Modeling, and Relevance Modeling in <br>Cross-Language Information Retrieval. To appear in <br><i>Information Processing and Management Special Issue on <br>Cross Language Information Retrieval</i>, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:484"><nobr><span class="ft52">[8]</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:521;left:511"><nobr><span class="ft13">Larkey, L. S., Ballesteros, L., and Connell, M. E. Improving <br>stemming for Arabic information retrieval: Light stemming <br>and co-occurrence analysis. In Proceedings of <i>SIGIR 2002,</i> <br>275-282, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:484"><nobr><span class="ft52">[9]</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:501"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:592;left:511"><nobr><span class="ft13">Lavrenko, V. and Croft, W. B. Relevance-based language <br>models. In <i>Proceedings of SIGIR 2001.</i> New Orleans: <br>ACM, 120-127, 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:646;left:484"><nobr><span class="ft52">[10]</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:509"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:511"><nobr><span class="ft13">Lavrenko, V. and Croft, W. B. Relevance models in <br>information retrieval. In <i>Language modeling for informa-<br>tion retrieval</i>, W. B. Croft and J. Lafferty (eds.). Boston: <br>Kluwer, 11-56, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:484"><nobr><span class="ft52">[11]</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:509"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:718;left:511"><nobr><span class="ft14">Lavrenko, V., Allan, J., DeGuzman, E., LaFlamme, D., Pol-<br>lard, V., and Thomas, S. Relevance models for topic detec-<br>tion and tracking. In <i>Proceedings of the Conference on <br>Human Language Technology</i>, 104-110, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:788;left:484"><nobr><span class="ft52">[12]</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:509"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:789;left:511"><nobr><span class="ft13">Leek, T., Schwartz, R. M., and Sista, S. Probabilistic ap-<br>proaches to topic detection and tracking. In <i>Topic detection <br>and tracking: Event-based information organization</i>, J. <br>Allan (ed.). Boston, MA: Kluwer, 67-83, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:858;left:484"><nobr><span class="ft52">[13]</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:509"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:511"><nobr><span class="ft13">Levow, G.-A. and Oard, D. W. Signal boosting for translin-<br>gual topic tracking: Document expansion and n-best trans-<br>lation. In <i>Topic detection and tracking: Event-based infor-<br>mation organization</i>, J. Allan (ed.). Boston, MA: Kluwer, <br>175-195, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:945;left:484"><nobr><span class="ft52">[14]</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:509"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:946;left:511"><nobr><span class="ft13">Oard, D. W. <i>Adaptive vector space text filtering for <br>monolingual and cross-language applications</i>. PhD dis-<br>sertation, University of Maryland, College Park, 1996. <br>http://www.glue.umd.edu/~dlrg/filter/papers/thesis.ps.gz</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:81"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft3">409</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
