<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>p42-barbara.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2002-10-23T10:38:58+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:6px;font-family:Times;color:#000000;}
	.ft2{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:15px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:9px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Courier;color:#000000;}
	.ft9{font-size:11px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft11{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
	.ft12{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="58001.png" alt="background image">
<DIV style="position:absolute;top:108;left:109"><nobr><span class="ft0"><b>COOLCAT: An entropy-based algorithm for categorical</b></span></nobr></DIV>
<DIV style="position:absolute;top:138;left:390"><nobr><span class="ft0"><b>clustering</b></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:525"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:178"><nobr><span class="ft2">Daniel Barbar ´a</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:154"><nobr><span class="ft3">George Mason University</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:184"><nobr><span class="ft3">ISE Department</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:206"><nobr><span class="ft3">MSN 4A4</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:179"><nobr><span class="ft3">Fairfax, VA 22030</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:158"><nobr><span class="ft2">dbarbara@gmu.edu</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:413"><nobr><span class="ft2">Julia Couto</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:370"><nobr><span class="ft3">James Madison University</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:400"><nobr><span class="ft3">ISAT Department</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:411"><nobr><span class="ft3">P.O. Box 1212</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:378"><nobr><span class="ft3">Harrisonburg, VA 22807</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:390"><nobr><span class="ft2">coutoji@jmu.edu</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:659"><nobr><span class="ft2">Yi Li</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:593"><nobr><span class="ft3">George Mason University</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:623"><nobr><span class="ft3">ISE Department</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:645"><nobr><span class="ft3">MSN 4A4</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:618"><nobr><span class="ft3">Fairfax, VA 22030</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:619"><nobr><span class="ft2">yli1@gmu.edu</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:81"><nobr><span class="ft4"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:373;left:81"><nobr><span class="ft10">In this paper we explore the connection between clustering<br>categorical data and entropy: clusters of similar poi lower<br>entropy than those of dissimilar ones. We use this connec-<br>tion to design an incremental heuristic algorithm, COOL-<br>CAT, which is capable of efficiently clustering large data sets<br>of records with categorical attributes, and data streams. In<br>contrast with other categorical clustering algorithms pub-<br>lished in the past, COOLCAT's clustering results are very<br>stable for different sample sizes and parameter settings. Also,<br>the criteria for clustering is a very intuitive one, since it is<br>deeply rooted on the well-known notion of entropy. Most<br>importantly, COOLCAT is well equipped to deal with clus-<br>tering of data streams (continuously arriving streams of data<br>point) since it is an incremental algorithm capable of clus-<br>tering new points without having to look at every point that<br>has been clustered so far. We demonstrate the efficiency and<br>scalability of COOLCAT by a series of experiments on real<br>and synthetic data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:81"><nobr><span class="ft4"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:81"><nobr><span class="ft10">I.5.3 [Computing Methodologies]: Pattern Recognition--<br>Clustering</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:81"><nobr><span class="ft4"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:768;left:81"><nobr><span class="ft5">Entropy</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft4"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:824;left:81"><nobr><span class="ft5">clustering, entropy, data streams</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:81"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:856;left:112"><nobr><span class="ft4"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:878;left:94"><nobr><span class="ft5">Clustering is a widely used technique in which data points</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:81"><nobr><span class="ft5">are partitioned into groups, in such a way that points in the</span></nobr></DIV>
<DIV style="position:absolute;top:912;left:81"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:917;left:88"><nobr><span class="ft5">This work has been supported by NSF grant IIS-0208519</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft11">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>CIKM </i>'02, November 4-9, 2002, McLean, VA, USA.<br>Copyright 2002 ACM 1-58113-492-4/02/0011 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:316"><nobr><span class="ft5">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:323"><nobr><span class="ft6">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:475"><nobr><span class="ft10">same group, or cluster, are more similar among themselves<br>than to those in other clusters. Clustering of categorical<br>attributes (i.e., attributes whose domain is not numeric) is<br>a difficult, yet important task: many fields, from statistics<br>to psychology deal with categorical data.</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:747"><nobr><span class="ft5">In spite of its</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:475"><nobr><span class="ft10">importance, the task of categorical clustering has received<br>scant attention in the KDD community as of late, with only<br>a handful of publications addressing the problem ([18, 14,<br>12]).</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:489"><nobr><span class="ft5">Much of the published algorithms to cluster categorical</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:475"><nobr><span class="ft10">data rely on the usage of a distance metric that captures<br>the separation between two vectors of categorical attributes,<br>such as the Jaccard coefficient. In this paper, we present<br>COOLCAT (the name comes from the fact that we reduce<br>the entropy of the clusters, thereby "cooling" them), a novel<br>method which uses the notion of entropy to group records.<br>We argue that a classical notion such as entropy is a more<br>natural and intuitive way of relating records, and more im-<br>portantly does not rely in arbitrary distance metrics. COOL-<br>CAT is an incremental algorithm that aims to minimize<br>the expected entropy of the clusters. Given a set of clus-<br>ters, COOLCAT will place the next point in the cluster<br>where it minimizes the overall expected entropy. COOL-<br>CAT acts incrementally, and it is capable to cluster every<br>new point without having to re-process the entire set. There-<br>fore, COOLCAT is suited to cluster data streams (contin-<br>uosly incoming data points) [2].</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:687"><nobr><span class="ft5">This makes COOLCAT</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:475"><nobr><span class="ft10">applicable in a large variety of emerging applications such<br>as intrusion detection, and e-commerce data.</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:489"><nobr><span class="ft5">This paper is set up as follows. Section 2 offers the back-</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:475"><nobr><span class="ft10">ground and relationship between entropy and clustering, and<br>formulates the problem. Section 3 reviews the related work.<br>Section 4 describes COOLCAT, our algorithm. Section 5<br>presents the experimental evidence that demonstrates the<br>advantages of COOLCAT. Finally, Section 6 presents con-<br>clusions and future work.</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:475"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:931;left:506"><nobr><span class="ft12"><b>BACKGROUND AND PROBLEM FOR-<br>MULATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:974;left:489"><nobr><span class="ft5">In this section, we present the background of entropy and</span></nobr></DIV>
<DIV style="position:absolute;top:990;left:475"><nobr><span class="ft5">clustering and formulate the problem.</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:475"><nobr><span class="ft4"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:515"><nobr><span class="ft4"><b>Entropy and Clustering</b></span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:489"><nobr><span class="ft5">Entropy is the measure of information and uncertainty of</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:475"><nobr><span class="ft10">a random variable [28]. Formally, if X is a random variable,<br>S(X) the set of values that X can take, and p(x) the prob-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">582</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
	.ft14{font-size:15px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="58002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">ability function of X, the entropy E(X) is defined as shown<br>in Equation 1.</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:151"><nobr><span class="ft5">E(X) =</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:216"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:230"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:241"><nobr><span class="ft5"> S(X)</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:289"><nobr><span class="ft5">p(x)log(p(x))</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:422"><nobr><span class="ft5">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:94"><nobr><span class="ft5">The entropy of a multivariate vector ^</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:320"><nobr><span class="ft5">x =</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:356"><nobr><span class="ft5">{X</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:374"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:380"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:386"><nobr><span class="ft5">· · · , X</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:425"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:432"><nobr><span class="ft5">}</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:81"><nobr><span class="ft5">can be computed as shown in Equation 2, where p(^</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:396"><nobr><span class="ft5">x) =</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:81"><nobr><span class="ft5">p(x</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:101"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:107"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:113"><nobr><span class="ft5">· · · , x</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:148"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:156"><nobr><span class="ft5">) is the multivariate probability distribution.</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:123"><nobr><span class="ft5">E(^</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:140"><nobr><span class="ft5">x)</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:156"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:169"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:177"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:187"><nobr><span class="ft5"> S(X</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:226"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:233"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:240"><nobr><span class="ft5">...</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:254"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:262"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:273"><nobr><span class="ft5"> S(X</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:312"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:320"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:328"><nobr><span class="ft5">p(^</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:340"><nobr><span class="ft5">x)logp(^</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:384"><nobr><span class="ft5">x)</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:422"><nobr><span class="ft5">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:94"><nobr><span class="ft5">Entropy is sometimes referred to as a measure of the</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:81"><nobr><span class="ft10">amount of "disorder" in a system. A room with socks strewn<br>all over the floor has more entropy than a room in which<br>socks are paired up, neatly folded, and placed in one side of<br>your sock and underwear drawer.</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:81"><nobr><span class="ft4"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:384;left:121"><nobr><span class="ft4"><b>Problem formulation</b></span></nobr></DIV>
<DIV style="position:absolute;top:406;left:94"><nobr><span class="ft5">The problem we are trying to solve can be formulated as</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:81"><nobr><span class="ft5">follows. Given a data set D of N points ^</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:332"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:339"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:345"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:351"><nobr><span class="ft5">· · · , ^</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:378"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:385"><nobr><span class="ft1">N</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:395"><nobr><span class="ft5">, where</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:81"><nobr><span class="ft10">each point is a multidimensional vector of d categorical at-<br>tributes, i.e., ^</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:162"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:169"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:183"><nobr><span class="ft5">= (p</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:214"><nobr><span class="ft13">1<br>j</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:221"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:227"><nobr><span class="ft5">· · · , p</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:260"><nobr><span class="ft1">d</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:260"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:267"><nobr><span class="ft5">), and given an integer k, we</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft5">would like to separate the points into k groups C</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:380"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:386"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:392"><nobr><span class="ft5">· · · , C</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:429"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:436"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:81"><nobr><span class="ft10">or clusters, in such a way that we minimize the entropy of<br>the whole arrangement. Unfortunately, this problem is NP-<br>Complete, and moreover, difficult to approximate [13]. In<br>fact, the problem is NP-Complete for any distance function<br>d(x, y), defined over pairs of points x, y, such that the func-<br>tion maps pairs of points to real numbers (and hence, our<br>entropy function qualifies), therefore we need to resort to<br>heuristics to solve it.</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:94"><nobr><span class="ft5">We first have to resolve the issue of what we mean by the</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:81"><nobr><span class="ft10">"whole entropy of the system." In other words, we have<br>to make our objective function clear. We aim to minimize<br>the expected entropy, whose expression is shown in Equa-<br>tion 3, where E(C</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:192"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:199"><nobr><span class="ft5">),</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:210"><nobr><span class="ft5">· · · , E(C</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:263"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:270"><nobr><span class="ft5">), represent the entropies of</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:81"><nobr><span class="ft5">each cluster, C</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:175"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:186"><nobr><span class="ft5">denotes the points assigned to cluster i,</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:81"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:91"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:106"><nobr><span class="ft5"> D, with the property that C</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:305"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:319"><nobr><span class="ft5"> C</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:347"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:364"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:386"><nobr><span class="ft5">, for all</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:81"><nobr><span class="ft5">i, j</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:110"><nobr><span class="ft5">= 1, .., k i = j. The symbol </span></nobr></DIV>
<DIV style="position:absolute;top:721;left:315"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:337"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:360"><nobr><span class="ft5">{C</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:376"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:383"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:389"><nobr><span class="ft5">· · · , C</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:426"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:432"><nobr><span class="ft5">}</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:81"><nobr><span class="ft5">represents the clustering.</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:178"><nobr><span class="ft5">¯</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:175"><nobr><span class="ft5">E( </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:191"><nobr><span class="ft5">C) =</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:244"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:257"><nobr><span class="ft5">( |C</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:278"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:285"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:267"><nobr><span class="ft5">|D| (E(C</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:322"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:329"><nobr><span class="ft5">)))</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:422"><nobr><span class="ft5">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:94"><nobr><span class="ft5">This function, as we will see later, allows us to imple-</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:81"><nobr><span class="ft10">ment an incremental algorithm that can effectively deal with<br>large datasets, since we do not need to look at the entire set<br>of points to decide about the entropy of an arrangement.<br>Rather, we will be able to decide for each point, how it<br>would affect the entropy of each of the existing clusters if<br>placed in each one of them.</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:94"><nobr><span class="ft5">The solution we propose in this paper (and present in Sec-</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:81"><nobr><span class="ft10">tion 4) is a heuristic based in finding a set of initial clusters<br>(using the entropic criteria), and then incrementally (greed-<br>ily) add points to the clusters according to a criteria that<br>minimizes Equation 3.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:94"><nobr><span class="ft5">Furthermore, we make a simplification in the computation</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft10">of entropy of a set of records. We assume independence of<br>the attributes of the record, transforming Equation 2 into<br>Equation 5.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:166"><nobr><span class="ft5">In other words, the joint probability of the</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft5">combined attribute values becomes the product of the prob-</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:575"><nobr><span class="ft5">members</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:723"><nobr><span class="ft5">E</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:774"><nobr><span class="ft10">Exp.<br>En-<br>tropy</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:493"><nobr><span class="ft5">Cluster0</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:575"><nobr><span class="ft5">{"red", "heavy"}</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:723"><nobr><span class="ft5">1.0</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:774"><nobr><span class="ft5">0.66</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:575"><nobr><span class="ft5">{"red", "medium"}</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:493"><nobr><span class="ft5">Cluster1</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:575"><nobr><span class="ft5">{"blue", "light"}</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:723"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:493"><nobr><span class="ft5">Cluster0</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:575"><nobr><span class="ft5">{"red", "heavy"}</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:723"><nobr><span class="ft5">2.0</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:774"><nobr><span class="ft5">1.33</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:575"><nobr><span class="ft5">{"blue", "light"}</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:493"><nobr><span class="ft5">Cluster1</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:575"><nobr><span class="ft5">{"red", "medium"}</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:723"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:493"><nobr><span class="ft5">Cluster0</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:575"><nobr><span class="ft5">{"red", heavy"}</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:723"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:774"><nobr><span class="ft5">1.33</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:493"><nobr><span class="ft5">Cluster1</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:575"><nobr><span class="ft5">{"red", "medium"}</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:723"><nobr><span class="ft5">2.0</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:575"><nobr><span class="ft5">{"blue", "light"}</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:475"><nobr><span class="ft5">Figure 1:</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:555"><nobr><span class="ft5">Three different clusterings for the set</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:475"><nobr><span class="ft5">v</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:482"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:488"><nobr><span class="ft5">, v</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:501"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:507"><nobr><span class="ft5">, v</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:520"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:526"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:543"><nobr><span class="ft5">Clustering 1 minimizes the expected en-</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:475"><nobr><span class="ft5">tropy of the two clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:475"><nobr><span class="ft10">abilities of each attribute, and hence the entropy can be<br>calculated as the sum of entropies of the attributes.</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:526"><nobr><span class="ft5">E(^</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:542"><nobr><span class="ft5">x)</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:568"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:593"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:606"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:614"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:624"><nobr><span class="ft5"> S(X</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:663"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:669"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:677"><nobr><span class="ft5">· · ·</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:695"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:703"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:714"><nobr><span class="ft5"> S(X</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:753"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:761"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:816"><nobr><span class="ft5">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:600"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:610"><nobr><span class="ft5">(p(x</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:636"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:641"><nobr><span class="ft5">))log(</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:682"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:695"><nobr><span class="ft5">p(x</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:715"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:720"><nobr><span class="ft5">))</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:568"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:597"><nobr><span class="ft5">E(X</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:625"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:631"><nobr><span class="ft5">) + E(X</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:681"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:687"><nobr><span class="ft5">) +</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:710"><nobr><span class="ft5">· · · + E(X</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:771"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:778"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:816"><nobr><span class="ft5">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:489"><nobr><span class="ft5">Assume that we have a set of three records, v</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:760"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:774"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:793"><nobr><span class="ft5">{"red"</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:475"><nobr><span class="ft5">, "heavy"</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:531"><nobr><span class="ft5">}, v</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:555"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:575"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:599"><nobr><span class="ft5">{"blue", "light"}, and v</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:746"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:765"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:789"><nobr><span class="ft5">{"red",</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:475"><nobr><span class="ft5">"medium"</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:540"><nobr><span class="ft5">}, and we want to form two clusters with them.</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:475"><nobr><span class="ft10">Figure 1 shows all the possible arrangements, with the en-<br>tropy of each cluster, and the expected entropy in each ar-<br>rangement. As we can see, the minimum expected entropy<br>is that of arrangement 1, which obviously is the correct way<br>of clustering the records (using two clusters).</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:489"><nobr><span class="ft5">Even though the assumption of attribute independence</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:475"><nobr><span class="ft10">is not true in every data set, it proves to work very well<br>in practice (as shall be shown in the experimental section<br>of this paper). Moreover, in the cases we can demonstrate<br>that there is a correlation between two or more attributes of<br>the data set, we can always change the data points by creat-<br>ing attributes that reflect these correlations and then apply<br>Equation 5 to compute the join entropy. For instance, if the<br>data set is composed of records of attributes A, B, C, D, E, F<br>and we know that (A, B), (A, C) and (E, F ) are correlated.<br>we can convert the data set into one having records with<br>attributes AB, AC, D, EF and compute the entropy assum-<br>ing that these new attributes are independent. Notice that<br>for the grouped attributes, we are in effect computing their<br>joint probabilities. The correlations between attributes can<br>be easily found by techniques such as the Chi-Square and<br>likelihood ratio tests. In our experimental experience, the<br>gains obtained by doing this are small enough to justify the<br>usage of the independence assumption.</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:475"><nobr><span class="ft4"><b>2.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:980;left:515"><nobr><span class="ft14"><b>Expected entropy and the Minimum De-<br>scription Length principle</b></span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:489"><nobr><span class="ft5">The Minimum Description Length principle (MDL) [26,</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft10">27] recommends choosing the model that minimizes the sum<br>of the model's algorithmic complexity and the description of<br>the data with respect to that model. This principle is widely</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">583</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft15{font-size:11px;line-height:18px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="58003.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">used to compare classifiers (see [23]) but it has not been used<br>much to deal with clustering.</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:94"><nobr><span class="ft5">Formally, the complexity of the model can be stated as</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:81"><nobr><span class="ft10">shown in Equation 6, where K() indicates the complexity, h<br>is the model, and D denotes the data set. The term K(h)<br>denotes the complexity of the model, or model encoding,<br>while K(D using h) is the complexity of the data encoding<br>with respect to the chosen model.</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:156"><nobr><span class="ft5">K(h, D) = K(h) + K(D using h)</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:422"><nobr><span class="ft5">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:94"><nobr><span class="ft5">Consider first the term K(h). To encode the model, we</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:81"><nobr><span class="ft10">need to encode for each cluster the probability distribution<br>for the attribute values. This can be done by encoding the<br>number of times each attribute value appears in the cluster,<br>and the number of points in each cluster. Assuming that<br>there are d attributes in the data, and that attribute A</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:434"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:81"><nobr><span class="ft5">can assume v</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:162"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:174"><nobr><span class="ft5">different values. As usual, k represents the</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:81"><nobr><span class="ft10">number of clusters in the model. K(h) can be written as<br>shown in Equation 7. In each cluster i, we need to encode<br>c</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:87"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:102"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:141"><nobr><span class="ft13">d-1<br>j=0</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:164"><nobr><span class="ft5">v</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:170"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:182"><nobr><span class="ft5">values. So, the total number of values we</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:81"><nobr><span class="ft5">need to encode is</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:201"><nobr><span class="ft13">k-1<br>i=0</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:224"><nobr><span class="ft5">c</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:230"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:242"><nobr><span class="ft5">= k, where  is a constant. We</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:81"><nobr><span class="ft10">also need to encode the number of points in each cluster, or<br>k values. The number of bits needed to encode the number<br>of times each attribute value occurs in the cluster, or the<br>number of points in a cluster is equal to log(</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:353"><nobr><span class="ft5">|D|), since the</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:81"><nobr><span class="ft10">maximum number for these values is the size of the entire<br>data set. Therefore K(h) is a linear function of k, with <br>a constant that represents all the contributions described<br>above.</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:198"><nobr><span class="ft5">K(h) = klog(</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:297"><nobr><span class="ft5">|D|)</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:422"><nobr><span class="ft5">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:94"><nobr><span class="ft5">On the other hand, the encoding of the data given the</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:81"><nobr><span class="ft5">model can be stated as shown in Equation 8.</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:383"><nobr><span class="ft5">Once the</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:81"><nobr><span class="ft10">probabilities of occurrence of each attribute value in each<br>cluster are known, an optimal code (Huffman) can be chosen<br>to represent each attribute value in the cluster. Each point<br>is simply represented by the encoding of its attributes' val-<br>ues. The optimal code is achieved by giving to each value a<br>number of bits proportional to log(Pijl), where P (ijl) is the<br>probability that the l value of attribute j occurs in cluster i.<br>The second term in the equation simply indicates the mem-<br>bership of all the points, needing log(k) for the encoding of<br>the individual memberships.</span></nobr></DIV>
<DIV style="position:absolute;top:839;left:83"><nobr><span class="ft5">K(D using h) =</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:197"><nobr><span class="ft1">k-1</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:198"><nobr><span class="ft1">i=0</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:222"><nobr><span class="ft5">|C</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:235"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:240"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:223"><nobr><span class="ft5">|D|</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:248"><nobr><span class="ft1">d-1</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:248"><nobr><span class="ft1">j=0</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:270"><nobr><span class="ft1">v-1</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:271"><nobr><span class="ft1">l=0</span></nobr></DIV>
<DIV style="position:absolute;top:839;left:293"><nobr><span class="ft5">Pijllog(Pijl) + Dlog(k)</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:422"><nobr><span class="ft5">(8)</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:94"><nobr><span class="ft5">Noticing that the first term of Equation 8 is simply the</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:81"><nobr><span class="ft10">expected entropy of the clustering, we can write K(h, D) as<br>shown in Equation 9. Notice that for a fixed k, the MDL<br>principle indicates that the best model can be found by min-<br>imizing the expected entropy of the clustering, which is pre-<br>cisely our goal.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:136"><nobr><span class="ft5">K(h, D) = log(</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:245"><nobr><span class="ft5">|D|) + Dlog(k) + ¯</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:352"><nobr><span class="ft5">E( </span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:368"><nobr><span class="ft5">C)</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:422"><nobr><span class="ft5">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:81"><nobr><span class="ft4"><b>2.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:121"><nobr><span class="ft4"><b>Evaluating clustering results</b></span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:94"><nobr><span class="ft5">A frequent problem one encounters when applying cluster-</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft5">ing algorithms in practice is the difficulty in evaluating the</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft10">solutions. Different clustering algorithms (and sometimes<br>multiple applications of the same algorithm using slight vari-<br>ations of initial conditions or parameters) result in very dif-<br>ferent solutions, all of them looking plausible. This stems<br>from the fact that there is no unifying criteria to define clus-<br>ters, and more often than not, the final clusters found by the<br>algorithm are in fact the ones that correspond to the criteria<br>used to drive the algorithm. Methods to evaluate whether<br>or not the structure found is a property of the data set and<br>not one imposed by the algorithm are needed.</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:489"><nobr><span class="ft5">Authors have pondered about good ways to validate clus-</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:475"><nobr><span class="ft10">ters found by algorithms (e.g., see [21, 1]). Two widely used<br>methods are the following:</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:495"><nobr><span class="ft5">· Significance Test on External Variables This technique</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:509"><nobr><span class="ft10">calls for the usage of significance tests that compare<br>the clusters on variables not used to generate them.<br>One way of doing this is to compute the entropy of<br>the solution using a variable that did not participate<br>in the clustering. (A class attribute.) The entropy of<br>an attribute C in a cluster C</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:688"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:700"><nobr><span class="ft5">is computed as shown</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:509"><nobr><span class="ft5">in Equation 10, where V</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:658"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:669"><nobr><span class="ft5">denotes one of the possible</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:509"><nobr><span class="ft10">values that C can take. The evaluation is performed<br>by computing the expected entropy (taken into con-<br>sideration the cluster sizes). The smaller the value of<br>E(C</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:535"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:542"><nobr><span class="ft5">), the better the clustering fares.</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:531"><nobr><span class="ft5">E(C</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:557"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:564"><nobr><span class="ft5">) =</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:607"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:621"><nobr><span class="ft5">P (C = V</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:684"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:690"><nobr><span class="ft5">)logP (C = V</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:776"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:782"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:809"><nobr><span class="ft5">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:495"><nobr><span class="ft5">· The category utility function The category utility (CU)</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:509"><nobr><span class="ft10">function [15] attempts to maximize both the probabil-<br>ity that two objects in the same cluster have attribute<br>values in common and the probability that objects<br>from different clusters have different attributes. The<br>expression to calculate the expected value of the CU<br>function is shown in Equation 11, where P (A</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:773"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:785"><nobr><span class="ft5">= V</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:810"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:820"><nobr><span class="ft5">|C</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:834"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:841"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:509"><nobr><span class="ft10">is the conditional probability that the attribute i has<br>the value V</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:577"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:592"><nobr><span class="ft5">given the cluster C</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:707"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:714"><nobr><span class="ft5">, and P (A</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:777"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:791"><nobr><span class="ft5">= V</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:819"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:829"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:509"><nobr><span class="ft10">is the overall probability of the attribute i having the<br>value V</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:555"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:571"><nobr><span class="ft5">(in the entire set).</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:700"><nobr><span class="ft5">The function aims to</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:509"><nobr><span class="ft10">measure if the clustering improves the likelihood of<br>similar values falling in the same cluster. Obviously,<br>the higher the value of CU, the better the clustering<br>fares.</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:535"><nobr><span class="ft5">CU</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:575"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:613"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:637"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:647"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:636"><nobr><span class="ft5">|D|</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:673"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:695"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:604"><nobr><span class="ft5">[P (A</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:634"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:643"><nobr><span class="ft5">= V</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:665"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:675"><nobr><span class="ft5">|C</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:689"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:696"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:701"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:710"><nobr><span class="ft5">- P (A</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:751"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:759"><nobr><span class="ft5">= V</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:782"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:792"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:797"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:804"><nobr><span class="ft5">] (11)</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:489"><nobr><span class="ft5">We have used both techniques in validating our results,</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:475"><nobr><span class="ft5">as shall be seen in the experimental section.</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:475"><nobr><span class="ft4"><b>2.5</b></span></nobr></DIV>
<DIV style="position:absolute;top:951;left:515"><nobr><span class="ft4"><b>Number of clusters</b></span></nobr></DIV>
<DIV style="position:absolute;top:973;left:489"><nobr><span class="ft5">The issue of choosing the number of clusters is one com-</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:475"><nobr><span class="ft10">mon to all clustering methods, and our technique is no ex-<br>ception. Many methods have been proposed for determin-<br>ing the right number of clusters (e.g.,[4, 9]). Unfortunately<br>many of these methods (e.g., [4]) assume that it is possible<br>to compute a centroid for each cluster, which in categorical<br>data is not easy. We consider this issue out of the scope of</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">584</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:5px;font-family:Times;color:#000000;}
	.ft17{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="58004.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">this paper since we plan to examine good ways of selecting<br>the optimal number of clusters in the context of our metric.</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:81"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:135;left:112"><nobr><span class="ft4"><b>RELATED WORK</b></span></nobr></DIV>
<DIV style="position:absolute;top:157;left:94"><nobr><span class="ft5">Clustering is an extensively researched area not only by</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:81"><nobr><span class="ft10">data mining and database researchers [31, 11, 17, 18, 3], but<br>also by people in other disciplines [10]. Among the numer-<br>ical clustering algorithms, ENCLUS [6] uses entropy as a<br>criteria to drive the algorithm. However, ENCLUS follows<br>a completely different algorithm to our approach, dividing<br>the hyperspace recursively. For each subspace, ENCLUS es-<br>timates its density and entropy and determines if it satisfies<br>the goodness criteria: its entropy has to be lower than a<br>threshold. However, it is not possible to translate either the<br>algorithm or the relationships to the area of categorical clus-<br>tering, since the notion of density has no intuitive meaning<br>when the attributes are categorical. In a recent paper [16],<br>the authors use Renyi's definition of entropy [25] to define<br>a clustering evaluation function that measures the distance<br>between clusters as the information potential [24] between<br>them. Using this function, they describe an algorithm that,<br>starting with a random placing of points in clusters, perturbs<br>the placement until the improvement on the information po-<br>tential is not appreciable. This algorithm, however, cannot<br>scale to large data sets since it requires all points to perform<br>the calculation of the distance.</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:94"><nobr><span class="ft5">In the area of clustering categorical records, a few re-</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:81"><nobr><span class="ft10">cent publications are worth mentioning. In [19], the authors<br>address the problem of clustering transactions in a market<br>basket database by representing frequent item sets as hyper-<br>edges in a weighted hypergraph. The weight of the graph is<br>computed as the average of the confidences for all possible<br>association rules that can be generated from the item set.<br>Then, a hypergraph partitioning algorithm is employed to<br>partition the items, minimizing the weight of the cut hyper-<br>edges. The algorithm does not produce a clustering of the<br>transactions and it is not obvious how to obtain one from<br>the item clusters. A related paper by Gibson et al [14] also<br>treats categorical clustering as hypergraph partitioning, but<br>uses a less combinatorial approach to solving it, based on<br>non-linear dynamical systems.</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:94"><nobr><span class="ft5">CACTUS [12], is an agglomerative algorithm that uses the</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:81"><nobr><span class="ft10">author's definitions of support, strong connection and simi-<br>larity to cluster categorical data. Support for an attribute<br>value pair (a</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:161"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:165"><nobr><span class="ft5">, a</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:179"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:185"><nobr><span class="ft5">), where a</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:249"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:260"><nobr><span class="ft5">is in the domain of attribute</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft5">A</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:91"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:102"><nobr><span class="ft5">and a</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:137"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:148"><nobr><span class="ft5">in the domain of attribute A</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:327"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:339"><nobr><span class="ft5">is defined as the</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:81"><nobr><span class="ft10">number of tuples that have these two values. The two at-<br>tributes a</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:141"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:146"><nobr><span class="ft5">, a</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:160"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:172"><nobr><span class="ft5">are strongly connected if their support ex-</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft10">ceeds the value expected under the attribute-independence.<br>This concept is then extended to sets of attributes. A clus-<br>ter is defined as a region of attributes that are pairwise<br>strongly connected, no sub-region has the property, and its<br>support exceeds the expected support under the attribute-<br>independence assumption.</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:94"><nobr><span class="ft5">ROCK [18] computes distances between records using the</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:81"><nobr><span class="ft5">Jaccard coefficient.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:209"><nobr><span class="ft5">Using a threshold, it determines, for</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:81"><nobr><span class="ft10">each record, who are its neighbors. For a given point p, a<br>point q is a neighbor of p if the Jaccard coefficient J(p, q)<br>exceeds the threshold. Then, it computes the values of a<br>matrix LIN K, in which the entries link(p, q) are the num-<br>ber of common neighbors between p and q. The algorithm<br>then proceeds to cluster the records in an agglomerative way,<br>trying to maximize for the k clusters (k is a predefined in-</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft5">teger) the function</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:607"><nobr><span class="ft13">k<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:627"><nobr><span class="ft5">n</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:636"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:658"><nobr><span class="ft1">p,qC</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:688"><nobr><span class="ft16">i</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:708"><nobr><span class="ft1">link(p,q)</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:697"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:94;left:704"><nobr><span class="ft5">1 + 2f ()</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:704"><nobr><span class="ft16">i</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:764"><nobr><span class="ft5">, where  is</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:475"><nobr><span class="ft10">the threshold, and f () is a function selected by the user.<br>The choice of f () is critical in defining the fitness of the<br>clusters formed the the ROCK algorithm, and, as the au-<br>thors point out, the function is dependent on the data set<br>as well as on the kind of cluster the user is interested in. We<br>feel that choosing the function is a delicate and difficult task<br>for users that may be a roadblock to using ROCK efficiently.</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:489"><nobr><span class="ft5">Snob [29, 30] is an unsupervised learning algorithm based</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:475"><nobr><span class="ft10">on the notion of Minimum Message Length (MML). MML<br>is an information theoretic criterion for parameter estima-<br>tion and model selection. Although MML is similar to the<br>MDL criterion of Rissanen, MML is a Bayesian criterion<br>and therefore uses an a-priori distribution of parameter val-<br>ues. Snob is in the category of mixture model algorithms<br>[22]. Snob is iterative in nature and therefore does not scale<br>with large data sets. Moreover, contrary to COOLCAT, it<br>is difficult to envision how Snob can be used to cluster data<br>streams. AUTOCLASS [5] also uses mixture models and<br>Bayesian criteria to cluster data sets. Again, AUTOCLASS<br>does not scale well with large data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:475"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:450;left:506"><nobr><span class="ft4"><b>OUR ALGORITHM</b></span></nobr></DIV>
<DIV style="position:absolute;top:472;left:489"><nobr><span class="ft5">Our entropy-based algorithm, COOLCAT, consists of two</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:475"><nobr><span class="ft5">steps: initialization and incremental step.</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:475"><nobr><span class="ft4"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:515;left:515"><nobr><span class="ft4"><b>Initialization</b></span></nobr></DIV>
<DIV style="position:absolute;top:537;left:489"><nobr><span class="ft5">The initialization step "bootstraps" the algorithm, finding</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:475"><nobr><span class="ft10">a suitable set of clusters out of a sample S, taken from the<br>data set (</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:532"><nobr><span class="ft5">|S| &lt;&lt; N), where N is the size of the entire data</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:475"><nobr><span class="ft10">set. We first find the k most "dissimilar" records from the<br>sample set by maximizing the minimum pairwise entropy<br>of the chosen points. We start by finding the two points<br>ps</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:489"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:496"><nobr><span class="ft5">, ps</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:515"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:526"><nobr><span class="ft5">that maximize E(ps</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:646"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:653"><nobr><span class="ft5">, ps</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:673"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:680"><nobr><span class="ft5">) and placing them in two</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:475"><nobr><span class="ft5">separate clusters (C</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:597"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:604"><nobr><span class="ft5">, C</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:620"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:626"><nobr><span class="ft5">), marking the records (this takes</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:475"><nobr><span class="ft5">O(</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:491"><nobr><span class="ft5">|S|</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:508"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:515"><nobr><span class="ft5">)). From there, we proceed incrementally, i.e., to find</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:475"><nobr><span class="ft10">the record we will put in the j-th cluster, we choose an un-<br>marked point ps</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:572"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:581"><nobr><span class="ft5">that maximizes min</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:701"><nobr><span class="ft1">i=1,..,j-1</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:753"><nobr><span class="ft5">(E(ps</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:788"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:793"><nobr><span class="ft5">, ps</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:813"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:819"><nobr><span class="ft5">)).</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:489"><nobr><span class="ft5">The rest of the sample unmarked points (</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:735"><nobr><span class="ft5">|S| - k), as well</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:475"><nobr><span class="ft10">as the remaining points (outside the sample), are placed in<br>the clusters using the incremental step.</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:489"><nobr><span class="ft5">We are interested in determining the size of the sample</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:475"><nobr><span class="ft17">that guarantees with high probability the existence in the<br>sample of at least one member of each cluster, given the<br>number of clusters. In [17], the authors address the same<br>problem and use Chernoff bounds[7] to bound the size of the<br>sample given an estimate of the size of the smallest cluster<br>with respect to the average size (</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:685"><nobr><span class="ft1">|D|</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:690"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:703"><nobr><span class="ft5">), and the confidence</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:475"><nobr><span class="ft10">level  for the probability of finding at least a member of<br>each cluster. The estimate of the size of the smallest cluster<br>with respect to the average size is given in the form of a</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:475"><nobr><span class="ft5">parameter  =</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:588"><nobr><span class="ft16">|D|</span></nobr></DIV>
<DIV style="position:absolute;top:920;left:592"><nobr><span class="ft16">k</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:590"><nobr><span class="ft1">m</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:606"><nobr><span class="ft5">, where m is the size of the smallest</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:475"><nobr><span class="ft10">cluster. The parameter  is then a number greater than<br>1. The bound on the size of the sample is then given by<br>Equation 12.</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:500"><nobr><span class="ft5">s = k + klog( 1</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:606"><nobr><span class="ft5"> ) + k</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:665"><nobr><span class="ft5">(log( 1</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:696"><nobr><span class="ft5"> ))</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:715"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:725"><nobr><span class="ft5">+ 2log( 1</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:771"><nobr><span class="ft5"> )</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:809"><nobr><span class="ft5">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft5">It is important to remark that Equation 12 does not</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft5">depend on the size of the data set, which makes the bound</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">585</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="58005.png" alt="background image">
<DIV style="position:absolute;top:84;left:81"><nobr><span class="ft5">1.Given an initial set of clusters </span></nobr></DIV>
<DIV style="position:absolute;top:84;left:278"><nobr><span class="ft5">C = C</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:326"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:333"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:81;left:339"><nobr><span class="ft5">· · · , C</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:375"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:100;left:131"><nobr><span class="ft10">2.Bring points to memory from disk and<br>for each point p do</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:181"><nobr><span class="ft5">3. For i = 1, .., k</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:222"><nobr><span class="ft5">4. Tentatively place p in C</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:384"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:394"><nobr><span class="ft5">and</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:222"><nobr><span class="ft5">compute ¯</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:278"><nobr><span class="ft5">E( </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:294"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:305"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:310"><nobr><span class="ft5">) where </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:360"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:371"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:222"><nobr><span class="ft10">denotes the clustering obtained<br>by placing p in cluster C</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:371"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:222"><nobr><span class="ft5">5. Let j = argmin</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:344"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:349"><nobr><span class="ft5">( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:354"><nobr><span class="ft5">E( </span></nobr></DIV>
<DIV style="position:absolute;top:214;left:371"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:381"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:386"><nobr><span class="ft5">))</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:181"><nobr><span class="ft5">6. Place p in C</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:272"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:131"><nobr><span class="ft10">7. Until all points have been placed in<br>some cluster</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:164"><nobr><span class="ft5">Figure 2: Incremental step.</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:81"><nobr><span class="ft10">very favorable for larger sets (and unfavorable for small ones,<br>but this is not a problem since for small sets we can simply<br>use the entire set as a sample).</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:81"><nobr><span class="ft4"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:121"><nobr><span class="ft4"><b>Incremental Step</b></span></nobr></DIV>
<DIV style="position:absolute;top:445;left:94"><nobr><span class="ft5">After the initialization, we process the remaining records</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:81"><nobr><span class="ft10">of the data set (the rest of the sample and points outside<br>the sample) incrementally, finding a suitable cluster for each<br>record. This is done by computing the expected entropy<br>that results of placing the point in each of the clusters and<br>selecting the cluster for which that expected entropy is the<br>minimum. We proceed in the incremental step by bringing<br>a buffer of points to main memory and clustering them one<br>by one.</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:94"><nobr><span class="ft5">The order of processing points has a definite impact on</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:81"><nobr><span class="ft10">the quality of the clusters obtained. It is possible that a<br>point that seems a good fit for a cluster at a given point<br>in time, becomes a poor fit as more points are clustered.<br>In order to reduce this effect, we enhanced the heuristic by<br>re-processing a fraction of the points in the batch. After a<br>batch of points is clustered, we select a fraction m of points<br>in the batch that can be considered the worst fit for the clus-<br>ters they were put in. We proceed to remove these points<br>from their clusters and re-cluster them. The way we figure<br>out how good a fit a point is for the cluster where it landed<br>originally, is by keeping track of the number of occurrences<br>of each of its attributes' values in that cluster. That is, at<br>the end of the batch, we know the values of q</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:370"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:380"><nobr><span class="ft5">, for each</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:81"><nobr><span class="ft5">record i in the batch and each attribute j, where q</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:389"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:404"><nobr><span class="ft5">repre-</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:81"><nobr><span class="ft5">sent the number of times that the value V</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:337"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:352"><nobr><span class="ft5">appears in the</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:81"><nobr><span class="ft10">cluster where i was placed. We convert these numbers into<br>probabilities by dividing q</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:242"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:257"><nobr><span class="ft5">by the cluster size (i.e.,</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:414"><nobr><span class="ft5">C</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:424"><nobr><span class="ft1">l</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:435"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:81"><nobr><span class="ft5">where C</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:130"><nobr><span class="ft1">l</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:138"><nobr><span class="ft5">is the cluster where i was placed). Let us call these</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:81"><nobr><span class="ft5">numbers p</span></nobr></DIV>
<DIV style="position:absolute;top:889;left:142"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:152"><nobr><span class="ft5">. For each record, we can compute a fitting prob-</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:81"><nobr><span class="ft5">ability p</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:131"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:145"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:181"><nobr><span class="ft1">j</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:187"><nobr><span class="ft5">(p</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:199"><nobr><span class="ft1">ij</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:209"><nobr><span class="ft5">). Notice that the lower the p</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:392"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:402"><nobr><span class="ft5">is, the</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:81"><nobr><span class="ft10">worst fit the record is in that cluster (we can say that the<br>global combination of attributes is not very common in the<br>cluster). We then sort records according to p</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:348"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:357"><nobr><span class="ft5">and select the</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:81"><nobr><span class="ft5">m records in the batch with lowest p</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:307"><nobr><span class="ft1">i</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:317"><nobr><span class="ft5">as the records to be</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:81"><nobr><span class="ft10">reprocessed. Each re-processed record is placed in the clus-<br>ter that minimizes the expected entropy (as done originally<br>in the incremental step).</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:81"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:112"><nobr><span class="ft4"><b>EXPERIMENTAL RESULTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:94"><nobr><span class="ft5">Our experiments were run in a DELL server equipped</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft10">with a Pentium III running at 800 MHz, and 1 Gigabyte of<br>main memory, running Red Hat Linux 2.2.14. We used two<br>kinds of data sets: real data sets (for evaluating the quality<br>of our algorithm) and synthetic data sets (for the evaluation<br>of scalability). The experiments were conducted using the<br>following datasets (plus a synthetically generated data set<br>to test the scalability of the algorithm).</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:495"><nobr><span class="ft5">· Archaeological data set</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:509"><nobr><span class="ft10">Our first data set is a hypothetical collection of human<br>tombs and artifacts from an archaeological site. Al-<br>though the data set is not "real," it is realistic enough<br>and so we include it in this section. It has also the<br>property of being small, so brute force can be used<br>to find the optimal clustering. The data set is taken<br>from [1] The first attribute (not used for clustering<br>but for verification) indicates the sex (M for male, F<br>for female) of the individuals buried. The other eight<br>attributes are binary (1 present, 0 non-present), and<br>represent artifacts types (e.g., ceramics, bracelets, ar-<br>row points) that were found (or not found) in the tomb.</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:495"><nobr><span class="ft5">· Congressional votes This data set was obtained from</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:509"><nobr><span class="ft10">the UCI KDD Archive ([20]) and contains the United<br>States Congressional Voting Records for the year 1984.<br>Each record contains a Congressman's votes on 16 is-<br>sues. All the attributes are boolean ("yes" or "no"),<br>with a few of the votes containing missing values. We<br>decided to treat missing values as another domain value<br>for the attribute. A classification field with the la-<br>bels "Democrat," or "Republican" is provided for each<br>record, which are not used for clustering, but can be<br>loosely used for quality measuring. (Some congress-<br>men "crossed" parties to vote.) There are 435 records<br>in the set (267 Democrats and 168 Republicans).</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:495"><nobr><span class="ft5">· KDD Cup 1999 data This data set can be obtained</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:509"><nobr><span class="ft10">from the UCI Archive [20], and was used for the the<br>Third International Knowledge Discovery and Data<br>Mining Tools Competition. This database contains a<br>standard set of network audit data, which includes a<br>wide variety of simulated intrusions. Each record, cor-<br>responding to a connection, contains 42 features, some<br>of them categorical, and the rest continuous variables.<br>We transformed the continuous variables in categori-<br>cal by a simple process of discretization: we computed<br>the median of each attribute, and assigned any value<br>below and including the median a label "0," while the<br>rest of the values were assigned a label "1." There are<br>many intrusion data sets in the repository, some of<br>them to be used as training sets and some as test sets.<br>We utilized the set that corresponds to 10% of the<br>training data. In this set, records have an extra at-<br>tribute (class), labeled with a "1" if the connection is<br>part of an attack, or a "0" if it is not. We use this<br>attribute for the evaluation of external entropy (not in<br>the clustering process).</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:475"><nobr><span class="ft4"><b>5.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:998;left:515"><nobr><span class="ft4"><b>Archaeological Data</b></span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:489"><nobr><span class="ft5">Figure 3 show the results of using COOLCAT in the ar-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft5">chaeological data set.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:620"><nobr><span class="ft5">We performed experiments with 2</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:475"><nobr><span class="ft10">clusters, since the attribute with which we evaluate the ex-<br>ternal entropy (not used in the clustering) is Sex (and the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">586</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="58006.png" alt="background image">
<DIV style="position:absolute;top:86;left:109"><nobr><span class="ft5">Alg.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:187"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:236"><nobr><span class="ft5">CU</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:306"><nobr><span class="ft10">Ext<br>E.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:352"><nobr><span class="ft5">Expected</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:306"><nobr><span class="ft5">(sex)</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:352"><nobr><span class="ft5">entropy</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:109"><nobr><span class="ft5">COOLCAT 0%</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:236"><nobr><span class="ft5">0.7626</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:306"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:352"><nobr><span class="ft5">4.8599</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:187"><nobr><span class="ft5">10%</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:236"><nobr><span class="ft5">0.7626</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:306"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:352"><nobr><span class="ft5">4.8599</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:187"><nobr><span class="ft5">20%</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:236"><nobr><span class="ft5">0.7626</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:306"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:352"><nobr><span class="ft5">4.8599</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:109"><nobr><span class="ft10">Brute<br>Force</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:187"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:236"><nobr><span class="ft5">0.7626</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:306"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:352"><nobr><span class="ft5">4.8599</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:109"><nobr><span class="ft5">ROCK</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:187"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:236"><nobr><span class="ft5">0.3312</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:306"><nobr><span class="ft5">0.9622 n/a</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:81"><nobr><span class="ft10">Figure 3: Results for COOLCAT, ROCK and brute<br>force in the Archaeological data set.</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:81"><nobr><span class="ft10">data set is small, so we believed that the clustering could ef-<br>fectively separate the two sexes). We conducted experiments<br>with the original data set (which we label "independent"),<br>and a modified data set in which we grouped attributes in<br>the following way: (1), (24), (26), (34), (35), (46), (78), to re-<br>flect the correlations found among the attributes of the set<br>(found by using a Likelihood ratio test). However, we only<br>report the results for independent data, since the correlated<br>set results are essentially the same. (The same phenomena<br>was observed in the other experiments.) We also conducted<br>"brute force" experiments, in which we found the optimum<br>clustering, i.e., that for which the expected entropy was the<br>minimum.</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:154"><nobr><span class="ft5">We did this to compare how well our heuris-</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:81"><nobr><span class="ft10">tic (COOLCAT) performed. We also report in the table<br>the best results found by ROCK (which have to be found<br>by varying the parameter  over a range of values). The<br>results shown in Figure 3 show that the expected entropy<br>function does an excellent job in clustering this data. The<br>results obtained by COOLCAT (in terms of CU , and ex-<br>ternal entropy with respect to the variable sex, which is<br>not used in the clustering), and expected entropy are the<br>same obtained by the brute force (optimal) approach. In<br>all cases, both the CU function and the external entropy<br>of the COOLCAT solutions are better than those found for<br>the best ROCK solution. Particularly encouraging is the<br>fact that the external entropy for the variable SEX (which<br>the authors of the data set indicated as the one being more<br>correlated with the clusters), is 0 in all the COOLCAT solu-<br>tions, so a perfect separation is achieved. (ROCK's solution<br>does not achieve this, resulting in a high external entropy.)<br>In this data set, the re-processing step does not have any<br>effect, as seen by the fact that the results are the same for<br>all the values of m. This is attributed to the size of the data<br>set (only 20 records). Both COOLCAT and ROCK took<br>0.01 seconds to find a solution for this data set.</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:81"><nobr><span class="ft4"><b>5.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:888;left:121"><nobr><span class="ft4"><b>Congressional Voting results</b></span></nobr></DIV>
<DIV style="position:absolute;top:910;left:94"><nobr><span class="ft5">Figure 4 summarizes the results obtained by COOLCAT</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:81"><nobr><span class="ft10">in the Congressional Voting records (no grouping of attributes<br>was performed), for three values of m. The results obtained<br>for various sample sizes are extremely stable. The CU val-<br>ues for the clusterings obtained with COOLCAT are, in<br>all the cases superior to the one obtained by ROCK. The<br>values show no fluctuations on our results as m changes,<br>while the value for CU is 11% better than ROCK's value.<br>The external entropy for the COOLCAT solutions is slightly<br>better than the value in ROCK's solution. The buffer size<br>(batch) in this experiment was 100 records, making the num-</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:483"><nobr><span class="ft5">Alg.</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:538"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:582"><nobr><span class="ft5">CU</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:648"><nobr><span class="ft5">Ext.Ent.</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:715"><nobr><span class="ft5">Expected</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:786"><nobr><span class="ft5">Running</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:648"><nobr><span class="ft10">(pol.<br>affl.)</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:715"><nobr><span class="ft5">entropy</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:786"><nobr><span class="ft10">time<br>(sec.)</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:483"><nobr><span class="ft5">COOL</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:538"><nobr><span class="ft5">0%</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:582"><nobr><span class="ft5">2.9350</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:648"><nobr><span class="ft5">0.4975</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:715"><nobr><span class="ft5">13.8222</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:786"><nobr><span class="ft5">0.16</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:483"><nobr><span class="ft5">CAT</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:538"><nobr><span class="ft5">10%</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:582"><nobr><span class="ft5">2.9350</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:648"><nobr><span class="ft5">0.4975</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:715"><nobr><span class="ft5">13.8222</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:786"><nobr><span class="ft5">0.26</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:538"><nobr><span class="ft5">20%</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:582"><nobr><span class="ft5">2.9350</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:648"><nobr><span class="ft5">0.4975</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:715"><nobr><span class="ft5">13.8222</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:786"><nobr><span class="ft5">0.28</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:483"><nobr><span class="ft5">ROCK</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:538"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:582"><nobr><span class="ft5">2.6282</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:648"><nobr><span class="ft5">0.4993</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:715"><nobr><span class="ft5">N/A</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:786"><nobr><span class="ft5">0.51</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:475"><nobr><span class="ft10">Figure 4: Results for COOLCAT and ROCK in the<br>Congressional Voting data set</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:475"><nobr><span class="ft10">ber of re-processed points 0,10, and 20 (m = 0%, 10%, 20%).<br>(Again, these numbers correspond to the means of 500 runs.)<br>The running time of COOLCAT is significantly better than<br>the one for ROCK (a decrease of 45% in the slowest case,<br>m = 20%, of COOLCAT).</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:475"><nobr><span class="ft4"><b>5.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:383;left:515"><nobr><span class="ft4"><b>KDD Cup 1999 data set</b></span></nobr></DIV>
<DIV style="position:absolute;top:405;left:489"><nobr><span class="ft5">Since we did not have explicit knowledge of how many</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:475"><nobr><span class="ft10">clusters we could find in this data set, we decided to find<br>clusterings for many k values, and report, in each case, the<br>expected entropy, external entropy (with respect to the at-<br>tribute that denotes whether the record is an attack or not),<br>and CU . The results are shown in the form of a graph in<br>Figure 5.</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:545"><nobr><span class="ft5">In the figure, the left hand side scale is used</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:475"><nobr><span class="ft10">for expected entropy and CU , while the right hand side is<br>used for external entropy (the values of external entropy are<br>between 0 and 1, while the other parameters have larger<br>ranges). The figure shows that all the parameters tend to<br>an asymptotic limit as k grows. The saturation starts to<br>occur in the value k = 10, which exhibits an external en-<br>tropy of 0.09, which indicates that most of the clusters are<br>"inhabited" by either attack records or attack-free records.<br>In other words, the clustering achieves a good separation of<br>the points. The experiments were conducted using a sample<br>size of 1,000 points, which guarantees a level of confidence<br>of 95% (for  = 10).</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:475"><nobr><span class="ft4"><b>5.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:715;left:515"><nobr><span class="ft4"><b>Synthetic data set</b></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:489"><nobr><span class="ft5">We used a synthetic data generator ([8]) to generate data</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft10">sets with different number of records and attributes. We<br>used these data sets to test the scalability of COOLCAT.<br>The results are shown in the graph of Figure 7, where the<br>y-axis shows the execution time of COOLCAT in seconds,<br>and the x-axis the number of records (in multiples of 10</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:818"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:825"><nobr><span class="ft5">),</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:475"><nobr><span class="ft10">for four different number of attributes (A = 5, 10, 20, 40).<br>In all the cases, COOLCAT behaves linearly with respect to<br>the number of records, due to the incremental nature of the<br>algorithm (it processes each record in the data set at most<br>twice: those that are selected for re-processing are clustered<br>twice, the rest only once; moreover, points are brought from<br>disk to memory only once). We used for these experiments<br>an m equal to 20%, and a buffer size of 300 records. Notice<br>that in this experiment, we do not report running times for<br>ROCK. The reason for this is that ROCK is designed to<br>be a main memory algorithm. In [18], the authors make it<br>explicit that ROCK deals with large data sets by using ran-<br>dom sampling (not by looking at the entire set). Therefore,<br>it would have been unfair to compare COOLCAT's running<br>times with those of ROCK (over samples of the sets).</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:489"><nobr><span class="ft5">We performed another experiment with synthetic data</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">587</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:3px;font-family:Helvetica;color:#000000;}
	.ft19{font-size:5px;font-family:Helvetica;color:#000000;}
	.ft20{font-size:3px;line-height:5px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="58007.png" alt="background image">
<DIV style="position:absolute;top:318;left:81"><nobr><span class="ft10">Figure 5: Expected entropy, external entropy and<br>CU vs.</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:141"><nobr><span class="ft5">Number of Clusters (k) in the KDD Cup</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:81"><nobr><span class="ft5">1999 data set.</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:197"><nobr><span class="ft5">The left scale (y-axis) is used for</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:81"><nobr><span class="ft10">xpected entropy and CU , while the right one is used<br>for external entropy.</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:81"><nobr><span class="ft10">sets generated by [8]. In this experiment, each synthetic<br>set contained 8,124 records of 23 attributes each. Twenty<br>two of the attributes are used for clustering and one (Indx)<br>for the evaluation of external entropy. Each data set was<br>generated using 21 different types of rules. A rule involves<br>12 attributes. An example of the rules used is: A = c&amp;C =<br>a&amp;D = b&amp;K = c&amp;N = a&amp;O = c&amp;Q = b&amp;R = c&amp;S =<br>c&amp;T = b&amp;U = b</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:189"><nobr><span class="ft5"> Indx = r</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:267"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:273"><nobr><span class="ft5">. This rule says that when</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:81"><nobr><span class="ft10">the 11 attributes on the left hand side take the values shown,<br>the attribute Indx takes the value r</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:304"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:310"><nobr><span class="ft5">. Every record obeys</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:81"><nobr><span class="ft10">one of the rules. Two sets were generated, using different<br>probability distributions for the rules. In the first one (uni-<br>form), every rule is used in the same number of records in<br>the data set. (In other words the number of records that<br>obey a particular rule is equal to the size of the data set<br>divided by 21.) In the normal distribution, the populations<br>are distributed following a Gaussian distribution (some rules<br>receive more records than others). The 23rd attribute takes<br>the value of the rule number (rule index).</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:360"><nobr><span class="ft5">The external</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:81"><nobr><span class="ft10">entropy is calculated using this attribute (which does not<br>participate in the clustering). Figure 6 shows the evaluation<br>of clusters obtained by COOLCAT over different synthetic<br>data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:154"><nobr><span class="ft5">The table shows also the results obtained by</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:81"><nobr><span class="ft10">using ROCK. As we can see, COOLCAT results are signif-<br>icantly better than those obtained by ROCK for both data<br>sets. Particularly significant is the fact that the external<br>entropy for the COOLCAT solutions in the Uniform case<br>with m = 10%, 20% are 0, indicating a perfect separation<br>of rules. The values for other cases are extremely close to<br>0 as well. As expected, re-processing (increasing m) helps<br>in finding a better clustering. However, the impact is more<br>marked when going from no re-processing (m = 0) to re-<br>processing 10% of the points, leveling out from then on.<br>The running times of COOLCAT are more than one order<br>of magnitude smaller than those of ROCK.</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:81"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:998;left:112"><nobr><span class="ft4"><b>CONCLUSIONS</b></span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:94"><nobr><span class="ft5">In this paper we have introduced a new categorical clus-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft10">tering algorithm, COOLCAT, based in the notion of en-<br>tropy. The algorithm groups points in the data set trying<br>to minimize the expected entropy of the clusters. The ex-</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:483"><nobr><span class="ft5">Dist.</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:545"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:589"><nobr><span class="ft5">CU</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:648"><nobr><span class="ft5">Ext.Ent.</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:715"><nobr><span class="ft5">Expected</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:786"><nobr><span class="ft5">Running</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:648"><nobr><span class="ft10">(rule in-<br>dex)</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:715"><nobr><span class="ft5">entropy</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:786"><nobr><span class="ft10">time<br>(sec.)</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:621"><nobr><span class="ft5">COOLCAT</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:483"><nobr><span class="ft5">Uniform 0%</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:589"><nobr><span class="ft5">6.9187</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:648"><nobr><span class="ft5">0.00816</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:715"><nobr><span class="ft5">17.4302</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:786"><nobr><span class="ft5">6.73</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:545"><nobr><span class="ft5">10%</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:589"><nobr><span class="ft5">6.9268</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:648"><nobr><span class="ft5">0.00000</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:715"><nobr><span class="ft5">17.2958</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:786"><nobr><span class="ft5">11.85</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:545"><nobr><span class="ft5">20%</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:589"><nobr><span class="ft5">6.9268</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:648"><nobr><span class="ft5">0.00000</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:715"><nobr><span class="ft5">17.3958</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:786"><nobr><span class="ft5">12.95</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:483"><nobr><span class="ft5">Normal</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:545"><nobr><span class="ft5">0%</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:589"><nobr><span class="ft5">6.8893</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:648"><nobr><span class="ft5">0.02933</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:715"><nobr><span class="ft5">17.4969</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:786"><nobr><span class="ft5">6.88</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:545"><nobr><span class="ft5">10%</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:589"><nobr><span class="ft5">6.8996</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:648"><nobr><span class="ft5">0.00813</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:715"><nobr><span class="ft5">17.4458</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:786"><nobr><span class="ft5">11.99</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:545"><nobr><span class="ft5">20%</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:589"><nobr><span class="ft5">6.9008</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:648"><nobr><span class="ft5">0.00742</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:715"><nobr><span class="ft5">17.4328</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:786"><nobr><span class="ft5">13.07</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:635"><nobr><span class="ft5">ROCK</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:483"><nobr><span class="ft5">Uniform -</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:589"><nobr><span class="ft5">6.6899</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:648"><nobr><span class="ft5">0.09861</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:715"><nobr><span class="ft5">n/a</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:786"><nobr><span class="ft5">207.37</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:483"><nobr><span class="ft5">Normal</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:545"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:589"><nobr><span class="ft5">6.2749</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:648"><nobr><span class="ft5">0.34871</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:715"><nobr><span class="ft5">n/a</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:786"><nobr><span class="ft5">223.49</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:475"><nobr><span class="ft10">Figure 6: Results for COOLCAT and ROCK in the<br>synthetic data sets</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:512"><nobr><span class="ft18">0</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:500"><nobr><span class="ft18">2000</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:500"><nobr><span class="ft18">4000</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:500"><nobr><span class="ft18">6000</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:500"><nobr><span class="ft18">8000</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:496"><nobr><span class="ft18">10000</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:496"><nobr><span class="ft18">12000</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:496"><nobr><span class="ft18">14000</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:496"><nobr><span class="ft18">16000</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:496"><nobr><span class="ft18">18000</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:496"><nobr><span class="ft18">20000</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:496"><nobr><span class="ft18">22000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:518"><nobr><span class="ft18">0</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:543"><nobr><span class="ft18">1000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:574"><nobr><span class="ft18">2000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:605"><nobr><span class="ft18">3000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:636"><nobr><span class="ft18">4000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:668"><nobr><span class="ft18">5000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:699"><nobr><span class="ft18">6000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:730"><nobr><span class="ft18">7000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:761"><nobr><span class="ft18">8000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:792"><nobr><span class="ft18">9000</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:821"><nobr><span class="ft18">10000</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:485"><nobr><span class="ft19">execution time</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:634"><nobr><span class="ft18">Number of records x 1000</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:781"><nobr><span class="ft18">A = 5</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:777"><nobr><span class="ft20">A = 10<br>A = 20<br>A = 40</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:475"><nobr><span class="ft5">Figure 7:</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:554"><nobr><span class="ft5">COOLCAT's performance for the syn-</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:475"><nobr><span class="ft5">thetic data sets:</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:609"><nobr><span class="ft5">response time (in seconds) vs.</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:475"><nobr><span class="ft10">the number of records in the data set (in mul-<br>tiples of 10</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:558"><nobr><span class="ft1">3</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:564"><nobr><span class="ft5">), for different number of attributes</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:475"><nobr><span class="ft5">(A = 5, 10, 20, 40).</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:475"><nobr><span class="ft10">perimental evaluation supports our claim that COOLCAT<br>is an efficient algorithm, whose solutions are stable for dif-<br>ferent samples (and sample sizes) and it is scalable for large<br>data sets (since it incrementally adds points to the initial<br>clusters). We have evaluated our results using category util-<br>ity function, and the external entropy which determines if<br>the clusters have significance with respect to external vari-<br>ables (i.e., variables not used in the clustering process). In<br>our comparisons with ROCK, COOLCAT always shows a<br>small advantage in terms of the quality measures (CU and<br>external entropy). However, the real advantage of COOL-<br>CAT resides in the fact that ROCK is extremely difficult to<br>tune (finding the right ), while COOLCAT's behavior to<br>its only parameter (m) is extremely stable: small values of<br>m are sufficient to obtain a good result. In the largest data<br>set for which we compared both techniques (Mushrooms),<br>COOLCAT had a significantly better running time.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:489"><nobr><span class="ft5">The incremental nature of COOLCAT makes it possible</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:475"><nobr><span class="ft10">to apply the algorithm to data streams, and as the results in<br>scalability show, the algorithm can cope with large volumes<br>of data. We are currently doing research in tracking evolving<br>clusters using COOLCAT.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">588</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="58008.png" alt="background image">
<DIV style="position:absolute;top:84;left:81"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:84;left:112"><nobr><span class="ft4"><b>ACKNOWLEDGMENTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:106;left:94"><nobr><span class="ft5">We like to thank Vipin Kumar and Eui-Hong (Sam) Han</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:81"><nobr><span class="ft5">for lending us their implementation of ROCK.</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:81"><nobr><span class="ft4"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:158;left:112"><nobr><span class="ft4"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:183;left:88"><nobr><span class="ft5">[1] M.S. Aldenderfer and R.K. Blashfield. Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:109"><nobr><span class="ft10">Analysis. Sage Publications, (Sage University Paper<br>series on Quantitative Applications in the Social<br>Sciences, No. 44), 1984.</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:88"><nobr><span class="ft5">[2] D. Barbar´</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:170"><nobr><span class="ft5">a. Requirements for clustering data streams.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:109"><nobr><span class="ft10">SIGKDD Explorations (Special Issue on Online,<br>Interactive, and Anytime Data Mining), 3(2), 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:88"><nobr><span class="ft5">[3] D. Barbar´</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:170"><nobr><span class="ft5">a and P. Chen. Using the fractal dimension</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:109"><nobr><span class="ft10">to cluster datasets. In Proceedings of the ACM<br>SIGKDD International Conference on Knowledge<br>Discovery and Data Mining, Boston, MA, August<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:88"><nobr><span class="ft5">[4] R.B. Calinski and J. Harabasz. A dendrite method for</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:109"><nobr><span class="ft10">cluster analysis. Communications in Statistics, pages<br>1­27, 1974.</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:88"><nobr><span class="ft5">[5] P. Cheeseman and J. Stutz. Bayesian classification</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:109"><nobr><span class="ft10">(AUTOCLASS): Theory and Results. In U.M. Fayyad,<br>G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,<br>editors, Advances in Knowledge Discovery and Data<br>Mining. AAAI Press, Menlo Park, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:88"><nobr><span class="ft5">[6] C. CHen, A.W. Fu, and Y. Zhang. Entropy-based</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:109"><nobr><span class="ft10">Subspace Clustering for Mining Numerical Data. In<br>Proceedings of ACM SIGKDD International<br>Conference on Knowledge Discovery and Data Mining,<br>San Diego, CA, August 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:88"><nobr><span class="ft5">[7] H. Chernoff. A Measure of Asymptotic Efficiency for</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:109"><nobr><span class="ft10">Tests of a Hypothesis Based on the Sum of<br>Observations. Annals of Mathematical Statistics, pages<br>493­509, 1952.</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:88"><nobr><span class="ft5">[8] DataGen. Data Generator: Perfect data for an</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:109"><nobr><span class="ft5">imperfect world. http://www.datasetgenerator.com/.</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:88"><nobr><span class="ft5">[9] R.C. Dubes and A.K. Jain. Validity studies in</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:109"><nobr><span class="ft10">clustering methodologies. Pattern Recognition, pages<br>235­254, 1979.</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:81"><nobr><span class="ft5">[10] R.O. Duda and P.E. Hart. Pattern Classification and</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:109"><nobr><span class="ft5">Scene Analysis. Wiley-Interscience, New York, 1973.</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:81"><nobr><span class="ft5">[11] M. Ester, H.P. Kriegel, and X. Wu. A density-based</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:109"><nobr><span class="ft10">algorithm for discovering clusters in large spatial<br>database with noise. In Proceedings of the<br>International Conference on Knowledge Discovery and<br>Data Mining, Portland, Oregon, August 1996.</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:81"><nobr><span class="ft5">[12] V. Ganti, J. Gehrke, and R. Ramakrishnan.</span></nobr></DIV>
<DIV style="position:absolute;top:858;left:109"><nobr><span class="ft10">CACTUS-Clustering Categorical Data Using<br>Summaries. In Proceedings of the ACM-SIGKDD<br>International Conference on Knowledge Discovery and<br>Data Mining, San Diego, CA, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:81"><nobr><span class="ft5">[13] M. Garey and D. Johnson. Computers and</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:109"><nobr><span class="ft10">Intractability: A Guide to the Theory of<br>NP-Completeness. W.H. Freeman, 1979.</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:81"><nobr><span class="ft5">[14] D. Gibson, J. Kleinberg, and P. Raghavan. Clustering</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:109"><nobr><span class="ft10">Categorical Data: An Approach Based on Dynamical<br>Systems. In Proceedings of the International<br>Conference on Very Large Databases (VLDB), New<br>York, NY, September 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft5">[15] A. Gluck and J. Corter. Information, uncertainty, and</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:109"><nobr><span class="ft5">the utility of categories. In Proceedings of the Seventh</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:503"><nobr><span class="ft10">Annual Conference of the Cognitive Science Society,<br>1985.</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:475"><nobr><span class="ft5">[16] E. Gokcay and J.C. Principe. Information Theoretic</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:503"><nobr><span class="ft10">Clustering. IEEE Transactions on Pattern Analysis<br>and Machine Intelligence, 24(2), February 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:475"><nobr><span class="ft5">[17] S. Guha, R. Rastogi, and K. Shim. CURE: A</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:503"><nobr><span class="ft10">clustering algorithm for large databases. In<br>Proceedings of the ACM SIGMOD Conference on<br>Management of Data, Seattle, WA, May 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:475"><nobr><span class="ft5">[18] S. Guha, R. Rastogi, and K. Shim. ROCK: A Robust</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:503"><nobr><span class="ft10">Clustering Algorithm for Categorical Attributes. In<br>Proceedings of the 15th International Conference on<br>Data Engineering, Sydney, Australia, April 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:475"><nobr><span class="ft5">[19] E.H. Han, G. Karypis, V. Kumar, and B. Mobasher.</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:503"><nobr><span class="ft10">Clustering based on association rule hypergraphs. In<br>Proceedings of the SIGMOD Workshop on Research<br>Issues on Data Mining and Knowledge Discovery,<br>June 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:475"><nobr><span class="ft5">[20] S. Hettich(librarian). UCI KDD Archive.</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:503"><nobr><span class="ft5">http://kdd.ics.uci.edu/.</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:475"><nobr><span class="ft5">[21] A.K. Jain and R.C. Dubes. Algorithms for clustering</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:503"><nobr><span class="ft5">data. Prentice Hall, 1988.</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:475"><nobr><span class="ft5">[22] G.J McLachlan and K.E. Basford. Mixture Models.</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:503"><nobr><span class="ft5">Marcel Dekker, New York, 1988.</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:475"><nobr><span class="ft17">[23] T.M. Mitchell. Machine Learning. McGraw-Hill, 1997.<br>[24] J.C. Pincipe, D. Xu, and J. Fisher. Information</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:503"><nobr><span class="ft10">theoretic learning. In S. Haykin, editor, Unsupervised<br>Adaptive Filtering. John Wiley &amp; Sons, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:475"><nobr><span class="ft5">[25] A. Renyi. On Measures of Entropy and Information.</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:503"><nobr><span class="ft10">In Proc. of the Fourth Berkeley Symp. Math.,<br>Statistics, and Probability, 1960.</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:475"><nobr><span class="ft5">[26] J. Rissanen. A universal prior for integers and</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:503"><nobr><span class="ft10">estimation by minimum description length. The<br>Annals of Statistics, 1983.</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:475"><nobr><span class="ft5">[27] J. Rissanen. Stochastic complexity in statistical</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:503"><nobr><span class="ft5">inquiry. World Scientific Pub., 1989.</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:475"><nobr><span class="ft5">[28] C.E. Shannon. A mathematical theory of</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:503"><nobr><span class="ft10">communication. Bell System Techical Journal, pages<br>379­423, 1948.</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:475"><nobr><span class="ft5">[29] C.S. Wallace and D.M. Boulton. An information</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:503"><nobr><span class="ft10">measure for classification. The Computer Journal,<br>11(2), 1968.</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:475"><nobr><span class="ft5">[30] C.S. Wallace and D.L. Dowe. Intrinsic classification by</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:503"><nobr><span class="ft10">MML, the Snob program. In Proceedings of the 7th<br>Australian Joint Conference on Artificial Intelligence,<br>1994.</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:475"><nobr><span class="ft5">[31] R. Zhang, R. Ramakrishnan, and M.Livny. Birch: An</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:503"><nobr><span class="ft10">efficient data clustering method for very large<br>databases. In Proceedings of the ACM SIGMOD<br>Conference on Data Management, Montreal, Canada,<br>June 1996.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft9">589</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
