<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\199</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-08-05T21:38:52+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:15px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:9px;font-family:Times;color:#000000;}
	.ft8{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft9{font-size:11px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft11{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="199001.png" alt="background image">
<DIV style="position:absolute;top:108;left:233"><nobr><span class="ft0"><b>Tracking Dynamics of Topic Trends</b></span></nobr></DIV>
<DIV style="position:absolute;top:138;left:275"><nobr><span class="ft0"><b>Using a Finite Mixture Model</b></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:218"><nobr><span class="ft1">Satoshi Morinaga</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:231"><nobr><span class="ft2">NEC Corporation</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:205"><nobr><span class="ft2">4-1-1,Miyazaki,Miyamae,</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:160"><nobr><span class="ft2">Kawasaki,Kanagawa 216-8555,JAPAN</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:191"><nobr><span class="ft3">morinaga@ccm.cl.nec.co.jp</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:564"><nobr><span class="ft1">Kenji Yamanishi</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:570"><nobr><span class="ft2">NEC Corporation</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:544"><nobr><span class="ft2">4-1-1,Miyazaki,Miyamae,</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:499"><nobr><span class="ft2">Kawasaki,Kanagawa 216-8555,JAPAN</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:527"><nobr><span class="ft3">k-yamanishi@cw.jp.nec.com</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:81"><nobr><span class="ft4"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:357;left:81"><nobr><span class="ft10">In a wide range of business areas dealing with text data<br>streams, including CRM, knowledge management, and Web<br>monitoring services, it is an important issue to discover topic<br>trends and analyze their dynamics in real-time.Specifi-<br>cally we consider the following three tasks in topic trend<br>analysis: 1)Topic Structure Identification; identifying what<br>kinds of main topics exist and how important they are,<br>2)Topic Emergence Detection; detecting the emergence of a<br>new topic and recognizing how it grows, 3)Topic Characteri-<br>zation; identifying the characteristics for each of main topics.<br>For real topic analysis systems, we may require that these<br>three tasks be performed in an on-line fashion rather than in<br>a retrospective way, and be dealt with in a single framework.<br>This paper proposes a new topic analysis framework which<br>satisfies this requirement from a unifying viewpoint that a<br>topic structure is modeled using a finite mixture model and<br>that any change of a topic trend is tracked by learning the<br>finite mixture model dynamically.In this framework we pro-<br>pose the usage of a time-stamp based discounting learning<br>algorithm in order to realize real-time topic structure iden-<br>tification.This enables tracking the topic structure adap-<br>tively by forgetting out-of-date statistics.Further we apply<br>the theory of dynamic model selection to detecting changes<br>of main components in the finite mixture model in order<br>to realize topic emergence detection.We demonstrate the<br>effectiveness of our framework using real data collected at<br>a help desk to show that we are able to track dynamics of<br>topic trends in a timely fashion.</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:81"><nobr><span class="ft4"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:81"><nobr><span class="ft10">H.2.8 [Database Management]: Database Applications -<br>Data Mining</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:81"><nobr><span class="ft4"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:915;left:81"><nobr><span class="ft5">topic analysis, model selection, CRM, text mining</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:81"><nobr><span class="ft11">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br><i>KDD'04, </i>August 22­25, 2004, Seattle, Washington, USA.<br>Copyright 2004 ACM 1-58113-888-1/04/0008 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:315"><nobr><span class="ft5">$</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:322"><nobr><span class="ft6">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:475"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:332;left:507"><nobr><span class="ft4"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:370;left:475"><nobr><span class="ft4"><b>1.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:370;left:516"><nobr><span class="ft4"><b>Problem Setting</b></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:489"><nobr><span class="ft5">In a wide range of business areas dealing with text streams,</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:475"><nobr><span class="ft10">including CRM, knowledge management, and Web monitor-<br>ing services, it is an important issue to discover topic trends<br>and analyze their dynamics in real-time.For example, it is<br>desired in the CRM area to grasp a new trend of topics in<br>customers' claims every day and to track a new topic as soon<br>as it emerges.A topic is here defined as a seminal event or<br>activity.Specifically we consider the following three tasks<br>in topic analysis:</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:489"><nobr><span class="ft5">1) Topic Structure Identification; learning a topic struc-</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:475"><nobr><span class="ft10">ture in a text stream, in other words, identifying what kinds<br>of main topics exist and how important they are.<br>2) Topic Emergence Detection; detecting the emergence of<br>a new topic and recognizing how rapidly it grows, similarly,<br>detecting the disappearance of an existing topic.<br>3) Topic Characterization; identifying the characteristics for<br>each of main topics.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:489"><nobr><span class="ft5">For real topic analysis systems, we may require that these</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:475"><nobr><span class="ft10">three tasks be performed in an on-line fashion rather than in<br>a retrospective way, and be dealt with in a single framework.</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:489"><nobr><span class="ft5">The main purpose of this paper is to propose a new topic</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:475"><nobr><span class="ft10">analysis framework that satisfies the requirement as above,<br>and to demonstrate its effectiveness through its experimen-<br>tal evaluations for real data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:489"><nobr><span class="ft5">Our framework is designed from a unifying viewpoint that</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:475"><nobr><span class="ft10">a topic structure in a text stream is modeled using a finite<br>mixture model (a model of the form of a weighted average<br>of a number of probabilistic models) and that any change<br>of a topic trend is tracked by learning the finite mixture<br>model dynamically.Here each topic corresponds to a single<br>mixture component in the model.</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:489"><nobr><span class="ft5">All of the tasks 1)-3) are formalized in terms of a finite</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:475"><nobr><span class="ft10">mixture model as follows: As for the task 1), the topic struc-<br>ture is identified by statistical parameters of a finite mix-<br>ture model.They are learned using our original time-stamp<br>based discounting learning algorithm, which incrementally<br>and adaptively estimates statistical parameters of the model<br>by gradually forgetting out-of-date statistics, making use of<br>time-stamps of data.This makes the learning procedure<br>adaptive to changes of the nature of text streams.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:489"><nobr><span class="ft5">As for the task 2), any change of a topic structure is rec-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft10">ognized by tracking the change of main components in a<br>mixture model.We apply the theory of dynamic model se-<br>lection [7] to detecting changes of the optimal number of</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">811</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:593"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft12{font-size:6px;font-family:Times;color:#000000;}
	.ft13{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="199002.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">main components and their organization in the finite mix-<br>ture model.We may recognize that a new topic has emerged<br>if a new mixture component is detected in the model and re-<br>mains for a while.Unlike conventional approaches to statis-<br>tical model selection under the stationary environment, dy-<br>namic model selection is performed under the non-stationary<br>one in which the optimal model may change over time.Fur-<br>ther note that we deal with a complicated situation where<br>the dimension of input data, i.e., the number of features of<br>a text vector, may increase as time goes by.</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:94"><nobr><span class="ft5">As for the task 3), we classify every text into the cluster</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:81"><nobr><span class="ft10">for which the posterior probability is largest, and then we<br>characterize each topic using feature terms characterizing<br>texts classified into its corresponding cluster.These feature<br>terms are extracted as those of highest information gain,<br>which are computed in real-time.</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:94"><nobr><span class="ft5">We demonstrate the validity of the topic trend analysis</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:81"><nobr><span class="ft10">framework, by showing experimental results on its applica-<br>tions to real domains.Specifically we emphasize that it is<br>really effective for discovering trends in questions at a help<br>desk.</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:81"><nobr><span class="ft4"><b>1.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:121"><nobr><span class="ft4"><b>Related Work</b></span></nobr></DIV>
<DIV style="position:absolute;top:455;left:94"><nobr><span class="ft5">The technologies similar to 1)-3) have extensively been ex-</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:81"><nobr><span class="ft10">plored in the area of topic detection and tracking (TDT) (see<br>[1]).Actually 1) and 2) are closely related to the subprob-<br>lems in TDT called topic tracking and new event detection,<br>respectively.Here topic tracking is to classify texts into one<br>of topics specified by a user, while new event detection, for-<br>merly called first story detection, is to identify texts that<br>discuss a topic that has not already been reported in earlier<br>texts.The latter problem is also related to work on topic-<br>conditioned novelty detection by Yang et.al.[16]. In most of<br>related TDT works, however, topic tracking or new event<br>detection is conducted without identifying main topics or<br>a topic structure, hence the tasks 1)-3) cannot be unified<br>within a conventional TDT framework.Further topic time-<br>line analysis has not been addressed in it.</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:94"><nobr><span class="ft5">Swan and Allen [12] addressed the issue of how to auto-</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:81"><nobr><span class="ft10">matically overview timelines of a set of news stories.They<br>used the </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:145"><nobr><span class="ft12">2</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:151"><nobr><span class="ft5">-method to identify at each time a burst of fea-</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:81"><nobr><span class="ft10">ture terms that more frequently appear than at other times.<br>Similar issues are addressed in the visualization commu-<br>nity [3].However, all of the methods proposed there are<br>not designed to perform in an on-line fashion.</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:94"><nobr><span class="ft5">Kleinberg [4] proposed a formal model of "bursts of ac-</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:81"><nobr><span class="ft10">tivity" using an infinite-state automaton.This is closely<br>related to topic emergence detection in our framework.A<br>burst has a somewhat different meaning from a topic in<br>the sense that the former is a series of texts including a<br>specific feature, while the latter is a cluster of categorized<br>texts.Hence topic structure identification and characteriza-<br>tion cannot be dealt with in his model.Further note that<br>Kleinberg's model is not designed for real-time analysis but<br>for retrospective one.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:94"><nobr><span class="ft5">Related to our statistical modeling of a topic structure,</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:81"><nobr><span class="ft10">Liu et.al. [2] and Li and Yamanishi [6] also proposed meth-<br>ods for topic analysis using a finite mixture model.Specif-<br>ically, Liu et.al.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:192"><nobr><span class="ft5">considered the problem of selecting the</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft10">optimal number of mixture components in the context of<br>text clustering.In their approach a single model is selected<br>as an optimal model under the assumption that the opti-<br>mal model does not change over time.Meanwhile, in our</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft10">approach, a sequence of optimal models is selected dynam-<br>ically under the assumption that the optimal model may<br>change over time.</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:489"><nobr><span class="ft5">Related to topic emergence detection, Matsunaga and Ya-</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:475"><nobr><span class="ft10">manishi [7] proposed a basic method of dynamic model se-<br>lection, by which one can dynamically track the change of<br>number of components in the mixture model.However, any<br>of all of these technologies cannot straightforwardly be ap-<br>plied to real-time topic analysis in which the dimension of<br>data may increase as time goes by.</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:489"><nobr><span class="ft5">Related to topic structure identification, an on-line dis-</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:475"><nobr><span class="ft10">counting learning algorithm for estimating parameters in a<br>finite mixture model has been proposed by Yamanishi et.<br>al.[14]. The main difference between our algorithm and<br>theirs is that the former makes use of time-stamps in order<br>to make the topic structure affected by a timeline of top-<br>ics while the latter considers only the time-order of data<br>ignoring their time-stamps.</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:489"><nobr><span class="ft5">The rest of this paper is organized as follows: Section 2</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:475"><nobr><span class="ft10">describes a basic model of topic structure.Section 3 gives<br>a method for topic structure identification.Section 4 gives<br>a method for topic emergence detection.Section 5 gives a<br>method for topic characterization.Section 6 gives experi-<br>mental results.Section 7 gives concluding remarks.</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:475"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:484;left:507"><nobr><span class="ft4"><b>MODEL</b></span></nobr></DIV>
<DIV style="position:absolute;top:506;left:489"><nobr><span class="ft5">We employ a probabilistic model called a finite mixture</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:475"><nobr><span class="ft10">model for the representation of topic generation in a text<br>stream.Let</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:550"><nobr><span class="ft5">W = {w</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:600"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:607"><nobr><span class="ft5">, · · · , w</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:650"><nobr><span class="ft12">d</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:656"><nobr><span class="ft5">} be the complete vocabulary</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:475"><nobr><span class="ft10">set of the document corpus after the stop-words removal<br>and words stemming operations.For a given document x,<br>let tf(w</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:522"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:527"><nobr><span class="ft5">) be the term frequency of word w</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:727"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:735"><nobr><span class="ft5">in x.Let idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:824"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:829"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:475"><nobr><span class="ft5">be the idf value of w</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:603"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:608"><nobr><span class="ft5">, i. e. , idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:677"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:682"><nobr><span class="ft5">) = log(N/df(w</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:778"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:782"><nobr><span class="ft5">)) where</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:475"><nobr><span class="ft5">N is the total number of texts for reference and df(w</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:809"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:813"><nobr><span class="ft5">) is</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:475"><nobr><span class="ft5">the frequency of texts in which w</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:680"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:690"><nobr><span class="ft5">appears.Let tf-idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:824"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:829"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:475"><nobr><span class="ft5">be the tf-idf value of w</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:621"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:632"><nobr><span class="ft5">in x, i. e. , tf-idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:747"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:752"><nobr><span class="ft5">) = tf(w</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:809"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:814"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:823"><nobr><span class="ft5">×</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:475"><nobr><span class="ft5">log(N/df(w</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:545"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:550"><nobr><span class="ft5">)). We may represent a text x of the form:</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:584"><nobr><span class="ft5">x = (tf(w</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:644"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:650"><nobr><span class="ft5">), ..., tf (w</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:708"><nobr><span class="ft12">d</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:714"><nobr><span class="ft5">))</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:475"><nobr><span class="ft5">or</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:553"><nobr><span class="ft5">x = (tf-idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:642"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:648"><nobr><span class="ft5">), ..., tf -idf(w</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:734"><nobr><span class="ft12">d</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:741"><nobr><span class="ft5">)).</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:475"><nobr><span class="ft5">We may use either type of the representation forms.</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:489"><nobr><span class="ft5">Let K be a given positive integer representing the num-</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:475"><nobr><span class="ft10">ber of different topics.We suppose that a text has only<br>one topic and a text having the i-th topic is distributed<br>according to the probability distribution with a density:<br>p</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:482"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:487"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:510"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:515"><nobr><span class="ft5">) (i = 1, 2, · · · , K), where </span></nobr></DIV>
<DIV style="position:absolute;top:849;left:680"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:689"><nobr><span class="ft5">is a real-valued parame-</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:475"><nobr><span class="ft10">ter vector.We suppose here that x is distributed according<br>to a finite mixture distribution (see e.g., [8]) with K compo-<br>nents given by</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:575"><nobr><span class="ft5">p(x| : K) =</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:659"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:654"><nobr><span class="ft5">X</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:655"><nobr><span class="ft12">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:677"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:935;left:685"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:689"><nobr><span class="ft5">p(x|</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:720"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:725"><nobr><span class="ft5">),</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:816"><nobr><span class="ft5">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft5">where </span></nobr></DIV>
<DIV style="position:absolute;top:977;left:525"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:538"><nobr><span class="ft5">&gt; 0 (i = 1, · · · , K) and P</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:714"><nobr><span class="ft13">K<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:735"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:977;left:743"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:755"><nobr><span class="ft5">= 1. We set</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:475"><nobr><span class="ft5"> = (</span></nobr></DIV>
<DIV style="position:absolute;top:991;left:514"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:521"><nobr><span class="ft5">, · · · , </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:562"><nobr><span class="ft12">K-1</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:586"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:991;left:599"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:605"><nobr><span class="ft5">, · · · , </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:645"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:655"><nobr><span class="ft5">).Here </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:712"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:721"><nobr><span class="ft5">denotes the degree</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:475"><nobr><span class="ft10">to what the i-th topic is likely to appear in a text stream.<br>Note that each component in the mixture defines a single<br>cluster in the sense of soft-clustering.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft5">Throughout this paper we suppose that each p</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:762"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:766"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:790"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:795"><nobr><span class="ft5">) takes</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft5">a form of a Gaussian density: Letting d be the dimension of</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">812</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:80"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft14{font-size:9px;font-family:Helvetica;color:#000000;}
	.ft15{font-size:5px;font-family:Times;color:#000000;}
	.ft16{font-size:9px;line-height:13px;font-family:Helvetica;color:#000000;}
	.ft17{font-size:6px;line-height:11px;font-family:Times;color:#000000;}
	.ft18{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
	.ft19{font-size:6px;line-height:12px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="199003.png" alt="background image">
<DIV style="position:absolute;top:113;left:334"><nobr><span class="ft14">Text Data Stream</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:191"><nobr><span class="ft14">Discount Learning</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:208"><nobr><span class="ft16">Finite Mixture<br>Model 1</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:185"><nobr><span class="ft14">Dynamic Model Selection</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:398"><nobr><span class="ft14">Topic Emergence Detection</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:421"><nobr><span class="ft14">Topic Characteriztion</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:639"><nobr><span class="ft14">Timeline of Topics</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:346"><nobr><span class="ft14">Discount Learning</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:558"><nobr><span class="ft14">Discount Learning</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:368"><nobr><span class="ft16">Finite Mixture<br>Model 2</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:587"><nobr><span class="ft16">Finite Mixture<br>Model K</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:320"><nobr><span class="ft5">Figure 1: Topic Trend Analysis System</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:81"><nobr><span class="ft5">each datum,</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:81"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:88"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:92"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:116"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:121"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:140"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:164"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:445;left:172"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:177"><nobr><span class="ft5">(x|µ</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:202"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:207"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:445;left:223"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:228"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:437"><nobr><span class="ft5">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:140"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:204"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:165"><nobr><span class="ft5">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:192"><nobr><span class="ft12">d/2</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:209"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:223"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:228"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:476;left:232"><nobr><span class="ft12">1/2</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:253"><nobr><span class="ft5">exp</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:277"><nobr><span class="ft5">,,</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:287"><nobr><span class="ft5">- 12(x - µ</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:346"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:351"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:357"><nobr><span class="ft12">T</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:365"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:464;left:375"><nobr><span class="ft17">-1<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:390"><nobr><span class="ft5">(x - µ</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:429"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:433"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:439"><nobr><span class="ft5">«</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:451"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:81"><nobr><span class="ft5">where µ</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:128"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:137"><nobr><span class="ft5">is a d-dimensional real-valued vector, </span></nobr></DIV>
<DIV style="position:absolute;top:507;left:373"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:382"><nobr><span class="ft5">is a d × d-</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:81"><nobr><span class="ft5">dimensional matrix, and we set </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:285"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:295"><nobr><span class="ft5">= (µ</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:325"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:330"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:346"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:351"><nobr><span class="ft5">).In this case</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:81"><nobr><span class="ft10">(1) is so-called a Gaussian mixture.Note that a Gaussian<br>density may be replaced with any other form of probability<br>distributions, such as a multinomial distribution.</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:94"><nobr><span class="ft5">In terms of a finite mixture model, a topic structure is</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:81"><nobr><span class="ft10">identified by A) the number of components K (how many<br>topics exist), B) the weight vector (</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:311"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:317"><nobr><span class="ft5">, · · · , </span></nobr></DIV>
<DIV style="position:absolute;top:617;left:358"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:368"><nobr><span class="ft5">) indicating</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:81"><nobr><span class="ft10">how likely each topic appears, and C) the parameter values<br></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:87"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:95"><nobr><span class="ft5">(i = 1, · · · , K) indicating how each topic is distributed.A</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:81"><nobr><span class="ft10">topic structure in a text stream must be learned in an on-line<br>fashion.Topic emergence detection is conducted by track-<br>ing the change of main components in the mixture model.<br>Topic characterization is conducted by classifying each text<br>into the component for which the posterior is largest and<br>then by extracting feature terms characterizing the classi-<br>fied texts.Topic drift may be detected by tracking changes<br>of a parameter value </span></nobr></DIV>
<DIV style="position:absolute;top:774;left:215"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:770;left:224"><nobr><span class="ft5">for each topic i.These tasks will be</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:81"><nobr><span class="ft5">described in details in the sessions to follow.</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:94"><nobr><span class="ft5">The overall flow of the tasks is illustrated in Figure 1.</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:81"><nobr><span class="ft10">A text is sequentially input to the system.We prepare a<br>number of finite mixture models, for each of which we learn<br>statistical parameters using the time-stamp based learning<br>algorithm to perform topic identification.These tasks are<br>performed in parallel.On the basis of the input data and<br>learned models, we conduct dynamic model selection for<br>choosing the optimal finite mixture model.We then com-<br>pare the new optimal model with the last one to conduct<br>topic emergence detection.Finally for each component of<br>the optimal model, we conduct topic characterization.</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:81"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:992;left:112"><nobr><span class="ft18"><b>TOPIC STRUCTURE IDENTIFICATION<br>WITH DISCOUNTING LEARNING</b></span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:94"><nobr><span class="ft5">In this section we propose an algorithm for learning a topic</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft10">structure, which we call an time-stamp based discounting<br>topic learning algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:489"><nobr><span class="ft5">The algorithm is basically designed as a variant of the in-</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:475"><nobr><span class="ft10">cremental EM algorithm for learning a finite mixture model (see,<br>e.g., Neal and Hinton [9]). Our proposed one is distinguished<br>from existing ones with regards to the following three main<br>features:<br>1) Adaptive to the change of the topic structure. The pa-<br>rameters are updated by forgetting out-of-date statistics as<br>time goes on.This is realized by putting a larger weight to<br>the statistics for a more recent data.<br>2) Making use of time stamps for texts. Not only the time<br>order of texts but also their time stamps are utilized to make<br>the topic structure depend on the timeline.For example, for<br>two text data x</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:567"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:609;left:572"><nobr><span class="ft15">1</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:578"><nobr><span class="ft5">, x</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:592"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:609;left:596"><nobr><span class="ft15">2</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:607"><nobr><span class="ft5">(t</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:617"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:627"><nobr><span class="ft5">&lt; t</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:647"><nobr><span class="ft12">2</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:653"><nobr><span class="ft5">), if the length t</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:747"><nobr><span class="ft12">2</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:755"><nobr><span class="ft5">-t</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:772"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:782"><nobr><span class="ft5">is larger,</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:475"><nobr><span class="ft5">the topic structure learned at time t</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:690"><nobr><span class="ft12">2</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:701"><nobr><span class="ft5">will be less affected by</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:475"><nobr><span class="ft5">that at time t</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:559"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:565"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:475"><nobr><span class="ft10">3) Normalizing data of different dimensions. We consider<br>the on-line situation where the dimension of a datum may<br>increase as time goes by.This situation actually occurs be-<br>cause new words may possibly be added to the list of words<br>every time a new text is input.Hence it is needed for nor-<br>malizing data of different dimensions.</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:489"><nobr><span class="ft5">We suppose that text data x</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:676"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:683"><nobr><span class="ft5">, x</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:697"><nobr><span class="ft12">2</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:703"><nobr><span class="ft5">, ... are given in this</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:475"><nobr><span class="ft10">order, and each has a time-stamp indicating when it ap-<br>peared.Here is a description of the algorithm, in which <br>is a discounting parameter, </span></nobr></DIV>
<DIV style="position:absolute;top:796;left:650"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:660"><nobr><span class="ft5">denotes the posterior density</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:475"><nobr><span class="ft10">of the ith component, and m is introduced for calculation<br>of weights for old statistics.</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:475"><nobr><span class="ft5">Time-stamp Based Discounting Learning Algorithm</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:475"><nobr><span class="ft10">Initialization:<br>Set initial values of </span></nobr></DIV>
<DIV style="position:absolute;top:896;left:606"><nobr><span class="ft12">(0)</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:606"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:621"><nobr><span class="ft5">, µ</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:635"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:650"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:896;left:666"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:681"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:700"><nobr><span class="ft12">(0)</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:719"><nobr><span class="ft5">(i = 1, · · · , k).Let</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:475"><nobr><span class="ft5"> &gt; 0, 0 &lt;  &lt; 1 be given.</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:475"><nobr><span class="ft10">Iteration:<br>For t = 1, 2, .. do the following procedure.<br>For the t-th data be x</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:608"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:617"><nobr><span class="ft5">and its time stamp be t</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:759"><nobr><span class="ft12">new</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:780"><nobr><span class="ft5">.Let the</span></nobr></DIV>
<DIV style="position:absolute;top:996;left:475"><nobr><span class="ft5">time stamp of the (t - 1)-th data be t</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:705"><nobr><span class="ft12">old</span></nobr></DIV>
<DIV style="position:absolute;top:996;left:721"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:475"><nobr><span class="ft5">For i = 1, · · · , k, update the parameters according to the</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">813</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:593"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft20{font-size:11px;line-height:18px;font-family:Times;color:#000000;}
	.ft21{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
	.ft22{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft23{font-size:6px;line-height:13px;font-family:Times;color:#000000;}
	.ft24{font-size:15px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="199004.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft5">following rules:</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:84"><nobr><span class="ft5">p(i|x</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:113"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:118"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:184"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:108;left:192"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:192"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:220"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:227"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:231"><nobr><span class="ft5">(x</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:245"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:250"><nobr><span class="ft5">|µ</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:262"><nobr><span class="ft19">(t-1)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:290"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:306"><nobr><span class="ft19">(t-1)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:333"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:166"><nobr><span class="ft5">P</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:181"><nobr><span class="ft13">k<br>l=1</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:201"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:129;left:210"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:209"><nobr><span class="ft12">l</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:237"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:244"><nobr><span class="ft12">l</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:249"><nobr><span class="ft5">(x</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:262"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:267"><nobr><span class="ft5">|µ</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:279"><nobr><span class="ft19">(t-1)<br>l</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:307"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:129;left:323"><nobr><span class="ft19">(t-1)<br>l</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:351"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:101"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:153;left:109"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:109"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:164"><nobr><span class="ft5">WA(p(i|x</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:224"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:229"><nobr><span class="ft5">), 1/k|1, )</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:101"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:176;left:109"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:109"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:164"><nobr><span class="ft5">WA(</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:204"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:204"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:218"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:176;left:232"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:231"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:245"><nobr><span class="ft5">|m</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:261"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:289"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:177;left:303"><nobr><span class="ft12">-(t</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:321"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:341"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:354"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:369"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:374"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:101"><nobr><span class="ft5">µ</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:109"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:164"><nobr><span class="ft5">WA(µ</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:204"><nobr><span class="ft19">(t-1)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:232"><nobr><span class="ft5">, x</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:246"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:251"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:262"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:267"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:279"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:307"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:200;left:321"><nobr><span class="ft12">-(t</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:339"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:359"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:372"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:387"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:392"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:199;left:400"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:399"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:413"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:100"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:109"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:164"><nobr><span class="ft5">WA(</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:205"><nobr><span class="ft19">(t-1)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:233"><nobr><span class="ft5">, x</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:247"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:252"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:260"><nobr><span class="ft12">T</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:260"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:268"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:280"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:285"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:297"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:325"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:223;left:339"><nobr><span class="ft12">-(t</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:356"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:376"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:389"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:404"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:409"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:417"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:235;left:417"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:431"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:99"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:109"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:164"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:174"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:191"><nobr><span class="ft5">- µ</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:213"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:218"><nobr><span class="ft5">µ</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:226"><nobr><span class="ft12">T</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:226"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:97"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:109"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:136"><nobr><span class="ft5">:=</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:164"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:269;left:172"><nobr><span class="ft12">(t</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:181"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:201"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:214"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:229"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:234"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:246"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:277"><nobr><span class="ft5">+ 1,</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:81"><nobr><span class="ft5">where</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:121"><nobr><span class="ft5">WA denotes the operation such that</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:139"><nobr><span class="ft5">WA(X, Y |A, B) =</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:271"><nobr><span class="ft5">A</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:257"><nobr><span class="ft5">A + B X +</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:341"><nobr><span class="ft5">B</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:328"><nobr><span class="ft5">A + B Y.</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:94"><nobr><span class="ft5">Generally, we set the initial value </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:307"><nobr><span class="ft12">(0)</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:306"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:325"><nobr><span class="ft5">= 1/K, m</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:384"><nobr><span class="ft12">(0)</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:403"><nobr><span class="ft5">= 0, a</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:81"><nobr><span class="ft5">small value to </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:175"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:190"><nobr><span class="ft5">, and set µ</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:251"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:269"><nobr><span class="ft5">the first x</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:326"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:331"><nobr><span class="ft5">s that are different</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:81"><nobr><span class="ft5">each other.This algorithm updates </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:316"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:321"><nobr><span class="ft5">, µ</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:339"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:344"><nobr><span class="ft5">, and </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:392"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:402"><nobr><span class="ft5">as the</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:81"><nobr><span class="ft10">weighted average of the latest parameter value and the new<br>statistics.The weight ratio is m</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:278"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:310"><nobr><span class="ft5">: </span></nobr></DIV>
<DIV style="position:absolute;top:429;left:326"><nobr><span class="ft12">-(t</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:343"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:363"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:376"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:429;left:391"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:401"><nobr><span class="ft5">for </span></nobr></DIV>
<DIV style="position:absolute;top:439;left:431"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:436"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:81"><nobr><span class="ft5">and </span></nobr></DIV>
<DIV style="position:absolute;top:456;left:116"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:120"><nobr><span class="ft5">m</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:132"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:164"><nobr><span class="ft5">: </span></nobr></DIV>
<DIV style="position:absolute;top:447;left:180"><nobr><span class="ft12">-(t</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:197"><nobr><span class="ft15">new</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:217"><nobr><span class="ft12">-t</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:230"><nobr><span class="ft15">old</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:245"><nobr><span class="ft12">)</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:250"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:258"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:257"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:276"><nobr><span class="ft5">for µ</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:306"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:315"><nobr><span class="ft5">and </span></nobr></DIV>
<DIV style="position:absolute;top:456;left:352"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:357"><nobr><span class="ft5">, respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:94"><nobr><span class="ft5">Note that Yamanishi et.al.'s sequentially discounting learn-</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:81"><nobr><span class="ft10">ing algorithm [14] can be thought of as a special case of this<br>algorithm in which the time interval t</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:306"><nobr><span class="ft12">l+1</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:326"><nobr><span class="ft5">- t</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:344"><nobr><span class="ft12">l</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:352"><nobr><span class="ft5">is independent</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:81"><nobr><span class="ft10">of l.In that case if we further let  = 1, the algorithm<br>becomes an ordinary incremental EM algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:94"><nobr><span class="ft5">In real implementation, we supposed that </span></nobr></DIV>
<DIV style="position:absolute;top:550;left:356"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:365"><nobr><span class="ft5">is a diagonal</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft10">matrix for the sake of computational complexity issues.The<br>scalability issue for dealing with a general matrix </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:384"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:392"><nobr><span class="ft5">remains</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:81"><nobr><span class="ft5">for future study.</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:81"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:629;left:112"><nobr><span class="ft18"><b>TOPIC EMERGENCE DETECTION WITH<br>DYNAMIC MODEL SELECTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:672;left:94"><nobr><span class="ft5">In this section we are concerned with the issue of topic</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:81"><nobr><span class="ft10">emergence detection, i.e., tracking the emergence of a new<br>topic.We reduce here this issue to that of selecting the<br>optimal components in the mixture model dynamically.We<br>call this statistical issue dynamic model selection (see also<br>[7]).</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:94"><nobr><span class="ft5">The key idea of dynamic model selection is to first learn a</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:81"><nobr><span class="ft10">finite mixture model with a relatively large number of com-<br>ponents, then to select main components dynamically from<br>among them on the basis of Rissanen's predictive stochastic<br>complexity [10].</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:94"><nobr><span class="ft5">The procedure of dynamic model selection is described as</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:81"><nobr><span class="ft5">follows:</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:81"><nobr><span class="ft10">Initialization:<br>Let K</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:118"><nobr><span class="ft12">max</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:146"><nobr><span class="ft5">(maximum number of mixture components) and</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:81"><nobr><span class="ft20">W (window size) be given positive integers.<br>Set initial values of </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:204"><nobr><span class="ft12">(0)</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:203"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:219"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:231"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:250"><nobr><span class="ft5">= (µ</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:278"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:293"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:309"><nobr><span class="ft19">(0)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:324"><nobr><span class="ft5">) (i = 1, · · · , K</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:413"><nobr><span class="ft12">max</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:436"><nobr><span class="ft5">).</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:81"><nobr><span class="ft10">Iteration:<br>For t = 1, 2, · · · , do the following procedure 1 to 4:</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:81"><nobr><span class="ft10">1. Model Class Construction:<br>Let G</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:117"><nobr><span class="ft21">t<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:127"><nobr><span class="ft5">be the window average of the posterior probability</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:81"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:94"><nobr><span class="ft12">t-W</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:93"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:121"><nobr><span class="ft5">+</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:133"><nobr><span class="ft5">· · ·+</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:170"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:170"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:175"><nobr><span class="ft5">)/W .For k = 1, · · · , K</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:314"><nobr><span class="ft12">max</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:337"><nobr><span class="ft5">, do the following</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft5">procedure:</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:475"><nobr><span class="ft5">Let</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:507"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:513"><nobr><span class="ft5">, · · · ,</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:552"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:564"><nobr><span class="ft5">be the indices of k highest scores such that</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:475"><nobr><span class="ft5">G</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:486"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:491"><nobr><span class="ft15">1</span></nobr></DIV>
<DIV style="position:absolute;top:101;left:521"><nobr><span class="ft5"> · · ·  G</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:590"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:595"><nobr><span class="ft15">k</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:618"><nobr><span class="ft5">.Construct the following mixture</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:475"><nobr><span class="ft5">model with k components: For s = t - W, · · · , t,</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:512"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:519"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:546"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:569"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:575"><nobr><span class="ft5">, · · · ,</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:614"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:621"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:640"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:664"><nobr><span class="ft12">k-1</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:664"><nobr><span class="ft5">X</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:665"><nobr><span class="ft12">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:686"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:151;left:695"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:699"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:723"><nobr><span class="ft5">p</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:734"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:741"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:764"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:769"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:792"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:664"><nobr><span class="ft5">+</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:677"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:688"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:698"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:712"><nobr><span class="ft12">k-1</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:712"><nobr><span class="ft5">X</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:712"><nobr><span class="ft12">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:734"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:201;left:743"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:747"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:770"><nobr><span class="ft5">!</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:784"><nobr><span class="ft5">U.</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:475"><nobr><span class="ft5">where</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:515"><nobr><span class="ft5">U is a uniform distribution over the domain.</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:475"><nobr><span class="ft10">2. Predictive Stochastic Complexity Calculation:<br>When the t-th input data x</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:652"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:664"><nobr><span class="ft5">with dimension d</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:771"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:783"><nobr><span class="ft5">is given,</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:475"><nobr><span class="ft5">compute</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:499"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:509"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:522"><nobr><span class="ft5">(k) =</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:576"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:568"><nobr><span class="ft5">X</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:559"><nobr><span class="ft12">s=t-W</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:600"><nobr><span class="ft5">"-logp</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:648"><nobr><span class="ft12">(s)</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:663"><nobr><span class="ft5">(x</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:676"><nobr><span class="ft12">s</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:682"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:691"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:698"><nobr><span class="ft5">, · · · ,</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:736"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:743"><nobr><span class="ft5">)/d</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:762"><nobr><span class="ft12">s</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:769"><nobr><span class="ft5">"</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:779"><nobr><span class="ft5">.</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:816"><nobr><span class="ft5">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:475"><nobr><span class="ft10">3. Model Selection:<br>Select k</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:521"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:410;left:521"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:531"><nobr><span class="ft5">minimizing S</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:610"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:624"><nobr><span class="ft5">(k).Let p</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:686"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:692"><nobr><span class="ft5">(x|</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:716"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:721"><nobr><span class="ft15">j</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:744"><nobr><span class="ft5">) (j = 1, · · · , k</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:830"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:410;left:830"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:837"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:475"><nobr><span class="ft5">be main components at time t, which we write as {C</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:781"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:780"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:795"><nobr><span class="ft5">, · · · C</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:831"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:830"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:836"><nobr><span class="ft15"></span></nobr></DIV>
<DIV style="position:absolute;top:438;left:836"><nobr><span class="ft15">t</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:844"><nobr><span class="ft5">}.</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:475"><nobr><span class="ft10">4. Estimation of Parameters:<br>Learn a finite mixture model with K</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:698"><nobr><span class="ft12">max</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:726"><nobr><span class="ft5">components using</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:475"><nobr><span class="ft22">the time-stamp based discounting learning algorithm.Let<br>the estimated parameter be (</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:655"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:654"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:668"><nobr><span class="ft5">, · · · , </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:710"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:709"><nobr><span class="ft12">K</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:718"><nobr><span class="ft15">max</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:741"><nobr><span class="ft5">, </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:754"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:753"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:768"><nobr><span class="ft5">, · · · , </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:807"><nobr><span class="ft23">(t)<br>K</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:816"><nobr><span class="ft15">max</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:838"><nobr><span class="ft5">).</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:489"><nobr><span class="ft5">Note that the S</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:589"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:602"><nobr><span class="ft5">(k) can be thought of as a variant of</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:475"><nobr><span class="ft10">Rissanen's predictive stochastic complexity [10] normalized<br>by the dimension for each datum, which can be interpreted<br>as the total code length required for encoding a data stream<br>x</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:483"><nobr><span class="ft12">t-W</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:509"><nobr><span class="ft5">, ..., x</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:540"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:550"><nobr><span class="ft5">into a binary string sequentially.</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:489"><nobr><span class="ft5">Once main components C</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:652"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:639;left:651"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:665"><nobr><span class="ft5">, · · · , C</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:709"><nobr><span class="ft12">(t)</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:708"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:714"><nobr><span class="ft15"></span></nobr></DIV>
<DIV style="position:absolute;top:646;left:714"><nobr><span class="ft15">t</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:730"><nobr><span class="ft5">are obtained, we</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:475"><nobr><span class="ft5">compare them with C</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:612"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:611"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:640"><nobr><span class="ft5">, · · · , C</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:684"><nobr><span class="ft12">(t-1)</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:683"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:689"><nobr><span class="ft15"></span></nobr></DIV>
<DIV style="position:absolute;top:669;left:689"><nobr><span class="ft15">t-1</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:718"><nobr><span class="ft5">to check the emer-</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:475"><nobr><span class="ft10">gence of a new topic or the disappearance of an existing<br>topic in the following way.If a new component is selected<br>at some point and remains for a longer time than a specified<br>threshold, we may determine that a new topic has emerged.<br>Specifically, if the optimal number k</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:706"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:706"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:719"><nobr><span class="ft5">of components be-</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:475"><nobr><span class="ft5">comes larger than k</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:597"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:761;left:597"><nobr><span class="ft12">t-1</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:616"><nobr><span class="ft5">, we can recognize that a new topic</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:475"><nobr><span class="ft10">has emerged.Similarly, if an existing component is not se-<br>lected at some time and does not appear any longer, then<br>we may determine that the topic has disappeared.</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:475"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:835;left:507"><nobr><span class="ft24"><b>TOPIC CHARACTERIZATION WITH IN-<br>FORMATION GAIN</b></span></nobr></DIV>
<DIV style="position:absolute;top:878;left:489"><nobr><span class="ft5">Once the optimal finite mixture model is obtained, we</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:475"><nobr><span class="ft10">are concerned with the issue of how to characterize each<br>topic.We address this issue by extracting terms character-<br>izing each topic and by observing the growth or decay of<br>each topic component.Details are shown below.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:489"><nobr><span class="ft5">A) Extracting terms characterizing each topic.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:791"><nobr><span class="ft5">We at-</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft10">tempt to characterize each topic by extracting characteris-<br>tic words for it.We perform this task by computing the<br>information gain of possible words.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:489"><nobr><span class="ft5">In the time-stamp based discounting topic learning algo-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft10">rithm, the posterior probability distribution over the set of<br>clusters is estimated every time a text data is input.Ac-<br>cording to that posterior distribution an input text will be</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">814</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:80"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft25{font-size:9px;font-family:Times;color:#000000;}
	.ft26{font-size:7px;font-family:Helvetica;color:#000000;}
	.ft27{font-size:4px;font-family:Helvetica;color:#000000;}
	.ft28{font-size:8px;font-family:Times;color:#000000;}
	.ft29{font-size:6px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="199005.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft10">categorized into the component for which the posterior prob-<br>ability is largest.This clustering task can be performed in<br>an on-line fashion.</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:94"><nobr><span class="ft5">After observing the t-th datum x</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:302"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:306"><nobr><span class="ft5">, for i = 1, · · · , k, let</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:81"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:89"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:94"><nobr><span class="ft5">(i) be the set of texts in x</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:256"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:266"><nobr><span class="ft5">= x</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:289"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:295"><nobr><span class="ft5">, .., x</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:323"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:333"><nobr><span class="ft5">classified into the</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft5">i-th component and let t</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:237"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:248"><nobr><span class="ft5">be the size of</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:339"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:347"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:352"><nobr><span class="ft5">(i).Let S</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:417"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:429"><nobr><span class="ft5">=</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:81"><nobr><span class="ft5"></span></nobr></DIV>
<DIV style="position:absolute;top:178;left:90"><nobr><span class="ft12">k</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:90"><nobr><span class="ft12">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:109"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:117"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:122"><nobr><span class="ft5">(i).</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:94"><nobr><span class="ft5">Below we show the method for computing the information</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft10">gain of a term w for each topic component.For any term<br>w, let S(w) be a set of vectors in S</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:289"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:298"><nobr><span class="ft5">such that the frequency</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:81"><nobr><span class="ft5">of w is larger than a given threshold, and let m</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:363"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:376"><nobr><span class="ft5">be the size</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:81"><nobr><span class="ft5">of</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:97"><nobr><span class="ft5">S(w).Let S( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:181"><nobr><span class="ft5">w) be a set of vectors in S</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:347"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:357"><nobr><span class="ft5">such that the</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:81"><nobr><span class="ft5">frequency of w is not larger than the threshold, and m</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:414"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:412"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:425"><nobr><span class="ft5">be</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:81"><nobr><span class="ft5">the size of</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:147"><nobr><span class="ft5">S( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:161"><nobr><span class="ft5">w).</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:94"><nobr><span class="ft5">For a specified topic component, say, the i-th component,</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:81"><nobr><span class="ft5">let m</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:114"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:114"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:128"><nobr><span class="ft5">be the number of vectors in</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:304"><nobr><span class="ft5">S(w) that are also in-</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft5">cluded in</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:142"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:150"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:155"><nobr><span class="ft5">(i).Let m</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:220"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:222"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:220"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:234"><nobr><span class="ft5">be the number of vectors in</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:409"><nobr><span class="ft5">S( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:424"><nobr><span class="ft5">w)</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:81"><nobr><span class="ft5">that are also included in</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:233"><nobr><span class="ft5">S</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:241"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:246"><nobr><span class="ft5">(i).</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:94"><nobr><span class="ft5">Then we define the information gain of w for the i-th topic</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:81"><nobr><span class="ft5">component as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:115"><nobr><span class="ft5">IG(w|i) = I(t, t</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:210"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:214"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:223"><nobr><span class="ft5">- `I(m</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:268"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:276"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:295"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:295"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:304"><nobr><span class="ft5">) + I(m</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:353"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:351"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:360"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:378"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:413;left:380"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:378"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:387"><nobr><span class="ft5">)´ ,</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:81"><nobr><span class="ft10">where I(x, y) is the information measure such as stochastic<br>complexity [10], extended stochastic complexity [13][5].The<br>stochastic complexity [10] is given as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:146"><nobr><span class="ft5">I(x, y) = xH</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:227"><nobr><span class="ft5">" y</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:237"><nobr><span class="ft5">x</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:246"><nobr><span class="ft5">"</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:258"><nobr><span class="ft5">+ 1</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:273"><nobr><span class="ft5">2 log</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:304"><nobr><span class="ft5">" x</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:319"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:332"><nobr><span class="ft5">"</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:343"><nobr><span class="ft5">,</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft10">where H(x) = -x log x - (1 - x) log(1 - x) log(1 - x) is the<br>binary entropy function, and log's base is 2.A special case<br>of extended stochastic complexity is given as follows [13][5]:</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:137"><nobr><span class="ft5">I(x, y) = min{y, x - y} + cpx log x,</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:81"><nobr><span class="ft5">where c is a constant.</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:94"><nobr><span class="ft5">We select a specified number of terms ws of largest in-</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:81"><nobr><span class="ft10">formation gains.We can think of them the set of terms<br>characterizing the i-th topic component.</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:94"><nobr><span class="ft5">The statistics: m</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:205"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:214"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:233"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:233"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:242"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:262"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:260"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:269"><nobr><span class="ft5">, m</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:287"><nobr><span class="ft12">+</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:290"><nobr><span class="ft12">¯</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:287"><nobr><span class="ft12">w</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:303"><nobr><span class="ft5">needed for computing</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:81"><nobr><span class="ft10">information gain can be calculated in an on-line fashion.<br>Hence topic characterization task is conducted in real-time.</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:94"><nobr><span class="ft5">B) Observing the growth or decay of each cluster. Let G</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:426"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:739;left:81"><nobr><span class="ft10">be the window average of the posterior probability of the i-<br>th topic component, that is, G</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:274"><nobr><span class="ft21">t<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:285"><nobr><span class="ft5">= (</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:315"><nobr><span class="ft12">t-W</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:315"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:341"><nobr><span class="ft5">+, · · · , +</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:403"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:760;left:403"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:408"><nobr><span class="ft5">)/W .</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:81"><nobr><span class="ft5">G</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:91"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:110"><nobr><span class="ft5">increases when texts corresponding to the i-th topic is</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:81"><nobr><span class="ft22">input, and decreases when the other is input.We can see<br>how rapidly this topic grows by observing G</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:348"><nobr><span class="ft19">(t)<br>i</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:366"><nobr><span class="ft5">as t goes by.</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:81"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:841;left:112"><nobr><span class="ft4"><b>EXPERIMENTAL RESULTS</b></span></nobr></DIV>
<DIV style="position:absolute;top:863;left:94"><nobr><span class="ft5">We conducted an experiment on real data: contact data of</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft10">a help desk for an internal e-mail service.An example of the<br>data record is presented in Table 1.It has the field of contact<br>date/time, question/request, answered date/time, answer,<br>and so on.The number of the records is 1202.The date of<br>the first/last contact is Feb 21 2004/May 20 respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:94"><nobr><span class="ft5">We input contact dates as the time-stamps, and ques-</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:81"><nobr><span class="ft5">tions/requests as the text data to our system.We set K</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:417"><nobr><span class="ft12">max</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:81"><nobr><span class="ft10">to 50 and  to 0.99.Our system ran on an NEC Express5800<br>with 1GHz Pentium III and a 1GB memory.The system was<br>implemented using C, and OS was Windows 2000 Server.<br>Processing 1202 records of data took about five minute.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:94"><nobr><span class="ft5">Figure 2 shows the number of components k</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:362"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:362"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:373"><nobr><span class="ft5">selected by</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft5">our system as main topics .The number increases at the</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:804"><nobr><span class="ft26">Date</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:494"><nobr><span class="ft26">k</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:499"><nobr><span class="ft27">*t</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:539"><nobr><span class="ft5">Figure 2: Number of main topics</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:475"><nobr><span class="ft10">beginning of March, and has a peak in the middle of April.<br>Since a fiscal year begins at April in Japan, we can suppose<br>that the number of topics at the help desk is increasing<br>around the first day of April.</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:489"><nobr><span class="ft5">Let us look into a few of the components, because we do</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:475"><nobr><span class="ft10">not have enough space for all of the components.Here, we<br>observe Component 27 and 42 in detail.Figure 3 shows the<br>window averages G</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:591"><nobr><span class="ft12">27</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:603"><nobr><span class="ft5">, G</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:620"><nobr><span class="ft12">42</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:636"><nobr><span class="ft5">of the posterior probabilities and</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:475"><nobr><span class="ft10">the periods where the components are selected as main top-<br>ics. G</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:511"><nobr><span class="ft12">27</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:527"><nobr><span class="ft5">increases in the beginning of April and has the first</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:715"><nobr><span class="ft26">G</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:722"><nobr><span class="ft27">27</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:566"><nobr><span class="ft26">G</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:573"><nobr><span class="ft27">42</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:773"><nobr><span class="ft29">C27 is main.</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:661"><nobr><span class="ft29">C42 is main.</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:495"><nobr><span class="ft5">Figure 3: G</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:576"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:586"><nobr><span class="ft5">and Period of Component 27, 42</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:475"><nobr><span class="ft10">peak at April 12.Then it repeats increase and decrease until<br>the middle of May.The corresponding component is selected<br>as main from the first week of April, and remains as main<br>until the middle of May (with short discontinuances). G</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:822"><nobr><span class="ft12">42</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:475"><nobr><span class="ft10">is positive during April, and also the corresponding topic is<br>main during April.The lines of G</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:679"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:683"><nobr><span class="ft5">s indicate how important</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:475"><nobr><span class="ft10">the corresponding topics are in each time.Moreover, we<br>can observe how the emerged topics grows and disappears<br>from the figure.The topic corresponding to Component 42<br>emerges at the beginning of April, grows for two weeks, is<br>attenuated, then drops out from the main topics at the end<br>of April.</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:489"><nobr><span class="ft5">Term "transfer" was extracted as a characteristic word</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">815</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:593"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="199006.png" alt="background image">
<DIV style="position:absolute;top:96;left:302"><nobr><span class="ft5">Table 1: Examples of help desk data records</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:98"><nobr><span class="ft25">Contact date/time</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:273"><nobr><span class="ft25">Question/Request</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:444"><nobr><span class="ft25">Answered date/time</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:573"><nobr><span class="ft25">Answer</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:802"><nobr><span class="ft25">,,,</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:99"><nobr><span class="ft25">Feb 26 2004 14:05</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:223"><nobr><span class="ft25">I forgot my password. How can I ...</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:451"><nobr><span class="ft25">Feb 26 2004 14:48</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:574"><nobr><span class="ft25">You can get a new ...</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:99"><nobr><span class="ft25">Feb 26 2004 14:08</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:217"><nobr><span class="ft25">Until what time is an account for a ...</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:451"><nobr><span class="ft25">Feb 26 2004 14:25</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:574"><nobr><span class="ft25">It is valid for 14 days after retirement.</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:99"><nobr><span class="ft25">Feb 26 2004 14:09</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:217"><nobr><span class="ft25">Is it possible to forward mails from ...</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:451"><nobr><span class="ft25">Feb 26 2004 15:09</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:574"><nobr><span class="ft25">Yes. You can set up by ....</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:142"><nobr><span class="ft25">....</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:316"><nobr><span class="ft25">....</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:494"><nobr><span class="ft25">....</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:573"><nobr><span class="ft25">....</span></nobr></DIV>
<DIV style="position:absolute;top:226;left:81"><nobr><span class="ft10">for Component 27.Texts classified into this component are<br>questions like "Is it possible to use Service XXX after I am<br>transfered to YYY?".That kind of questions may increase<br>around the beginning of a fiscal year."Service ZZZ" and<br>"failure" were extracted as chracteristic words for Compo-<br>nent 42.Actually, Service ZZZ failed in the beginning of<br>April, then, the topic consists of related complaints and<br>questions.</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:94"><nobr><span class="ft5">In this way we can recognize the emergence, growth, and</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:81"><nobr><span class="ft10">decay of each topic from the system.Through this example<br>it has turned out that our framework for topic trend analysis<br>are very effective for tracking dynamics of topic trends in<br>contact data at a help desk.</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:81"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:112"><nobr><span class="ft4"><b>CONCLUSION AND FUTURE STUDY</b></span></nobr></DIV>
<DIV style="position:absolute;top:468;left:94"><nobr><span class="ft5">In this paper we have proposed a framework for tracking</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:81"><nobr><span class="ft10">dynamics of topic trends using a finite mixture model.In<br>this framework the three main tasks: topic structure identi-<br>fication, topic emergence detection, and topic characteriza-<br>tion are unified within a single framework.Topic structure<br>identification has been realized by our unique time-stamp<br>based learning algorithm.It enables tracking topic struc-<br>tures adaptively by forgetting out-of-date statistics.Topic<br>emergence detection has been realized on the basis of the<br>theory of dynamic model selection.It enables detecting<br>changes of the optimal number of components in the finite<br>mixture model to check whether a new topic has appeared<br>or not.Topic characterization has been realized by on-line<br>text clustering and feature extraction based on information<br>gain.Through the experiments using real data collected at a<br>help desk, it is demonstrated that our framework works well<br>in the sense that dynamics of topic trends can be tracked in<br>a timely fashion.</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:94"><nobr><span class="ft10">The following issues remain open for future study:<br>Context-based topic trend analysis: In this paper we have</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:81"><nobr><span class="ft10">proposed an approach to word-based topic trend analysis.<br>However, we need to further analyze contexts, i.e., relations<br>among words, in order to more deeply analyze the semantics<br>of topics.</span></nobr></DIV>
<DIV style="position:absolute;top:844;left:94"><nobr><span class="ft5">Multi-topics analysis: We supposed that one text comes</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:81"><nobr><span class="ft10">from a single mixture component corresponding to a single<br>topic.It is our future study how to deal with texts having<br>multi topics.</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:81"><nobr><span class="ft4"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:112"><nobr><span class="ft4"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:940;left:95"><nobr><span class="ft5">[1] J.Allen, R.Papka, and V.Lavrenko: On-line new</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:116"><nobr><span class="ft10">event detection and tracking, in Proceedings of<br>SIGIR International Conference on Information<br>Retrieval, pp:37-45, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:94"><nobr><span class="ft5">[2] X.Liu, Y.Gong, W.Xu, and S.Zhu: Document</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:116"><nobr><span class="ft10">clustering with cluster refinement and model<br>selection capabilities, in Proceedings of SIGIR<br>International Conference on Information Retrieval,<br>pp:191-198, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:226;left:489"><nobr><span class="ft5">[3] S.Harve, B.Hetzler, and L.Norwell: ThemeRiver:</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:510"><nobr><span class="ft10">Visualizing theme changes over time, in Proceesings<br>of IEEE Symposium on Information Visualization,<br>pp:115-123, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:489"><nobr><span class="ft5">[4] J.Kleiberg: Bursty and hierarchical structure in</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:510"><nobr><span class="ft10">streams, in Proceedings of KDD2002, pp:91-101,<br>ACM Press, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:489"><nobr><span class="ft5">[5] H.Li and K.Yamanishi: Text classification using</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:510"><nobr><span class="ft10">ESC-based decision lists, Information Processing and<br>Management, vol.38/3, pp:343-361, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:489"><nobr><span class="ft5">[6] H.Li and K.Yamanishi: Topic analysis using a finite</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:510"><nobr><span class="ft10">mixture model, Information Processing and<br>Management, Vol.39/4, pp 521-541, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:489"><nobr><span class="ft5">[7] Y.Matsunaga and K.Yamanishi: An</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:510"><nobr><span class="ft10">information-theoretic approach to detecting<br>anomalous behaviors, in Information Technology<br>Letters vol.2 (Proc. of the 2nd Forum on Information<br>Technologies), pp:123-124, (in Japanese) 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:489"><nobr><span class="ft5">[8] G.McLahlan and D.Peel: Finite Mixture Models,</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:510"><nobr><span class="ft10">Wiley Series in Probability and Statistics, John<br>Wiley and Sons, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:489"><nobr><span class="ft5">[9] R.M.Neal and G.E.Hinton: A view of the EM</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:510"><nobr><span class="ft10">algorithm that justifies incremental sparse, and other<br>variants, Learning in Graphical Models, M. Jordan<br>(editor), MIT Press, Cambridge MA, USA.</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:482"><nobr><span class="ft5">[10] J.Rissanen: Universal coding, information, and</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:510"><nobr><span class="ft10">estimation, IEEE Trans. on Inform. Theory,<br>30:629-636, 1984.</span></nobr></DIV>
<DIV style="position:absolute;top:678;left:482"><nobr><span class="ft5">[11] R.Swan and J.Allen: Extracting significant</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:510"><nobr><span class="ft10">time-varying features from text, in Proceedings of 8th<br>International Conference on Information Knowledge<br>Management, pp:38-45, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:482"><nobr><span class="ft5">[12] R.Swan and J.Allen: Automatic generation of</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:510"><nobr><span class="ft10">overview timelines, in Proceedings of SIGIR<br>International Conference on Information Retrieval,<br>pp:49-56, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:482"><nobr><span class="ft5">[13] K.Yamanishi: A Decision-theoretic Extension of</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:510"><nobr><span class="ft10">Stochastic Complexity and Its Applications to<br>Learning, IEEE Trans. on Inform. Theory, vol.44/4,<br>pp:1424-1439, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:482"><nobr><span class="ft5">[14] K.Yamanishi, J.Takeuchi, G.Williams, and P.Milne:</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:510"><nobr><span class="ft10">On-line unsupervised outlier detection using finite<br>mixtures with discounting learning algorithms," in<br>Proceedings of KDD2000, ACM Press, pp:320­324<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:482"><nobr><span class="ft5">[15] Y.Yang, T.Pierce, J.G.Carbonell: A study on</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:510"><nobr><span class="ft10">retrospective and on-line event detection, in<br>Proceedings of SIGIR International Conference on<br>Information Retrieval, pp:28-30, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:482"><nobr><span class="ft5">[16] Y.Yang, J.Zang, J.Carbonell, and C.Jin:</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:510"><nobr><span class="ft10">Topic-conditioned novelty detection, in Proceedings<br>of KDD 2002, pp:688-693, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft9">816</span></nobr></DIV>
<DIV style="position:absolute;top:54;left:80"><nobr><span class="ft8"><b>Industry/Government Track Poster</b></span></nobr></DIV>
</DIV>
</BODY>
</HTML>
