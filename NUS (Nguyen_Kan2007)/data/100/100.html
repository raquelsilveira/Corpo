<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>crawler.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2004-10-06T20:49:36+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:6px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Courier;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Times;color:#000000;}
	.ft9{font-size:10px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft11{font-size:10px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="100001.png" alt="background image">
<DIV style="position:absolute;top:128;left:230"><nobr><span class="ft0"><b>High Performance Crawling System</b></span></nobr></DIV>
<DIV style="position:absolute;top:196;left:237"><nobr><span class="ft1">Youn `es Hafri</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:184"><nobr><span class="ft2">Ecole Polytechnique de Nantes</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:183"><nobr><span class="ft2">Institut National de l'Audiovisuel</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:215"><nobr><span class="ft2">4, avenue de l'Europe</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:164"><nobr><span class="ft2">94366 Bry sur Marne - cedex, France</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:237"><nobr><span class="ft1">yhafri@ina.fr</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:557"><nobr><span class="ft1">Chabane Djeraba</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:475"><nobr><span class="ft2">Laboratoire d'Informatique Fondamentale Lille</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:521"><nobr><span class="ft2">UMR CNRS 8022- Batiment M3</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:492"><nobr><span class="ft2">59655 Villeneuve d'Ascq C ´edex - France</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:571"><nobr><span class="ft1">djeraba@lifl.fr</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:81"><nobr><span class="ft3"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:356;left:81"><nobr><span class="ft10">In the present paper, we will describe the design and imple-<br>mentation of a real-time distributed system of Web crawling<br>running on a cluster of machines.</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:320"><nobr><span class="ft4">The system crawls</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:81"><nobr><span class="ft10">several thousands of pages every second, includes a high-<br>performance fault manager, is platform independent and is<br>able to adapt transparently to a wide range of configurations<br>without incurring additional hardware expenditure. We will<br>then provide details of the system architecture and describe<br>the technical choices for very high performance crawling.<br>Finally, we will discuss the experimental results obtained,<br>comparing them with other documented systems.</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:81"><nobr><span class="ft3"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:571;left:81"><nobr><span class="ft10">D.1.3 [Concurrent Programming]: Distributed program-<br>ming; C.4 [Performance of Systems]: Fault tolerance;<br>H.3.7 [Digital Libraries]: Systems issues</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:81"><nobr><span class="ft3"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:660;left:81"><nobr><span class="ft10">Web Crawler, Hierarchical Cooperation, High Availability<br>System</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:81"><nobr><span class="ft3"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:112"><nobr><span class="ft3"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:731;left:94"><nobr><span class="ft4">With the World Wide Web containing the vast amount</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:81"><nobr><span class="ft10">of information (several thousands in 1993, 3 billion today)<br>that it does and the fact that it is ever expanding, we<br>need a way to find the right information (multimedia of<br>textual).</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:148"><nobr><span class="ft4">We need a way to access the information on</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:81"><nobr><span class="ft4">specific subjects that we require.</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:300"><nobr><span class="ft4">To solve the problems</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:81"><nobr><span class="ft10">above several programs and algorithms were designed that<br>index the web, these various designs are known as search<br>engines, spiders, crawlers, worms or knowledge robots graph<br>in its simplest terms. The pages are the nodes on the graph<br>and the links are the arcs on the graph. What makes this so<br>difficult is the vast amount of data that we have to handle,<br>and then we must also take into account the fact that the<br>World Wide Web is constantly growing and the fact that</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:475"><nobr><span class="ft10">people are constantly updating the content of their web<br>pages.</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:489"><nobr><span class="ft4">Any High performance crawling system should offer at</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:475"><nobr><span class="ft4">least the following two features.</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:707"><nobr><span class="ft4">Firstly, it needs to</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:475"><nobr><span class="ft10">be equipped with an intelligent navigation strategy, i.e.<br>enabling it to make decisions regarding the choice of sub-<br>sequent actions to be taken (pages to be downloaded etc).<br>Secondly, its supporting hardware and software architecture<br>should be optimized to crawl large quantities of documents<br>per unit of time (generally per second). To this we may add<br>fault tolerance (machine crash, network failure etc.) and<br>considerations of Web server resources.</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:489"><nobr><span class="ft4">Recently we have seen a small interest in these two</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:475"><nobr><span class="ft10">field. Studies on the first point include crawling strategies<br>for important pages [9, 17], topic-specific document down-<br>loading [5, 6, 18, 10], page recrawling to optimize overall<br>refresh frequency of a Web archive [8, 7] or scheduling the<br>downloading activity according to time [22]. However, little<br>research has been devoted to the second point, being very<br>difficult to implement [20, 13]. We will focus on this latter<br>point in the rest of this paper.</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:489"><nobr><span class="ft4">Indeed, only a few crawlers are equipped with an opti-</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:475"><nobr><span class="ft10">mized scalable crawling system, yet details of their internal<br>workings often remain obscure (the majority being propri-<br>etary solutions).</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:589"><nobr><span class="ft4">The only system to have been given a</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:475"><nobr><span class="ft10">fairly in-depth description in existing literature is Mercator<br>by Heydon and Najork of DEC/Compaq [13] used in the<br>AltaVista search engine (some details also exist on the first<br>version of the Google [3] and Internet Archive [4] robots).</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:489"><nobr><span class="ft4">Most recent studies on crawling strategy fail to deal with</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:475"><nobr><span class="ft10">these features, contenting themselves with the solution of<br>minor issues such as the calculation of the number of pages<br>to be downloaded in order to maximize/minimize some<br>functional objective. This may be acceptable in the case<br>of small applications, but for real time</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:722"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:735"><nobr><span class="ft4">applications the</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:475"><nobr><span class="ft10">system must deal with a much larger number of constraints.<br>We should also point out that little academic research<br>is concerned with high performance search engines, as<br>compared with their commercial counterparts (with the<br>exception of the WebBase project [14] at Stanford).</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:489"><nobr><span class="ft4">In the present paper, we will describe a very high</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:475"><nobr><span class="ft10">availability, optimized and distributed crawling system.<br>We will use the system on what is known as breadth-<br>first crawling, though this may be easily adapted to other<br>navigation strategies. We will first focus on input/output,<br>on management of network traffic and robustness when<br>changing scale. We will also discuss download policies in</span></nobr></DIV>
<DIV style="position:absolute;top:1081;left:476"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:482"><nobr><span class="ft4">"Soft" real time</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">299</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:135"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:985;left:81"><nobr><span class="ft11">Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are not <br>made or distributed for profit or commercial advantage and that copies bear <br>this notice and the full citation on the first page. To copy otherwise, or <br>republish, to post on servers or to redistribute to lists, requires prior specific <br>permission and/or a fee. <br>MIR'04, October 15­16, 2004, New York, New York, USA. <br>Copyright 2004 ACM 1-58113-940-3/04/0010...$5.00.   </span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft12{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="100002.png" alt="background image">
<DIV style="position:absolute;top:106;left:81"><nobr><span class="ft10">terms of speed regulation, fault management by supervisors<br>and the introduction/suppression of machine nodes without<br>system restart during a crawl.</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:94"><nobr><span class="ft4">Our system was designed within the experimental frame-</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:81"><nobr><span class="ft4">work of the D´</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:173"><nobr><span class="ft4">ep^</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:186"><nobr><span class="ft4">ot L´</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:214"><nobr><span class="ft4">egal du Web Fran¸</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:328"><nobr><span class="ft4">cais (French Web</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:81"><nobr><span class="ft10">Legal Deposit). This consists of archiving only multimedia<br>documents in French available on line, indexing them and<br>providing ways for these archives to be consulted. Legal<br>deposit requires a real crawling strategy in order to ensure<br>site continuity over time.</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:257"><nobr><span class="ft4">The notion of registration is</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:81"><nobr><span class="ft10">closely linked to that of archiving, which requires a suitable<br>strategy to be useful. In the course of our discussion, we<br>will therefore analyze the implication and impact of this<br>experimentation for system construction.</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:81"><nobr><span class="ft3"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:112"><nobr><span class="ft3"><b>STATE OF THE ART</b></span></nobr></DIV>
<DIV style="position:absolute;top:371;left:81"><nobr><span class="ft3"><b>2.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:371;left:121"><nobr><span class="ft3"><b>Prerequisites of a Crawling System</b></span></nobr></DIV>
<DIV style="position:absolute;top:394;left:94"><nobr><span class="ft4">In order to set our work in this field in context, listed</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:81"><nobr><span class="ft10">below are definitions of services that should be considered<br>the minimum requirements for any large-scale crawling<br>system.</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:101"><nobr><span class="ft4">· Flexibility: as mentioned above, with some minor</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:114"><nobr><span class="ft10">adjustments our system should be suitable for various<br>scenarios. However, it is important to remember that<br>crawling is established within a specific framework:<br>namely, Web legal deposit.</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:101"><nobr><span class="ft4">· High Performance: the system needs to be scalable</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:114"><nobr><span class="ft10">with a minimum of one thousand pages/second and<br>extending up to millions of pages for each run on<br>low cost hardware. Note that here, the quality and<br>efficiency of disk access are crucial to maintaining high<br>performance.</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:101"><nobr><span class="ft4">· Fault Tolerance: this may cover various aspects. As</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:114"><nobr><span class="ft10">the system interacts with several servers at once,<br>specific problems emerge.</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:288"><nobr><span class="ft4">First, it should at least</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:114"><nobr><span class="ft10">be able to process invalid HTML code, deal with<br>unexpected Web server behavior, and select good<br>communication protocols etc. The goal here is to avoid<br>this type of problem and, by force of circumstance, to<br>be able to ignore such problems completely. Second,<br>crawling processes may take days or weeks, and it is<br>imperative that the system can handle failure, stopped<br>processes or interruptions in network services, keeping<br>data loss to a minimum. Finally, the system should<br>be persistent, which means periodically switching large<br>data structures from memory to the disk (e.g. restart<br>after failure).</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:101"><nobr><span class="ft4">· Maintainability and Configurability: an appropriate</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:114"><nobr><span class="ft10">interface is necessary for monitoring the crawling<br>process, including download speed, statistics on the<br>pages and amounts of data stored. In online mode, the<br>administrator may adjust the speed of a given crawler,<br>add or delete processes, stop the system, add or delete<br>system nodes and supply the black list of domains not<br>to be visited, etc.</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:81"><nobr><span class="ft3"><b>2.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:121"><nobr><span class="ft3"><b>General Crawling Strategies</b></span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:94"><nobr><span class="ft4">There are many highly accomplished techniques in terms</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:81"><nobr><span class="ft10">of Web crawling strategy. We will describe the most relevant<br>of these here.</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:495"><nobr><span class="ft4">· Breadth-first Crawling: in order to build a wide Web</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:509"><nobr><span class="ft10">archive like that of the Internet Archive [15], a crawl<br>is carried out from a set of Web pages (initial URLs<br>or seeds).</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:586"><nobr><span class="ft4">A breadth-first exploration is launched</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:509"><nobr><span class="ft10">by following hypertext links leading to those pages<br>directly connected with this initial set. In fact, Web<br>sites are not really browsed breadth-first and various<br>restrictions may apply, e.g. limiting crawling processes<br>to within a site, or downloading the pages deemed<br>most interesting first</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:635"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:495"><nobr><span class="ft4">· Repetitive Crawling: once pages have been crawled,</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:509"><nobr><span class="ft10">some systems require the process to be repeated<br>periodically so that indexes are kept updated. In the<br>most basic case, this may be achieved by launching<br>a second crawl in parallel.</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:692"><nobr><span class="ft4">A variety of heuristics</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:509"><nobr><span class="ft4">exist to overcome this problem:</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:731"><nobr><span class="ft4">for example, by</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:509"><nobr><span class="ft10">frequently relaunching the crawling process of pages,<br>sites or domains considered important to the detriment<br>of others.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:584"><nobr><span class="ft4">A good crawling strategy is crucial for</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:509"><nobr><span class="ft10">maintaining a constantly updated index list. Recent<br>studies by Cho and Garcia-Molina [8, 7] have focused<br>on optimizing the update frequency of crawls by using<br>the history of changes recorded on each site.</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:495"><nobr><span class="ft4">· Targeted Crawling: more specialized search engines</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:509"><nobr><span class="ft10">use crawling process heuristics in order to target a<br>certain type of page, e.g. pages on a specific topic or<br>in a particular language, images, mp3 files or scientific<br>papers. In addition to these heuristics, more generic<br>approaches have been suggested. They are based on<br>the analysis of the structures of hypertext links [6,<br>5] and techniques of learning [9, 18]: the objective<br>here being to retrieve the greatest number of pages<br>relating to a particular subject by using the minimum<br>bandwidth. Most of the studies cited in this category<br>do not use high performance crawlers, yet succeed in<br>producing acceptable results.</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:495"><nobr><span class="ft4">· Random Walks and Sampling: some studies have</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:509"><nobr><span class="ft10">focused on the effect of random walks on Web graphs<br>or modified versions of these graphs via sampling in<br>order to estimate the size of documents on line [1, 12,<br>11].</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:495"><nobr><span class="ft4">· Deep Web Crawling: a lot of data accessible via</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:509"><nobr><span class="ft10">the Web are currently contained in databases and<br>may only be downloaded through the medium of<br>appropriate requests or forms. Recently, this often-<br>neglected but fascinating problem has been the focus<br>of new interest. The Deep Web is the name given to<br>the Web containing this category of data [9].</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:509"><nobr><span class="ft10">Lastly, we should point out the acknowledged differ-<br>ences that exist between these scenarios. For example,<br>a breadth-first search needs to keep track of all pages<br>already crawled.</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:628"><nobr><span class="ft4">An analysis of links should use</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:509"><nobr><span class="ft10">structures of additional data to represent the graph<br>of the sites in question, and a system of classifiers in<br>order to assess the pages' relevancy [6, 5]. However,<br>some tasks are common to all scenarios, such as</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:476"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:482"><nobr><span class="ft4">See [9] for the heuristics that tend to find the most</span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:475"><nobr><span class="ft12">important pages first and [17] for experimental results<br>proving that breadth-first crawling allows the swift retrieval<br>of pages with a high PageRank.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">300</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:14px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft15{font-size:6px;line-height:13px;font-family:Times;color:#000000;}
	.ft16{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="100003.png" alt="background image">
<DIV style="position:absolute;top:106;left:114"><nobr><span class="ft10">respecting robot exclusion files (robots.txt), crawling<br>speed, resolution of domain names . . .</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:94"><nobr><span class="ft4">In the early 1990s, several companies claimed that their</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:81"><nobr><span class="ft10">search engines were able to provide complete Web coverage.<br>It is now clear that only partial coverage is possible at<br>present.</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:141"><nobr><span class="ft4">Lawrence and Giles [16] carried out two exper-</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:81"><nobr><span class="ft10">iments in order to measure coverage performance of data<br>established by crawlers and of their updates. They adopted<br>an approach known as overlap analysis to estimate the size<br>of the Web that may be indexed (See also Bharat and Broder<br>1998 on the same subject). Let W be the total set of Web<br>pages and W</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:161"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:174"><nobr><span class="ft4"> W and W</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:254"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:266"><nobr><span class="ft4"> W the pages downloaded</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:81"><nobr><span class="ft4">by two different crawlers a and b. What is the size of W</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:433"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:81"><nobr><span class="ft4">and W</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:121"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:131"><nobr><span class="ft4">as compared with W ? Let us assume that uniform</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:81"><nobr><span class="ft10">samples of Web pages may be taken and their membership of<br>both sets tested. Let P (W</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:242"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:248"><nobr><span class="ft4">) and P (W</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:315"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:320"><nobr><span class="ft4">) be the probability</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:81"><nobr><span class="ft10">that a page is downloaded by a or b respectively. We know<br>that:</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:173"><nobr><span class="ft4">P (W</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:203"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:212"><nobr><span class="ft4"> W</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:238"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:243"><nobr><span class="ft4">|W</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:260"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:266"><nobr><span class="ft4">) = W</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:304"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:314"><nobr><span class="ft4"> W</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:340"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:305"><nobr><span class="ft4">|W</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:322"><nobr><span class="ft5">b</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:328"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:422"><nobr><span class="ft4">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:81"><nobr><span class="ft10">Now, if these two crawling processes are assumed to be<br>independent, the left side of equation 1may be reduced to<br>P (W</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:110"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:117"><nobr><span class="ft4">), that is data coverage by crawler a. This may be</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:81"><nobr><span class="ft10">easily obtained by the intersection size of the two crawling<br>processes. However, an exact calculation of this quantity<br>is only possible if we do not really know the documents<br>crawled. Lawrence and Giles used a set of controlled data of<br>575 requests to provide page samples and count the number<br>of times that the two crawlers retrieved the same pages. By<br>taking the hypothesis that the result P (W</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:341"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:348"><nobr><span class="ft4">) is correct, we</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:81"><nobr><span class="ft4">may estimate the size of the Web as</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:320"><nobr><span class="ft4">|W</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:337"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:344"><nobr><span class="ft4">|/P (W</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:384"><nobr><span class="ft5">a</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:391"><nobr><span class="ft4">).</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:412"><nobr><span class="ft4">This</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:81"><nobr><span class="ft10">approach has shown that the Web contained at least 320<br>million pages in 1997 and that only 60% was covered by the<br>six major search engines of that time. It is also interesting<br>to note that a single search engine would have covered only<br>1/3 of the Web. As this approach is based on observation, it<br>may reflect a visible Web estimation, excluding for instance<br>pages behind forms, databases etc. More recent experiments<br>assert that the Web contains several billion pages.</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:85"><nobr><span class="ft13"><i>2.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:749;left:134"><nobr><span class="ft13"><i>Selective Crawling</i></span></nobr></DIV>
<DIV style="position:absolute;top:770;left:94"><nobr><span class="ft4">As demonstrated above, a single crawler cannot archive</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:81"><nobr><span class="ft10">the whole Web. The fact is that the time required to carry<br>out the complete crawling process is very long, and impos-<br>sible given the technology currently available. Furthermore,<br>crawling and indexing very large amounts of data implies<br>great problems of scalability, and consequently entails not<br>inconsiderable costs of hardware and maintenance.</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:419"><nobr><span class="ft4">For</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:81"><nobr><span class="ft10">maximum optimization, a crawling system should be able<br>to recognize relevant sites and pages, and restrict itself to<br>downloading within a limited time.</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:94"><nobr><span class="ft4">A document or Web page's relevancy may be officially</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:81"><nobr><span class="ft14">recognized in various ways. The idea of selective crawling<br>may be introduced intuitively by associating each URL u<br>with a score calculation function s</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:296"><nobr><span class="ft15">()<br></span></nobr></DIV>
<DIV style="position:absolute;top:976;left:316"><nobr><span class="ft4">respecting relevancy</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:81"><nobr><span class="ft10">criterion  and parameters . In the most basic case, we<br>may assume a Boolean relevancy function, i.e. s(u) = 1 if<br>the document designated by u is relevant and s(u) = 0 if not.<br>More generally, we may think of s(d) as a function with real<br>values, such as a conditional probability that a document<br>belongs to a certain category according to its content. In all<br>cases, we should point out that the score calculation function</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:475"><nobr><span class="ft10">depends only on the URL and  and not on the time or state<br>of the crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:489"><nobr><span class="ft4">A general approach for the construction of a selective</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:475"><nobr><span class="ft10">crawler consists of changing the URL insertion and extrac-<br>tion policy in the queue Q of the crawler. Let us assume<br>that the URLs are sorted in the order corresponding to the<br>value retrieved by s(u). In this case, we obtain the best-<br>first strategy (see [19]) which consists of downloading URLs<br>with the best scores first). If s(u) provides a good relevancy<br>model, we may hope that the search process will be guided<br>towards the best areas of the Web.</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:489"><nobr><span class="ft4">Various studies have been carried out in this direction: for</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:475"><nobr><span class="ft10">example, limiting the search depth in a site by specifying<br>that pages are no longer relevant after a certain depth. This<br>amounts to the following equation:</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:535"><nobr><span class="ft4">s</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:541"><nobr><span class="ft15">(depth)<br></span></nobr></DIV>
<DIV style="position:absolute;top:358;left:578"><nobr><span class="ft4">(u) =</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:633"><nobr><span class="ft4">1, if</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:663"><nobr><span class="ft4">|root(u)  u| &lt; </span></nobr></DIV>
<DIV style="position:absolute;top:366;left:633"><nobr><span class="ft4">0, else</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:816"><nobr><span class="ft4">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:475"><nobr><span class="ft4">where root(u) is the root of the site containing u.</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:810"><nobr><span class="ft4">The</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:475"><nobr><span class="ft10">interest of this approach lies in the fact that maximizing<br>the search breadth may make it easier for the end-user to<br>retrieve the information. Nevertheless, pages that are too<br>deep may be accessed by the user, even if the robot fails to<br>take them into account.</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:489"><nobr><span class="ft4">A second possibility is the estimation of a page's popu-</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:475"><nobr><span class="ft10">larity. One method of calculating a document's relevancy<br>would relate to the number of backlinks.</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:527"><nobr><span class="ft4">s</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:534"><nobr><span class="ft15">(backlinks)<br></span></nobr></DIV>
<DIV style="position:absolute;top:547;left:591"><nobr><span class="ft4">(u) =</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:645"><nobr><span class="ft10">1, if indegree(u) &gt; <br>0, else</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:816"><nobr><span class="ft4">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:475"><nobr><span class="ft4">where  is a threshold.</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:489"><nobr><span class="ft4">It is clear that s</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:593"><nobr><span class="ft15">(backlinks)<br></span></nobr></DIV>
<DIV style="position:absolute;top:596;left:651"><nobr><span class="ft4">(u) may only be calculated if</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:475"><nobr><span class="ft10">we have a complete site graph (site already downloaded<br>beforehand).</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:565"><nobr><span class="ft4">In practice, we make take an approximate</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft10">value and update it incrementally during the crawling<br>process. A derivative of this technique is used in Google's<br>famous PageRank calculation.</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:475"><nobr><span class="ft3"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:708;left:506"><nobr><span class="ft16"><b>OUR APPROACH: THE DOMINOS SYS-<br>TEM</b></span></nobr></DIV>
<DIV style="position:absolute;top:750;left:489"><nobr><span class="ft4">As mentioned above, we have divided the system into two</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:475"><nobr><span class="ft10">parts: workers and supervisors. All of these processes may<br>be run on various operating systems (Windows, MacOS X,<br>Linux, FreeBSD) and may be replicated if need be. The<br>workers are responsible for processing the URL flow coming<br>from their supervisors and for executing crawling process<br>tasks in the strict sense. They also handle the resolution of<br>domain names by means of their integrated DNS resolver,<br>and adjust download speed in accordance with node policy.<br>A worker is a light process in the Erlang sense, acting as<br>a fault tolerant and highly available HTTP client.</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:810"><nobr><span class="ft4">The</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:475"><nobr><span class="ft10">process-handling mode in Erlang makes it possible to create<br>several thousands of workers in parallel.</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:489"><nobr><span class="ft4">In our system, communication takes place mainly by send-</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:475"><nobr><span class="ft10">ing asynchronous messages as described in the specifications<br>for Erlang language. The type of message varies according to<br>need: character string for short messages and binary format<br>for long messages (large data structures or files). Disk access<br>is reduced to a minimum as far as possible and structures<br>are stored in the real-time Mnesia</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:696"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:709"><nobr><span class="ft4">database that forms</span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:476"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:1072;left:482"><nobr><span class="ft4">http://www.erlang.org/doc/r9c/lib/mnesia-</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:475"><nobr><span class="ft4">4.1.4/doc/html/</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">301</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft17{font-size:5px;font-family:Times;color:#000000;}
	.ft18{font-size:9px;font-family:Times;color:#000000;}
	.ft19{font-size:9px;font-family:Times;color:#ff0000;}
	.ft20{font-size:5px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="100004.png" alt="background image">
<DIV style="position:absolute;top:106;left:81"><nobr><span class="ft10">a standard part of the Erlang development kit. Mnesia's<br>features give it a high level of homogeneity during the base's<br>access, replication and deployment. It is supported by two<br>table management modules ETS and DETS. ETS allows<br>tables of values to be managed by random access memory,<br>while DETS provides a persistent form of management on<br>the disk. Mnesia's distribution faculty provides an efficient<br>access solution for distributed data. When a worker moves<br>from one node to another (code migration), it no longer need<br>be concerned with the location of the base or data. It simply<br>has to read and write the information transparently.</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:61"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:81"><nobr><span class="ft18">loop(InternalState) -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:227"><nobr><span class="ft19">% Supervisor main</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:61"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:224"><nobr><span class="ft19">% loop</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:61"><nobr><span class="ft17">3</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:81"><nobr><span class="ft18">receive {From,{migrate,Worker,Src,Dest}} -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:61"><nobr><span class="ft17">4</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:86"><nobr><span class="ft19">% Migrate the Worker process from</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:61"><nobr><span class="ft17">5</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:86"><nobr><span class="ft19">% Src node to Dest node</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:61"><nobr><span class="ft17">6</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:86"><nobr><span class="ft18">spawn(supervisor,migrate,</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:61"><nobr><span class="ft17">7</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:126"><nobr><span class="ft18">[Worker,Src,Dest]),</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:61"><nobr><span class="ft17">8</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:86"><nobr><span class="ft19">% Infinite loop</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:61"><nobr><span class="ft17">9</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:92"><nobr><span class="ft18">loop(InternalState);</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:56"><nobr><span class="ft20">10<br>11</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:92"><nobr><span class="ft18">{From,{replace,OldPid,NewPid,State}} -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:56"><nobr><span class="ft17">12</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:98"><nobr><span class="ft19">% Add the new worker to</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:56"><nobr><span class="ft17">13</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:98"><nobr><span class="ft19">% the supervisor state storage</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:56"><nobr><span class="ft17">14</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:98"><nobr><span class="ft18">NewInternalState =</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:56"><nobr><span class="ft17">15</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:103"><nobr><span class="ft18">replace(OldPid,NewPid,InternalState),</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:56"><nobr><span class="ft17">16</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:98"><nobr><span class="ft19">% Infinite loop</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:56"><nobr><span class="ft17">17</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:98"><nobr><span class="ft18">loop(NewInternalState);</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:56"><nobr><span class="ft17">18</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:109"><nobr><span class="ft18">...</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:56"><nobr><span class="ft17">19</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:86"><nobr><span class="ft18">end.</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:56"><nobr><span class="ft20">20<br>21</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:86"><nobr><span class="ft18">migrate(Pid,Src,Dest) -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:245"><nobr><span class="ft19">% Migration</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:56"><nobr><span class="ft17">22</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:241"><nobr><span class="ft19">% process</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:56"><nobr><span class="ft17">23</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:86"><nobr><span class="ft18">receive</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:56"><nobr><span class="ft17">24</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:98"><nobr><span class="ft18">Pid ! {self(), stop},</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:56"><nobr><span class="ft17">25</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:98"><nobr><span class="ft18">receive</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:56"><nobr><span class="ft17">26</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:103"><nobr><span class="ft18">{Pid,{stopped,LastState}} -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:56"><nobr><span class="ft17">27</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:121"><nobr><span class="ft18">NewPid = spawn{Dest,worker,proc,</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:56"><nobr><span class="ft17">28</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:201"><nobr><span class="ft18">[LastState]},</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:56"><nobr><span class="ft17">29</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:121"><nobr><span class="ft18">self() ! {self(), {replace,Pid,</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:56"><nobr><span class="ft17">30</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:178"><nobr><span class="ft18">NewPid,LastState}};</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:56"><nobr><span class="ft17">31</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:98"><nobr><span class="ft18">{Pid,Error} -&gt; ...</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:56"><nobr><span class="ft17">32</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:86"><nobr><span class="ft18">end.</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:160"><nobr><span class="ft4">Listing 1: Process Migration</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:94"><nobr><span class="ft4">Code 1describes the migration of a worker process from</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:81"><nobr><span class="ft4">one node Src to another Dest.</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:276"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:295"><nobr><span class="ft4">The supervisor receives</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft10">the migration order for process P id (line 4). The migration<br>action is not blocking and is performed in a different Erlang<br>process (line 7). The supervisor stops the worker with the<br>identifier P id (line 25) and awaits the operation result (line<br>26). It then creates a remote worker in the node Dest with<br>the latest state of the stopped worker (line 28) and updates<br>its internal state (lines 30 and 12).</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:81"><nobr><span class="ft3"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:916;left:121"><nobr><span class="ft3"><b>Dominos Process</b></span></nobr></DIV>
<DIV style="position:absolute;top:938;left:94"><nobr><span class="ft4">The Dominos system is different from all the other crawl-</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:81"><nobr><span class="ft10">ing systems cited above. Like these, the Dominos offering is<br>on distributed architecture, but with the difference of being<br>totally dynamic. The system's dynamic nature allows its<br>architecture to be changed as required. If, for instance, one<br>of the cluster's nodes requires particular maintenance, all of<br>the processes on it will migrate from this node to another.<br>When servicing is over, the processes revert automatically</span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:81"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:1073;left:88"><nobr><span class="ft4">The character % indicates the beginning of a comment in</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:81"><nobr><span class="ft4">Erlang.</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:475"><nobr><span class="ft10">to their original node. Crawl processes may change pool<br>so as to reinforce one another if necessary. The addition or<br>deletion of a node in the cluster is completely transparent in<br>its execution. Indeed, each new node is created containing a<br>completely blank system. The first action to be undertaken<br>is to search for the generic server in order to obtain the<br>parameters of the part of the system that it is to belong<br>to. These parameters correspond to a limited view of the<br>whole system. This enables Dominos to be deployed more<br>easily, the number of messages exchanged between processes<br>to be reduced and allows better management of exceptions.<br>Once the generic server has been identified, binaries are sent<br>to it and its identity is communicated to the other nodes<br>concerned.</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:495"><nobr><span class="ft4">· Dominos Generic Server (GenServer): Erlang process</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:509"><nobr><span class="ft10">responsible for managing the process identifiers on the<br>whole cluster. To ensure easy deployment of Dominos,<br>it was essential to mask the denominations of the<br>process identifiers. Otherwise, a minor change in the<br>names of machines or their IP would have required<br>complete reorganization of the system.</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:772"><nobr><span class="ft4">GenServer</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:509"><nobr><span class="ft10">stores globally the identifiers of all processes existing<br>at a given time.</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:495"><nobr><span class="ft4">· Dominos RPC Concurrent (cRPC): as its name sug-</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:509"><nobr><span class="ft10">gests, this process is responsible for delegating the<br>execution of certain remote functions to other pro-<br>cesses. Unlike conventional RPCs where it is necessary<br>to know the node and the object providing these<br>functions (services), our RPCC completely masks the<br>information.</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:595"><nobr><span class="ft4">One need only call the function, with</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:509"><nobr><span class="ft10">no concern for where it is located in the cluster or<br>for the name of the process offering this function.<br>Moreover, each RPCC process is concurrent, and<br>therefore manages all its service requests in parallel.<br>The results of remote functions are governed by two<br>modes: blocking or non-blocking. The calling process<br>may therefore await the reply of the remote function<br>or continue its execution.</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:688"><nobr><span class="ft4">In the latter case, the</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:509"><nobr><span class="ft10">reply is sent to its mailbox. For example, no worker<br>knows the process identifier of its own supervisor. In<br>order to identify it, a worker sends a message to the<br>process called supervisor. The RPCC deals with the<br>message and searches the whole cluster for a super-<br>visor process identifier, starting with the local node.<br>The address is therefore resolved without additional<br>network overhead, except where the supervisor does<br>not exist locally.</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:495"><nobr><span class="ft4">· Dominos Distributed Database (DDB): Erlang process</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:509"><nobr><span class="ft10">responsible for Mnesia real-time database manage-<br>ment. It handles the updating of crawled information,<br>crawling progress and the assignment of URLs to be<br>downloaded to workers.</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:675"><nobr><span class="ft4">It is also responsible for</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:509"><nobr><span class="ft10">replicating the base onto the nodes concerned and for<br>the persistency of data on disk.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:495"><nobr><span class="ft4">· Dominos Nodes: a node is the physical representation</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:509"><nobr><span class="ft10">of a machine connected (or disconnected as the case<br>may be) to the cluster. This connection is considered<br>in the most basic sense of the term, namely a simple<br>plugging-in (or unplugging) of the network outlet.<br>Each node clearly reflects the dynamic character of<br>the Dominos system.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">302</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft21{font-size:9px;font-family:Times;color:#0000ff;}
-->
</STYLE>
<IMG width="918" height="1188" src="100005.png" alt="background image">
<DIV style="position:absolute;top:102;left:101"><nobr><span class="ft4">· Dominos Group Manager: Erlang process responsible</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:114"><nobr><span class="ft10">for controlling the smooth running of its child pro-<br>cesses (supervisor and workers).</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:101"><nobr><span class="ft4">· Dominos Master-Supervisor Processes: each group</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:114"><nobr><span class="ft10">manager has a single master process dealing with the<br>management of crawling states of progress. It therefore<br>controls all the slave processes (workers) contained<br>within it.</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:101"><nobr><span class="ft4">· Dominos Slave-Worker Processes: workers are the</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:114"><nobr><span class="ft4">lowest-level elements in the crawling process.</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:412"><nobr><span class="ft4">This</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:114"><nobr><span class="ft10">is the very heart of the Web client wrapping the<br>libCURL.</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:94"><nobr><span class="ft4">With Dominos architecture being completely dynamic and</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:81"><nobr><span class="ft10">distributed, we may however note the hierarchical character<br>of processes within a Dominos node. This is the only way to<br>ensure very high fault tolerance. A group manager that fails<br>is regenerated by the node on which it depends. A master<br>process (supervisor) that fails is regenerated by its group<br>manager. Finally, a worker is regenerated by its supervisor.<br>As for the node itself, it is controlled by the Dominos kernel<br>(generally on another remote machine). The following code<br>describes the regeneration of a worker process in case of<br>failure.</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:61"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:81"><nobr><span class="ft19">% Activate error handling</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:61"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:81"><nobr><span class="ft18">process_flag(trap_exit,</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:227"><nobr><span class="ft21">true</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:252"><nobr><span class="ft18">),</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:61"><nobr><span class="ft17">3</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:92"><nobr><span class="ft18">...</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:61"><nobr><span class="ft17">4</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:81"><nobr><span class="ft18">loop(InternalState) -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:227"><nobr><span class="ft19">% Supervisor main loop</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:61"><nobr><span class="ft17">5</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:81"><nobr><span class="ft18">receive</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:61"><nobr><span class="ft17">6</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:86"><nobr><span class="ft18">{From,{job,</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:156"><nobr><span class="ft21">Name</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:182"><nobr><span class="ft18">,finish}, State} -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:61"><nobr><span class="ft17">7</span></nobr></DIV>
<DIV style="position:absolute;top:600;left:92"><nobr><span class="ft19">% Informe the GenServer that the download is ok</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:61"><nobr><span class="ft17">8</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:92"><nobr><span class="ft18">?ServerGen ! {job,</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:206"><nobr><span class="ft21">Name</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:232"><nobr><span class="ft18">,finish},</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:61"><nobr><span class="ft17">9</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:56"><nobr><span class="ft17">10</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:92"><nobr><span class="ft19">% Save the new worker state</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:56"><nobr><span class="ft17">11</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:92"><nobr><span class="ft18">NewInternalState=save_state(From,State,InternalState),</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:56"><nobr><span class="ft20">12<br>13</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:92"><nobr><span class="ft19">% Infinite loop</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:56"><nobr><span class="ft17">14</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:92"><nobr><span class="ft18">loop(NewInternalState);</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:56"><nobr><span class="ft17">15</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:92"><nobr><span class="ft18">...</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:56"><nobr><span class="ft17">16</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:92"><nobr><span class="ft18">{From,Error} -&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:194"><nobr><span class="ft19">% Worker crash</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:56"><nobr><span class="ft17">17</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:92"><nobr><span class="ft19">% Get the last operational state before the crash</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:56"><nobr><span class="ft17">18</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:92"><nobr><span class="ft18">WorkerState = last_state(From,InternalState),</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:56"><nobr><span class="ft20">19<br>20</span></nobr></DIV>
<DIV style="position:absolute;top:774;left:92"><nobr><span class="ft19">% Free all allocated resources</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:56"><nobr><span class="ft17">21</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:92"><nobr><span class="ft18">free_resources(From,InternalState),</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:56"><nobr><span class="ft20">22<br>23</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:92"><nobr><span class="ft19">% Create a new worker with the last operational</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:56"><nobr><span class="ft17">24</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:92"><nobr><span class="ft19">% state of the crashed worker</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:56"><nobr><span class="ft17">25</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:92"><nobr><span class="ft18">Pid = spawn(worker,proc,[WorkerState]),</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:56"><nobr><span class="ft20">26<br>27</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:92"><nobr><span class="ft19">% Add the new worker to the supervisor state</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:56"><nobr><span class="ft17">28</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:92"><nobr><span class="ft19">% storage</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:56"><nobr><span class="ft17">29</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:92"><nobr><span class="ft18">NewInternalState =replace(From,Pid,InternalState),</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:56"><nobr><span class="ft20">30<br>31</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:92"><nobr><span class="ft19">% Infinite loop</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:56"><nobr><span class="ft17">32</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:92"><nobr><span class="ft18">loop(NewInternalState);</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:56"><nobr><span class="ft17">33</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:81"><nobr><span class="ft18">end.</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:81"><nobr><span class="ft10">Listing 2: Regeneration of a Worker Process in Case<br>of Failure</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:94"><nobr><span class="ft4">This represents the part of the main loop of the supervisor</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:81"><nobr><span class="ft10">process dealing with the management of the failure of a<br>worker.</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:138"><nobr><span class="ft4">As soon as a worker error is received (line 19),</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:81"><nobr><span class="ft10">the supervisor retrieves the last operational state of the<br>worker that has stopped (line 22), releases all of its allocated</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:475"><nobr><span class="ft10">resources (line 26) and recreates a new worker process with<br>the operational state of the stopped process (line 31). The<br>supervisor continually turns in loop while awaiting new<br>messages (line 40). The loop function call (lines 17 and 40)<br>is tail recursive, thereby guaranteeing that the supervision<br>process will grow in a constant memory space.</span></nobr></DIV>
<DIV style="position:absolute;top:210;left:475"><nobr><span class="ft3"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:210;left:515"><nobr><span class="ft3"><b>DNS Resolution</b></span></nobr></DIV>
<DIV style="position:absolute;top:232;left:489"><nobr><span class="ft4">Before contacting a Web server, the worker process</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:475"><nobr><span class="ft10">needs to convert the Domain Name Server (DNS) into<br>a valid IP address.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:615"><nobr><span class="ft4">Whereas other systems (Mercator,</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:475"><nobr><span class="ft10">Internet Archive) are forced to set up DNS resolvers each<br>time a new link is identified, this is not necessary with<br>Dominos.</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:547"><nobr><span class="ft4">Indeed, in the framework of French Web le-</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:475"><nobr><span class="ft10">gal deposit, the sites to be archived have been iden-<br>tified beforehand, thus requiring only one DNS resolu-<br>tion per domain name. This considerably increases crawl<br>speed.</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:530"><nobr><span class="ft4">The sites concerned include all online newspa-</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:475"><nobr><span class="ft10">pers, such as LeMonde (http://www.lemonde.fr/ ), LeFigaro<br>(http://www.lefigaro.fr/ ) . . . , online television/radio such as<br>TF1(http://www.tf1.fr/ ), M6 (http://www.m6.fr/ ) . . .</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:475"><nobr><span class="ft3"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:455;left:506"><nobr><span class="ft3"><b>DETAILS OF IMPLEMENTATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:475"><nobr><span class="ft3"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:515"><nobr><span class="ft3"><b>Web Client</b></span></nobr></DIV>
<DIV style="position:absolute;top:510;left:489"><nobr><span class="ft4">The workers are the medium responsible for physically</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:475"><nobr><span class="ft4">crawling on-line contents.</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:659"><nobr><span class="ft4">They provide a specialized</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:475"><nobr><span class="ft4">wrapper around the libCURL</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:657"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:669"><nobr><span class="ft4">library that represents the</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:475"><nobr><span class="ft4">heart of the HTTP client.</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:655"><nobr><span class="ft4">Each worker is interfaced to</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:475"><nobr><span class="ft10">libCURL by a C driver (shared library). As the system seeks<br>maximum network accessibility (communication protocol<br>support), libCURL appeared to be the most judicious choice<br>when compared with other available libraries.</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:751"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:757"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:489"><nobr><span class="ft4">The protocols supported include: FTP, FTPS, HTTP,</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:475"><nobr><span class="ft4">HTTPS, LDAP, Certifications, Proxies, Tunneling etc.</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:489"><nobr><span class="ft4">Erlang's portability was a further factor favoring the</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:475"><nobr><span class="ft10">choice of libCURL. Indeed, libCURL is available for various<br>architectures:</span></nobr></DIV>
<DIV style="position:absolute;top:698;left:569"><nobr><span class="ft4">Solaris, BSD, Linux, HPUX, IRIX, AIX,</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:475"><nobr><span class="ft10">Windows, Mac OS X, OpenVMS etc. Furthermore, it is<br>fast, thread-safe and IPv6 compatible.</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:489"><nobr><span class="ft4">This choice also opens up a wide variety of functions.</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:475"><nobr><span class="ft10">Redirections are accounted for and powerful filtering is<br>possible according to the type of content downloaded,<br>headers, and size (partial storage on RAM or disk depending<br>on the document's size).</span></nobr></DIV>
<DIV style="position:absolute;top:834;left:475"><nobr><span class="ft3"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:834;left:515"><nobr><span class="ft3"><b>Document Fingerprint</b></span></nobr></DIV>
<DIV style="position:absolute;top:856;left:489"><nobr><span class="ft4">For each download, the worker extracts the hypertext</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:475"><nobr><span class="ft10">links included in the HTML documents and initiates a fin-<br>gerprint (signature operation). A fast fingerprint (HAVAL<br>on 256 bits) is calculated for the document's content itself<br>so as to differentiate those with similar contents (e.g. mirror<br>sites). This technique is not new and has already been used<br>in Mercator[13]. It allows redundancies to be eliminated in<br>the archive.</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:475"><nobr><span class="ft3"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:992;left:515"><nobr><span class="ft3"><b>URL Extraction and Normalization</b></span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:489"><nobr><span class="ft4">Unlike other systems that use libraries of regular expres-</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:475"><nobr><span class="ft4">sions such as PCRE</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:601"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:614"><nobr><span class="ft4">for URL extraction, we have opted</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:476"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:482"><nobr><span class="ft4">Available at http://curl.haxx.se/libcurl/</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:476"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:482"><nobr><span class="ft4">See http://curl.haxx.se/libcurl/competitors.html</span></nobr></DIV>
<DIV style="position:absolute;top:1081;left:476"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:482"><nobr><span class="ft4">Available at http://www.pcre.org/</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">303</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft22{font-size:7px;font-family:Times;color:#000000;}
	.ft23{font-size:7px;font-family:Courier;color:#000000;}
	.ft24{font-size:6px;font-family:Courier;color:#0000e9;}
-->
</STYLE>
<IMG width="918" height="1188" src="100006.png" alt="background image">
<DIV style="position:absolute;top:106;left:81"><nobr><span class="ft10">for the Flex tool that definitely generates a faster parser.<br>Flex was compiled using a 256Kb buffer in which all table<br>compression options were activated during parsing "-8 -f -<br>Cf -Ca -Cr -i". Our current parser analyzes around 3,000<br>pages/second for a single worker for an average 49Kb per<br>page.</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:94"><nobr><span class="ft4">According to [20], a URL extraction speed of 300 pages/sec-</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:81"><nobr><span class="ft10">ond may generate a list of more than 2,000 URLs on average.<br>A naive representation of structures in the memory may<br>soon saturate the system.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:94"><nobr><span class="ft4">Various solutions have been proposed to alleviate this</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:81"><nobr><span class="ft4">problem.</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:151"><nobr><span class="ft4">The Internet Archive [4] crawler uses Bloom</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:81"><nobr><span class="ft10">filters in random access memory. This makes it possible<br>to have a compact representation of links retrieved, but also<br>generates errors (false-positive), i.e. certain pages are never<br>downloaded as they create collisions with other pages in the<br>Bloom filter. Compression without loss may reduce the size<br>of URLs to below 10Kb [2, 21], but this remains insufficient<br>in the case of large-scale crawls. A more ingenious approach<br>is to use persistent structures on disk coupled with a cache<br>as in Mercator [13].</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:81"><nobr><span class="ft3"><b>4.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:444;left:121"><nobr><span class="ft3"><b>URL Caching</b></span></nobr></DIV>
<DIV style="position:absolute;top:466;left:94"><nobr><span class="ft4">In order to speed up processing, we have developed a</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:81"><nobr><span class="ft10">scalable cache structure for the research and storage of URLs<br>already archived. Figure 1describes how such a cache works:</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:113"><nobr><span class="ft22">Links</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:218"><nobr><span class="ft22">Local Cache - Worker</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:104"><nobr><span class="ft22">Rejected</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:111"><nobr><span class="ft22">Links</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:213"><nobr><span class="ft23">0 1 2</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:372"><nobr><span class="ft23">255</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:153"><nobr><span class="ft22">JudyL-Array</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:285"><nobr><span class="ft22">URL CRC</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:280"><nobr><span class="ft22">URL</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:312"><nobr><span class="ft22">#URL</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:282"><nobr><span class="ft24">key</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:312"><nobr><span class="ft24">value</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:148"><nobr><span class="ft22">JudySL-Array</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:173"><nobr><span class="ft4">Figure 1: Scalable Cache</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:94"><nobr><span class="ft4">The cache is available at the level of each worker.</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:429"><nobr><span class="ft4">It</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:81"><nobr><span class="ft10">acts as a filter on URLs found and blocks those already<br>encountered.</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:170"><nobr><span class="ft4">The cache needs to be scalable to be able</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:81"><nobr><span class="ft10">to deal with increasing loads. Rapid implementation using<br>a non-reversible hash function such as HAVAL, TIGER,<br>SHA1 , GOST, MD5, RIPEMD . . . would be fatal to the<br>system's scalability. Although these functions ensure some<br>degree of uniqueness in fingerprint constructionthey are too<br>slow to be acceptable in these constructions. We cannot<br>allow latency as far as lookup or URL insertion in the cache<br>is concerned, if the cache is apt to exceed a certain size (over<br>10</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:94"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:106"><nobr><span class="ft4">key-value on average). This is why we have focused on</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:81"><nobr><span class="ft10">the construction of a generic cache that allows key-value<br>insertion and lookup in a scalable manner.</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:373"><nobr><span class="ft4">The Judy-</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:81"><nobr><span class="ft4">Array API</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:146"><nobr><span class="ft5">8</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:158"><nobr><span class="ft4">enabled us to achieve this objective. Without</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:81"><nobr><span class="ft10">going into detail about Judy-Array (see their site for more<br>information), our cache is a coherent coupling between<br>a JudyL-Array and N JudySL-Array.</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:330"><nobr><span class="ft4">The JudyL-Array</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:81"><nobr><span class="ft4">represents a hash table of N = 2</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:272"><nobr><span class="ft5">8</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:282"><nobr><span class="ft4">or N = 2</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:335"><nobr><span class="ft5">16</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:351"><nobr><span class="ft4">buckets able to</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:81"><nobr><span class="ft10">fit into the internal cache of the CPU. It is used to store "key-<br>numeric value" pairs where the key represents a CRC of the</span></nobr></DIV>
<DIV style="position:absolute;top:1081;left:81"><nobr><span class="ft5">8</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:88"><nobr><span class="ft4">Judy Array at the address: http://judy.sourceforge.net/</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:475"><nobr><span class="ft10">URL and whose value is a pointer to a JudySL-Array. The<br>second, JudySL-Array, is a "key-compressed character string<br>value" type of hash, in which the key represents the URL<br>identifier and whose value is the number of times that the<br>URL has been viewed. This cache construction is completely<br>scalable and makes it possible to have sub-linear response<br>rates, or linear in the worst-case scenario (see Judy-Array at<br>for an in-depth analysis of their performance). In the section<br>on experimentation (section 5) we will see the results of this<br>type of construction.</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:475"><nobr><span class="ft3"><b>4.5</b></span></nobr></DIV>
<DIV style="position:absolute;top:278;left:515"><nobr><span class="ft3"><b>Limiting Disk Access</b></span></nobr></DIV>
<DIV style="position:absolute;top:300;left:489"><nobr><span class="ft4">Our aim here is to eliminate random disk access com-</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:475"><nobr><span class="ft4">pletely.</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:533"><nobr><span class="ft4">One simple idea used in [20] is periodically to</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:475"><nobr><span class="ft10">switch structures requiring much memory over onto disk.<br>For example, random access memory can be used to keep<br>only those URLs found most recently or most frequently,<br>in order to speed up comparisons.</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:726"><nobr><span class="ft4">This requires no</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:475"><nobr><span class="ft10">additional development and is what we have decided to<br>use. The persistency of data on disk depends on the size<br>of data in DS memory, and their DA age.</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:775"><nobr><span class="ft4">The data</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:475"><nobr><span class="ft10">in the memory are distributed transparently via Mnesia,<br>specially designed for this kind of situation. Data may be<br>duplicated (</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:550"><nobr><span class="ft4">{ram copies, [Nodes]}, {disc copies, [Nodes]})</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:475"><nobr><span class="ft4">or fragmented (</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:578"><nobr><span class="ft4">{frag properties, .....}) on the nodes in</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft4">question.</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:489"><nobr><span class="ft4">According to [20], there are on average 8 non-duplicated</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:475"><nobr><span class="ft4">hypertext links per page downloaded.</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:729"><nobr><span class="ft4">This means that</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:475"><nobr><span class="ft10">the number of pages retrieved and not yet archived is<br>considerably increased.</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:628"><nobr><span class="ft4">After archiving 20 million pages,</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:475"><nobr><span class="ft4">over 100 million URLs would still be waiting.</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:780"><nobr><span class="ft4">This has</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:475"><nobr><span class="ft10">various repercussions, as newly-discovered URLs will be<br>crawled only several days, or even weeks, later. Given this<br>speed, the base's data refresh ability is directly affected.</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:475"><nobr><span class="ft3"><b>4.6</b></span></nobr></DIV>
<DIV style="position:absolute;top:661;left:515"><nobr><span class="ft3"><b>High Availability</b></span></nobr></DIV>
<DIV style="position:absolute;top:683;left:489"><nobr><span class="ft4">In order to apprehend the very notion of High Availability,</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:475"><nobr><span class="ft10">we first need to tackle the differences that exist between<br>a system's reliability and its availability.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:754"><nobr><span class="ft4">Reliability is</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:475"><nobr><span class="ft10">an attribute that makes it possible to measure service<br>continuity when no failure occurs.</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:489"><nobr><span class="ft4">Manufacturers generally provide a statistical estimation of</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:475"><nobr><span class="ft10">this value for this equipment: we may use the term MTBF<br>(Mean Time Between Failure). A strong MTBF provides a<br>valuable indication of a component's ability to avoid overly<br>frequent failure.</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:489"><nobr><span class="ft4">In the case of a complex system (that can be broken</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:475"><nobr><span class="ft10">down into hardware or software parts), we talk about MTTF<br>(Mean Time To Failure).</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:644"><nobr><span class="ft4">This denotes the average time</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:475"><nobr><span class="ft10">elapsed until service stops as the result of failure in a<br>component or software.</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:489"><nobr><span class="ft4">The attribute of availability is more difficult to calculate</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:475"><nobr><span class="ft10">as it includes a system's ability to react correctly in case of<br>failure in order to restart service as quickly as possible.</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:489"><nobr><span class="ft4">It is therefore necessary to quantify the time interval dur-</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:475"><nobr><span class="ft10">ing which service is unavailable before being re-established:<br>the acronym MTTR (Mean Time To Repair) is used to<br>represent this value.</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:489"><nobr><span class="ft4">The formula used to calculate the rate of a system's</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:475"><nobr><span class="ft4">availability is as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:555"><nobr><span class="ft4">availability =</span></nobr></DIV>
<DIV style="position:absolute;top:1073;left:676"><nobr><span class="ft4">M T T F</span></nobr></DIV>
<DIV style="position:absolute;top:1091;left:645"><nobr><span class="ft4">M T T F + M T T R</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:816"><nobr><span class="ft4">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">304</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft25{font-size:2px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="100007.png" alt="background image">
<DIV style="position:absolute;top:106;left:81"><nobr><span class="ft10">A system that looks to have a high level of availability should<br>have either a strong MTTF, or a weak MTTR.</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:94"><nobr><span class="ft4">Another more practical approach consists in measuring</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:81"><nobr><span class="ft10">the time period during which service is down in order to<br>evaluate the level of availability. This is the method most<br>frequently adopted, even if it fails to take account of the<br>frequency of failure, focusing rather on its duration.</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:94"><nobr><span class="ft4">Calculation is usually based on a calendar year.</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:416"><nobr><span class="ft4">The</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:81"><nobr><span class="ft10">higher the percentage of service availability, the nearer it<br>comes to High Availability.</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:94"><nobr><span class="ft4">It is fairly easy to qualify the level of High Availability of a</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:81"><nobr><span class="ft10">service from the cumulated downtime, by using the normal-<br>ized principle of "9's" (below 3 nine, we are no longer talking<br>about High Availability, but merely availability). In order<br>to provide an estimation of Dominos' High Availability, we<br>carried out performance tests by fault injection. It is clear<br>that a more accurate way of measuring this criterion would<br>be to let the system run for a whole year as explained above.<br>However, time constraints led us to adopt this solution. Our<br>injector consists in placing pieces of false code in each part<br>of the system and then measuring the time required for the<br>system to make the service available. Once again, Erlang has<br>proved to be an excellent choice for the setting up of these<br>regression tests. The table below shows the average time<br>required by Dominos to respond to these cases of service<br>unavailability.</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:94"><nobr><span class="ft4">Table 1clearly shows Dominos' High Availability.</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:420"><nobr><span class="ft4">We</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:113"><nobr><span class="ft4">Service</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:193"><nobr><span class="ft4">Error</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:298"><nobr><span class="ft4">MTTR (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:113"><nobr><span class="ft4">GenServer</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:206"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:217"><nobr><span class="ft4">bad match</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:342"><nobr><span class="ft4">320</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:113"><nobr><span class="ft4">cRPC</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:206"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:217"><nobr><span class="ft4">bad match</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:346"><nobr><span class="ft4">70</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:113"><nobr><span class="ft4">DDB</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:206"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:217"><nobr><span class="ft4">tuples</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:332"><nobr><span class="ft4">9</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:343"><nobr><span class="ft4"> 10</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:366"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:113"><nobr><span class="ft4">Node</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:206"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:217"><nobr><span class="ft4">bad match</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:342"><nobr><span class="ft4">250</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:113"><nobr><span class="ft4">Supervisor</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:206"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:217"><nobr><span class="ft4">bad match</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:346"><nobr><span class="ft4">60</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:113"><nobr><span class="ft4">Worker</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:193"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:206"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:217"><nobr><span class="ft4">bad match</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:342"><nobr><span class="ft4">115</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:171"><nobr><span class="ft4">Table 1: MTTR Dominos</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:81"><nobr><span class="ft4">see that for 10</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:180"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:194"><nobr><span class="ft4">matches of error, the system resumes</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:81"><nobr><span class="ft4">service virtually instantaneously.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:295"><nobr><span class="ft4">The DB was tested on</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:81"><nobr><span class="ft4">10</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:94"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:107"><nobr><span class="ft4">tuples in random access memory and resumed service</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:81"><nobr><span class="ft4">after approximately 9 seconds.</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:291"><nobr><span class="ft4">This corresponds to an</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:81"><nobr><span class="ft10">excellent MTTR, given that the injections were made on<br>a PIII-966Mhz with 512Mb of RAM. From these results, we<br>may label our system as being High Availability, as opposed<br>to other architectures that consider High Availability only<br>in the sense of failure not affecting other components of<br>the system, but in which service restart of a component<br>unfortunately requires manual intervention every time.</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:81"><nobr><span class="ft3"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:918;left:112"><nobr><span class="ft3"><b>EXPERIMENTATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:940;left:94"><nobr><span class="ft4">This section describes Dominos' experimental results tested</span></nobr></DIV>
<DIV style="position:absolute;top:956;left:81"><nobr><span class="ft4">on 5 DELL machines:</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:101"><nobr><span class="ft4">· nico: Intel Pentium 4 - 1.6 Ghz, 256 Mb RAM. Crawl</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:114"><nobr><span class="ft4">node (supervisor, workers). Activates a local cRPC.</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:101"><nobr><span class="ft4">· zico: Intel Pentium 4 - 1.6 Ghz, 256 Mb RAM. Crawl</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:114"><nobr><span class="ft4">node (supervisor, workers). Activates a local cRPC.</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:101"><nobr><span class="ft4">· chopin: Intel Pentium 3 - 966 Mhz, 512 Mb RAM.</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:114"><nobr><span class="ft4">Main node loaded on ServerGen and DB. Also handles</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:509"><nobr><span class="ft4">crawling (supervisor, workers).</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:724"><nobr><span class="ft4">Activates a local</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:509"><nobr><span class="ft4">cRPC.</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:495"><nobr><span class="ft4">· gao: Intel Pentium 3 - 500 Mhz, 256 Mb RAM. Node</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:509"><nobr><span class="ft4">for DB fragmentation. Activates a local cRPC.</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:495"><nobr><span class="ft4">· margo: Intel Pentium 2 - 333 Mhz, 256 Mb RAM.</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:509"><nobr><span class="ft4">Node for DB fragmentation. Activates a local cRPC.</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:489"><nobr><span class="ft4">Machines chopin, gao and margo are not dedicated solely</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:475"><nobr><span class="ft10">to crawling and are used as everyday workstations. Disk<br>size is not taken into account as no data were actually<br>stored during these tests.</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:648"><nobr><span class="ft4">Everything was therefore car-</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:475"><nobr><span class="ft10">ried out using random access memory with a network of<br>100 Mb/second.</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:592"><nobr><span class="ft4">Dominos performed 25,116,487 HTTP</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:475"><nobr><span class="ft10">requests after 9 hours of crawling with an average of<br>816 documents/second for 49Kb per document.</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:798"><nobr><span class="ft4">Three</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:475"><nobr><span class="ft10">nodes (nico, zico and chopin) were used in crawling, each<br>having 400 workers.</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:617"><nobr><span class="ft4">We restricted ourselves to a total</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:475"><nobr><span class="ft10">of 1,200 workers, due to problems generated by Dominos<br>at intranet level.</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:604"><nobr><span class="ft4">The firewall set up to filter access</span></nobr></DIV>
<DIV style="position:absolute;top:413;left:475"><nobr><span class="ft10">is considerably detrimental to performance because of its<br>inability to keep up with the load imposed by Dominos.<br>Third-party tests have shown that peaks of only 4,000<br>HTTP requests/second cause the immediate collapse of the<br>firewall. The firewall is not the only limiting factor, as the<br>same tests have shown the incapacity of Web servers such<br>as Apache2, Caudium or Jigsaw to withstand such loads<br>(see http://www.sics.se/</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:621"><nobr><span class="ft4">joe/apachevsyaws.html). Figure 2</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:475"><nobr><span class="ft10">(left part) shows the average URL extraction per document<br>crawled using a single worker. The abscissa (x) axis rep-<br>resents the number of documents treated, and the ordered<br>(y) axis gives the time in microseconds corresponding to<br>extraction.</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:558"><nobr><span class="ft4">In the right-hand figure, the abscissa axis</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:475"><nobr><span class="ft10">represents the same quantity, though this time in terms<br>of data volume (Mb). We can see a high level of parsing<br>reaching an average of 3,000 pages/second at a speed of<br>70Mb/second. In Figure 3 we see that URL normalization</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:505"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:491"><nobr><span class="ft25"> 500000</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:494"><nobr><span class="ft25"> 1e+06</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:490"><nobr><span class="ft25"> 1.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:494"><nobr><span class="ft25"> 2e+06</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:490"><nobr><span class="ft25"> 2.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:494"><nobr><span class="ft25"> 3e+06</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:490"><nobr><span class="ft25"> 3.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:510"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:529"><nobr><span class="ft25"> 2000</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:553"><nobr><span class="ft25"> 4000</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:576"><nobr><span class="ft25"> 6000</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:599"><nobr><span class="ft25"> 8000</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:621"><nobr><span class="ft25"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:482"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:558"><nobr><span class="ft25">Documents</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:528"><nobr><span class="ft25">Average number of parsed documents</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:514"><nobr><span class="ft25">PD</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:699"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:685"><nobr><span class="ft25"> 500000</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:688"><nobr><span class="ft25"> 1e+06</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:684"><nobr><span class="ft25"> 1.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:688"><nobr><span class="ft25"> 2e+06</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:684"><nobr><span class="ft25"> 2.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:688"><nobr><span class="ft25"> 3e+06</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:684"><nobr><span class="ft25"> 3.5e+06</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:704"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:717"><nobr><span class="ft25"> 20</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:732"><nobr><span class="ft25"> 40</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:746"><nobr><span class="ft25"> 60</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:761"><nobr><span class="ft25"> 80</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:774"><nobr><span class="ft25"> 100</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:789"><nobr><span class="ft25"> 120</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:803"><nobr><span class="ft25"> 140</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:818"><nobr><span class="ft25"> 160</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:675"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:742"><nobr><span class="ft25">Document Size (Mb)</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:725"><nobr><span class="ft25">Average size of parsed documents  </span></nobr></DIV>
<DIV style="position:absolute;top:708;left:708"><nobr><span class="ft25">PDS</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:564"><nobr><span class="ft4">Figure 2: Link Extraction</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:475"><nobr><span class="ft10">is as efficient as extraction in terms of speed. The abscissa<br>axis at the top (and respectively at the bottom) represents<br>the number of documents processed per normalization phase<br>(respectively the quantity of documents in terms of volume).<br>Each worker normalizes on average 1,000 documents/sec-<br>ond, which is equivalent to 37,000 URLs/second at a speed<br>of 40Mb/second. Finally, the URL cache structure ensures<br>a high degree of scalability (Figure 3). The abscissa axis in<br>this figure represents the number of key-values inserted or<br>retrieved. The cache is very close to a step function due to<br>key compression in the Judy-Array. Following an increase in<br>insertion/retrieval time in the cache, it appears to plateau<br>by 100,000 key-value bands. We should however point out<br>that URL extraction and normalization also makes use of<br>this type of cache so as to avoid processing a URL already<br>encountered.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">305</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="100008.png" alt="background image">
<DIV style="position:absolute;top:204;left:105"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:94"><nobr><span class="ft25"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:94"><nobr><span class="ft25"> 20000</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:94"><nobr><span class="ft25"> 30000</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:94"><nobr><span class="ft25"> 40000</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:94"><nobr><span class="ft25"> 50000</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:94"><nobr><span class="ft25"> 60000</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:110"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:130"><nobr><span class="ft25"> 2000</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:155"><nobr><span class="ft25"> 4000</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:179"><nobr><span class="ft25"> 6000</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:204"><nobr><span class="ft25"> 8000</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:227"><nobr><span class="ft25"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:87"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:148"><nobr><span class="ft25">Normalized documents</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:126"><nobr><span class="ft25">Average number of normalized documents</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:114"><nobr><span class="ft25">AD</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:298"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:287"><nobr><span class="ft25"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:287"><nobr><span class="ft25"> 20000</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:287"><nobr><span class="ft25"> 30000</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:287"><nobr><span class="ft25"> 40000</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:287"><nobr><span class="ft25"> 50000</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:287"><nobr><span class="ft25"> 60000</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:303"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:315"><nobr><span class="ft25"> 2000  4000  6000  8000  10000 12000 14000 16000</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:281"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:362"><nobr><span class="ft25">Urls</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:328"><nobr><span class="ft25">Average number of normalized Url  </span></nobr></DIV>
<DIV style="position:absolute;top:117;left:308"><nobr><span class="ft25">AU</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:105"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:94"><nobr><span class="ft25"> 10000</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:94"><nobr><span class="ft25"> 20000</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:94"><nobr><span class="ft25"> 30000</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:94"><nobr><span class="ft25"> 40000</span></nobr></DIV>
<DIV style="position:absolute;top:258;left:94"><nobr><span class="ft25"> 50000</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:94"><nobr><span class="ft25"> 60000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:110"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:124"><nobr><span class="ft25"> 20</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:139"><nobr><span class="ft25"> 40</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:154"><nobr><span class="ft25"> 60</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:170"><nobr><span class="ft25"> 80</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:184"><nobr><span class="ft25"> 100</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:199"><nobr><span class="ft25"> 120</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:214"><nobr><span class="ft25"> 140</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:229"><nobr><span class="ft25"> 160</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:87"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:150"><nobr><span class="ft25">Document Size (Mb)</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:129"><nobr><span class="ft25">Average size of normalized documents  </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:114"><nobr><span class="ft25">ADS</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:301"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:290"><nobr><span class="ft25"> 50000</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:288"><nobr><span class="ft25"> 100000</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:288"><nobr><span class="ft25"> 150000</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:288"><nobr><span class="ft25"> 200000</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:288"><nobr><span class="ft25"> 250000</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:288"><nobr><span class="ft25"> 300000</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:288"><nobr><span class="ft25"> 350000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:306"><nobr><span class="ft25"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:325"><nobr><span class="ft25"> 20000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:349"><nobr><span class="ft25"> 40000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:373"><nobr><span class="ft25"> 60000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:396"><nobr><span class="ft25"> 80000</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:419"><nobr><span class="ft25"> 100000</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:281"><nobr><span class="ft25">Time (microsec)</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:357"><nobr><span class="ft25">Key-Value</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:326"><nobr><span class="ft25">Scalable Cache : Insertion vs Retrieval</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:321"><nobr><span class="ft25">Cache Insertion</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:321"><nobr><span class="ft25">Cache Retrieval</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:81"><nobr><span class="ft4">Figure 3:</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:158"><nobr><span class="ft4">URL Normalization and Cache Perfor-</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:81"><nobr><span class="ft4">mance</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:81"><nobr><span class="ft3"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:418;left:112"><nobr><span class="ft3"><b>CONCLUSION</b></span></nobr></DIV>
<DIV style="position:absolute;top:440;left:94"><nobr><span class="ft4">In the present paper, we have introduced a high avail-</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:81"><nobr><span class="ft4">ability system of crawling called Dominos.</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:364"><nobr><span class="ft4">This system</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:81"><nobr><span class="ft10">has been created in the framework of experimentation<br>for French Web legal deposit carried out at the Institut<br>National de l'Audiovisuel (INA). Dominos is a dynamic<br>system, whereby the processes making up its kernel are<br>mobile.</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:137"><nobr><span class="ft4">90% of this system was developed using Erlang</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:81"><nobr><span class="ft10">programming language, which accounts for its highly flexible<br>deployment, maintainability and enhanced fault tolerance.<br>Despite having different objectives, we have been able to<br>compare it with other documented Web crawling systems<br>(Mercator, InternetArchive . . . ) and have shown it to be<br>superior in terms of crawl speed, document parsing and<br>process management without system restart.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:94"><nobr><span class="ft4">Dominos is more complex than its description here. We</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:81"><nobr><span class="ft10">have not touched upon archival storage and indexation.<br>We have preferred to concentrate rather on the detail of<br>implementation of the Dominos kernel itself, a strategic<br>component that is often overlooked by other systems (in par-<br>ticular those that are proprietary, others being inefficient).</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:94"><nobr><span class="ft4">However, there is still room for the system's improvement.</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:81"><nobr><span class="ft10">At present, crawled archives are managed by NFS, a file<br>system that is moderately efficient for this type of problem.<br>Switchover to Lustre</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:213"><nobr><span class="ft5">9</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:220"><nobr><span class="ft4">, a distributed file system with a</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:81"><nobr><span class="ft4">radically higher level of performance, is underway.</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:81"><nobr><span class="ft3"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:854;left:112"><nobr><span class="ft3"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:880;left:88"><nobr><span class="ft4">[1] Z. BarYossef, A. Berg, S. Chien, J. Fakcharoenphol,</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:109"><nobr><span class="ft10">and D. Weitz. Approximating aggregate queries about<br>web pages via random walks. In Proc. of 26th Int.<br>Conf. on Very Large Data Bases, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:88"><nobr><span class="ft4">[2] K. Bharat, A. Broder, M. Henzinger, P. Kumar, and</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:109"><nobr><span class="ft10">S. Venkatasubramanian. The connectivity server: Fast<br>access to linkage. Information on the Web, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:993;left:88"><nobr><span class="ft4">[3] S. Brin and L. Page. The anatomy of a large-scale</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:109"><nobr><span class="ft10">hypertextual web search engine. In Proc. of the<br>Seventh World-Wide Web Conference, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:88"><nobr><span class="ft4">[4] M. Burner. Crawling towards eternity: Building an</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:109"><nobr><span class="ft4">archive of the world wide web.</span></nobr></DIV>
<DIV style="position:absolute;top:1081;left:81"><nobr><span class="ft5">9</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:88"><nobr><span class="ft4">http://www.lustre.org/</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:503"><nobr><span class="ft10">http://www.webtechniques.com/archives/1997/05/burner/,<br>1997.</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:482"><nobr><span class="ft4">[5] S. Chakrabarti, M. V. D. Berg, and B. Dom.</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:503"><nobr><span class="ft10">Distributed hypertext resource discovery through<br>example. In Proc. of 25th Int. Conf. on Very Large<br>Data Base, pages 375­386, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:482"><nobr><span class="ft4">[6] S. Chakrabarti, M. V. D. Berg, and B. Dom. Focused</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:503"><nobr><span class="ft10">crawling: A new approach to topic-specific web<br>resource discovery. In Proc. of the 8th Int. World<br>Wide Web Conference, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:482"><nobr><span class="ft4">[7] J. Cho and H. Garcia-Molina. The evolution of the</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:503"><nobr><span class="ft10">web and implications for an incremental crawler. In<br>Proc. of 26th Int. Conf. on Very Large Data Bases,<br>pages 117­128, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:482"><nobr><span class="ft4">[8] J. Cho and H. Garcia-Molina. Synchronizing a</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:503"><nobr><span class="ft10">database to improve freshness. In Proc. of the ACM<br>SIGMOD Int. Conf. on Management of Data, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:482"><nobr><span class="ft4">[9] J. Cho, H. Garcia-Molina, and L. Page. Efficient</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:503"><nobr><span class="ft10">crawling through url ordering. In 7th Int. World Wide<br>Web Conference, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:475"><nobr><span class="ft4">[10] M. Diligenti, F. Coetzee, S. Lawrence, C. Giles, and</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:503"><nobr><span class="ft10">M. Gori. Focused crawling using context graphs. In<br>Proc. of 26th Int. Conf. on Very Large Data Bases,<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:475"><nobr><span class="ft4">[11] M. Henzinger, A. Heydon, M. Mitzenmacher, and</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:503"><nobr><span class="ft10">M. Najork. Measuring index quality using random<br>walks on the web. In Proc. of the 8th Int. World Wide<br>Web Conference (WWW8), pages 213­225, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:475"><nobr><span class="ft4">[12] M. Henzinger, A. Heydon, M. Mitzenmacher, and</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:503"><nobr><span class="ft10">M. Najork. On near-uniform url sampling. In Proc. of<br>the 9th Int. World Wide Web Conference, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:475"><nobr><span class="ft4">[13] A. Heydon and M. Najork. Mercator: A scalable,</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:503"><nobr><span class="ft10">extensible web crawler. World Wide Web Conference,<br>pages 219­229, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:475"><nobr><span class="ft4">[14] J. Hirai, S. Raghavan, H. Garcia-Molina, and</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:503"><nobr><span class="ft10">A. Paepcke. Webbase : : A repository of web pages. In<br>Proc. of the 9th Int. World Wide Web Conference,<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:475"><nobr><span class="ft4">[15] B. Kahle. Archiving the internet. Scientific American,</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:503"><nobr><span class="ft4">1997.</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:475"><nobr><span class="ft4">[16] S. Lawrence and C. L. Giles. Searching the world wide</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:503"><nobr><span class="ft4">web. Science 280, pages 98­100, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:475"><nobr><span class="ft4">[17] M. Najork and J. Wiener. Breadth-first search</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:503"><nobr><span class="ft10">crawling yields high-quality pages. In 10th Int. World<br>Wide Web Conference, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:475"><nobr><span class="ft4">[18] J. Rennie and A. McCallum. Using reinforcement</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:503"><nobr><span class="ft10">learning to spider the web efficiently. In Proc. of the<br>Int. Conf. on Machine Learning, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:475"><nobr><span class="ft4">[19] S. Russel and P. Norvig. Artificial Intelligence: A</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:503"><nobr><span class="ft4">modern Approach. Prentice Hall, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:475"><nobr><span class="ft4">[20] V. Shkapenyuk and T. Suel. Design and</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:503"><nobr><span class="ft10">implementation of a high-performance distributed web<br>crawler. Polytechnic University: Brooklyn, Mars 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:475"><nobr><span class="ft4">[21] T. Suel and J. Yuan. Compressing the graph structure</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:503"><nobr><span class="ft10">of the web. In Proc. of the IEEE Data Compression<br>Conference, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:475"><nobr><span class="ft4">[22] J. Talim, Z. Liu, P. Nain, and E. Coffman. Controlling</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:503"><nobr><span class="ft10">robots of web search engines. In SIGMETRICS<br>Conference, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft7">306</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
