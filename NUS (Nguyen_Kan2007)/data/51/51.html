<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\51</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-04-27T11:58:41+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft0{font-size:11px;font-family:Times;color:#000000;}
	.ft1{font-size:24px;font-family:Times;color:#000000;}
	.ft2{font-size:15px;font-family:Times;color:#000000;}
	.ft3{font-size:12px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Times;color:#000000;}
	.ft7{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51001.png" alt="background image">
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:1388"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:282"><nobr><span class="ft1"><b>Can Machine Learning Be Secure?</b></span></nobr></DIV>
<DIV style="position:absolute;top:238;left:163"><nobr><span class="ft2">Marco Barreno</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:308"><nobr><span class="ft2">Blaine Nelson</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:445"><nobr><span class="ft2">Russell Sears</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:583"><nobr><span class="ft2">Anthony D. Joseph</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:759"><nobr><span class="ft2">J. D. Tygar</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:413"><nobr><span class="ft3">Computer Science Division</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:395"><nobr><span class="ft3">University of California, Berkeley</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:298"><nobr><span class="ft2">{barreno,nelsonb,sears,adj,tygar}@cs.berkeley.edu</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:126"><nobr><span class="ft2">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:126"><nobr><span class="ft7">Machine learning systems offer unparalled flexibility in deal-<br>ing with evolving input in a variety of applications, such as<br>intrusion detection systems and spam e-mail filtering. How-<br>ever, machine learning algorithms themselves can be a target<br>of attack by a malicious adversary. This paper provides a<br>framework for answering the question, "Can machine learn-<br>ing be secure?" Novel contributions of this paper include<br>a taxonomy of different types of attacks on machine learn-<br>ing techniques and systems, a variety of defenses against<br>those attacks, a discussion of ideas that are important to<br>security for machine learning, an analytical model giving a<br>lower bound on attacker's work function, and a list of open<br>problems.</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:126"><nobr><span class="ft2">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:126"><nobr><span class="ft7">D.4.6 [Security and Protection]: Invasive software (e.g.,<br>viruses, worms, Trojan horses); I.5.1 [Models]: Statistical;<br>I.5.2 [Design Methodology]</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:126"><nobr><span class="ft2">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:126"><nobr><span class="ft4">Security</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:126"><nobr><span class="ft2">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:126"><nobr><span class="ft7">Adversarial Learning, Computer Networks, Computer Secu-<br>rity, Game Theory, Intrusion Detection, Machine Learning,<br>Security Metrics, Spam Filters, Statistical Learning</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:126"><nobr><span class="ft2">1. INTRODUCTION</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:126"><nobr><span class="ft7">Machine learning techniques are being applied to a growing<br>number of systems and networking problems, particularly<br>those problems where the intention is to detect anomalous<br>system behavior. For instance, network Intrusion Detection<br>Systems (IDS) monitor network traffic to detect abnormal<br>activities, such as attacks against hosts or servers. Machine<br>learning techniques offer the benefit that they can detect<br>novel differences in traffic (which presumably represent at-<br>tack traffic) by being trained on normal (known good) and</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:126"><nobr><span class="ft5">Permission to make digital or hard copies of all or part of this work for</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:126"><nobr><span class="ft5">personal or classroom use is granted without fee provided that copies are</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:126"><nobr><span class="ft5">not made or distributed for profit or commercial advantage and that copies</span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:126"><nobr><span class="ft5">bear this notice and the full citation on the first page. To copy otherwise, to</span></nobr></DIV>
<DIV style="position:absolute;top:1093;left:126"><nobr><span class="ft5">republish, to post on servers or to redistribute to lists, requires prior specific</span></nobr></DIV>
<DIV style="position:absolute;top:1107;left:126"><nobr><span class="ft5">permission and/or a fee.</span></nobr></DIV>
<DIV style="position:absolute;top:1120;left:126"><nobr><span class="ft5">ASIACCS'06, March 21­24, 2006, Taipei, Taiwan</span></nobr></DIV>
<DIV style="position:absolute;top:1134;left:126"><nobr><span class="ft5">Copyright 2006 ACM 1-59593-272-0/06/0003 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1134;left:361"><nobr><span class="ft4">$</span></nobr></DIV>
<DIV style="position:absolute;top:1134;left:368"><nobr><span class="ft5">5.00</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:521"><nobr><span class="ft7">attack (known bad) traffic. The traditional approach to de-<br>signing an IDS relied on an expert codifying rules defining<br>normal behavior and intrusions [26]. Because this approach<br>often fails to detect novel intrusions, a variety of researchers<br>have proposed incorporating machine learning techniques<br>into intrusion detection systems [1, 16, 18, 24, 38, 41]. On<br>the other hand, use of machine learning opens the possibility<br>of an adversary who maliciously "mis-trains" a learning sys-<br>tem in an IDS. A natural question arises: what techniques<br>(in their attacks) can an adversary use to confuse a learning<br>system?</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:521"><nobr><span class="ft7">This paper explores the larger question, as posed in the ti-<br>tle of this paper, can machine learning be secure? Specific<br>questions that we examine include:</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:541"><nobr><span class="ft4">· Can the adversary manipulate a learning system to</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:555"><nobr><span class="ft7">permit a specific attack? For example, can an attacker<br>leverage knowledge about the machine learning system<br>used by a spam e-mail filtering system to bypass the<br>filtering?</span></nobr></DIV>
<DIV style="position:absolute;top:733;left:541"><nobr><span class="ft4">· Can an adversary degrade the performance of a learn-</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:555"><nobr><span class="ft7">ing system to the extent that system administrators<br>are forced to disable the IDS? For example, could the<br>attacker confuse the system and cause valid e-mail to<br>be rejected?</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:541"><nobr><span class="ft4">· What defenses exist against adversaries manipulating</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:555"><nobr><span class="ft4">(attacking) learning systems?</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:541"><nobr><span class="ft4">· More generally, what is the potential impact from a se-</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:555"><nobr><span class="ft7">curity standpoint of using machine learning on a sys-<br>tem? Can an attacker exploit properties of the ma-<br>chine learning technique to disrupt the system?</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:521"><nobr><span class="ft7">The issue of machine learning security goes beyond intru-<br>sion detection systems and spam e-mail filters. Machine<br>learning is a powerful technique and has been used in a va-<br>riety of applications, including web services, online agent<br>systems, virus detection, cluster monitoring, and a variety<br>of applications that must deal with dynamically changing<br>data patterns.</span></nobr></DIV>
<DIV style="position:absolute;top:1084;left:521"><nobr><span class="ft7">Novel contributions of this paper include a taxonomy of dif-<br>ferent types of attacks on machine learning techniques and<br>systems, a variety of defenses against those attacks, a dis-<br>cussion of ideas that are important to security for machine</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:122"><nobr><span class="ft6">Invited Talk</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft8{font-size:6px;font-family:Times;color:#000000;}
	.ft9{font-size:5px;font-family:Times;color:#000000;}
	.ft10{font-size:15px;line-height:28px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51002.png" alt="background image">
<DIV style="position:absolute;top:1192;left:-397"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:126"><nobr><span class="ft7">learning, an analytical model giving a lower bound on at-<br>tacker's work function, and a list of open problems.</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:126"><nobr><span class="ft7">The rest of this paper is organized as follows: Section 2 dis-<br>cusses machine learning and how it is typically used in a<br>system, Section 3 develops a taxonomy of attacks, Section 4<br>introduces potential defenses against attacks and explores<br>their potential costs, Section 5 identifies several of the ideas<br>that are important to security for machine learning, Sec-<br>tion 6 presents an analytical model that examines an attack<br>to manipulate a naive learning algorithm, Section 7 discusses<br>related work, potential research directions, and our conclu-<br>sions.</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:126"><nobr><span class="ft10">2. REVIEW<br>2.1 The Learning Problem</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:126"><nobr><span class="ft7">A machine learning system attempts to find a hypothesis<br>function f that maps events (which we call points below)<br>into different classes. For example, an intrusion detection<br>system would find a hypothesis function f that maps an<br>event point (an instance of network behavior) into one of<br>two results: normal or intrusion.</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:126"><nobr><span class="ft7">One kind of learning system called supervised learning works<br>by taking a training data set together with labels identifying<br>the class for every point in the training data set.</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:126"><nobr><span class="ft7">For example, a supervised learning algorithm for an IDS<br>would have a training set consisting of points correspond-<br>ing to normal behavior and points corresponding to intru-<br>sion behavior. The learning algorithm selects the hypothesis<br>function f that best predicts the classification of a point.<br>More complicated learning algorithms can deal with event<br>points that are both labeled and unlabeled and furthermore<br>can deal with continuous streams of unlabeled points so that<br>training is an ongoing process. In this paper, we call these<br>algorithms online learning systems.</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:126"><nobr><span class="ft7">This remainder of this subsection presents a concise overview<br>of concepts in statistical learning theory. The presentation<br>below is formal and can be skipped on a first reading. For<br>a fuller discussion with motivation, refer to [11, 31].</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:126"><nobr><span class="ft7">A predictive learning problem is defined over an input space<br>X, an output space Y, and a loss function  : Y × Y  R.</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:126"><nobr><span class="ft4">The input to the problem is a training set</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:396"><nobr><span class="ft4">S, specified as</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:126"><nobr><span class="ft4">{(x</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:147"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:151"><nobr><span class="ft4">, y</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:164"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:169"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:178"><nobr><span class="ft4"> X × Y}, and the output is a hypothesis function</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:126"><nobr><span class="ft4">f :</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:147"><nobr><span class="ft4">X  Y. We choose f from a hypothesis space (or func-</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:126"><nobr><span class="ft4">tion class)</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:193"><nobr><span class="ft4">F to minimize the prediction error given by the</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:126"><nobr><span class="ft7">loss function. In many cases, researchers assume stationar-<br>ity, that the distribution of data points encountered in the<br>future will be the same as the distribution of the training<br>set. Stationarity allows us to reduce the predictive learning<br>problem to a minimization of the sum of the loss over the<br>training set:</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:209"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:1072;left:217"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:227"><nobr><span class="ft4">= argmin</span></nobr></DIV>
<DIV style="position:absolute;top:1090;left:252"><nobr><span class="ft8">f</span></nobr></DIV>
<DIV style="position:absolute;top:1090;left:258"><nobr><span class="ft8">F</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:301"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:1095;left:286"><nobr><span class="ft8">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1098;left:297"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1095;left:302"><nobr><span class="ft8">,y</span></nobr></DIV>
<DIV style="position:absolute;top:1098;left:311"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1095;left:316"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:1095;left:320"><nobr><span class="ft8">S</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:337"><nobr><span class="ft4">(f (x</span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:370"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:374"><nobr><span class="ft4">), y</span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:393"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:397"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:1075;left:467"><nobr><span class="ft4">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:126"><nobr><span class="ft7">Loss functions are typically defined to be non-negative over<br>all inputs and zero when f (x</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:301"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:306"><nobr><span class="ft4">) = y</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:336"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:341"><nobr><span class="ft4">. A commonly used loss</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:126"><nobr><span class="ft4">function is the squared-error loss </span></nobr></DIV>
<DIV style="position:absolute;top:1156;left:329"><nobr><span class="ft8">sq</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:340"><nobr><span class="ft4">(f (x</span></nobr></DIV>
<DIV style="position:absolute;top:1156;left:367"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:371"><nobr><span class="ft4">), y) = (f (x</span></nobr></DIV>
<DIV style="position:absolute;top:1156;left:441"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:446"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:1150;left:451"><nobr><span class="ft4">-y)</span></nobr></DIV>
<DIV style="position:absolute;top:1148;left:475"><nobr><span class="ft8">2</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:481"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:521"><nobr><span class="ft4">The hypothesis space (or function class)</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:762"><nobr><span class="ft4">F can be any repre-</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:521"><nobr><span class="ft4">sentation of functions from</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:686"><nobr><span class="ft4">X to Y, such as linear functions,</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:521"><nobr><span class="ft7">polynomials, boolean functions, or neural networks. The<br>choice of</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:579"><nobr><span class="ft4">F involves a tradeoff between expressiveness and</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:521"><nobr><span class="ft4">ability to generalize. If</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:672"><nobr><span class="ft4">F is too expressive, it can overfit</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:521"><nobr><span class="ft7">the training data. The extreme case is a lookup table that<br>maps x</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:565"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:575"><nobr><span class="ft4">to y</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:599"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:609"><nobr><span class="ft4">for each instance of the training set but will</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:521"><nobr><span class="ft7">not generalize to new data. A linear function, on the other<br>hand, will generalize what it learns on the training set to<br>new points, though it may not be sufficiently expressive to<br>describe intricate data sets. We typically use simple func-<br>tion classes, such as linear functions, to avoid overfitting.</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:521"><nobr><span class="ft7">We can describe a more general learning problem by drop-<br>ping the requirement that the training examples include all<br>the labels y</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:588"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:593"><nobr><span class="ft4">. The case where all labels are present is referred</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:521"><nobr><span class="ft7">to as supervised learning, when no labels are present the<br>problem is unsupervised, and when some labels are present<br>the problem is semi-supervised. In all these cases we can<br>pose the learning problem as the minimization of some mea-<br>sure over the training set:</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:620"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:628"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:638"><nobr><span class="ft4">= argmin</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:662"><nobr><span class="ft8">f</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:669"><nobr><span class="ft8">F</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:704"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:697"><nobr><span class="ft8">(x</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:708"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:712"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:717"><nobr><span class="ft8">S</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:734"><nobr><span class="ft4">L(x</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:757"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:761"><nobr><span class="ft4">, f )</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:862"><nobr><span class="ft4">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:521"><nobr><span class="ft2">2.2 Terminology and Running Example</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:521"><nobr><span class="ft7">To illustrate some of our contributions, we use a running ex-<br>ample throughout this paper: a network Intrusion Detection<br>System (IDS). This IDS receives network events x</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:828"><nobr><span class="ft4"> X and</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:521"><nobr><span class="ft7">classifies each event x as either f (x) = normal or f (x) =<br>intrusion. The literature describes a number of algorithms<br>for learning f over time, but we wish to consider the impact<br>of malicious input on the learning algorithm. This paper<br>poses the question: can a malicious party send events to the<br>IDS that will cause it to malfunction? Possible types of at-<br>tacks on the IDS include attacks on the learning algorithm,<br>causing the IDS to create an f that misclassifies events. As<br>we discuss in the next section, this is only one of a number<br>of types of attacks that an adversary can make on an IDS.</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:521"><nobr><span class="ft7">It is important to be careful about notation here. When we<br>speak of attacks, we mean an attack on the learning system<br>(e.g., the learner in an IDS). Attacks may try to make the<br>learner mis-learn, fail because of denial of service, report<br>information about its internal state, etc. "Attack" should be<br>distinguished from "intrusion." An attack targets a learning<br>system; an intrusion targets a computer system (such as a<br>system protected by an IDS). While many researchers use<br>the word "attack" to include intrusions, in this paper we are<br>careful to use the word "attack" only to mean an attack on<br>a learner.</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:521"><nobr><span class="ft7">We do not want to restrict ourselves to particular learning<br>algorithms used by intrusion detection systems to choose<br>hypotheses. However, we allow adversaries that have deep<br>understanding of the learning algorithms.</span></nobr></DIV>
<DIV style="position:absolute;top:1072;left:521"><nobr><span class="ft7">Similarly, we do not discuss mechanisms for translating net-<br>work level events into a form relevant to the learner. We call<br>each unit of data seen by the learner a data point, or simply<br>a point. In the context of the IDS, our discussion encom-<br>passes continuous, discrete, or mixed data. We assume that<br>X is a metric space, allowing us to freely discuss distances</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft11{font-size:11px;line-height:12px;font-family:Times;color:#000000;}
	.ft12{font-size:11px;line-height:24px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51003.png" alt="background image">
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:1388"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:458"><nobr><span class="ft4">Integrity</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:682"><nobr><span class="ft4">Availability</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:177"><nobr><span class="ft4">Causative:</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:305"><nobr><span class="ft4">Targeted</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:376"><nobr><span class="ft4">Permit a specific intrusion</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:608"><nobr><span class="ft7">Create sufficient errors to make sys-<br>tem unusable for one person or ser-<br>vice</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:268"><nobr><span class="ft4">Indiscriminate</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:376"><nobr><span class="ft4">Permit at least one intrusion</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:608"><nobr><span class="ft7">Create sufficient errors to make<br>learner unusable</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:177"><nobr><span class="ft4">Exploratory:</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:305"><nobr><span class="ft4">Targeted</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:376"><nobr><span class="ft7">Find a permitted intrusion from a<br>small set of possibilities</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:608"><nobr><span class="ft7">Find a set of points misclassified by<br>the learner</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:268"><nobr><span class="ft4">Indiscriminate</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:376"><nobr><span class="ft4">Find a permitted intrusion</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:408"><nobr><span class="ft4">Table 1: The attack model.</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:126"><nobr><span class="ft7">between points. Furthermore, we assume the set of points<br>classified as normal by the IDS forms multiple contiguous<br>subsets in</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:192"><nobr><span class="ft4">X. The border of this set is called the decision</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:126"><nobr><span class="ft4">boundary.</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:126"><nobr><span class="ft4">Below, we consider a variety of scenarios and assumptions.</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:126"><nobr><span class="ft2">3. ATTACKS</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:126"><nobr><span class="ft2">3.1 Attack Model</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:126"><nobr><span class="ft7">We give relevant properties for analyzing attacks on machine<br>learning systems.</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:126"><nobr><span class="ft4">Influence</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:160"><nobr><span class="ft4">Causative - Causative attacks alter the training pro-</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:190"><nobr><span class="ft4">cess through influence over the training data.</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:160"><nobr><span class="ft4">Exploratory - Exploratory attacks do not alter the</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:190"><nobr><span class="ft7">training process but use other techniques, such as<br>probing the learner or offline analysis, to discover<br>information.</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:126"><nobr><span class="ft4">Specificity</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:160"><nobr><span class="ft4">Targeted - The specificity of an attack is a continu-</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:190"><nobr><span class="ft7">ous spectrum. At the targeted end, the focus of<br>the attack is on a particular point or a small set<br>of points.</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:160"><nobr><span class="ft4">Indiscriminate - At the indiscriminate end, the ad-</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:190"><nobr><span class="ft7">versary has a more flexible goal that involves a<br>very general class of points, such as "any false<br>negative."</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:126"><nobr><span class="ft4">Security violation</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:160"><nobr><span class="ft4">Integrity - An integrity attack results in intrusion</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:190"><nobr><span class="ft4">points being classified as normal (false negatives).</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:160"><nobr><span class="ft4">Availability - An availability attack is a broader class</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:190"><nobr><span class="ft7">of attack than an integrity attack. An availabil-<br>ity attack results in so many classification errors,<br>both false negatives and false positives, that the<br>system becomes effectively unusable.</span></nobr></DIV>
<DIV style="position:absolute;top:1076;left:126"><nobr><span class="ft7">These three axes define a space of attacks; Table 1 provides<br>a concise summary.</span></nobr></DIV>
<DIV style="position:absolute;top:1123;left:126"><nobr><span class="ft7">In causative attacks, the adversary has some measure of con-<br>trol over the training of the learner. An attack that causes<br>the learner to misclassify intrusion points, for example an</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:521"><nobr><span class="ft7">attack that fools an IDS into not flagging a known exploit<br>as an intrusion, is a causative integrity attack. The distinc-<br>tion between targeted and indiscriminate causative integrity<br>attacks is the difference between choosing one particular ex-<br>ploit or just finding any exploit. A causative availability<br>attack causes the learner's performance to degrade. For ex-<br>ample, an adversary might cause an IDS to reject many le-<br>gitimate HTTP connections. A causative availability attack<br>may be used to force the system administrator to disable<br>the IDS. A targeted attack focuses on a particular service,<br>while an indiscriminate attack has a wider scope.</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:521"><nobr><span class="ft7">Exploratory attacks do not attempt to influence learning;<br>they instead attempt to discover information about the state<br>of the learner. Exploratory integrity attacks seek to find<br>intrusions that are not recognized by the learner.</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:521"><nobr><span class="ft2">3.2 Online Learning</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:521"><nobr><span class="ft7">A learner can have an explicit training phase or can be<br>continuously trained (online learner). Online learning al-<br>lows the learner to adapt to changing conditions; the as-<br>sumption of stationarity is weakened to accommodate long-<br>term changes in the distribution of data seen by the learner.<br>Online learning is more flexible, but potentially simplifies<br>causative attacks. By definition, an online learner changes<br>its prediction function over time, so an adversary has the op-<br>portunity to shape this change. Gradual causative attacks<br>may be difficult to detect.</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:521"><nobr><span class="ft2">4. DEFENSES</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:521"><nobr><span class="ft7">In this section we discuss potential defenses against attacks.<br>This section describes speculative work, and the efficacy of<br>these techniques in practice is a topic for future research.</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:521"><nobr><span class="ft2">4.1 Robustness</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:521"><nobr><span class="ft7">To increase robustness against causative attacks we con-<br>strain the class of functions (hypotheses) that the learner<br>considers. The constraint we consider is the statistical tech-<br>nique of regularization. Regularization extends the basic<br>learning optimization in Equation (1) by adding a term J(f )<br>that penalizes complex hypotheses:</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:565"><nobr><span class="ft4">f</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:573"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:583"><nobr><span class="ft4">= argmin</span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:607"><nobr><span class="ft8">f</span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:614"><nobr><span class="ft8">F</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:642"><nobr><span class="ft12">8<br>&lt;<br>: X</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:655"><nobr><span class="ft8">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1089;left:665"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:670"><nobr><span class="ft8">,y</span></nobr></DIV>
<DIV style="position:absolute;top:1089;left:679"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:684"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:688"><nobr><span class="ft8">S</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:705"><nobr><span class="ft4">(f (x</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:738"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:742"><nobr><span class="ft4">), y</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:761"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:766"><nobr><span class="ft4">) + J(f )</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:824"><nobr><span class="ft12">9<br>=<br>; (3)</span></nobr></DIV>
<DIV style="position:absolute;top:1107;left:521"><nobr><span class="ft7">Here  adjusts the trade-off. The penalty term J(f ) can<br>be as simple as the sum of squares of the parameters of f .<br>Regularization is used in statistics to restrict or bias the<br>choice of hypothesis when the problem suffers from lack of</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft13{font-size:11px;line-height:26px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51004.png" alt="background image">
<DIV style="position:absolute;top:1192;left:-397"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:458"><nobr><span class="ft4">Integrity</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:682"><nobr><span class="ft4">Availability</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:177"><nobr><span class="ft4">Causative:</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:305"><nobr><span class="ft4">Targeted</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:396"><nobr><span class="ft13">· Regularization<br>· Randomization</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:628"><nobr><span class="ft13">· Regularization<br>· Randomization</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:268"><nobr><span class="ft4">Indiscriminate</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:396"><nobr><span class="ft4">· Regularization</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:628"><nobr><span class="ft4">· Regularization</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:177"><nobr><span class="ft4">Exploratory:</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:305"><nobr><span class="ft4">Targeted</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:396"><nobr><span class="ft13">· Information hiding<br>· Randomization</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:628"><nobr><span class="ft4">· Information hiding</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:268"><nobr><span class="ft4">Indiscriminate</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:396"><nobr><span class="ft4">· Information hiding</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:333"><nobr><span class="ft4">Table 2: Defenses against the attacks in Table 1.</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:126"><nobr><span class="ft7">data or noisy data. It can also be interpreted as encoding a<br>prior distribution on the parameters, penalizing parameter<br>choices that are less likely a priori. Regularization and prior<br>distributions can both be viewed as penalty functions in<br>Equation (3) [42].</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:126"><nobr><span class="ft7">The constraint added to the learning problem by the penalty<br>term may help our defenses in two ways. First, it has the ef-<br>fect of smoothing the solution, removing complexity that an<br>adversary might exploit in attacks. Second, prior distribu-<br>tions can be a useful way to encode expert knowledge about<br>a domain or use domain structure learned from a preprocess-<br>ing step. In the simplest case, we might have a reasonable<br>guess for the parameters (such as the mean) that we wish<br>to refine; in a more complex situation, we could perform an<br>analysis of a related dataset giving correlation information<br>which informs a multivariate Gaussian prior on the param-<br>eters [28]. When the learner has more prior information (or<br>constraints) on which to base the learning, there is less de-<br>pendence on exact data fitting, so there is less opportunity<br>for the adversary to exert influence over the learning process.</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:126"><nobr><span class="ft2">4.2 Detecting Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:126"><nobr><span class="ft7">The learner can benefit from the ability to detect attacks<br>even if they are not prevented. Detecting attacks can be dif-<br>ficult even when the adversary is not attempting to conceal<br>them. However, we may be able to detect causative attacks<br>by using a special test set. This test set could include sev-<br>eral known intrusions and intrusion variants, as well as some<br>random points that are similar to the intrusions. After the<br>learner has been trained, misclassifying a disproportionately<br>high number of intrusions could indicate compromises.</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:126"><nobr><span class="ft7">To detect naive exploratory attacks, a separate clustering<br>algorithm could be run against data classified by the learner.<br>The sudden appearance of a large cluster near the decision<br>boundary could indicate systematic probing. This type of<br>defense is akin to port scan detection, which has become an<br>arms race between port scanners and IDS [26].</span></nobr></DIV>
<DIV style="position:absolute;top:1091;left:126"><nobr><span class="ft7">Detecting an attack gives the learner information about the<br>adversary's capabilities. This information may be used to<br>reformulate defense strategies.</span></nobr></DIV>
<DIV style="position:absolute;top:1154;left:126"><nobr><span class="ft4">As the adversary's control over the data increases, the best</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:521"><nobr><span class="ft7">strategy for the learner is to ignore potentially tainted data.<br>Otherwise, the adversary can exploit misplaced trust. These<br>ideas have been formalized within the context of deception<br>games [14, 32], which typically assume all players know the<br>extent to which other players may manipulate data. How-<br>ever, if the parties estimate each other's abilities, more so-<br>phisticated strategies emerge.</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:521"><nobr><span class="ft2">4.3 Disinformation</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:521"><nobr><span class="ft7">In some circumstances, the learner may be able to alter the<br>data seen by the adversary. This strategy of disinformation<br>has the goal of confusing the adversary's estimate of the<br>learner's state. In the simplest case, the adversary would<br>then be faced with a situation not unlike a learner under<br>an indiscriminate causative availability attack. The goal of<br>the learner is to prevent the adversary from learning the<br>decision boundary. Please note how the roles of adversary<br>and learner have been reversed.</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:521"><nobr><span class="ft7">A more sophisticated learner could trick the adversary into<br>believing that a particular intrusion was not included in the<br>training set. This apparently permitted "intrusion" would<br>act as a honeypot [27], causing the adversary to reveal itself.<br>An increase in the incidence of that particular attack would<br>be detected, revealing the existence of an adversary. In this<br>case again, roles would reverse, and the adversary would face<br>a situation analogous to a learner subjected to a targeted<br>causative integrity attack.</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:521"><nobr><span class="ft2">4.4 Randomization for Targeted Attacks</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:521"><nobr><span class="ft7">Targeted attacks hinge on the classification of one point or a<br>small set of points. They are more sensitive to variations in<br>the decision boundary than indiscriminate attacks because<br>boundary movement is more likely to change the classifica-<br>tion of the relevant points.</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:521"><nobr><span class="ft7">This suggests randomization as a potential tool against tar-<br>geted causative attacks. In such an attack, the adversary<br>has to do a particular amount of work to move the decision<br>boundary past the targeted point. If there is some random-<br>ization in the placement of the boundary and the adversary<br>has imperfect feedback from the learner, more work is re-<br>quired.</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft14{font-size:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51005.png" alt="background image">
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0">0</span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:1388"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:126"><nobr><span class="ft2">4.5 Cost of Countermeasures</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:126"><nobr><span class="ft7">The more we know about the distribution of training data,<br>the less room there is for an adversary to manipulate the<br>learner. The disadvantage, however, is that the legitimate<br>data has less influence in the learning process. A tension<br>exists between expressivity and constraint: as the learner<br>includes more prior information, it loses flexibility to adapt<br>to the data, but as it incorporates more information from<br>the data, it becomes more vulnerable to attack.</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:126"><nobr><span class="ft7">Equation (3) makes this tradeoff explicit with . In the ad-<br>versarial scenario, this tradeoff becomes more relevant be-<br>cause the adversary may have influence over the data.</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:126"><nobr><span class="ft7">Randomization increases the adversary's work, but it also<br>will increase the learner's base error rate. Determining the<br>right amount of randomization is an open problem.</span></nobr></DIV>
<DIV style="position:absolute;top:456;left:126"><nobr><span class="ft2">4.6 Summary of Defenses</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:126"><nobr><span class="ft7">Table 2 shows how our defenses discussed here relate to at-<br>tack classes presented in Table 1. (Information hiding is an<br>additional technique discussed in Section 5 below.)</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:126"><nobr><span class="ft2">5. DISCUSSION</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:126"><nobr><span class="ft2">5.1 Secrets and Computational Complexity</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:126"><nobr><span class="ft7">A number of defenses and attacks upon machine learning<br>algorithms hinge upon the types of information available to<br>the adversary. Some of these involve information about the<br>decision boundary. Below we consider factors that influence<br>the security and secrecy of the decision boundary.</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:126"><nobr><span class="ft2">5.2 Scale of Training</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:126"><nobr><span class="ft7">Some machine learning systems are trained by the end user,<br>while others are trained using data from many users or or-<br>ganizations. The choice between these two models is some-<br>times cast as a tradeoff between the amount of training data<br>and the secrecy of the resulting classifier [3]. This issue also<br>applies to an IDS; if an IDS is trained each time it is de-<br>ployed then it will have comparatively little data regarding<br>normal network traffic. It will also have no chance to learn<br>about novel intrusions before seeing them in the wild.</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:126"><nobr><span class="ft7">Conversely, an IDS that uses a global set of rules would<br>be able to adapt to novel intrusion attempts more quickly.<br>Unfortunately, any adversary with access to a public IDS<br>classification function can test to ensure that its intrusion<br>points will be accepted by deployments of the same classifi-<br>cation function.</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:126"><nobr><span class="ft7">These issues are instances of a more general problem. In<br>some cases, it seems reasonable to assume the adversary has<br>little access to information available to the learner. How-<br>ever, unless the adversary has no prior knowledge about the<br>learning problem at hand, we cannot assume all of the in-<br>formation provided in the training set is secret. Therefore,<br>it is unclear how much is gained by attempting to keep the<br>training set, and therefore the state of the classifier, secret.</span></nobr></DIV>
<DIV style="position:absolute;top:1108;left:126"><nobr><span class="ft7">Many systems already attempt to achieve a balance between<br>global and local retraining [3]. Systems that take this ap-<br>proach have the potential to outperform systems that per-<br>form training at a single level. However, the relationships</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:521"><nobr><span class="ft7">between multilevel training, the adversary's domain knowl-<br>edge, and secrecy are not yet well understood.</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:525"><nobr><span class="ft14">5.2.1 Adversary Observations</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:521"><nobr><span class="ft7">Even without prior knowledge regarding a particular sys-<br>tem, an adversary still may deduce the state of the learn-<br>ing algorithm. For example, if the learning system provides<br>feedback to the adversary (e.g., "Request denied"), then a<br>probing attack could be used to map the space of acceptable<br>inputs.</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:521"><nobr><span class="ft7">If the adversary has no information regarding the type of<br>decision boundary used by the learner, this process could<br>require a number of probes proportional to the size of the<br>space. On the other hand, if the adversary knows which<br>learning algorithm is being used, a few well-chosen probes<br>could give the adversary sufficient knowledge of the learner's<br>state. As a standard security practice, we assume the learn-<br>ing algorithm itself to be common knowledge.</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:521"><nobr><span class="ft7">Instead of expecting the learning algorithm to be a secret,<br>some systems attempt to prevent the adversary from discov-<br>ering the set of features the learning algorithm uses. This<br>may be realistic in systems with a small number of deploy-<br>ments.</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:521"><nobr><span class="ft7">Ideally, we could produce an information theoretic bound<br>on the amount of information an adversary could gain by<br>observing the behavior of a particular algorithm on a par-<br>ticular point. Using these bounds, we could reason about<br>the algorithm's robustness against probing attacks. In this<br>setting, it may also be interesting to distinguish between in-<br>formation gained from normal points drawn from the data's<br>underlying distribution, intrusion points from a third party,<br>and (normal or intrusion) attack points of the adversary's<br>choosing.</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:521"><nobr><span class="ft7">An adversary with sufficient information regarding training<br>data, classifications of data points, or the internal state of a<br>learner would be able to deduce the learner's decision bound-<br>ary. This knowledge could simplify other types of attacks.</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:521"><nobr><span class="ft7">For instance, the adversary could avoid detection by choos-<br>ing intrusion points that will be misclassified by the learner,<br>or launch an availability attack by manipulating normal<br>points in a way that leads to misclassification. In either<br>case, by increasing the number of points that are in the re-<br>gion that the defender incorrectly classifies, the adversary<br>could increase the error rate.</span></nobr></DIV>
<DIV style="position:absolute;top:967;left:521"><nobr><span class="ft7">Some algorithms classify points by translating them into an<br>abstract space and performing the actual classification in<br>that space. The mapping between raw data and the abstract<br>space is often difficult to reason about. Therefore, it may be<br>computationally difficult for an adversary to use knowledge<br>of a classifier's decision boundary to generate "interesting"<br>attack points that will be misclassified.</span></nobr></DIV>
<DIV style="position:absolute;top:1092;left:521"><nobr><span class="ft7">One can imagine classes of decision boundaries that are<br>meaningful, yet provably provide an adversary with no in-<br>formation regarding unclassified points. Even with complete<br>knowledge of the state of a learner that uses such a decision<br>boundary, it would be computationally intractable to find</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft15{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft16{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51006.png" alt="background image">
<DIV style="position:absolute;top:1192;left:-397"><nobr><span class="ft0">0</span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:126"><nobr><span class="ft7">one of a few "interesting" points in a sufficiently large search<br>space.</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:126"><nobr><span class="ft7">In some cases, the decision boundary itself may contain sen-<br>sitive information. For example, knowledge of the bound-<br>ary may allow an adversary to infer confidential information<br>about the training set. Alternatively, the way the decision<br>boundary was constructed might be a secret.</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:131"><nobr><span class="ft14">5.2.2 Security Properties</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:126"><nobr><span class="ft7">The performance of different algorithms will likely degrade<br>differently as the adversary controls larger fractions of the<br>training set. A measurement of an algorithm's ability to<br>deal with malicious training errors could help system de-<br>signers reason about and decide between different learners.<br>A simple approach would be to characterize an algorithm's<br>performance when subjected to a particular type of attack,<br>but this would lead to an arms race as adversaries devise<br>classes of attacks not well represented during the evaluation<br>of the algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:126"><nobr><span class="ft7">Depending on the exact nature of the classification prob-<br>lem, it may be possible to make statements regarding the<br>strength of predictions. For example, after making a classi-<br>fication a learning algorithm could examine the training set<br>for that classification. It could measure the effect of small<br>changes to that training set; if small changes generate large<br>effects, the training set is more vulnerable to manipulation.</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:126"><nobr><span class="ft2">6. THEORETICAL RESULTS</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:126"><nobr><span class="ft7">In this section we present an analytic model that examines<br>a causative attack to manipulate a naive learning algorithm.<br>The model's simplicity yields an optimal policy for the ad-<br>versary and a bound on the effort required to achieve the<br>adversary's objective. We interpret the resulting bound and<br>discuss possible extensions to this model to capture more<br>realistic settings.</span></nobr></DIV>
<DIV style="position:absolute;top:791;left:126"><nobr><span class="ft7">We discuss an outlier detection technique. Outlier detection<br>is the task of identifying anomalous data and is a widely used<br>paradigm in fault detection [40], intrusion detection [23],<br>and virus detection [33, 34]. We find the smallest region<br>that contains some fixed percentage of the observed data,<br>which is called the support of the data's distribution. The<br>outlier detector classifies points inside the support as normal<br>and those outside as anomalous. Outlier detection is often<br>used in scenarios where anomalous data is scarce or novel<br>anomalies could arise.</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:126"><nobr><span class="ft2">6.1 A Simple Model</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:126"><nobr><span class="ft15">One simple approach to outlier detection is to estimate the<br>support of the normal data by a multi-dimensional hyper-<br>sphere. As depicted in Figure 1(a) every point in the hyper-<br>sphere is classified as normal and those outside the hyper-<br>sphere are classified as outliers. The training algorithm fixes<br>the radius of the hypersphere and centers it at the mean of<br>the training data. The hypersphere can be fit into the learn-<br>ing framework presented above by a squared loss function,<br></span></nobr></DIV>
<DIV style="position:absolute;top:1124;left:132"><nobr><span class="ft8">sphere</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:166"><nobr><span class="ft4">( ¯</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:172"><nobr><span class="ft4">X, x</span></nobr></DIV>
<DIV style="position:absolute;top:1124;left:198"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:203"><nobr><span class="ft4">) =</span></nobr></DIV>
<DIV style="position:absolute;top:1108;left:229"><nobr><span class="ft4">`x</span></nobr></DIV>
<DIV style="position:absolute;top:1124;left:243"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:251"><nobr><span class="ft4">- ¯X</span></nobr></DIV>
<DIV style="position:absolute;top:1108;left:277"><nobr><span class="ft4">´</span></nobr></DIV>
<DIV style="position:absolute;top:1115;left:283"><nobr><span class="ft8">2</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:290"><nobr><span class="ft4">, where ¯</span></nobr></DIV>
<DIV style="position:absolute;top:1119;left:339"><nobr><span class="ft4">X is the centroid of the</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:126"><nobr><span class="ft4">data</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:158"><nobr><span class="ft4">{x</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:173"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:177"><nobr><span class="ft4">}. It is easy to show that the parameter that mini-</span></nobr></DIV>
<DIV style="position:absolute;top:1151;left:126"><nobr><span class="ft4">mizes Equation (1) is the mean of the training data.</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:521"><nobr><span class="ft7">To make the hypersphere adaptive, the hypersphere is re-<br>trained on new data allowing for a repeated attack. To<br>prevent arbitrary data from being introduced, we employ a<br>conservative retraining strategy that only admits new points<br>to the training set if they are classified as normal; we say<br>the classifier bootstraps itself. This learning framework is<br>not meant to represent the state of the art in learning tech-<br>niques; instead, it is a illustrative technique that allows for<br>an exact analysis.</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:521"><nobr><span class="ft2">6.2 Attack Strategy</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:521"><nobr><span class="ft7">The attack we analyze involves an adversary determined to<br>alter our detector to include a specific point G by construct-<br>ing data to shift the hypersphere toward the target as the<br>hypersphere is retrained. We assume the goal G is initially<br>correctly classified as an anomaly by our algorithm. For in-<br>stance, in the IDS domain, the adversary has an intrusion<br>packet that our detector currently classifies as anomalous.<br>The adversary wants to change the state of our detector to<br>misclassify the packet as normal. This scenario is a causative<br>targeted integrity attack. Before the attack, the hypersphere<br>is centered at ¯</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:606"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:618"><nobr><span class="ft8">0</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:628"><nobr><span class="ft4">and it has a fixed radius R. The attack is</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:521"><nobr><span class="ft7">iterated over the course of T &gt; 1 training iterations. At the<br>i-th iteration the mean of the hypersphere is denoted by ¯</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:860"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:871"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:876"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:521"><nobr><span class="ft7">We give the adversary complete control: the adversary knows<br>the algorithm, its feature set, and its current state, and all<br>points are attack points. At each iteration, the bootstrap-<br>ping policy retrains on all points that were classified as nor-<br>mal in a previous iteration. Under this policy, the adver-<br>sary's optimal strategy is straightforward -- as depicted in<br>Figure 1(b) the adversary places points at the location where<br>the line between the mean and G intersects with the bound-<br>ary. This reduces the attack to a single dimension along<br>this line. Suppose that in the i-th iteration, the adversary<br>strategically places </span></nobr></DIV>
<DIV style="position:absolute;top:733;left:652"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:663"><nobr><span class="ft4">points at the i-th optimal location</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:521"><nobr><span class="ft7">achieving optimal displacement of the mean toward the ad-<br>versary's goal, G. The effort of the adversary is measured<br>by M defined as</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:624"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:639"><nobr><span class="ft16">T<br>i=1</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:660"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:780;left:669"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:673"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:521"><nobr><span class="ft7">Placing all attack points in the first iteration is not optimal.<br>It achieves a finite shift while optimal strategies achieve un-<br>bounded gains. As we discuss below, the attack strategy<br>must be balanced. The more points placed during an iter-<br>ation, the further the hypersphere is displaced on that iter-<br>ation. However, the points placed early in the attack effec-<br>tively weigh down the hypersphere making it more difficult<br>to move. The adversary must balance current gain against<br>future gain. Another tradeoff is the number of rounds of<br>iteration versus the total effort.</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:521"><nobr><span class="ft2">6.3 Optimal Attack Displacement</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:521"><nobr><span class="ft4">We calculate the displacement caused by a sequence</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:837"><nobr><span class="ft4">{</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:853"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:858"><nobr><span class="ft4">} of</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:521"><nobr><span class="ft7">attack points. For T iterations and M total attack points,<br>the function D</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:613"><nobr><span class="ft8">R,T</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:633"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:639"><nobr><span class="ft4">{</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:654"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:659"><nobr><span class="ft4">}) denotes the relative displacement</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:521"><nobr><span class="ft7">caused by the attack sequence. The relative displacement is<br>the total displacement over the radius of the hypersphere,</span></nobr></DIV>
<DIV style="position:absolute;top:1079;left:526"><nobr><span class="ft8">¯</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:523"><nobr><span class="ft8">X</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:532"><nobr><span class="ft9">T</span></nobr></DIV>
<DIV style="position:absolute;top:1081;left:540"><nobr><span class="ft8">- ¯</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:549"><nobr><span class="ft8">X</span></nobr></DIV>
<DIV style="position:absolute;top:1083;left:558"><nobr><span class="ft9">0</span></nobr></DIV>
<DIV style="position:absolute;top:1093;left:539"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:565"><nobr><span class="ft4">. Let M</span></nobr></DIV>
<DIV style="position:absolute;top:1089;left:623"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:635"><nobr><span class="ft4">be defined as</span></nobr></DIV>
<DIV style="position:absolute;top:1074;left:725"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:740"><nobr><span class="ft16">i<br>j=1</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:762"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:1089;left:771"><nobr><span class="ft8">j</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:776"><nobr><span class="ft4">, the cumulative</span></nobr></DIV>
<DIV style="position:absolute;top:1100;left:521"><nobr><span class="ft4">mass. Using these terms, the relative distance is</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:611"><nobr><span class="ft4">D</span></nobr></DIV>
<DIV style="position:absolute;top:1144;left:623"><nobr><span class="ft8">R,T</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:643"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:1139;left:649"><nobr><span class="ft4">{M</span></nobr></DIV>
<DIV style="position:absolute;top:1144;left:669"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1139;left:674"><nobr><span class="ft4">}) = T -</span></nobr></DIV>
<DIV style="position:absolute;top:1126;left:737"><nobr><span class="ft8">T</span></nobr></DIV>
<DIV style="position:absolute;top:1127;left:731"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:1159;left:732"><nobr><span class="ft8">i=2</span></nobr></DIV>
<DIV style="position:absolute;top:1131;left:755"><nobr><span class="ft4">M</span></nobr></DIV>
<DIV style="position:absolute;top:1136;left:769"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1135;left:773"><nobr><span class="ft8">-1</span></nobr></DIV>
<DIV style="position:absolute;top:1149;left:762"><nobr><span class="ft4">M</span></nobr></DIV>
<DIV style="position:absolute;top:1154;left:776"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:862"><nobr><span class="ft4">(4)</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
	.ft17{font-size:11px;line-height:19px;font-family:Times;color:#000000;}
	.ft18{font-size:15px;line-height:25px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="1003" height="1373" src="51007.png" alt="background image">
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:1388"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:451;left:211"><nobr><span class="ft4">(a) Hypersphere Outlier Detection</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:551"><nobr><span class="ft4">(b) Attack on a Hypersphere Outlier Detector</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:126"><nobr><span class="ft7">Figure 1: Depictions of the concept of hypersphere outlier detection and the vulnerability of naive approaches.<br>In Figure 1(a) a bounding hypersphere centered at ¯</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft4">X of fixed radius R is used to estimate the empirical support</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:126"><nobr><span class="ft7">of a distribution excluding outliers. Samples from the "normal" distribution being modeled are indicated by<br> with three outliers indicated by</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:370"><nobr><span class="ft4">. Meanwhile, Figure 1(b) depicts how an adversary with knowledge of</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:126"><nobr><span class="ft7">the state of the outlier detector could shift the outlier detector toward a first goal G. It could take several<br>iterations of attacks to shift the hypersphere further to include the second goal G</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:696"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:567;left:700"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:126"><nobr><span class="ft4">where we constrain M</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:260"><nobr><span class="ft8">1</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:270"><nobr><span class="ft4">= 1 and M</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:337"><nobr><span class="ft8">T</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:349"><nobr><span class="ft4">= M [25].</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:126"><nobr><span class="ft7">By finding an upper bound to Equation (4), we can bound<br>the minimal effort M</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:260"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:673;left:272"><nobr><span class="ft4">of the adversary. For a particular</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:126"><nobr><span class="ft4">M , we desire an optimal sequence</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:341"><nobr><span class="ft4">{M</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:362"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:369"><nobr><span class="ft4">} that achieves the</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:126"><nobr><span class="ft4">maximum relative displacement, D</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:335"><nobr><span class="ft8">R,T</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:356"><nobr><span class="ft4">(M ). If the adversary</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:126"><nobr><span class="ft4">has no time constraint, the solution is M</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:373"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:384"><nobr><span class="ft4">= i, which corre-</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:126"><nobr><span class="ft17">sponds to placing a single point at each iteration. However,<br>if the adversary expedites the attack to T &lt; M iterations,<br>the optimal strategy is given by M</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:343"><nobr><span class="ft8">i</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:354"><nobr><span class="ft4">= M</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:388"><nobr><span class="ft9">i</span></nobr></DIV>
<DIV style="position:absolute;top:764;left:392"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:386"><nobr><span class="ft9">T</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:394"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:410"><nobr><span class="ft4">. This value</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:126"><nobr><span class="ft4">is not always an integer, so we have:</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:193"><nobr><span class="ft4">D</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:204"><nobr><span class="ft8">R,T</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:225"><nobr><span class="ft4">(M )</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:254"><nobr><span class="ft4"> T - (T - 1) · M</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:371"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:367"><nobr><span class="ft9">T</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:374"><nobr><span class="ft9">-1</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:394"><nobr><span class="ft4"> T</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:467"><nobr><span class="ft4">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:126"><nobr><span class="ft2">6.4 Bounding the Adversary's Effort</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:126"><nobr><span class="ft7">From these results we find a bound on the adversary's effort<br>M . Since M</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:207"><nobr><span class="ft4"> 1 and T &gt; 1, Equation (5) is monotonically</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:126"><nobr><span class="ft7">increasing in M . If the desired relative displacement to the<br>goal is D</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:183"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:192"><nobr><span class="ft4">, the bound in Equation (5) can be inverted to</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:126"><nobr><span class="ft4">bound the minimal effort M</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:300"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:934;left:312"><nobr><span class="ft4">required to achieve the goal.</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:126"><nobr><span class="ft4">Since D</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:174"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:949;left:187"><nobr><span class="ft4">&lt; T , this bound is given by:</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:239"><nobr><span class="ft4">M</span></nobr></DIV>
<DIV style="position:absolute;top:982;left:254"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:985;left:264"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:966;left:279"><nobr><span class="ft4">,, T-1</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:291"><nobr><span class="ft4">T</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:304"><nobr><span class="ft4">- D</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:329"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:340"><nobr><span class="ft4">«</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:350"><nobr><span class="ft8">T</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:358"><nobr><span class="ft8">-1</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:467"><nobr><span class="ft4">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:126"><nobr><span class="ft7">The bound in Equation (6) gives us a worst-case bound on<br>the adversary's capability when the adversary has complete<br>control of the learner's training. For large relative displace-<br>ments D</span></nobr></DIV>
<DIV style="position:absolute;top:1087;left:180"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:1083;left:195"><nobr><span class="ft4">&gt; 1, the bound decreases exponentially as the</span></nobr></DIV>
<DIV style="position:absolute;top:1098;left:126"><nobr><span class="ft7">number of iterations is increased. The bound has a limiting<br>value of M</span></nobr></DIV>
<DIV style="position:absolute;top:1111;left:191"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:1114;left:201"><nobr><span class="ft4"> e</span></nobr></DIV>
<DIV style="position:absolute;top:1112;left:222"><nobr><span class="ft8">D</span></nobr></DIV>
<DIV style="position:absolute;top:1115;left:231"><nobr><span class="ft9">R</span></nobr></DIV>
<DIV style="position:absolute;top:1111;left:239"><nobr><span class="ft8">-1</span></nobr></DIV>
<DIV style="position:absolute;top:1114;left:254"><nobr><span class="ft4">. The adversary must tradeoff between</span></nobr></DIV>
<DIV style="position:absolute;top:1130;left:126"><nobr><span class="ft4">using a large number of attack points or extending the at-</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:521"><nobr><span class="ft7">tack over many iterations. A tightly-fit hypersphere with<br>small radius will be more robust since our displacement is<br>relative to its radius.</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:521"><nobr><span class="ft7">An apparent deficiency of this analysis is the weak bound of<br>M</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:536"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:547"><nobr><span class="ft4">  where 0 &lt;   1 that occurs when D</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:804"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:818"><nobr><span class="ft4"> 1. This</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:521"><nobr><span class="ft7">an important range since the adversary's goal may be near<br>the boundary. The deficiency comes directly from our as-<br>sumption of complete adversarial control. The lack of initial<br>non-adversarial data allows our adversary to ensure a first<br>step of one radius regardless of M . Therefore, the adversary<br>can reach the objective of D</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:699"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:714"><nobr><span class="ft4"> 1 with any M  1 in a</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:521"><nobr><span class="ft4">single iteration.</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:521"><nobr><span class="ft7">A more complex model could allow for initial data. By con-<br>sidering an initial N training points that support the hyper-<br>sphere before the attack, we can obtain a stronger bound:</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:642"><nobr><span class="ft4">M</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:657"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:907;left:667"><nobr><span class="ft4"> N</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:697"><nobr><span class="ft4">he</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:710"><nobr><span class="ft8">D</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:719"><nobr><span class="ft9">R</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:731"><nobr><span class="ft4">- 1</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:752"><nobr><span class="ft4">i</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:862"><nobr><span class="ft4">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:521"><nobr><span class="ft4">This stronger bound ensures that even for small D</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:841"><nobr><span class="ft8">R</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:850"><nobr><span class="ft4">, the</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:521"><nobr><span class="ft7">adversary's effort is a multiple of N that increases exponen-<br>tially in the desired displacement [25].</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:521"><nobr><span class="ft7">We could extend the model by adding non-adversarial data<br>at every training iteration, for this corresponds to scenarios<br>where the adversary only controls part of the data.</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:521"><nobr><span class="ft18">7. CONCLUSIONS<br>7.1 Related Work</span></nobr></DIV>
<DIV style="position:absolute;top:1114;left:521"><nobr><span class="ft7">The earliest theoretical work we know of that approaches<br>learning in the presence of an adversary was done by Kearns</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="1003" height="1373" src="51008.png" alt="background image">
<DIV style="position:absolute;top:1192;left:-397"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:123"><nobr><span class="ft7">and Li [15]. They worked in the context of Valiant's Prob-<br>ably Approximately Correct (PAC) learning framework [35,<br>36], extending it to prove bounds for maliciously chosen er-<br>rors in the training data. Specifically, they proved that if<br>the learner is to perform correctly, in general the fraction<br>of training points controlled by the adversary must be less<br>than /(1 + ), where  is the desired bound on classification<br>errors by the learner [4, 6, 30].</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:123"><nobr><span class="ft7">Results from game theory may be relevant to adversarial<br>learning systems.</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:243"><nobr><span class="ft4">In particular, deception games involve</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:123"><nobr><span class="ft7">players that have partial information and influence the infor-<br>mation seen by other players. Some of these games involve<br>continuous variables generated by various probability distri-<br>butions [5, 9, 17, 29, 32], while others apply to scenarios<br>with discrete states [14]. This work and adversarial learning<br>both ask many of the same questions, and they both address<br>the same underlying issues. Integration of game theoretic<br>concepts is a promising direction for work in this area.</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:123"><nobr><span class="ft7">Dalvi et al. examine the learn-adapt-relearn cycle from a<br>game-theoretic point of view [8]. In their model, the learner<br>has a cost for measuring each feature of the data and the<br>adversary has a cost for changing each feature in attack<br>points. If the adversary and learner have complete infor-<br>mation about each other and we accept some other assump-<br>tions, they find an optimal strategy for the learner to defend<br>against the adversary's adaptations.</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:123"><nobr><span class="ft7">Research has also begun to examine the vulnerability of<br>learners to reverse engineering. Lowd and Meek introduce a<br>novel learning problem for adversarial classifier reverse engi-<br>neering in which an adversary conducts an attack that min-<br>imizes a cost function [21]. Under their framework, Lowd<br>and Meek construct algorithms for reverse engineering lin-<br>ear classifiers. Moreover, they build an attack to reverse<br>engineer spam filters [22].</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:123"><nobr><span class="ft7">Although they are not machine learning systems, publicly<br>verifiable digital watermarks also must deal with sensitiv-<br>ity (probing) attacks. An information theoretic analysis of<br>the sensitivity attack quantifies the amount of information<br>revealed per probe.</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:258"><nobr><span class="ft4">Randomization of thresholds within</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:123"><nobr><span class="ft7">the watermark verification algorithm increase the number<br>of probes necessary to remove a digital watermark [19].</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:123"><nobr><span class="ft7">An interesting junction of learning and game theory has<br>dealt with combining advice from a set of experts to predict<br>a sequence with the goal of doing at least as well as the best<br>expert in all possible sequences [7, 13, 37]. In this domain,<br>adaptive weighting schemes are used to combine the ex-<br>perts, each accessed by how well it performs compared to the<br>best expert for an adversarially chosen sequence. Amongst<br>these schemes are the Aggregating Algorithm [37] and the<br>Weighted Majority Algorithm [20].</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:123"><nobr><span class="ft7">There has also been work on attacking statistical spam fil-<br>ters. Wittel and Wu [39] discuss the possibility of crafting<br>attacks designed to take advantage of the statistical nature<br>of such spam filters, and they implement a simple attack.<br>John Graham-Cumming describes implementing an attack<br>he calls "Bayes vs. Bayes," in which the adversary trains<br>a second statistical spam filter based on feedback from the</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:518"><nobr><span class="ft7">filter under attack and then uses the second filter to find<br>words that make spam messages undetectable by the origi-<br>nal filter [10].</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:518"><nobr><span class="ft7">Methods exist to perform exact learning of a concept using<br>answers to a series of queries. These queries return a coun-<br>terexample when a "no" response is generated. In many<br>scenarios, it has been shown that learning is possible even<br>in the worst case [2].</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:518"><nobr><span class="ft7">Control theory has been proposed as an alternative to game<br>theory and search oriented expert-systems for military com-<br>mand and control systems [12]. The motivation behind this<br>proposal is the difficulty associated with modeling (or even<br>predicting) the goals of a military adversary.</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:518"><nobr><span class="ft2">7.2 Research Directions</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:518"><nobr><span class="ft4">Can machine learning be secure?</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:742"><nobr><span class="ft4">Does adding machine</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:518"><nobr><span class="ft7">learning to a system introduce vulnerability? This paper<br>proposes a framework for understanding these questions.<br>We present a model for describing attacks against learn-<br>ing algorithms, and we analyze a simple attack in detail.<br>We discuss potential defenses against attacks and speculate<br>about their effectiveness.</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:518"><nobr><span class="ft7">Here we lay out the directions for research that we see as<br>most promising. To evaluate and ensure the security of ma-<br>chine learning, these are among the most important areas<br>that must be addressed:</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:518"><nobr><span class="ft4">Information</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:552"><nobr><span class="ft7">How crucial is it to keep information secret from an<br>adversary? If an adversary has full knowledge of the<br>system, are all the exploratory attacks trivial? If the<br>adversary has no knowledge about the system, which<br>attacks are still possible?</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:518"><nobr><span class="ft4">Arms race</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:552"><nobr><span class="ft7">Can we avoid arms races in online learning systems?<br>Arms races have occurred in spam filters. Can game<br>theory suggest a strategy for secure re-training?</span></nobr></DIV>
<DIV style="position:absolute;top:862;left:518"><nobr><span class="ft4">Quantitative measurement</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:552"><nobr><span class="ft7">Can we measure the effects of attacks? Such infor-<br>mation would allow comparison of the security per-<br>formance of learning algorithms. We could calculate<br>risk based on probability and damage assessments of<br>attacks.</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:518"><nobr><span class="ft4">Security proofs</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:552"><nobr><span class="ft7">Can we bound the amount of information leaked by the<br>learner? If so, we can bound the accuracy of the ad-<br>versary's approximation of the learner's current state.</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:518"><nobr><span class="ft4">Detecting adversaries</span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:552"><nobr><span class="ft7">Attacks introduce potentially detectable side effects<br>such as drift, unusual patterns in the data observed by<br>the learner, etc. These attacks are more pronounced<br>in online learning. When do these side effects reveal<br>the adversary's attack?</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="1003" height="1373" src="51009.png" alt="background image">
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:1388"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:126"><nobr><span class="ft2">8. ACKNOWLEDGMENTS</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:126"><nobr><span class="ft7">Thanks to Michael I. Jordan, Peter Bartlett and David Mol-<br>nar for their insightful discussions and comments regarding<br>this work. We gratefully acknowledge support from the Na-<br>tional Science Foundation and the Homeland Security Ad-<br>vanced Research Projects Agency. The views expressed here<br>are solely those of the authors and do not necessarily reflect<br>the views of the funding agencies or any agency of the U.S.<br>government.</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:126"><nobr><span class="ft2">9. REFERENCES</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:133"><nobr><span class="ft4">[1] I. Androutsopoulos, J. Koutsias, K. V. Chandrinos,</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:155"><nobr><span class="ft7">G. Paliouras, and C. D. Spyropolous. An evaluation of<br>naive Bayesian anti-spam filtering. Proceedings of the<br>Workshop on Machine Learning in the New<br>Information Age, pages 9­17, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:133"><nobr><span class="ft4">[2] D. Angluin. Queries and concept learning. Machine</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:155"><nobr><span class="ft4">Learning, 2(4):319­342, Apr. 1988.</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:133"><nobr><span class="ft4">[3] Apache, http://spamassassin.apache.org/.</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:155"><nobr><span class="ft4">SpamAssassin.</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:133"><nobr><span class="ft4">[4] P. Auer. Learning nested differences in the presence of</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:155"><nobr><span class="ft7">malicious noise. Theoretical Computer Science,<br>185(1):159­175, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:133"><nobr><span class="ft4">[5] V. J. Baston and F. Bostock. Deception games.</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:155"><nobr><span class="ft7">International Journal of Game Theory, 17(2):129­134,<br>1988.</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:133"><nobr><span class="ft4">[6] N. H. Bshouty, N. Eiron, and E. Kushilevitz. PAC</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:155"><nobr><span class="ft7">learning with nasty noise. Theoretical Computer<br>Science, 288(2):255­275, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:133"><nobr><span class="ft4">[7] N. Cesa-Bianchi, Y. Freund, D. P. Helmbold,</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:155"><nobr><span class="ft7">D. Haussler, R. E. Schapire, and M. K. Warmuth.<br>How to use expert advice. Journal of the ACM,<br>44(3):427­485, May 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:133"><nobr><span class="ft4">[8] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:155"><nobr><span class="ft7">D. Verma. Adversarial classification. In Proceedings of<br>the Tenth ACM SIGKDD International Conference on<br>Knowledge Discovery and Data Mining, pages 99­108,<br>Seattle, WA, 2004. ACM Press.</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:133"><nobr><span class="ft4">[9] B. Fristedt. The deceptive number changing game in</span></nobr></DIV>
<DIV style="position:absolute;top:899;left:155"><nobr><span class="ft7">the absence of symmetry. International Journal of<br>Game Theory, 26:183­191, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:943;left:126"><nobr><span class="ft4">[10] J. Graham-Cumming. How to beat an adaptive spam</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:155"><nobr><span class="ft7">filter. Presentation at the MIT Spam Conference, Jan.<br>2004.</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:126"><nobr><span class="ft4">[11] T. Hastie, R. Tibshirani, and J. Friedman. The</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:155"><nobr><span class="ft7">Elements of Statistical Learning: Data Mining,<br>Inference and Prediction. Springer, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:126"><nobr><span class="ft4">[12] S. A. Heise and H. S. Morse. The DARPA JFACC</span></nobr></DIV>
<DIV style="position:absolute;top:1079;left:155"><nobr><span class="ft7">program: Modeling and control of military operations.<br>In Proceedings of the 39th IEEE Conference on<br>Decision and Control, pages 2551­2555. IEEE, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:126"><nobr><span class="ft4">[13] M. Herbster and M. K. Warmuth. Tracking the best</span></nobr></DIV>
<DIV style="position:absolute;top:1155;left:155"><nobr><span class="ft4">expert. Machine Learning, 32(2):151­178, Aug. 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:521"><nobr><span class="ft4">[14] J. P. Hespanha, Y. S. Ate¸skan, and H. H. Kizilocak.</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:549"><nobr><span class="ft7">Deception in non-cooperative games with partial<br>information. In Proceedings of the 2nd<br>DARPA-JFACC Symposium on Advances in<br>Enterprise Control, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:521"><nobr><span class="ft4">[15] M. Kearns and M. Li. Learning in the presence of</span></nobr></DIV>
<DIV style="position:absolute;top:279;left:549"><nobr><span class="ft7">malicious errors. SIAM Journal on Computing,<br>22:807­837, 1993.</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:521"><nobr><span class="ft4">[16] A. Lazarevic, L. Ert¨</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:671"><nobr><span class="ft4">oz, V. Kumar, A. Ozgur, and</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:549"><nobr><span class="ft7">J. Srivastava. A comparative study of anomaly<br>detection schemes in network intrusion detection. In<br>D. Barbar´</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:610"><nobr><span class="ft4">a and C. Kamath, editors, Proceedings of</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:549"><nobr><span class="ft7">the Third SIAM International Conference on Data<br>Mining, May 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:521"><nobr><span class="ft4">[17] K.-T. Lee. On a deception game with three boxes.</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:549"><nobr><span class="ft7">International Journal of Game Theory, 22:89­95,<br>1993.</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:521"><nobr><span class="ft4">[18] Y. Liao and V. R. Vemuri. Using text categorization</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:549"><nobr><span class="ft7">techniques for intrusion detection. In Proceedings of<br>the 11th USENIX Security Symposium, pages 51­59,<br>Aug. 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:556;left:521"><nobr><span class="ft4">[19] J.-P. M. Linnartz and M. van Dijk. Analysis of the</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:549"><nobr><span class="ft7">sensitivity attack against electronic watermarks in<br>images. In D. Aucsmith, editor, Information Hiding<br>'98, pages 258­272. Springer-Verlag, 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:521"><nobr><span class="ft4">[20] N. Littlestone and M. K. Warmuth. The weighted</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:549"><nobr><span class="ft7">majority algorithm. Information and Computation,<br>108(2):212­261, 1994.</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:521"><nobr><span class="ft4">[21] D. Lowd and C. Meek. Adversarial learning. In</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:549"><nobr><span class="ft7">Proceedings of the Eleventh ACM SIGKDD<br>International Conference on Knowledge Discovery and<br>Data Mining, pages 641­647, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:521"><nobr><span class="ft4">[22] D. Lowd and C. Meek. Good word attacks on</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:549"><nobr><span class="ft7">statistical spam filters. In Proceedings of the Second<br>Conference on Email and Anti-Spam (CEAS), 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:521"><nobr><span class="ft4">[23] M. V. Mahoney and P. K. Chan. Learning</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:549"><nobr><span class="ft7">nonstationary models of normal network traffic for<br>detecting novel attacks. In Proceedings of the Eighth<br>ACM SIGKDD International Conference on<br>Knowledge Discovery and Data Mining, pages<br>376­385, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:521"><nobr><span class="ft4">[24] S. Mukkamala, G. Janoski, and A. Sung. Intrusion</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:549"><nobr><span class="ft7">detection using neural networks and support vector<br>machines. In Proceedings of the International Joint<br>Conference on Neural Networks (IJCNN'02), pages<br>1702­1707, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:521"><nobr><span class="ft4">[25] B. Nelson. Designing, Implementing, and Analyzing a</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:549"><nobr><span class="ft7">System for Virus Detection. Master's thesis,<br>University of California at Berkeley, Dec. 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:521"><nobr><span class="ft4">[26] V. Paxson. Bro: A system for detecting network</span></nobr></DIV>
<DIV style="position:absolute;top:1082;left:549"><nobr><span class="ft7">intruders in real-time. Computer Networks,<br>31(23):2435­2463, Dec. 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:1124;left:521"><nobr><span class="ft4">[27] N. Provos. A virtual honeypot framework. In</span></nobr></DIV>
<DIV style="position:absolute;top:1140;left:549"><nobr><span class="ft7">Proceedings of the 13th USENIX Security Symposium,<br>2004.</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:1003;height:1373;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="1003" height="1373" src="51010.png" alt="background image">
<DIV style="position:absolute;top:1192;left:-397"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:1192;left:495"><nobr><span class="ft0"></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:123"><nobr><span class="ft4">[28] R. Raina, A. Y. Ng, and D. Koller. Transfer learning</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:152"><nobr><span class="ft7">by constructing informative priors. In Neural<br>Information Processing Systems Workshop on<br>Inductive Transfer: 10 Years Later, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:123"><nobr><span class="ft4">[29] M. Sakaguchi. Effect of correlation in a simple</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:152"><nobr><span class="ft7">deception game. Mathematica Japonica,<br>35(3):527­536, 1990.</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:123"><nobr><span class="ft4">[30] R. A. Servedio. Smooth boosting and learning with</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:152"><nobr><span class="ft7">malicious noise. Journal of Machine Learning<br>Research (JMLR), 4:633­648, Sept. 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:123"><nobr><span class="ft4">[31] J. Shawe-Taylor and N. Cristianini. Kernel Methods</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:152"><nobr><span class="ft7">for Pattern Analysis. Cambridge University Press,<br>2004.</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:123"><nobr><span class="ft4">[32] J. Spencer. A deception game. American Math</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:152"><nobr><span class="ft4">Monthly, 80:416­417, 1973.</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:123"><nobr><span class="ft4">[33] S. J. Stolfo, S. Hershkop, K. Wang, O. Nimeskern, and</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:152"><nobr><span class="ft7">C. W. Hu. A behavior-based approach to secure email<br>systems. In Mathematical Methods, Models and<br>Architectures for Computer Networks Security, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:123"><nobr><span class="ft4">[34] S. J. Stolfo, W. J. Li, S. Hershkop, K. Wang, C. W.</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:152"><nobr><span class="ft7">Hu, and O. Nimeskern. Detecting viral propagations<br>using email behavior profiles. In ACM Transactions<br>on Internet Technology, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:123"><nobr><span class="ft4">[35] L. G. Valiant. A theory of the learnable.</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:152"><nobr><span class="ft7">Communications of the ACM, 27(11):1134­1142, Nov.<br>1984.</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:518"><nobr><span class="ft4">[36] L. G. Valiant. Learning disjunctions of conjunctions.</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:546"><nobr><span class="ft7">In Proceedings of the 9th International Joint<br>Conference on Artificial Intelligence, pages 560­566,<br>1985.</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:518"><nobr><span class="ft4">[37] V. Vovk. Aggregating strategies. In M. Fulk and</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:546"><nobr><span class="ft7">J. Case, editors, Proceedings of the 7th Annual<br>Workshop on Computational Learning Theory, pages<br>371­383, San Mateo, CA, 1990. Morgan-Kaufmann.</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:518"><nobr><span class="ft4">[38] L. Wehenkel. Machine learning approaches to power</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:546"><nobr><span class="ft7">system security assessment. IEEE Intelligent Systems<br>and Their Applications, 12(5):60­72, Sept.­Oct. 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:518"><nobr><span class="ft4">[39] G. L. Wittel and S. F. Wu. On attacking statistical</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:546"><nobr><span class="ft7">spam filters. In Proceedings of the First Conference on<br>Email and Anti-Spam (CEAS), 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:518"><nobr><span class="ft4">[40] W. Xu, P. Bodik, and D. Patterson. A flexible</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:546"><nobr><span class="ft7">architecture for statistical learning and data mining<br>from system log streams. In Temporal Data Mining:<br>Algorithms, Theory and Applications, Brighton, UK,<br>Nov. 2004. The Fourth IEEE International Conference<br>on Data Mining.</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:518"><nobr><span class="ft4">[41] D.-Y. Yeung and C. Chow. Parzen-window network</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:546"><nobr><span class="ft7">intrusion detectors. In Proceedings of the Sixteenth<br>International Conference on Pattern Recognition,<br>pages 385­388, Aug. 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:518"><nobr><span class="ft4">[42] K. Yu and V. Tresp. Learning to learn and</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:546"><nobr><span class="ft7">collaborative filtering. In Neural Information<br>Processing Systems Workshop on Inductive Transfer:<br>10 Years Later, 2005.</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
