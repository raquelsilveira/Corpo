<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - p7044-dumais-nomarkup.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2002-07-12T18:25:25+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:11px;font-family:Times;color:#000000;}
	.ft1{font-size:25px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:16px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft8{font-size:8px;font-family:Times;color:#000000;}
	.ft9{font-size:10px;font-family:Times;color:#000000;}
	.ft10{font-size:10px;font-family:Times;color:#000000;}
	.ft11{font-size:16px;font-family:Courier;color:#000000;}
	.ft12{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft13{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft15{font-size:10px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159001.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:114;left:139"><nobr><span class="ft1"><b>Web Question Answering: Is More Always Better?</b></span></nobr></DIV>
<DIV style="position:absolute;top:169;left:208"><nobr><span class="ft2">Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, Andrew Ng </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:396"><nobr><span class="ft3">Microsoft Research </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:397"><nobr><span class="ft3">One Microsoft Way </span></nobr></DIV>
<DIV style="position:absolute;top:223;left:368"><nobr><span class="ft3">Redmond, WA  98052  USA </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:264"><nobr><span class="ft2">{sdumais, mbanko, brill}@research.microsoft.com </span></nobr></DIV>
<DIV style="position:absolute;top:268;left:289"><nobr><span class="ft2">jimmylin@ai.mit.edu; ang@cs.berkeley.edu </span></nobr></DIV>
<DIV style="position:absolute;top:293;left:459"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:81"><nobr><span class="ft4"><b>ABSTRACT </b></span></nobr></DIV>
<DIV style="position:absolute;top:339;left:81"><nobr><span class="ft12">This paper describes a question answering system that is designed <br>to capitalize on the tremendous amount of data that is now <br>available online.   Most question answering systems use a wide <br>variety of linguistic resources.  We focus instead on the <br>redundancy available in large corpora as an important resource.   <br>We use this redundancy to simplify the query rewrites that we <br>need to use, and to support answer mining from returned snippets.  <br>Our system performs quite well given the simplicity of the <br>techniques being utilized.  Experimental results show that <br>question answering accuracy can be greatly improved by <br>analyzing more and more matching passages.  Simple passage <br>ranking and n-gram extraction techniques work well in our system <br>making it efficient to use with many backend retrieval engines. </span></nobr></DIV>
<DIV style="position:absolute;top:556;left:81"><nobr><span class="ft4"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:559;left:352"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:81"><nobr><span class="ft13">H.3.1. [<b>Content Analysis and Indexing</b>], H.3.3 [<b>Information <br>Search and Retrieval</b></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:202"><nobr><span class="ft0">].<i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:81"><nobr><span class="ft4"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:621;left:197"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:81"><nobr><span class="ft0">Algorithms, Experimentation. </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:81"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:672;left:94"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:673;left:105"><nobr><span class="ft4"><b>INTRODUCTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:693;left:81"><nobr><span class="ft12">Question answering has recently received attention from the <br>information retrieval, information extraction, machine learning, <br>and natural language processing communities [1][3][19][20] The <br>goal of a question answering system is to retrieve `answers' to <br>questions rather than full documents or even best-matching <br>passages as most information retrieval systems currently do.   The <br>TREC Question Answering Track which has motivated much of <br>the recent work in the field focuses on fact-based, short-answer <br>questions such as "<i>Who killed Abraham Lincoln?</i>" or "<i>How tall is <br>Mount Everest?</i>".   In this paper we focus on this kind of question <br>answering task, although the techniques we propose are more <br>broadly applicable. </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft12">The design of our question answering system is motivated by <br>recent observations in natural language processing that, for many <br>applications, significant improvements in accuracy can be attained </span></nobr></DIV>
<DIV style="position:absolute;top:318;left:477"><nobr><span class="ft12">simply by increasing the amount of data used for learning.  <br>Following the same guiding principle we take advantage of the <br>tremendous data resource that the Web provides as the backbone <br>of our question answering system.  Many groups working on <br>question answering have used a variety of linguistic resources ­ <br>part-of-speech tagging, syntactic parsing, semantic relations, <br>named entity extraction, dictionaries, WordNet, etc. (e.g., <br>[2][8][11][12][13][15][16]).We chose instead to focus on the <br>Web as gigantic data repository with tremendous redundancy that <br>can be exploited for question answering.   The Web, which is <br>home to billions of pages of electronic text, is orders of magnitude <br>larger than the TREC QA document collection, which consists of <br>fewer than 1 million documents.   This is a resource that can be <br>usefully exploited for question answering.    We view our <br>approach as complimentary to more linguistic approaches, but <br>have chosen to see how far we can get initially by focusing on <br>data per se as a key resource available to drive our system design. </span></nobr></DIV>
<DIV style="position:absolute;top:596;left:477"><nobr><span class="ft12">Automatic QA from a single, small information source is <br>extremely challenging, since there is likely to be only one answer <br>in the source to any user's question.  Given a source, such as the <br>TREC corpus, that contains only a relatively small number of <br>formulations of answers to a query, we may be faced with the <br>difficult task of mapping questions to answers by way of <br>uncovering complex lexical, syntactic, or semantic relationships <br>between question string and answer string.  The need for anaphor <br>resolution and synonymy, the presence of alternate syntactic <br>formulations, and indirect answers all make answer finding a <br>potentially challenging task.  However, the greater the answer <br>redundancy in the source data collection, the more likely it is that <br>we can find an answer that occurs in a simple relation to the <br>question.  Therefore, the less likely it is that we will need to resort <br>to solving the aforementioned difficulties facing natural language <br>processing systems. </span></nobr></DIV>
<DIV style="position:absolute;top:869;left:477"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:868;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:501"><nobr><span class="ft4"><b>EXPLOITING REDUNDANCY FOR QA </b></span></nobr></DIV>
<DIV style="position:absolute;top:889;left:477"><nobr><span class="ft12">We take advantage of the redundancy (multiple, differently <br>phrased, answer occurrences) available when considering massive <br>amounts of data in two key ways in our system. </span></nobr></DIV>
<DIV style="position:absolute;top:946;left:477"><nobr><span class="ft5"><b>Enables Simple Query Rewrites.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:946;left:685"><nobr><span class="ft0">The greater the number of </span></nobr></DIV>
<DIV style="position:absolute;top:961;left:477"><nobr><span class="ft14">information sources we can draw from, the easier the task of <br>rewriting the question becomes, since the answer is more likely to <br>be expressed in different manners.  For example, consider the <br>difficulty of gleaning an answer to the question "<i>Who killed <br>Abraham Lincoln?"</i> from a source which contains only the text <br><i>"John Wilkes Booth altered history with a bullet.  He will forever <br>be known as the man who ended Abraham Lincoln's life,"</i>   </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:87"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:970;left:87"><nobr><span class="ft15">Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are <br>not made or distributed for profit or commercial advantage and that <br>copies bear this notice and the full citation on the first page. To copy <br>otherwise, or republish, to post on servers or to redistribute to lists, <br>requires prior specific permission and/or a fee. <br><i>SIGIR'02</i>, August 11-15, 2002, Tampere, Finland. <br>Copyright 2002 ACM 1-58113-561-0/02/0008...$5.00. </span></nobr></DIV>
<DIV style="position:absolute;top:1084;left:87"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">291</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft17{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft18{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft19{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft20{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft21{font-size:12px;font-family:Courier;color:#000000;}
	.ft22{font-size:15px;line-height:20px;font-family:Helvetica;color:#000000;}
	.ft23{font-size:11px;line-height:16px;font-family:Helvetica;color:#000000;}
	.ft24{font-size:15px;line-height:21px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159002.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:96"><nobr><span class="ft0">     </span></nobr></DIV>
<DIV style="position:absolute;top:802;left:145"><nobr><span class="ft16">Question</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:362"><nobr><span class="ft16">Rewrite Query</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:609"><nobr><span class="ft16">&lt;Search Engine&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:597"><nobr><span class="ft16">Collect Summaries, </span></nobr></DIV>
<DIV style="position:absolute;top:907;left:619"><nobr><span class="ft16">Mine N-grams</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:613"><nobr><span class="ft16">Filter N-Grams</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:379"><nobr><span class="ft16">Tile N-Grams </span></nobr></DIV>
<DIV style="position:absolute;top:984;left:138"><nobr><span class="ft16">N-Best Answers</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:121"><nobr><span class="ft22"><b>Where is the Louvre<br>Museum located?</b></span></nobr></DIV>
<DIV style="position:absolute;top:826;left:332"><nobr><span class="ft23">"+the Louvre Museum +is located"<br>"+the Louvre Museum +is +in"<br>"+the Louvre Museum +is near"<br>"+the Louvre Museum +is"<br>Louvre AND Museum AND near</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:121"><nobr><span class="ft22"><b>in Paris France 59%<br>museums          12%<br>hostels              10%</b></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:322"><nobr><span class="ft19"><b>Figure 1.  </b>System Architecture</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:145"><nobr><span class="ft16">Question</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:362"><nobr><span class="ft16">Rewrite Query</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:609"><nobr><span class="ft16">&lt;Search Engine&gt;</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:597"><nobr><span class="ft16">Collect Summaries, </span></nobr></DIV>
<DIV style="position:absolute;top:907;left:619"><nobr><span class="ft16">Mine N-grams</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:613"><nobr><span class="ft16">Filter N-Grams</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:379"><nobr><span class="ft16">Tile N-Grams </span></nobr></DIV>
<DIV style="position:absolute;top:984;left:138"><nobr><span class="ft16">N-Best Answers</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:121"><nobr><span class="ft22"><b>Where is the Louvre<br>Museum located?</b></span></nobr></DIV>
<DIV style="position:absolute;top:826;left:332"><nobr><span class="ft23">"+the Louvre Museum +is located"<br>"+the Louvre Museum +is +in"<br>"+the Louvre Museum +is near"<br>"+the Louvre Museum +is"<br>Louvre AND Museum AND near</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:121"><nobr><span class="ft22"><b>in Paris France 59%<br>museums          12%<br>hostels              10%</b></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:322"><nobr><span class="ft19"><b>Figure 1.  </b>System Architecture</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:785"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">versus a source that also contains the transparent answer string, <br><i>"John Wilkes Booth killed Abraham Lincoln."</i><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:153;left:81"><nobr><span class="ft5"><b>Facilitates Answer Mining.</b></span></nobr></DIV>
<DIV style="position:absolute;top:152;left:245"><nobr><span class="ft0">  Even when no obvious answer </span></nobr></DIV>
<DIV style="position:absolute;top:168;left:81"><nobr><span class="ft12">strings can be found in the text, redundancy can improve the <br>efficacy of question answering.  For instance, consider the <br>question: "<i>How many times did Bjorn Borg win Wimbledon?</i>"  <br>Assume the system is unable to find any obvious answer strings, <br>but does find the following sentences containing "Bjorn Borg" <br>and "Wimbledon", as well as a number: </span></nobr></DIV>
<DIV style="position:absolute;top:273;left:81"><nobr><span class="ft0">(1)<b> Bjorn Borg</b> blah blah  <b>Wimbledon</b> blah blah <b>5</b> blah  </span></nobr></DIV>
<DIV style="position:absolute;top:298;left:81"><nobr><span class="ft0">(2) <b>Wimbledon</b> blah blah blah <b>Bjorn Borg</b> blah <b>37</b> blah. </span></nobr></DIV>
<DIV style="position:absolute;top:323;left:81"><nobr><span class="ft0">(3) blah <b>Bjorn Borg</b> blah blah <b>5</b> blah blah <b>Wimbledon</b>   </span></nobr></DIV>
<DIV style="position:absolute;top:348;left:81"><nobr><span class="ft0">(4) <b>5</b> blah blah <b>Wimbledon</b> blah blah <b>Bjorn Borg</b>. </span></nobr></DIV>
<DIV style="position:absolute;top:373;left:81"><nobr><span class="ft12">By virtue of the fact that the most frequent number in these <br>sentences is 5, we can posit that as the most likely answer.   </span></nobr></DIV>
<DIV style="position:absolute;top:425;left:81"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:424;left:94"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:425;left:105"><nobr><span class="ft4"><b>RELATED WORK </b></span></nobr></DIV>
<DIV style="position:absolute;top:444;left:81"><nobr><span class="ft12">Other researchers have recently looked to the web as a resource <br>for question answering.   The Mulder system described by Kwok <br>et al. [14] is similar to our approach in several respects.  For each <br>question, Mulder submits multiple queries to a web search engine <br>and analyzes the results.  Mulder does sophisticated parsing of the <br>query and the full-text of retrieved pages, which is far more <br>complex and compute-intensive than our analysis.  They also <br>require global idf term weights for answer extraction and <br>selection, which requires local storage of a database of term <br>weights.  They have done some interesting user studies of the <br>Mulder interface, but they have not evaluated it with TREC <br>queries nor have they looked at the importance of various system <br>components. </span></nobr></DIV>
<DIV style="position:absolute;top:659;left:81"><nobr><span class="ft12">Clarke et al. [9][10] investigated the importance of redundancy in <br>their question answering system.   In [9] they found that the best <br>weighting of passages for question answering involves using both <br>passage frequency (what they call redundancy) and a global idf <br>term weight.  They also found that analyzing more top-ranked <br>passages was helpful in some cases and not in others.   Their <br>system builds a full-content index of a document collection, in </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft12">this case TREC.   In [10] they use web data to reinforce the scores <br>of promising candidate answers by providing additional <br>redundancy, with good success.   Their implementation requires <br>an auxiliary web corpus be available for full-text analysis and <br>global term weighting.   In our work, the web is the primary <br>source of redundancy and we operate without a full-text index of <br>documents or a database of global term weights. </span></nobr></DIV>
<DIV style="position:absolute;top:231;left:477"><nobr><span class="ft12">Buchholz's Shapaqa NLP system [7] has been evaluated on both <br>TREC and Web collections.  Question answering accuracy was <br>higher with the Web collection (although both runs were poor in <br>absolute terms), but few details about the nature of the differences <br>are provided. </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:477"><nobr><span class="ft12">These systems typically perform complex parsing and entity <br>extraction for both queries and best matching web pages ([7][14]), <br>which limits the number of  web pages that they can analyze in <br>detail.   Other systems require term weighting for selecting or <br>ranking the best-matching passages ([10][14]) and this requires <br>auxiliary data structures.  Our approach is distinguished from <br>these in its simplicity (simple rewrites and string matching) and <br>efficiency in the use of web resources (use of only summaries and <br>simple ranking).   We now describe how our system uses <br>redundancy in detail and evaluate this systematically.    </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:477"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:501"><nobr><span class="ft4"><b>SYSTEM OVERVIEW </b></span></nobr></DIV>
<DIV style="position:absolute;top:517;left:477"><nobr><span class="ft12">A flow diagram of our system is shown in Figure 1.  The system <br>consists of four main components. </span></nobr></DIV>
<DIV style="position:absolute;top:558;left:477"><nobr><span class="ft5"><b>Rewrite Query</b></span></nobr></DIV>
<DIV style="position:absolute;top:558;left:563"><nobr><span class="ft0">. Given a question, the system generates a number </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:477"><nobr><span class="ft12">of rewrite strings, which are likely substrings of declarative <br>answers to the question.  To give a simple example, from the <br>question "<i>When was Abraham Lincoln born</i>?" we know that a <br>likely answer formulation takes the form "Abraham Lincoln was <br>born on &lt;DATE&gt;".  Therefore, we can look through the collection <br>of documents, searching for such a pattern.   </span></nobr></DIV>
<DIV style="position:absolute;top:678;left:477"><nobr><span class="ft12">We first classify the question into one of seven categories, each of <br>which is mapped to a particular set of rewrite rules.  Rewrite rule <br>sets range in size from one to five rewrite types.   The output of <br>the rewrite module is a set of 3-tuples of the form [</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:774"><nobr><span class="ft21">string, </span></nobr></DIV>
<DIV style="position:absolute;top:742;left:477"><nobr><span class="ft21">L/R/-, weight</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:598"><nobr><span class="ft0">], where "</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:660"><nobr><span class="ft21">string</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:714"><nobr><span class="ft0">" is the reformulated </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">292</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft25{font-size:7px;font-family:Times;color:#000000;}
	.ft26{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft27{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159003.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft0">search query, "</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:166"><nobr><span class="ft21">L/R/-</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:211"><nobr><span class="ft0">" indicates the position in the text where </span></nobr></DIV>
<DIV style="position:absolute;top:128;left:81"><nobr><span class="ft12">we expect to find the answer with respect to the query string (to <br>the left, right or anywhere) and "</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:259"><nobr><span class="ft21">weight</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:313"><nobr><span class="ft0">" reflects how much we </span></nobr></DIV>
<DIV style="position:absolute;top:161;left:81"><nobr><span class="ft12">prefer answers found with this particular query.  The idea behind <br>using a weight is that answers found using a high precision query <br>(e.g., "Abraham Lincoln was born on") are more likely to be <br>correct than those found using a lower precision query (e.g., <br>"Abraham" AND "Lincoln" AND "born"). </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:81"><nobr><span class="ft12">We do not use a parser or part-of-speech tagger for query <br>reformulation, but do use a lexicon in order to determine the <br>possible parts-of-speech of a word as well as its morphological <br>variants.   We created the rewrite rules and associated weights <br>manually for the current system, although it may be possible to <br>learn query-to-answer reformulations and weights (e.g. see <br>Agichtein et al. [4]; Radev et al. [17]).   </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:81"><nobr><span class="ft26">The rewrites generated by our system are simple string-based <br>manipulations.  For instance, some question types involve query <br>rewrites with possible verb movement; the verb "<i>is</i>" in the <br>question "<i>Where is the Louvre Museum located</i>?" should be <br>moved in formulating the desired rewrite to "The Louvre Museum <br><b>is</b></span></nobr></DIV>
<DIV style="position:absolute;top:442;left:90"><nobr><span class="ft0"> located in".  While we might be able to determine where to </span></nobr></DIV>
<DIV style="position:absolute;top:458;left:81"><nobr><span class="ft12">move a verb by analyzing the sentence syntactically, we took a <br>much simpler approach.  Given a query such as "Where is w</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:418"><nobr><span class="ft25">1</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:423"><nobr><span class="ft0"> w</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:437"><nobr><span class="ft25">2</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:441"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:490;left:81"><nobr><span class="ft0">... w</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:108"><nobr><span class="ft25">n</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:113"><nobr><span class="ft0">", where each of the w</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:238"><nobr><span class="ft25">i</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:240"><nobr><span class="ft0"> is a word, we generate a rewrite for </span></nobr></DIV>
<DIV style="position:absolute;top:506;left:81"><nobr><span class="ft0">each possible position the verb could be moved to (e.g. "w</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:406"><nobr><span class="ft25">1</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:410"><nobr><span class="ft0"> is w</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:437"><nobr><span class="ft25">2</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:441"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:521;left:81"><nobr><span class="ft0">... w</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:108"><nobr><span class="ft25">n</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:112"><nobr><span class="ft0">", "w</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:141"><nobr><span class="ft25">1</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:146"><nobr><span class="ft0"> w</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:159"><nobr><span class="ft25">2</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:164"><nobr><span class="ft0"> is ... w</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:208"><nobr><span class="ft25">n</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:212"><nobr><span class="ft0">", etc).  While such an approach results in </span></nobr></DIV>
<DIV style="position:absolute;top:537;left:81"><nobr><span class="ft12">many nonsensical rewrites (e.g. "The Louvre is Museum located <br>in"), these very rarely result in the retrieval of bad pages, and the <br>proper movement position is guaranteed to be found via <br>exhaustive search.  If we instead relied on a parser, we would <br>require fewer query rewrites, but a misparse would result in the <br>proper rewrite not being found. </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:81"><nobr><span class="ft12">For each query we also generate a final rewrite which is a backoff <br>to a simple ANDing of the non-stop words in the query.  We <br>could backoff even further to ranking using a best-match retrieval <br>system which doesn't require the presence of all terms and uses <br>differential term weights, but we did not find that this was <br>necessary when using the Web as a source of data.  There are an <br>average of 6.7 rewrites for the 500 TREC-9 queries used in the <br>experiments described below.   </span></nobr></DIV>
<DIV style="position:absolute;top:774;left:81"><nobr><span class="ft14">As an example, the rewrites for the query "<i>Who created the <br>character of Scrooge?</i>" are: </span></nobr></DIV>
<DIV style="position:absolute;top:810;left:135"><nobr><span class="ft0">LEFT_5_"created +the character +of Scrooge" </span></nobr></DIV>
<DIV style="position:absolute;top:831;left:135"><nobr><span class="ft12">RIGHT_5_"+the character +of Scrooge +was created <br>+by" </span></nobr></DIV>
<DIV style="position:absolute;top:867;left:135"><nobr><span class="ft12">AND_2_"created" AND "+the character" AND "+of <br>Scrooge" </span></nobr></DIV>
<DIV style="position:absolute;top:903;left:135"><nobr><span class="ft0">AND_1_"created" AND "character" AND "Scrooge" </span></nobr></DIV>
<DIV style="position:absolute;top:923;left:81"><nobr><span class="ft12">To date we have used only simple string matching techniques.  <br>Soubbotin and Soubbotin [18] have used much richer regular <br>expression matching to provide hints about likely answers, with <br>very good success in TREC 2001, and we could certainly <br>incorporate some of these ideas in our rewrites.    Note that many <br>of our rewrites require the matching of stop words like "in" and <br>"the", in the above example.  In our system stop words are <br>important indicators of likely answers, and we do not ignore them <br>as most ranked retrieval systems do, except in the final backoff <br>AND rewrite.   </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft12">The query rewrites are then formulated as search engine queries <br>and sent to a search engine from which page summaries are <br>collected and analyzed.  </span></nobr></DIV>
<DIV style="position:absolute;top:168;left:477"><nobr><span class="ft5"><b>Mine N-Grams</b></span></nobr></DIV>
<DIV style="position:absolute;top:168;left:564"><nobr><span class="ft0">.  From the page summaries returned by the search </span></nobr></DIV>
<DIV style="position:absolute;top:184;left:477"><nobr><span class="ft12">engine, n-grams are mined.   For reasons of efficiency, we use <br>only the returned summaries and not the full-text of the <br>corresponding web page.   The returned summaries contain the <br>query terms, usually with a few words of surrounding context.  In <br>some cases, this surrounding context has truncated the answer <br>string, which may negatively impact results.  The summary text is <br>then processed to retrieve only strings to the left or right of the <br>query string, as specified in the rewrite triple.   </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:477"><nobr><span class="ft12">1-, 2-, and 3-grams are extracted from the summaries.  An N-gram <br>is scored according the weight of the query rewrite that retrieved <br>it.  These scores are summed across the summaries that contain <br>the n-gram (which is the opposite of the usual inverse document <br>frequency component of document/passage ranking schemes).  <br>We do not count frequency of occurrence within a summary (the <br>usual tf component in ranking schemes).  Thus, the final score for <br>an n-gram is based on the rewrite rules that generated it and the <br>number of unique summaries in which it occurred.  When <br>searching for candidate answers, we enforce the constraint that at <br>most one stopword is permitted to appear in any potential n-gram <br>answers. </span></nobr></DIV>
<DIV style="position:absolute;top:516;left:477"><nobr><span class="ft0">The top-ranked n-grams for the Scrooge query are: </span></nobr></DIV>
<DIV style="position:absolute;top:533;left:531"><nobr><span class="ft27">Dickens 117 <br>Christmas Carol 78 <br>Charles Dickens 75 <br>Disney 72 <br>Carl Banks 54 <br>A Christmas 41 <br>uncle 31 </span></nobr></DIV>
<DIV style="position:absolute;top:662;left:477"><nobr><span class="ft5"><b>Filter/Reweight N-Grams.</b></span></nobr></DIV>
<DIV style="position:absolute;top:662;left:630"><nobr><span class="ft0">  Next, the n-grams are filtered and </span></nobr></DIV>
<DIV style="position:absolute;top:678;left:477"><nobr><span class="ft12">reweighted according to how well each candidate matches the <br>expected answer-type, as specified by a handful of handwritten <br>filters.  The system uses filtering in the following manner. First, <br>the query is analyzed and assigned one of seven question types, <br>such as <i>who-question, what-question, </i>or  <i>how-many-question</i>.  <br>Based on the query type that has been assigned, the system <br>determines what collection of filters to apply to the set of potential <br>answers found during n-gram harvesting. The answers are <br>analyzed for features relevant to the filters, and then rescored <br>according to the presence of such information.  <b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:842;left:477"><nobr><span class="ft12">A collection of approximately 15 filters were developed based on <br>human knowledge about question types and the domain from <br>which their answers can be drawn.  These filters used surface <br>string features, such as capitalization or the presence of digits, and <br>consisted of handcrafted regular expression patterns.  </span></nobr></DIV>
<DIV style="position:absolute;top:927;left:477"><nobr><span class="ft12">After the system has determined which filters to apply to a pool of <br>candidate answers, the selected filters are applied to each <br>candidate string and used to adjust the score of the string.  In most <br>cases, filters are used to boost the score of a potential answer <br>when it has been determined to possess the features relevant to the <br>query type. In other cases, filters are used to remove strings from <br>the candidate list altogether. This type of exclusion was only <br>performed when the set of correct answers was determined to be a </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">293</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft28{font-size:10px;font-family:Helvetica;color:#000000;}
	.ft29{font-size:10px;font-family:Helvetica;color:#000000;}
	.ft30{font-size:11px;line-height:15px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159004.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">closed set (e.g. "Which continent....?") or definable by a set of <br>closed properties (e.g. "How many...?"). </span></nobr></DIV>
<DIV style="position:absolute;top:150;left:81"><nobr><span class="ft5"><b>Tile N-Grams.</b></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:166"><nobr><span class="ft0"> Finally, we applied an answer tiling algorithm, </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft12">which both merges similar answers and assembles longer answers <br>out of answer fragments.  Tiling constructs longer n-grams from <br>sequences of overlapping shorter n-grams. For example, &quot;A B C&quot; <br>and &quot;B C D&quot; is tiled into &quot;A B C D.&quot; The algorithm proceeds <br>greedily from the top-scoring candidate - all subsequent <br>candidates (up to a certain cutoff) are checked to see if they can <br>be tiled with the current candidate answer. If so, the higher <br>scoring candidate is replaced with the longer tiled n-gram, and the <br>lower scoring candidate is removed. The algorithm stops only <br>when no n-grams can be further tiled. </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:81"><nobr><span class="ft0">The top-ranked n-grams after tiling for the Scrooge query are: </span></nobr></DIV>
<DIV style="position:absolute;top:347;left:135"><nobr><span class="ft27">Charles Dickens 117 <br>A Christmas Carol  78 <br>Walt Disney's uncle 72 <br>Carl Banks 54 <br>uncle 31 </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:81"><nobr><span class="ft12">Our system works most efficiently and naturally with a backend <br>retrieval system that returns best-matching passages or query-<br>relevant document summaries.   We can, of course, post-process <br>the full text of matching documents to extract summaries for n-<br>gram mining, but this is inefficient especially in Web applications <br>where the full text of documents would have to be downloaded <br>over the network at query time. </span></nobr></DIV>
<DIV style="position:absolute;top:567;left:81"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:566;left:94"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:567;left:105"><nobr><span class="ft4"><b>EXPERIMENTS </b></span></nobr></DIV>
<DIV style="position:absolute;top:586;left:81"><nobr><span class="ft12">For our experimental evaluations we used the first 500 TREC-9 <br>queries (201-700) [19]. For simplicity we ignored queries which <br>are syntactic rewrites of earlier queries (701-893), although <br>including them does not change the results in any substantive <br>way.   We used the patterns provided by NIST for automatic <br>scoring.  A few patterns were slightly modified to accommodate <br>the fact that some of the answer strings returned using the Web <br>were not available for judging in TREC-9.   We did this in a very <br>conservative manner allowing for more specific correct answers <br>(e.g., Edward J. Smith vs. Edward Smith) but not more general <br>ones (e.g., Smith vs. Edward Smith), and simple substitutions <br>(e.g., 9 months vs. nine months).   These changes influence the <br>absolute scores somewhat but do not change relative performance, <br>which is our focus here.   </span></nobr></DIV>
<DIV style="position:absolute;top:817;left:81"><nobr><span class="ft12">Many of the TREC queries are time sensitive ­ that is, the correct <br>answer depends on when the question is asked.  The TREC <br>database covers a period of time more than 10 years ago; the Web <br>is much more current.  Because of this mismatch, many correct <br>answers returned from the Web will be scored as incorrect using <br>the TREC answer patterns. 10-20% of the TREC queries have <br>temporal dependencies.  E.g., <i>Who is the president of Bolivia?  <br>What is the exchange rate between England and the U. S.?  </i>We <br>did not modify the answer key to accommodate these time <br>differences, because this is a subjective job and would make <br>comparison with earlier TREC results impossible.   </span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:81"><nobr><span class="ft12">For the main Web retrieval experiments we used Google as a <br>backend because it provides query-relevant summaries that make <br>our n-gram mining techniques more efficient.  Thus we have <br>access to more than 2 billion web pages.  For some experiments in <br>TREC retrieval we use the standard QA collection consisting of </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft12">news documents from Disks 1-5.  The TREC collection has just <br>under 1 million documents [19]. </span></nobr></DIV>
<DIV style="position:absolute;top:152;left:477"><nobr><span class="ft12">All runs are completely automatic, starting with queries and <br>generating a ranked list of 5 candidate answers.  Candidate <br>answers are a maximum of 50 bytes long, and typically much <br>shorter than that.  We report the Mean Reciprocal Rank (MRR) of <br>the first correct answer, the Number of Questions Correctly <br>Answered (NumCorrect), and the Proportion of Questions <br>Correctly Answered (PropCorrect).  Correct answers at any rank <br>are included in the number and proportion correct measures.  <br>Most correct answers are at the top of the list -- 70% of the correct <br>answers occur in the first position and 90% in the first or second <br>positions. </span></nobr></DIV>
<DIV style="position:absolute;top:335;left:477"><nobr><span class="ft12">Using our system with default settings for query rewrite weights, <br>number of summaries returned, etc. we obtain a MRR of 0.507 <br>and answer 61% of the queries.  The average answer length was <br>12 bytes, so the system is really returning short answers not <br>passages.   This is very good performance and would place us near <br>the top of 50-byte runs for TREC-9.  However, since we did not <br>take part in TREC-9 it is impossible to compare our results <br>precisely with those systems (e.g., we used TREC-9 for tuning our <br>TREC-10 system increasing our score somewhat, but we return <br>several correct answers that were not found in TREC-9 thus <br>decreasing our score somewhat).   </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:477"><nobr><span class="ft12">Redundancy is used in two key ways in our data-driven approach.  <br>First, the occurrence of multiple linguistic formulations of the <br>same answers increases the chances of being able to find an <br>answer that occurs within the context of a simple pattern match <br>with the query.  Second, answer redundancy facilitates the process <br>of answer extraction by giving higher weight to answers that <br>occur more often (i.e., in more different document summaries).   <br>We now evaluate the contributions of these experimentally. </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:477"><nobr><span class="ft4"><b>5.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:655;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:656;left:510"><nobr><span class="ft4"><b>Number of Snippets </b></span></nobr></DIV>
<DIV style="position:absolute;top:676;left:477"><nobr><span class="ft12">We begin by examining the importance of redundancy in answer <br>extraction.  To do this we vary the number of summaries <br>(snippets) that we get back from the search engine and use as <br>input to the n-gram mining process.   Our standard system uses <br>100 snippets.    We varied the number of snippets from 1 to 1000.  <br>The results are shown in Figure 2.      </span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:838"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:477"><nobr><span class="ft12">Performance improves sharply as the number of snippets increases <br>from 1 to 50 (0.243 MRR for 1 snippet, 0.370 MRR for 5, 0.423 <br>MRR for 10, and 0.501 for 50), somewhat more slowly after that </span></nobr></DIV>
<DIV style="position:absolute;top:914;left:546"><nobr><span class="ft28">0</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:535"><nobr><span class="ft28">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:535"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:535"><nobr><span class="ft28">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:842;left:535"><nobr><span class="ft28">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:535"><nobr><span class="ft28">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:535"><nobr><span class="ft28">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:558"><nobr><span class="ft28">1</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:633"><nobr><span class="ft28">10</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:710"><nobr><span class="ft28">100</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:785"><nobr><span class="ft28">1000</span></nobr></DIV>
<DIV style="position:absolute;top:956;left:618"><nobr><span class="ft29"><b>Num ber of Snippets</b></span></nobr></DIV>
<DIV style="position:absolute;top:870;left:525"><nobr><span class="ft29"><b>MR</b></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:525"><nobr><span class="ft29"><b>R</b></span></nobr></DIV>
<DIV style="position:absolute;top:995;left:492"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:981;left:509"><nobr><span class="ft30">Figure 2.  MRR as a function of number of <br>snippets returned.  TREC-9, queries 201-700. </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">294</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft31{font-size:14px;font-family:Times;color:#000000;}
	.ft32{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft33{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft34{font-size:12px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159005.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">(peaking 0.514 MRR with 200 snippets), and then falling off <br>somewhat after that as more snippets are included for n-gram <br>analysis.     Thus, over quite a wide range, the more snippets we <br>consider in selecting and ranking n-grams the better.  We believe <br>that the slight drop at the high end is due to the increasing <br>influence that the weaker rewrites have when many snippets are <br>returned.  The most restrictive rewrites return only a few matching <br>documents.   Increasing the number of snippets increases the <br>number of the least restrictive matches (the AND matches), thus <br>swamping the restrictive matches.  In addition, frequent n-grams <br>begin to dominate our rankings at this point. </span></nobr></DIV>
<DIV style="position:absolute;top:295;left:81"><nobr><span class="ft12">An example of failures resulting from too many AND matches is <br>Query 594: <i>What is the longest word in the English language?  <br></i>For this query, there are 40 snippets matching the rewrite "is the <br>longest word in the English language" with weight 5, 40 more <br>snippets matching the rewrite "the longest word in the English <br>language is" with the weight 5, and more than 100 snippets <br>matching the backoff AND query ("longest" AND "word" AND <br>"English" AND "language") with a weight of 1.  When 100 <br>snippets are used, the precise rewrites contribute almost as many <br>snippets as the AND rewrite.   In this case we find the correct <br>answer, "pneumonoultramicroscopicsilicovolcanokoniosis", in the <br>second rank.    The first answer, "1909 letters long", which is <br>incorrect, also matches many precise rewrites such as "the longest <br>word in English is ## letters long", and we pick up on this.    <br>When 1000 snippets are used, the weaker AND rewrites dominate <br>the matches.  In this case, the correct answer falls to seventh on <br>the list after "letters long", "one syllable", "is screeched", "facts", <br>"stewardesses" and "dictionary", all of which occur commonly in <br>results from the least restrictive AND rewrite.   A very common <br>AND match contains the phrase "the longest one-syllable word in <br>the English language is screeched", and this accounts for two of <br>our incorrect answers.   </span></nobr></DIV>
<DIV style="position:absolute;top:652;left:81"><nobr><span class="ft12">Using differential term weighting of answer terms, as many <br>retrieval systems do, should help overcome this problem to some <br>extent but we would like to avoid maintaining a database of global <br>term weights.  Alternatively we could refine our weight <br>accumulation scheme to dampen the effects of many weakly <br>restrictive matches by sub-linear accumulation, and we are <br>currently exploring several alternatives for doing this.      </span></nobr></DIV>
<DIV style="position:absolute;top:772;left:81"><nobr><span class="ft12">Our main results on snippet redundancy are consistent with those <br>reported by Clarke et al. [9], although they worked with the much <br>smaller TREC collection.  They examined a subset of the TREC-9 <br>queries requiring a person's name as the answer.  They varied the <br>number of passages retrieved (which they call depth) from 25 to <br>100, and observed some improvements in MRR.  When the <br>passages they retrieved were small (250 or 500 bytes) they found <br>improvement, but when the passages were larger (1000 or 2000 <br>bytes) no improvements were observed.   The snippets we used <br>are shorter than 250 bytes, so the results are consistent.  Clarke et <br>al. [9] also explored a different notion of redundancy (which they <br>refer to as c</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:145"><nobr><span class="ft25">i</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:147"><nobr><span class="ft0">).  c</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:169"><nobr><span class="ft25">i</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:171"><nobr><span class="ft0"> is the number of different passages that match an </span></nobr></DIV>
<DIV style="position:absolute;top:962;left:81"><nobr><span class="ft0">answer.  Their best performance is achieved when both c</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:413"><nobr><span class="ft25">i</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:415"><nobr><span class="ft0"> and </span></nobr></DIV>
<DIV style="position:absolute;top:978;left:81"><nobr><span class="ft12">term weighting are used to rank passages.  We too use the number <br>of snippets that an n-gram occurs in.   We do not, however, use <br>global term weights, but have tried other techniques for weighting <br>snippets as described below. </span></nobr></DIV>
<DIV style="position:absolute;top:113;left:477"><nobr><span class="ft4"><b>5.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:113;left:510"><nobr><span class="ft4"><b>TREC vs. Web Databases  </b></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:477"><nobr><span class="ft12">Another way to explore the importance of redundancy is to run <br>our system directly on the TREC documents.   As noted earlier, <br>there are three orders of magnitude more documents on the Web <br>than in the TREC QA collection.     Consequently, there will be <br>fewer alternative ways of saying the same thing and fewer <br>matching documents available for mining the candidate n-grams.   <br>We suspect that this lack of redundancy will limit the success of <br>our approach when applied directly on TREC documents.   </span></nobr></DIV>
<DIV style="position:absolute;top:268;left:477"><nobr><span class="ft12">While corpus size is an obvious and important difference between <br>the TREC and Web collections there are other differences as well.  <br>For example, text analysis, ranking, and snippet extraction <br>techniques will all vary somewhat in ways that we can not control.  <br>To better isolate the size factor, we also ran our system against <br>another Web search engine.     </span></nobr></DIV>
<DIV style="position:absolute;top:372;left:477"><nobr><span class="ft14">For these experiments we used only the AND rewrites and looked <br>at the first 100 snippets.   We had to restrict ourselves to AND <br>rewrites because some of the search engines we used do not <br>support the inclusion of stop words in phrases, e.g., "<i>created +the <br>character +of Scrooge</i>". </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:477"><nobr><span class="ft31"><i>5.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:464;left:510"><nobr><span class="ft32"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:464;left:521"><nobr><span class="ft31"><i>TREC Database </i></span></nobr></DIV>
<DIV style="position:absolute;top:483;left:477"><nobr><span class="ft12">The TREC QA collection consists of just under 1 million <br>documents.  We expect much less redundancy here compared to <br>the Web, and suspect that this will limit the success of our <br>approach.   An analysis of the TREC-9 query set (201-700) shows <br>that no queries have 100 judged relevant documents.   Only 10 of <br>the 500 questions have 50 or more relevant documents, which the <br>results in Figure 2 suggest are required for the good system <br>performance.  And a very large number of queries, 325, have <br>fewer than 10 relevant documents.   </span></nobr></DIV>
<DIV style="position:absolute;top:635;left:477"><nobr><span class="ft12">We used an Okapi backend retrieval engine for the TREC <br>collection.   Since we used only Boolean AND rewrites, we did <br>not take advantage of Okapi's best match ranking algorithm.   <br>However, most queries return fewer than 100 documents, so we <br>wind up examining most of the matches anyway. </span></nobr></DIV>
<DIV style="position:absolute;top:723;left:477"><nobr><span class="ft12">We developed two snippet extraction techniques to generate <br>query-relevant summaries for use in n-gram mining.  A <br><i>Contiguous</i> technique returned the smallest window containing all <br>the query terms along with 10 words of context on either side.  <br>Windows that were greater than 500 words were ignored.  This <br>approach is similar to passage retrieval techniques albeit without <br>differential term weighting.  A <i>Non-Contiguous</i> technique <br>returned the union of two word matches along with 10 words of <br>context on either side.  Single words not previously covered are <br>included as well.  The search engine we used for the initial Web <br>experiments returns both contiguous and non-contiguous snippets.     <br>Figure 3 shows the results of this experiment. </span></nobr></DIV>
<DIV style="position:absolute;top:941;left:656"><nobr><span class="ft33"><b>MRR</b></span></nobr></DIV>
<DIV style="position:absolute;top:941;left:707"><nobr><span class="ft33"><b>NumCorrect PropCorrect</b></span></nobr></DIV>
<DIV style="position:absolute;top:962;left:480"><nobr><span class="ft34"><i><b>Web1</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:962;left:655"><nobr><span class="ft3">0.450</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:726"><nobr><span class="ft3">281</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:790"><nobr><span class="ft3">0.562</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:480"><nobr><span class="ft34"><i><b>TREC, Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:980;left:655"><nobr><span class="ft3">0.186</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:726"><nobr><span class="ft3">117</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:790"><nobr><span class="ft3">0.234</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:480"><nobr><span class="ft34"><i><b>TREC, Non-Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:999;left:655"><nobr><span class="ft3">0.187</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:726"><nobr><span class="ft3">128</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:790"><nobr><span class="ft3">0.256</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:671"><nobr><span class="ft33"><b>AND Rewrites Only, Top 100</b></span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:574"><nobr><span class="ft3">Figure 3: Web vs. TREC as data source</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:836"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">295</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft35{font-size:11px;font-family:Times;color:#4b4b4b;}
-->
</STYLE>
<IMG width="918" height="1188" src="159006.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">Our baseline system using all rewrites and retrieving 100 snippets <br>achieves 0.507 MRR (Figure 2).  Using only the AND query <br>rewrites results in worse performance for our baseline system with <br>0.450 MRR (Figure 3).  More noticeable than this difference is <br>the drop in performance of our system using TREC as a data <br>source compared to using the much larger Web as a data source.</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:441"><nobr><span class="ft35">   </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:81"><nobr><span class="ft12">MRR drops from 0.450 to 0.186 for contiguous snippets and <br>0.187 for non-contiguous snippets, and the proportion of <br>questions answered correctly drops from 56% to 23% for <br>contiguous snippets and 26% for non-contiguous snippets.   It is <br>worth noting that the TREC MRR scores would still place this <br>system in the top half of the systems for the TREC-9 50-byte task, <br>even though we tuned our system to work on much larger <br>collections.  However, we can do much better simply by using <br>more data.  The lack of redundancy in the TREC collection <br>accounts for a large part of this drop off in performance.  Clarke et <br>al. [10] have also reported better performance using the Web <br>directly for TREC 2001 questions.   </span></nobr></DIV>
<DIV style="position:absolute;top:406;left:81"><nobr><span class="ft12">We expect that the performance difference between TREC and the <br>Web would increase further if all the query rewrites were used.   <br>This is because there are so few exact phrase matches in TREC <br>relative to the Web, and the precise matches improve performance <br>by 13% (0.507 vs. 0.450).     </span></nobr></DIV>
<DIV style="position:absolute;top:494;left:81"><nobr><span class="ft12">We believe that database size per se (and the associated <br>redundancy) is the most important difference between the TREC <br>and Web collections.  As noted above, however, there are other <br>differences between the systems such as text analysis, ranking, <br>and snippet extraction techniques.  While we can not control the <br>text analysis and ranking components of Web search engines, we <br>can use the same snippet extraction techniques.  We can also use a <br>different Web search engine to mitigate the effects of a specific <br>text analysis and ranking algorithms.  </span></nobr></DIV>
<DIV style="position:absolute;top:649;left:81"><nobr><span class="ft31"><i>5.2.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:114"><nobr><span class="ft32"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:649;left:125"><nobr><span class="ft31"><i>Another Web Search Engine </i></span></nobr></DIV>
<DIV style="position:absolute;top:668;left:81"><nobr><span class="ft12">For these experiments we used the MSNSearch search engine.  At <br>the time of our experiments, the summaries returned were <br>independent of the query.  So we retrieved the full text of the top <br>100 web pages and applied the two snippet extraction techniques <br>described above to generate query-relevant summaries.  As before, <br>all runs are completely automatic, starting with queries, retrieving <br>web pages, extracting snippets, and generating a ranked list of 5 <br>candidate answers.   The results of these experiments are shown in <br>Figure 4.   The original results are referred to as Web1 and the <br>new results as Web2. </span></nobr></DIV>
<DIV style="position:absolute;top:851;left:257"><nobr><span class="ft33"><b>MRR</b></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:308"><nobr><span class="ft33"><b>NumCorrect PropCorrect</b></span></nobr></DIV>
<DIV style="position:absolute;top:872;left:83"><nobr><span class="ft34"><i><b>Web1</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:872;left:256"><nobr><span class="ft3">0.450</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:327"><nobr><span class="ft3">281</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:390"><nobr><span class="ft3">0.562</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:83"><nobr><span class="ft34"><i><b>TREC, Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:891;left:256"><nobr><span class="ft3">0.186</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:327"><nobr><span class="ft3">117</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:390"><nobr><span class="ft3">0.234</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:83"><nobr><span class="ft34"><i><b>TREC, Non-Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:909;left:256"><nobr><span class="ft3">0.187</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:327"><nobr><span class="ft3">128</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:390"><nobr><span class="ft3">0.256</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:83"><nobr><span class="ft34"><i><b>Web2, Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:928;left:256"><nobr><span class="ft3">0.355</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:327"><nobr><span class="ft3">227</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:390"><nobr><span class="ft3">0.454</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:83"><nobr><span class="ft34"><i><b>Web2, Non-Contiguous Snippet</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:946;left:256"><nobr><span class="ft3">0.383</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:327"><nobr><span class="ft3">243</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:390"><nobr><span class="ft3">0.486</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:272"><nobr><span class="ft33"><b>AND Rewrites Only, Top 100</b></span></nobr></DIV>
<DIV style="position:absolute;top:985;left:176"><nobr><span class="ft3">Figure 4: Web vs. TREC as data source</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:435"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:81"><nobr><span class="ft12">The Web2 results are somewhat worse than the Web1 results, but <br>this is expected given that we developed our system using the <br>Web1 backend, and did not do any tuning of our snippet <br>extraction algorithms.  In addition, we believe that the Web2 </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:477"><nobr><span class="ft12">collection indexed somewhat less content than Web1 at the time <br>of our experiments, which should decrease performance in and of <br>itself.   More importantly, the Web2 results are much better than <br>the TREC results for both snippet extraction techniques, almost <br>doubling MRR in both cases.   Thus, we have shown that QA is <br>more successful using another large Web collection compared to <br>the small TREC collection.  The consistency of this result across <br>Web collections points to size and redundancy as the key factors. </span></nobr></DIV>
<DIV style="position:absolute;top:248;left:477"><nobr><span class="ft31"><i>5.2.3</i></span></nobr></DIV>
<DIV style="position:absolute;top:247;left:510"><nobr><span class="ft32"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:248;left:521"><nobr><span class="ft31"><i>Combining TREC and Web </i></span></nobr></DIV>
<DIV style="position:absolute;top:267;left:477"><nobr><span class="ft12">Given that the system benefits from having a large text collection <br>from which to search for potential answers, then we would expect <br>that combining both the Web and TREC corpus would result in <br>even greater accuracy.  We ran two experiments to test this.  <br>Because there was no easy way to merge the two corpora, we <br>instead combined the output of QA system built on each corpus.  <br>For these experiments we used the original Web1 system and our <br>TREC system.  We used only the AND query rewrites, looked at <br>the Top1000 search results for each rewrite, and used a slightly <br>different snippet extraction technique.  For these parameter <br>settings, the base TREC-based system had a MRR of .262, the <br>Web-based system had a MRR of .416. </span></nobr></DIV>
<DIV style="position:absolute;top:466;left:477"><nobr><span class="ft12">First, we ran an oracle experiment to assess the potential gain that <br>could be attained by combining the output of the Web-based and <br>TREC-based QA systems.  We implemented a "switching oracle", <br>which decides for each question whether to use the output from <br>the Web-based QA system or the TREC-based QA system, based <br>upon which system output had a higher ranking correct answer.  <br>The switching oracle had a MRR of .468, a 12.5% improvement <br>over the Web-based system.  Note that this oracle does not <br>precisely give us an upper bound, as combining algorithms (such <br>as that described below) could re-order the rankings of outputs. </span></nobr></DIV>
<DIV style="position:absolute;top:633;left:477"><nobr><span class="ft12">Next, we implemented a combining algorithm that merged the <br>outputs from the TREC-based and Web-based systems, by having <br>both systems vote on answers, where the vote is the score <br>assigned to a particular answer by the system.  For voting, we <br>defined string equivalence such that if a string X is a substring of <br>Y, then a vote for X is also a vote for Y.  The combined system <br>achieved a MRR of .433 (a 4.1% improvement over the Web-<br>based system) and answered 283 questions correctly. </span></nobr></DIV>
<DIV style="position:absolute;top:774;left:477"><nobr><span class="ft4"><b>5.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:774;left:510"><nobr><span class="ft4"><b>Snippet Weighting </b></span></nobr></DIV>
<DIV style="position:absolute;top:793;left:477"><nobr><span class="ft12">Until now, we have focused on the quantity of information <br>available and less on its quality.     Snippet weights are used in <br>ranking n-grams.   An n-gram weight is the sum of the weights for <br>all snippets in which that n-gram appears. </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:477"><nobr><span class="ft12">Each of our query rewrites has a weight associated with it <br>reflecting how much we prefer answers found with this particular <br>query.  The idea behind using a weight is that answers found <br>using a high precision query (e.g., "Abraham Lincoln was born <br>on") are more likely to be correct than those found using a lower <br>precision query (e.g., "Abraham" AND "Lincoln" AND "born").  <br>Our current system has 5 weights.  </span></nobr></DIV>
<DIV style="position:absolute;top:979;left:477"><nobr><span class="ft12">These rewrite weights are the only source of snippet weighting in <br>our system.    We explored how important these weight are and <br>considered several other factors that could be used as additional <br>sources of information for snippet weighting.  Although we <br>specify Boolean queries, the retrieval engine can provide a <br>ranking, based on factors like link analyses, proximity of terms, </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">296</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft36{font-size:13px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159007.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">location of terms in the document, etc.   So, different weights can <br>be assigned to matches at different positions in the ranked list.  <br>We also looked at the number of matching terms in the best fixed <br>width window, and the widow size of the smallest matching <br>passage as indicators of passage quality. </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:81"><nobr><span class="ft12"><i>Rewrite Wts </i>uses our heuristically determined rewrite weights as a <br>measure the quality of a snippet.   This is the current system <br>default.  <i>Equal Wts</i> gives equal weight to all snippets regardless of <br>the rewrite rule that generated them.  To the extent that more <br>precise rewrites retrieve better answers, we will see a drop in <br>performance when we make all weights equal.  <i>Rank Wts</i> uses the <br>rank of the snippet as a measure of its quality, <i>SnippetWt = (100-<br>rank)/100</i>.  <i>NMatch Wts</i> uses the number of matching terms in a <br>fixed-width window as the measure of snippet quality.  <i>Length <br>Wts</i> uses a measure of the length of the snippet needed to <br>encompass all query terms as the measure of snippet quality.  We <br>also look at combinations of these factors.  For example, <br><i>Rewrite+Rank Wts </i>uses both rewrite weight and rank according to <br>the following formula<i>, SnippetWt = RewriteScore + (100-<br>rank)/100</i>.   All of these measures are available from query-<br>relevant summaries returned by the search engine and do not <br>require analyzing the full text of the document. The results of <br>these experiments are presented in Figure 4. </span></nobr></DIV>
<DIV style="position:absolute;top:489;left:83"><nobr><span class="ft19"><b>Weighting</b></span></nobr></DIV>
<DIV style="position:absolute;top:489;left:236"><nobr><span class="ft19"><b>MRR</b></span></nobr></DIV>
<DIV style="position:absolute;top:489;left:308"><nobr><span class="ft19"><b>NumCorrect PropCorrect</b></span></nobr></DIV>
<DIV style="position:absolute;top:510;left:83"><nobr><span class="ft36"><i>Equal Wts</i></span></nobr></DIV>
<DIV style="position:absolute;top:510;left:235"><nobr><span class="ft20">0.489</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:328"><nobr><span class="ft20">298</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:391"><nobr><span class="ft20">0.596</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:83"><nobr><span class="ft36"><i>Rewrite Wts (Default)</i></span></nobr></DIV>
<DIV style="position:absolute;top:530;left:235"><nobr><span class="ft20">0.507</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:328"><nobr><span class="ft20">307</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:391"><nobr><span class="ft20">0.614</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:83"><nobr><span class="ft36"><i>Rank Wts</i></span></nobr></DIV>
<DIV style="position:absolute;top:548;left:235"><nobr><span class="ft20">0.483</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:328"><nobr><span class="ft20">292</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:391"><nobr><span class="ft20">0.584</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:83"><nobr><span class="ft36"><i>Rewrite + Rank Wts</i></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:235"><nobr><span class="ft20">0.508</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:328"><nobr><span class="ft20">302</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:391"><nobr><span class="ft20">0.604</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:83"><nobr><span class="ft36"><i>NMatch Wts</i></span></nobr></DIV>
<DIV style="position:absolute;top:586;left:235"><nobr><span class="ft20">0.506</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:328"><nobr><span class="ft20">301</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:391"><nobr><span class="ft20">0.602</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:83"><nobr><span class="ft36"><i>Length Wts</i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:235"><nobr><span class="ft20">0.506</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:328"><nobr><span class="ft20">302</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:391"><nobr><span class="ft20">0.604</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:198"><nobr><span class="ft20">Figure 5: Snippet Weighting</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:438"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:682;left:81"><nobr><span class="ft12">Our current default 5-level weighting scheme which reflects the <br>specificity of the query rewrites does quite well.   Equal weighting <br>is somewhat worse, as we expected.   Interestingly search engine <br>rank is no better for weighting candidate n-grams than equal <br>weighting.   None of the other techniques we looked at surpasses <br>the default weights in both MRR and PropCorrect.  Our heuristic <br>rewrite weights provide a simple and effective technique for <br>snippet weighting, that can be used with any backend retrieval <br>engine.   <br>Most question answering systems use IR-based measures of <br>passage quality, and do not typically evaluate the best measure of <br>similarity for purposes of extracting answers.  Clarke et al. [9] <br>noted above is an exception.   Soubbotin and Soubbotin [18] <br>mention different weights for different regular expression <br>matches, but they did not describe the mechanism in detail nor did <br>they evaluate how useful it is.  Harabagiu et al. [11] have a kind <br>of backoff strategy for matching which is similar to weighting, but <br>again we do not know of parametric evaluations of its importance <br>in their overall system performance.  The question of what kinds <br>of passages can best support answer mining for question <br>answering as opposed to document retrieval is an interesting one <br>that we are pursuing. </span></nobr></DIV>
<DIV style="position:absolute;top:113;left:477"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:113;left:501"><nobr><span class="ft4"><b>DISCUSSION AND FUTURE </b></span></nobr></DIV>
<DIV style="position:absolute;top:135;left:477"><nobr><span class="ft4"><b>DIRECTIONS </b></span></nobr></DIV>
<DIV style="position:absolute;top:154;left:477"><nobr><span class="ft12">The design of our question answering system was motivated by <br>the goal of exploiting the large amounts of text data that is <br>available on the Web and elsewhere as a useful resource.   While <br>many question answering systems take advantage of linguistic <br>resources, fewer depend primarily on data.   Vast amounts of data <br>provide several sources of redundancy that our system capitalizes <br>on.   Answer redundancy (i.e., multiple, differently phrased, <br>answer occurrences) enables us to use only simple query rewrites <br>for matching, and facilitates the extraction of candidate answers.    </span></nobr></DIV>
<DIV style="position:absolute;top:302;left:477"><nobr><span class="ft12">We evaluated the importance of redundancy in our system <br>parametrically.   First, we explored the relationship between the <br>number of document snippets examined and question answering <br>accuracy. Accuracy improves sharply as the number of snippets <br>included for n-gram analysis increases from 1 to 50, somewhat <br>more slowly after that peaking at 200 snippets, and then falls off <br>somewhat after that.  More is better up to a limit.  We believe that <br>we can increase this limit by improving our weight accumulation <br>algorithm so that matches from the least precise rewrites do not <br>dominate. Second, in smaller collections (like TREC), the <br>accuracy of our system drops sharply, although it is still quite <br>reasonable in absolute terms.     Finally, snippet quality is less <br>important to system performance than snippet quantity.  We have <br>a simple 5-level snippet weighting scheme based on the specificity <br>of the query rewrite, and this appears to be sufficient.   More <br>complex weighting schemes that we explored were no more <br>useful.   </span></nobr></DIV>
<DIV style="position:absolute;top:578;left:477"><nobr><span class="ft12">The performance of our system shows promise for approaches to <br>question answering which makes use of very large text databases <br>even with minimal natural language processing.  Our system does <br>not need to maintain its own index nor does it require global term <br>weights, so it can work in conjunction with any backend retrieval <br>engine.  Finally, since our system does only simple query <br>transformations and n-gram analysis, it is efficient and scalable. </span></nobr></DIV>
<DIV style="position:absolute;top:694;left:477"><nobr><span class="ft12">One might think that our system has limited applicability, because <br>it works best with large amounts of data.  But, this isn't <br>necessarily so.   First, we actually perform reasonably well in the <br>smaller TREC collection, and could perhaps tune system <br>parameters to work even better in that environment.    More <br>interestingly,  Brill et al. [6] described a projection technique that <br>can be used to combine the wealth of data available on the Web <br>with the reliability of data in smaller sources like TREC or an <br>encyclopedia.  The basic idea is to find candidate answers in a <br>large and possibly noisy source, and then expand the query to <br>include likely answers.   The expanded queries can then be used <br>on smaller but perhaps more reliable collections ­ either directly <br>to find support for the answer in the smaller corpus, or indirectly <br>as a new query which is issued and mined as we currently do.   <br>This approach appears to be quite promising.   Our approach <br>seems least applicable in applications that involve a small amount <br>of proprietary data.   In these cases, one might need to do much <br>more sophisticated analyses to map user queries to the exact <br>lexical form that occur in the text collection rather than depend on <br>primarily on redundancy as we have done. </span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:477"><nobr><span class="ft12">Although we have pushed the data-driven perspective, more <br>sophisticated language analysis might help as well by providing <br>more effective query rewrites or less noisy data for mining.    </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">297</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft37{font-size:12px;font-family:Times;color:#000000;}
	.ft38{font-size:7px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="159008.png" alt="background image">
<DIV style="position:absolute;top:57;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:81"><nobr><span class="ft12">Most question answering systems contain aspects of both ­ we use <br>some linguistic knowledge in our small query typology and <br>answer filtering, and more sophisticated systems often use simple <br>pattern matching for things like dates, zip codes, etc.   </span></nobr></DIV>
<DIV style="position:absolute;top:184;left:81"><nobr><span class="ft12">There are a number of open questions that we hope to explore.  In <br>the short term, we would like to look systematically at the <br>contributions of other system components.  Brill et al. [5] have <br>started to explore individual components in more detail, with <br>interesting results.  In addition, it is likely that we have made <br>several sub-optimal decisions in our initial implementation (e.g., <br>omitting most stop words from answers, simple linear <br>accumulation of scores over matching snippets) that we would <br>like to improve.   Most retrieval engines have been developed <br>with the goal of finding topically relevant documents.  Finding <br>accurate answers may require somewhat different matching <br>infrastructure.  We are beginning to explore how best to generate <br>snippets for use in answer mining.  Finally, time is an interesting <br>issue.  We noted earlier how the correct answer to some queries <br>changes over time.  Time also has interesting implications for <br>using redundancy.  For example, it would take a while for a news <br>or Web collection to correctly answer a question like "Who is the <br>U. S. President?" just after an election. </span></nobr></DIV>
<DIV style="position:absolute;top:478;left:81"><nobr><span class="ft12">An important goal of our work is to get system designers to treat <br>data as a first class resource that is widely available and <br>exploitable.   We have made good initial progress, and there are <br>several interesting issues remaining to explore. </span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:560;left:94"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:561;left:105"><nobr><span class="ft4"><b>REFERENCES </b></span></nobr></DIV>
<DIV style="position:absolute;top:581;left:81"><nobr><span class="ft37">[1]</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:108"><nobr><span class="ft14">AAAI Spring Symposium Series (2002).  <i>Mining Answers <br>from Text and Knowledge Bases</i>. </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:81"><nobr><span class="ft37">[2]</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:108"><nobr><span class="ft12">S. Abney, M. Collins and A. Singhal (2000).  Answer <br>extraction.  In <i>Proceedings of the 6</i></span></nobr></DIV>
<DIV style="position:absolute;top:634;left:332"><nobr><span class="ft38"><i>th</i></span></nobr></DIV>
<DIV style="position:absolute;top:637;left:339"><nobr><span class="ft6"><i> Applied Natural </i></span></nobr></DIV>
<DIV style="position:absolute;top:653;left:108"><nobr><span class="ft6"><i>Language Processing Conference (ANLP 2000), </i>296-301. </span></nobr></DIV>
<DIV style="position:absolute;top:675;left:81"><nobr><span class="ft37">[3]</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:676;left:108"><nobr><span class="ft14">ACL-EACL (2002). <i>Workshop on Open-domain Question <br>Answering</i>. </span></nobr></DIV>
<DIV style="position:absolute;top:714;left:81"><nobr><span class="ft37">[4]</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:108"><nobr><span class="ft12">E. Agichtein, S. Lawrence and L. Gravano (2001).  Learning <br>search engine specific query transformations for question <br>answering.  In <i>Proceedings of the 10</i></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:328"><nobr><span class="ft38"><i>th</i></span></nobr></DIV>
<DIV style="position:absolute;top:747;left:335"><nobr><span class="ft6"><i> World Wide Web </i></span></nobr></DIV>
<DIV style="position:absolute;top:762;left:108"><nobr><span class="ft6"><i>Conference (WWW10), </i>169-178. </span></nobr></DIV>
<DIV style="position:absolute;top:785;left:81"><nobr><span class="ft37">[5]</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:786;left:108"><nobr><span class="ft14">E. Brill, S. Dumais and M. Banko (2002).  An analysis of the <br>AskMSR question-answering system.  In <i>Proceedings of <br>2002 Conference on Empirical Methods in Natural <br>Language Processing (EMNLP 2002)</i>. </span></nobr></DIV>
<DIV style="position:absolute;top:855;left:81"><nobr><span class="ft37">[6]</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:856;left:108"><nobr><span class="ft14">E. Brill, J. Lin, M. Banko, S. Dumais and A. Ng (2002).  <br>Data-intensive question answering<i>.</i>   In <i>Proceedings of the <br>Tenth Text REtrieval Conference (TREC 2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:910;left:81"><nobr><span class="ft37">[7]</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:911;left:108"><nobr><span class="ft14">S. Buchholz (2002).  Using grammatical relations, answer <br>frequencies and the World Wide Web for TREC question <br>answering.   In <i>Proceedings of the Tenth Text REtrieval <br>Conference (TREC 2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:981;left:81"><nobr><span class="ft37">[8]</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:99"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:982;left:108"><nobr><span class="ft12">J. Chen, A. R. Diekema, M. D. Taffet, N. McCracken, N. E. <br>Ozgencil, O. Yilmazel, E. D. Liddy (2002).  Question </span></nobr></DIV>
<DIV style="position:absolute;top:111;left:504"><nobr><span class="ft14">answering: CNLP at the TREC-10 question answering track.  <br>In <i>Proceedings of the Tenth Text REtrieval Conference <br>(TREC 2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft37">[9]</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:494"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:504"><nobr><span class="ft14">C. Clarke, G. Cormack and T. Lyman (2001).  Exploiting <br>redundancy in question answering.   In <i>Proceedings of the <br>24</i></span></nobr></DIV>
<DIV style="position:absolute;top:195;left:517"><nobr><span class="ft38"><i>th</i></span></nobr></DIV>
<DIV style="position:absolute;top:198;left:525"><nobr><span class="ft6"><i> Annual International ACM SIGIR Conference on </i></span></nobr></DIV>
<DIV style="position:absolute;top:214;left:504"><nobr><span class="ft14"><i>Research and Development in Information Retrieval <br>(SIGIR'2001), </i>358-365. </span></nobr></DIV>
<DIV style="position:absolute;top:252;left:477"><nobr><span class="ft37">[10]</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:504"><nobr><span class="ft14">C. Clarke, G. Cormack and T. Lynam (2002).   Web <br>reinforced question answering.  In <i>Proceedings of the Tenth <br>Text REtrieval Conference (TREC 2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:307;left:477"><nobr><span class="ft37">[11]</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:308;left:504"><nobr><span class="ft14">S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. <br>Surdeanu, R. Bunescu, R. Girju, V. Rus and P. Morarescu <br>(2001).  FALCON: Boosting knowledge for question <br>answering.  In <i>Proceedings of the Ninth Text REtrieval <br>Conference (TREC-9), </i>479-488<i>.</i> </span></nobr></DIV>
<DIV style="position:absolute;top:393;left:477"><nobr><span class="ft37">[12]</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:394;left:504"><nobr><span class="ft14">E. Hovy, L. Gerber, U. Hermjakob, M. Junk and C. Lin <br>(2001).  Question answering in Webclopedia.  In <br><i>Proceedings of the Ninth Text REtrieval Conference (TREC-<br>9), </i>655-664<i>.</i> </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:477"><nobr><span class="ft37">[13]</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:465;left:504"><nobr><span class="ft14">E. Hovy, U. Hermjakob and C. Lin (2002).  The use of <br>external knowledge in factoid QA.  In <i>Proceedings of the <br>Tenth Text REtrieval Conference (TREC 2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:477"><nobr><span class="ft37">[14]</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:504"><nobr><span class="ft12">C. Kwok, O. Etzioni and D. Weld (2001).  Scaling question <br>answering to the Web.  In <i>Proceedings of the 10</i></span></nobr></DIV>
<DIV style="position:absolute;top:533;left:766"><nobr><span class="ft38"><i>th</i></span></nobr></DIV>
<DIV style="position:absolute;top:536;left:773"><nobr><span class="ft6"><i> World </i></span></nobr></DIV>
<DIV style="position:absolute;top:552;left:504"><nobr><span class="ft6"><i>Wide Web Conference (WWW'10), </i>150-161<i>.</i> </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:477"><nobr><span class="ft37">[15]</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:504"><nobr><span class="ft12">M. A. Pasca and S. M. Harabagiu (2001).  High performance <br>question/answering.  In <i>Proceedings of the 24</i></span></nobr></DIV>
<DIV style="position:absolute;top:588;left:753"><nobr><span class="ft38"><i>th</i></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:760"><nobr><span class="ft6"><i> Annual </i></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:504"><nobr><span class="ft12"><i>International ACM SIGIR Conference on Research and <br>Development in Information Retrieval (SIGIR'2001), </i>366-<br>374. </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:477"><nobr><span class="ft37">[16]</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:662;left:504"><nobr><span class="ft12">J. Prager, E. Brown, A. Coden and D. Radev (2000).  <br>Question answering by predictive annotation.  In <br><i>Proceedings of the 23</i></span></nobr></DIV>
<DIV style="position:absolute;top:690;left:623"><nobr><span class="ft38"><i>rd</i></span></nobr></DIV>
<DIV style="position:absolute;top:693;left:631"><nobr><span class="ft6"><i> Annual International ACM SIGIR </i></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:504"><nobr><span class="ft14"><i>Conference on Research and Development in Information <br>Retrieval (SIGIR'2000), </i>184-191. </span></nobr></DIV>
<DIV style="position:absolute;top:747;left:477"><nobr><span class="ft37">[17]</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:748;left:504"><nobr><span class="ft14">D. R. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z. <br>Zhang, W. Fan and J. Prager (2001). Mining the web for <br>answers to natural language questions. In <i>Proceeding of the <br>2001 </i>ACM <i>CIKM: Tenth International Conference on <br>Information and Knowledge Management</i>, 143-150  </span></nobr></DIV>
<DIV style="position:absolute;top:834;left:477"><nobr><span class="ft37">[18]</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:835;left:504"><nobr><span class="ft14">M. M. Soubbotin and S. M. Soubbotin (2002).  Patterns and <br>potential answer expressions as clues to the right answers.  In <br><i>Proceedings of the Tenth Text REtrieval Conference (TREC <br>2001).</i> </span></nobr></DIV>
<DIV style="position:absolute;top:904;left:477"><nobr><span class="ft37">[19]</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:905;left:504"><nobr><span class="ft12">E. Voorhees and D. Harman, Eds. (2001).  <i>Proceedings of <br>the Ninth Text REtrieval Conference (TREC-9).  </i>NIST <br>Special Publication 500-249. </span></nobr></DIV>
<DIV style="position:absolute;top:959;left:477"><nobr><span class="ft37">[20]</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:502"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:960;left:504"><nobr><span class="ft12">E. Voorhees and D. Harman, Eds. (2002).  <i>Proceedings of <br>the Tenth Text REtrieval Conference (TREC 2001). ).  </i>NIST <br>Special Publication 500-250.</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:81"><nobr><span class="ft0"> </span></nobr></DIV>
<DIV style="position:absolute;top:1122;left:449"><nobr><span class="ft0">298</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
