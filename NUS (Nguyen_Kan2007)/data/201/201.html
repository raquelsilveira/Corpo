<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - p65-Wang.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="jhwang">
<META name="date" content="2004-05-28T20:49:02+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:9px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:7px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft5{font-size:16px;font-family:Times;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:13px;font-family:Times;color:#000000;}
	.ft9{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft10{font-size:7px;font-family:Times;color:#000000;}
	.ft11{font-size:9px;font-family:Times;color:#000000;}
	.ft12{font-size:9px;font-family:Times;color:#000000;}
	.ft13{font-size:16px;font-family:Courier;color:#000000;}
	.ft14{font-size:11px;font-family:Times;color:#000000;}
	.ft15{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft16{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
	.ft17{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201001.png" alt="background image">
<DIV style="position:absolute;top:109;left:113"><nobr><span class="ft0"><b>Translating Unknown Cross-Lingual Queries in Digital </b></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:207"><nobr><span class="ft0"><b>Libraries Using a Web-based Approach   </b></span></nobr></DIV>
<DIV style="position:absolute;top:175;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:98"><nobr><span class="ft1">Jenq-Haur Wang</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:232"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:239"><nobr><span class="ft1">, Jei-Wen Teng</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:359"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:365"><nobr><span class="ft1">, Pu-Jen Cheng</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:488"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:495"><nobr><span class="ft1">, Wen-Hsiang Lu</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:626"><nobr><span class="ft2">2</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:633"><nobr><span class="ft1">, and Lee-Feng Chien</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:804"><nobr><span class="ft2">1,3</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:820"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:220;left:267"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:273"><nobr><span class="ft4"> Institute of Information Science, Academia Sinica, Taiwan </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:115"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:121"><nobr><span class="ft4"> Department of Computer Science and Information Engineering, National Cheng Kung University, Taiwan </span></nobr></DIV>
<DIV style="position:absolute;top:266;left:206"><nobr><span class="ft3">3</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:211"><nobr><span class="ft4"> Department of Information Management, National Taiwan University, Taiwan </span></nobr></DIV>
<DIV style="position:absolute;top:295;left:227"><nobr><span class="ft1">{jhwang, jackteng, pjcheng, whlu, lfchien}@iis.sinica.edu.tw </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:81"><nobr><span class="ft5"><b>ABSTRACT </b></span></nobr></DIV>
<DIV style="position:absolute;top:375;left:81"><nobr><span class="ft15">Users' cross-lingual queries to a digital library system might be <br>short and not included in a common translation dictionary <br>(unknown terms). In this paper, we investigate the feasibility of <br>exploiting the Web as the corpus source to translate unknown <br>query terms for cross-language information retrieval (CLIR) in <br>digital libraries. We propose a Web-based term translation <br>approach to determine effective translations for unknown query <br>terms by mining bilingual search-result pages obtained from a real <br>Web search engine. This approach can enhance the construction of <br>a domain-specific bilingual lexicon and benefit CLIR services in a <br>digital library that only has monolingual document collections. <br>Very promising results have been obtained in generating effective <br>translation equivalents for many unknown terms, including proper <br>nouns, technical terms and Web query terms. </span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft5"><b>Categories and Subject Descriptors </b></span></nobr></DIV>
<DIV style="position:absolute;top:622;left:81"><nobr><span class="ft15">H.3.3 [<b>Information Storage and Retrieval</b>]: Information Search <br>and Retrieval; H.3.7 [<b>Information Storage and Retrieval</b>]: <br>Digital Libraries.   </span></nobr></DIV>
<DIV style="position:absolute;top:678;left:81"><nobr><span class="ft5"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:197"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:698;left:81"><nobr><span class="ft6">Algorithms, Experimentation, Performance </span></nobr></DIV>
<DIV style="position:absolute;top:723;left:81"><nobr><span class="ft5"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:81"><nobr><span class="ft15">Cross-Language Information Retrieval, Digital Library, Term <br>Extraction, Term Translation, Web Mining </span></nobr></DIV>
<DIV style="position:absolute;top:784;left:81"><nobr><span class="ft5"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:784;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:784;left:113"><nobr><span class="ft5"><b>INTRODUCTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:804;left:81"><nobr><span class="ft15">With the development of digital library technologies, large <br>amounts of library content and cultural heritage material are being <br>digitized all over the world. As digital library systems become <br>commonly constructed and digitized content becomes widely <br>accessible on the Web, digital libraries that cross language and <br>regional boundaries will be in increasingly high demand globally. <br>Unfortunately, most of existing digital library systems only <br>provide monolingual content and search support in certain target </span></nobr></DIV>
<DIV style="position:absolute;top:350;left:477"><nobr><span class="ft15">languages. To facilitate a cross-language information retrieval <br>(CLIR) service in digital library systems, it is important to <br>develop a powerful query translation engine. This must be able to <br>automatically translate users' queries from multiple source <br>languages to the target languages that the systems accept. </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:477"><nobr><span class="ft15">Conventional approaches to CLIR incorporate parallel texts [16] <br>as the corpus. These texts contain bilingual sentences, from which <br>word or phrase translations can be extracted with appropriate <br>sentence alignment methods [7]. The basic assumption of such an <br>approach is that queries may be long so query expansion methods <br>can be used to enrich query terms not covered in parallel texts. <br>However, this approach presents some fundamental difficulties for <br>digital libraries that wish to support practical CLIR services. First, <br>since most existing digital libraries contain only monolingual text <br>collections, there is no bilingual corpus for cross-lingual training. <br>Second, real queries are often short, diverse and dynamic so that <br>only a subset of translations can be extracted through the corpora <br>in limited domains. How to efficiently construct a domain-specific <br>translation dictionary for each text collection has become a major <br>challenge for practical CLIR services in digital libraries. In this <br>paper, we propose a Web-based approach to deal with this <br>problem. We intend to exploit the Web as the corpus to find <br>effective translations automatically for query terms not included <br>in a dictionary (unknown terms). Besides, to speedup online <br>translation process of unknown terms, we extract possible key <br>terms from the document set in digital libraries and try to obtain <br>their translations in advance. </span></nobr></DIV>
<DIV style="position:absolute;top:787;left:477"><nobr><span class="ft15">For some language pairs, such as Chinese and English, as well as <br>Japanese and English, the Web offers rich texts in a mixture of <br>languages. Many of them contain bilingual translations of proper <br>nouns, such as company names and personal names. We want to <br>realize if this positive characteristic makes it possible to <br>automatically extract bilingual translations of a large number of <br>query terms. Real search engines, such as Google</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:747"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:751"><nobr><span class="ft6"> and AltaVista</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:833"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:837"><nobr><span class="ft6">, </span></nobr></DIV>
<DIV style="position:absolute;top:896;left:477"><nobr><span class="ft15">allow us to search English terms for pages in a certain language, <br>e.g. Chinese or Japanese. This has motivated us to develop the <br>proposed approach for mining bilingual search-result pages, <br>which are normally returned in a long, ordered list of snippets of <br>summaries to help users locate interesting documents. The <br>proposed approach uses the bilingual search-result pages of <br>unknown queries as the corpus for extracting translations by <br>utilizing the following useful techniques: (1) Term extraction </span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:477"><nobr><span class="ft8">                                                           </span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:477"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:481"><nobr><span class="ft6"> http://www.google.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:477"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:481"><nobr><span class="ft6"> http://www.altavista.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:937;left:92"><nobr><span class="ft17"> <br> <br>Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies <br>are not made or distributed for profit or commercial advantage and that <br>copies bear this notice and the full citation on the first page. To copy <br>otherwise, or republish, to post on servers or to redistribute to lists, <br>requires prior specific permission and/or a fee. <br>JCDL'04, June 7­11, 2004, Tucson, Arizona, USA. <br>Copyright 2004 ACM 1-58113-832-6/04/0006...$5.00.<b><i> <br></b></i> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">108</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:16px;line-height:20px;font-family:Times;color:#000000;}
	.ft19{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft20{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft21{font-size:13px;line-height:17px;font-family:Times;color:#000000;}
	.ft22{font-size:13px;line-height:22px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201002.png" alt="background image">
<DIV style="position:absolute;top:108;left:81"><nobr><span class="ft15">methods that extract translation candidates with correct lexical <br>boundaries. (2) Term translation methods that determine correct <br>translations based on co-occurrence and context similarity <br>analysis.  </span></nobr></DIV>
<DIV style="position:absolute;top:179;left:81"><nobr><span class="ft15">Several preliminary experiments have been conducted to test the <br>performance of the proposed approach. For example, very <br>promising translation accuracy has been obtained in generating <br>effective translation equivalents for many unknown terms, <br>including proper nouns, technical terms and Web query terms. <br>Also, it has been shown that the approach can enhance bilingual <br>lexicon construction in a very efficient manner and thereby benefit <br>CLIR services in digital libraries that only have monolingual <br>document collections. In Section 2 of this paper, we examine the <br>possibility of using search-result pages for term translation. The <br>technical details of the proposed approach, including the term <br>extraction and term translation methods, are presented with some <br>experiments in Sections 3 and 4 respectively. An application of <br>the proposed approach to bilingual lexicon construction is <br>described in Section 5. Finally, in Section 6 we list our <br>conclusions.   </span></nobr></DIV>
<DIV style="position:absolute;top:437;left:81"><nobr><span class="ft5"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:113"><nobr><span class="ft18"><b>OBSERVATIONS AND THE <br>PROPOSED APPROACH </b></span></nobr></DIV>
<DIV style="position:absolute;top:478;left:81"><nobr><span class="ft20">A large number of Web pages contain a mixture of multiple <br>languages. For example, Chinese pages on the Web consist of rich <br>texts in a mixture of Chinese (main language) and English <br>(auxiliary language), many of which contain translations of proper <br>nouns and foreign terms. In fact, in the Chinese writing style, the <br>first time a foreign term appears in the text, we might also write <br>its original word, e.g., "" (Yahoo). In our research, we are <br>seeking to determine if the percentage of correct translations for <br>real queries is high enough in the top search-result pages. If this is <br>the case, search-result-based methods can be useful in alleviating <br>the difficulty of term translation. According to our observations, <br>many query terms are very likely to appear simultaneously with <br>their translations in search-result pages. Figure 1 illustrates the <br>search-result page of the English query "National Palace <br>Museum", which was submitted to Google to search Chinese <br>pages. Many relevant results were obtained, including both the <br>query itself and its Chinese aliases, such as "" <br>(National Palace Museum), "" (an abbreviation of National <br>Palace Museum) and "" (Palace Museum), which <br>might not be covered in general-purpose translation dictionaries. </span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:431"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:82"><nobr><span class="ft7"><b>Figure 1. An illustration showing translation equivalents, such </b></span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:97"><nobr><span class="ft7"><b>as National Palace Museum/"</b><b>" ("</b><b>"), </b></span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:102"><nobr><span class="ft7"><b>which co-occur in search results returned from Google. </b></span></nobr></DIV>
<DIV style="position:absolute;top:108;left:477"><nobr><span class="ft15">Although search-result pages might contain translations, the <br>difficulties in developing a high-performance search-result-based <br>term translation approach still remain. For example, it is not <br>straightforward to extract translation candidates with correct <br>lexical boundaries and minimum noisy terms from a text. It is also <br>challenging to find correct translations for each unknown term <br>within an acceptable number of search-result pages and an <br>acceptable amount of network access time. To deal with these <br>problems, the proposed approach contains three major modules: <br>search-result collection, term extraction and term translation, as <br>shown in Figure 2 (a). In the search-result collection module, a <br>given source query (unknown term) is submitted to a real-world <br>search engine to collect top search-result pages. In the term <br>extraction module, translation candidates are extracted from the <br>collected search-result pages using the term extraction method. <br>Finally, the term translation module is used to determine the most <br>promising translations based on the similarity estimation between <br>source queries and target translations.   </span></nobr></DIV>
<DIV style="position:absolute;top:397;left:477"><nobr><span class="ft15">In fact there are two scenarios to which the proposed approach can <br>be applied. Except online translation of unknown queries, another <br>application is offline translation of key terms as in Figure 2 (b). <br>To reduce unnecessary online translation processes, the proposed <br>approach can be used to augment the bilingual lexicon via <br>translating key terms extracted from the document set in a digital <br>library. These extracted key terms are likely to be similar to terms <br>that users may use in real user queries. The proposed approach <br>can be applied to those unknown key terms to obtain their <br>translations with an offline batch process (the extracted <br>translations might be edited by indexers). Furthermore, the <br>constructed bilingual lexicon can be incrementally updated with <br>the input of unknown queries from users and the performing of <br>online translation processes. To facilitate the above scenarios the <br>proposed term extraction and term translation techniques are <br>required, which will be further described in the following sections.   </span></nobr></DIV>
<DIV style="position:absolute;top:953;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:974;left:482"><nobr><span class="ft7"><b>Figure 2. (a) An abstract diagram showing the concept of the </b></span></nobr></DIV>
<DIV style="position:absolute;top:990;left:490"><nobr><span class="ft7"><b>proposed approach for translating an unknown query. (b) </b></span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:486"><nobr><span class="ft7"><b>Two application scenarios of the proposed Web-based term </b></span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:483"><nobr><span class="ft7"><b>translation approach: online translation of unknown queries </b></span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:500"><nobr><span class="ft7"><b>and offline translation of key terms extracted from the </b></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:617"><nobr><span class="ft7"><b>document set. </b></span></nobr></DIV>
<DIV style="position:absolute;top:730;left:536"><nobr><span class="ft6">Search- </span></nobr></DIV>
<DIV style="position:absolute;top:746;left:539"><nobr><span class="ft6">Result </span></nobr></DIV>
<DIV style="position:absolute;top:762;left:531"><nobr><span class="ft6">Collection </span></nobr></DIV>
<DIV style="position:absolute;top:738;left:617"><nobr><span class="ft6">Search- </span></nobr></DIV>
<DIV style="position:absolute;top:754;left:620"><nobr><span class="ft6">Result </span></nobr></DIV>
<DIV style="position:absolute;top:770;left:621"><nobr><span class="ft6">Pages </span></nobr></DIV>
<DIV style="position:absolute;top:745;left:701"><nobr><span class="ft15">Translation <br>Candidates</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:712"><nobr><span class="ft6">Term  </span></nobr></DIV>
<DIV style="position:absolute;top:681;left:703"><nobr><span class="ft6">Extraction </span></nobr></DIV>
<DIV style="position:absolute;top:816;left:712"><nobr><span class="ft6">Term </span></nobr></DIV>
<DIV style="position:absolute;top:832;left:701"><nobr><span class="ft6">Translation </span></nobr></DIV>
<DIV style="position:absolute;top:659;left:541"><nobr><span class="ft6">Search </span></nobr></DIV>
<DIV style="position:absolute;top:675;left:540"><nobr><span class="ft6">Engine </span></nobr></DIV>
<DIV style="position:absolute;top:722;left:481"><nobr><span class="ft8">Source </span></nobr></DIV>
<DIV style="position:absolute;top:740;left:482"><nobr><span class="ft8">Query</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:784"><nobr><span class="ft8">Target </span></nobr></DIV>
<DIV style="position:absolute;top:737;left:771"><nobr><span class="ft8">Translations</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:485"><nobr><span class="ft8">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:485"><nobr><span class="ft8">(b)</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:619"><nobr><span class="ft6">Proposed </span></nobr></DIV>
<DIV style="position:absolute;top:912;left:618"><nobr><span class="ft6">Approach  </span></nobr></DIV>
<DIV style="position:absolute;top:875;left:513"><nobr><span class="ft6">Q </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:515"><nobr><span class="ft8">Doc</span></nobr></DIV>
<DIV style="position:absolute;top:884;left:706"><nobr><span class="ft21">Online Translation of <br>Unknown Queries <br>Offline Translation of <br>Key Terms </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">109</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft23{font-size:18px;font-family:Times;color:#000000;}
	.ft24{font-size:6px;font-family:Times;color:#000000;}
	.ft25{font-size:12px;font-family:Times;color:#000000;}
	.ft26{font-size:5px;font-family:Times;color:#000000;}
	.ft27{font-size:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201003.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft5"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:108;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:109;left:113"><nobr><span class="ft5"><b>TERM EXTRACTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:129;left:81"><nobr><span class="ft15">The first challenge of the proposed approach is: how to efficiently <br>and effectively extract translation candidates for an unknown <br>source term from a set of search-result pages. Other challenging <br>issues include: whether all possible translations can be extracted <br>and whether their lexical boundaries can be correctly segmented. <br>Conventionally, there are two types of term extraction methods <br>that can be employed. The first is the language-dependent <br>linguistics-based method that relies on lexical analysis, word <br>segmentation and syntactic analysis to extract named entities from <br>documents. The second type is the language-independent <br>statistics-based method that extracts significant lexical patterns <br>without length limitation, such as the local maxima method [19] <br>and the PAT-tree-based method [3]. Considering the diverse <br>applications in digital library and Web environments, we have <br>adopted the second approach. Our proposed term extraction <br>method, i.e., the PAT-tree-based local maxima method, is a hybrid <br>of the local maxima method [19] and the PAT-tree-based method <br>[3], which has been found more efficient and effective. First, we <br>construct a PAT tree data structure for the corpus, in this case, a <br>set of search-result pages retrieved using the source term as query. <br>(The same term extraction method will be applied to extract key <br>terms from digital libraries in Section 5 where the corpus is the <br>documents in digital libraries). By utilizing the PAT tree, we can <br>efficiently calculate the association measurement of every <br>character or word n-gram in the corpus and apply the local <br>maxima algorithm to extract the terms. The association <br>measurement is determined not only by the symmetric conditional <br>probability [19] but also by the context independency ratio [3] of <br>the  n-gram. We detail the proposed method in the following <br>subsections. </span></nobr></DIV>
<DIV style="position:absolute;top:604;left:81"><nobr><span class="ft5"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:604;left:103"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:604;left:123"><nobr><span class="ft5"><b>Association Measurement </b></span></nobr></DIV>
<DIV style="position:absolute;top:624;left:81"><nobr><span class="ft15">The proposed association measurement, called SCPCD, combines <br>the symmetric conditional probability (SCP) [19] with the concept <br>of context dependency (CD) [3]. SCP is the association estimation <br>of the correlation between its composed sub n-grams, which is as <br>defined below: </span></nobr></DIV>
<DIV style="position:absolute;top:740;left:81"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:787;left:227"><nobr><span class="ft23"></span></nobr></DIV>
<DIV style="position:absolute;top:735;left:227"><nobr><span class="ft23"></span></nobr></DIV>
<DIV style="position:absolute;top:788;left:248"><nobr><span class="ft24">-</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:246"><nobr><span class="ft24">=</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:368"><nobr><span class="ft24">+</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:248"><nobr><span class="ft24">-</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:246"><nobr><span class="ft24">=</span></nobr></DIV>
<DIV style="position:absolute;top:748;left:341"><nobr><span class="ft24">+</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:210"><nobr><span class="ft6">-</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:186"><nobr><span class="ft6">=</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:210"><nobr><span class="ft6">-</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:186"><nobr><span class="ft6">=</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:252"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:250"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:372"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:294"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:336"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:302"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:252"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:250"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:345"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:277"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:319"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:280"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:143"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:401"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:377"><nobr><span class="ft6">...</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:351"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:320"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:298"><nobr><span class="ft6">...</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:281"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:219"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:209"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:331"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:306"><nobr><span class="ft6">...</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:289"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:379"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:324"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:309"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:264"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:219"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:209"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:314"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:267"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:177"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:130"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:243"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:242"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:396"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:365"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:317"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:325"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:243"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:242"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:374"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:337"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:305"><nobr><span class="ft24">i</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:309"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:172"><nobr><span class="ft24">n</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:387"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:356"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:328"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:308"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:286"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:793;left:258"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:200"><nobr><span class="ft6">n</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:316"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:294"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:266"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:365"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:329"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:316"><nobr><span class="ft6">p</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:297"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:269"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:257"><nobr><span class="ft6">p</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:200"><nobr><span class="ft6">n</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:300"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:272"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:260"><nobr><span class="ft6">p</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:163"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:136"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:105"><nobr><span class="ft6">SCP</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:350"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:282"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:285"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:148"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:410"><nobr><span class="ft8">   (1) </span></nobr></DIV>
<DIV style="position:absolute;top:820;left:81"><nobr><span class="ft6">where  w</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:129"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:134"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:147"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:156"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:161"><nobr><span class="ft6"> is the n-gram to be estimated, p(w</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:367"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:818;left:372"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:385"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:394"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:399"><nobr><span class="ft6">) is the </span></nobr></DIV>
<DIV style="position:absolute;top:837;left:81"><nobr><span class="ft6">probability of the occurrence of the n-gram  w</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:376"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:381"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:394"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:403"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:408"><nobr><span class="ft6">, and </span></nobr></DIV>
<DIV style="position:absolute;top:852;left:81"><nobr><span class="ft6">freq(w</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:116"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:121"><nobr><span class="ft6">...w</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:141"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:146"><nobr><span class="ft6">) is the frequency of the n-gram. </span></nobr></DIV>
<DIV style="position:absolute;top:877;left:81"><nobr><span class="ft15">To a certain degree, SCP  can measure the cohesion holding the <br>words together within a word n-gram, but it cannot determine the <br>lexical boundaries of the n-gram. An n-gram with complete <br>lexical boundaries implies that it tends to have free association <br>with other n-grams appearing in the same context. Therefore, to <br>further ensure that an n-gram has complete lexical boundaries, the <br>concept of context dependency is introduced. Moreover, we <br>consolidate the concept with SCP to form one association <br>measurement. In order to achieve this goal, a refined measure, the <br>context independency ratio - which is a ratio value between 0 and <br>1 - is extended from [3]. It is defined as follows: </span></nobr></DIV>
<DIV style="position:absolute;top:112;left:477"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:129;left:736"><nobr><span class="ft26">2</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:692"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:730"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:654"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:564"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:730"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:678"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:768"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:716"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:692"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:640"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:602"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:550"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:723"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:761"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:686"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:595"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:714"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:683"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:655"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:752"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:721"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:697"><nobr><span class="ft6">RC</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:676"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:645"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:622"><nobr><span class="ft6">LC</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:586"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:555"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:530"><nobr><span class="ft6">CD</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:698"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:737"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:661"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:570"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:610"><nobr><span class="ft6">=</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:776"><nobr><span class="ft8">  </span></nobr></DIV>
<DIV style="position:absolute;top:112;left:824"><nobr><span class="ft8">(2) </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:477"><nobr><span class="ft6">where LC(w</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:544"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:548"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:562"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:571"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:575"><nobr><span class="ft6">) is the number of unique left adjacent words in </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft19">western languages, or characters in oriental languages, for the <br>n-gram in the corpus, or is equal to the frequency of the n-gram if <br>there is no left adjacent word/character. Similarly, RC(w</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:788"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:793"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:806"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:815"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:820"><nobr><span class="ft6">) is </span></nobr></DIV>
<DIV style="position:absolute;top:212;left:477"><nobr><span class="ft19">the number of unique right adjacent words/characters for the <br>n-gram, or is equal to the frequency of the n-gram if there is no <br>right adjacent word/character. Using this ratio we are able to judge <br>whether the appearance of an n-gram is dependent on a certain <br>string containing it. For example, if w</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:687"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:691"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:705"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:714"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:718"><nobr><span class="ft6"> is always a substring </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:477"><nobr><span class="ft6">of string xw</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:541"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:545"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:559"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:567"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:572"><nobr><span class="ft6">y in the corpus, then CD(w</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:717"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:721"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:735"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:744"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:748"><nobr><span class="ft6">) is close to 0. </span></nobr></DIV>
<DIV style="position:absolute;top:316;left:477"><nobr><span class="ft15">Combining formulae (1) and (2), the proposed association <br>measure SCPCD is as follows </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:477"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:393;left:633"><nobr><span class="ft27"></span></nobr></DIV>
<DIV style="position:absolute;top:394;left:652"><nobr><span class="ft26">-</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:650"><nobr><span class="ft26">=</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:772"><nobr><span class="ft26">+</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:616"><nobr><span class="ft6">-</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:594"><nobr><span class="ft6">=</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:681"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:355;left:594"><nobr><span class="ft6">=</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:656"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:654"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:777"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:696"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:740"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:671"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:720"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:641"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:543"><nobr><span class="ft26">1</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:810"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:757"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:728"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:684"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:624"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:614"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:774"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:728"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:705"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:659"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:753"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:707"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:674"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:628"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:577"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:531"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:647"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:647"><nobr><span class="ft26">i</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:805"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:770"><nobr><span class="ft26">i</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:724"><nobr><span class="ft26">i</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:769"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:700"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:748"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:669"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:572"><nobr><span class="ft26">n</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:797"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:762"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:735"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:716"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:688"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:662"><nobr><span class="ft6">freq</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:607"><nobr><span class="ft6">n</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:761"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:733"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:710"><nobr><span class="ft6">RC</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:692"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:664"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:642"><nobr><span class="ft6">LC</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:740"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:712"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:689"><nobr><span class="ft6">CD</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:661"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:633"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:605"><nobr><span class="ft6">SCP</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:563"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:536"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:488"><nobr><span class="ft6">SCPCD</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:782"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:701"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:746"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:677"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:725"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:646"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:549"><nobr><span class="ft6">K</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:818"><nobr><span class="ft8"> (3) </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:477"><nobr><span class="ft15">Note that the difference between the formulae of SCPCD and SCP <br>is in their numerator items. For SCP, those n-grams with low <br>frequency tend to be discarded, which is prevented in the case of <br>SCPCD. The proposed new measure determines a highly cohesive <br>term because of the frequencies of its substrings and the number <br>of its unique left and right adjacent words/characters. </span></nobr></DIV>
<DIV style="position:absolute;top:534;left:477"><nobr><span class="ft5"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:519"><nobr><span class="ft5"><b>Local Maxima Algorithm </b></span></nobr></DIV>
<DIV style="position:absolute;top:555;left:477"><nobr><span class="ft15">The local maxima algorithm, called LocalMaxs in [18], is based <br>on the idea that each n-gram has a kind of cohesion that holds the <br>words together within the n-gram. This is a heuristic algorithm <br>used to combine with the previous association measurements to <br>extract n-grams, which are supposed to be key terms from the text. <br>We know different n-grams usually have different cohesion values. <br>Given that: </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:487"><nobr><span class="ft11">·</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:493"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:504"><nobr><span class="ft6">An  antecedent (in size)  of  the  n-gram  w</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:789"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:794"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:803"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:807"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:821"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:830"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:834"><nobr><span class="ft6">, </span></nobr></DIV>
<DIV style="position:absolute;top:690;left:498"><nobr><span class="ft6">ant(w</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:529"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:533"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:547"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:556"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:560"><nobr><span class="ft6">),  is  a  sub-n-gram of the n-gram  w</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:760"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:765"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:778"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:787"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:792"><nobr><span class="ft6">, having </span></nobr></DIV>
<DIV style="position:absolute;top:706;left:498"><nobr><span class="ft6">size n - 1. i.e., the (n-1)-gram w</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:669"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:673"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:687"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:696"><nobr><span class="ft10">n-1 </span></nobr></DIV>
<DIV style="position:absolute;top:706;left:710"><nobr><span class="ft6">or w</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:734"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:738"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:752"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:761"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:765"><nobr><span class="ft6">. </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:487"><nobr><span class="ft11">·</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:493"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:504"><nobr><span class="ft6">A successor (in size) of the n-gram w</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:708"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:713"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:722"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:726"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:740"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:749"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:753"><nobr><span class="ft6">, succ(w</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:798"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:802"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:816"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:825"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:829"><nobr><span class="ft6">), </span></nobr></DIV>
<DIV style="position:absolute;top:744;left:498"><nobr><span class="ft6">is a (n+1)-gram N such that the n-gram w</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:732"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:736"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:750"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:759"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:763"><nobr><span class="ft6"> is an ant(N). </span></nobr></DIV>
<DIV style="position:absolute;top:761;left:498"><nobr><span class="ft6">i.e.,  succ(w</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:566"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:570"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:584"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:593"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:597"><nobr><span class="ft6">)  contains the n-gram  w</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:753"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:757"><nobr><span class="ft25">...</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:771"><nobr><span class="ft6">w</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:780"><nobr><span class="ft10">n</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:784"><nobr><span class="ft6"> and an </span></nobr></DIV>
<DIV style="position:absolute;top:776;left:498"><nobr><span class="ft6">additional word before (on the left) or after (on the right) it. </span></nobr></DIV>
<DIV style="position:absolute;top:796;left:477"><nobr><span class="ft15">The local maxima algorithm extracts each term whose cohesion, <br>i.e. association measure, is local maxima. That is, the term whose <br>association measure is greater than, or equal to, the association <br>measures of its antecedents and is greater than the association <br>measures of its successors. </span></nobr></DIV>
<DIV style="position:absolute;top:883;left:477"><nobr><span class="ft5"><b>3.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:883;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:883;left:519"><nobr><span class="ft18"><b>The PAT-Tree Based Local Maxima <br>Algorithm </b></span></nobr></DIV>
<DIV style="position:absolute;top:924;left:477"><nobr><span class="ft15">Despite the usefulness of the local maxima algorithm, without a <br>suitable data structure the time complexity of the algorithm is high. <br>The main time complexity problems occur in two areas. One is <br>calculating the context independency ratio (CD) for each unique <br>n-gram in the corpus and the other is to find the successor of an <br>n-gram. The two problems can be treated as one, i.e. finding the <br>successors of an n-gram. An intuitive way to do this is to find out <br>all  (n+1)-grams and then compare the n-gram with them <br>sequentially to see if they are the successors of it. As this is <br>time-consuming, we introduce PAT tree as the data structure. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">110</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft28{font-size:11px;font-family:Times;color:#ff0000;}
	.ft29{font-size:11px;line-height:18px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201004.png" alt="background image">
<DIV style="position:absolute;top:108;left:81"><nobr><span class="ft15">The above method is time consuming, however, so we use the <br>PAT tree, which is a more efficient data structure. It was <br>developed by Gonnet [8] from Morrison's PATRICIA algorithm <br>(Practical Algorithm to Retrieve Information Coded in <br>Alphanumeric) [15] for indexing a continuous data stream and <br>locating every possible position of a prefix in the stream. The <br>PAT tree structure is conceptually equivalent to a compressed <br>digital search tree, but smaller. The superior feature of this <br>structure mostly resulted from its use of semi-infinite strings [14] <br>to store the substream values in the nodes of the tree. This also <br>makes it easier and more efficient to find the successors of an <br>n-gram. More details on the PAT tree can be found in [3].   </span></nobr></DIV>
<DIV style="position:absolute;top:304;left:81"><nobr><span class="ft15">By utilizing the constructed PAT tree as the corpus, we can <br>efficiently retrieve all n-grams from the corpus, obtain their <br>frequencies and context dependency values, and then calculate the <br>association measures, SCPCD, of all of them. </span></nobr></DIV>
<DIV style="position:absolute;top:375;left:81"><nobr><span class="ft5"><b>3.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:375;left:103"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:375;left:123"><nobr><span class="ft5"><b>Experiments on Term Extraction </b></span></nobr></DIV>
<DIV style="position:absolute;top:395;left:81"><nobr><span class="ft15">To determine the effectiveness of the proposed association <br>measure SCPCD and the efficiency of the PAT-tree data structure, <br>we conducted several experiments on Web search-result pages <br>using the proposed PAT-tree-based local maxima algorithm. </span></nobr></DIV>
<DIV style="position:absolute;top:466;left:81"><nobr><span class="ft15">First, to test whether SCPCD can perform better than SCP and CD, <br>we randomly selected 50 real queries in English from a Chinese <br>search engine called Openfind</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:248"><nobr><span class="ft10">3</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:253"><nobr><span class="ft6">. We then submitted each of them </span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft15">to Google to search Chinese result pages. Most of these query <br>terms such as proper nouns and technical terms were not covered <br>in the common translation dictionary. After using the term <br>extraction method, the top 30 extracted</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:322"><nobr><span class="ft28"> </span></nobr></DIV>
<DIV style="position:absolute;top:560;left:332"><nobr><span class="ft6">Chinese translation </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:81"><nobr><span class="ft15">candidates were examined and the extraction accuracy of each <br>candidate to the source query was manually determined. We <br>applied this test mainly to determine whether the SCPCD <br>measurement can extract more relevant translation candidates and <br>segment them with correct lexical boundaries. A translation <br>candidate was taken as correctly extracted only if it was correctly <br>segmented and contained meanings relevant to the source term. A <br>relevant translation candidate was not necessarily a correct <br>translation. The whole relevant set was determined by examining <br>the terms extracted by all of the test methods, e.g., CD, SCP, and <br>SCPCD. Table 1 clearly shows that the method based on the <br>SCPCD measurement achieves the best performance.   </span></nobr></DIV>
<DIV style="position:absolute;top:770;left:81"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:786;left:81"><nobr><span class="ft7"><b>Table 1. The obtained extraction accuracy including precision, </b></span></nobr></DIV>
<DIV style="position:absolute;top:802;left:109"><nobr><span class="ft7"><b>recall, and average recall-precision of auto-extracted </b></span></nobr></DIV>
<DIV style="position:absolute;top:817;left:126"><nobr><span class="ft7"><b>translation candidates using different methods. </b></span></nobr></DIV>
<DIV style="position:absolute;top:848;left:101"><nobr><span class="ft7"><b>Association Measure </b></span></nobr></DIV>
<DIV style="position:absolute;top:848;left:243"><nobr><span class="ft7"><b>Precision </b></span></nobr></DIV>
<DIV style="position:absolute;top:848;left:319"><nobr><span class="ft7"><b>Recall </b></span></nobr></DIV>
<DIV style="position:absolute;top:848;left:378"><nobr><span class="ft7"><b>Avg. R-P</b></span></nobr></DIV>
<DIV style="position:absolute;top:873;left:152"><nobr><span class="ft6">CD </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:250"><nobr><span class="ft6">68.1 % </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:321"><nobr><span class="ft6">5.9 % </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:384"><nobr><span class="ft6">37.0 %</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:149"><nobr><span class="ft6">SCP </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:250"><nobr><span class="ft6">62.6 % </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:318"><nobr><span class="ft6">63.3 % </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:384"><nobr><span class="ft6">63.0 %</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:140"><nobr><span class="ft6">SCPCD </span></nobr></DIV>
<DIV style="position:absolute;top:915;left:250"><nobr><span class="ft6">79.3 % </span></nobr></DIV>
<DIV style="position:absolute;top:915;left:318"><nobr><span class="ft6">78.2 % </span></nobr></DIV>
<DIV style="position:absolute;top:915;left:384"><nobr><span class="ft6">78.7 %</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:98"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:952;left:81"><nobr><span class="ft15">In order to determine the efficiency of the PAT-tree data structure, <br>we compared the speed performance of the local maxima method <br>and the PAT-tree-based local maxima method. As Table 2 shows, <br>the PAT-tree data structure is more efficient in term extraction. <br>Although the PAT-tree construction phase took a little more time </span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:81"><nobr><span class="ft8">                                                           </span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:81"><nobr><span class="ft10">3</span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:85"><nobr><span class="ft6"> http://www.openfind.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:477"><nobr><span class="ft15">in a small corpus, in a real-world case for a large corpus - where <br>1,367 and 5,357 scientific documents were tested (refer to Section <br>5.2 for the details)</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:584"><nobr><span class="ft28"> </span></nobr></DIV>
<DIV style="position:absolute;top:139;left:590"><nobr><span class="ft6">- the PAT-tree-based local maxima method </span></nobr></DIV>
<DIV style="position:absolute;top:155;left:477"><nobr><span class="ft6">performed much better than the local maxima method. </span></nobr></DIV>
<DIV style="position:absolute;top:179;left:477"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:195;left:479"><nobr><span class="ft7"><b>Table 2. The obtained average speed performance of different </b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:584"><nobr><span class="ft7"><b>term extraction methods. </b></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:499"><nobr><span class="ft7"><b>Term Extraction Method </b></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:681"><nobr><span class="ft7"><b>Time for </b></span></nobr></DIV>
<DIV style="position:absolute;top:253;left:665"><nobr><span class="ft7"><b>Preprocessing</b></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:766"><nobr><span class="ft7"><b>Time for </b></span></nobr></DIV>
<DIV style="position:absolute;top:253;left:760"><nobr><span class="ft7"><b>Extraction</b></span></nobr></DIV>
<DIV style="position:absolute;top:273;left:500"><nobr><span class="ft6">LocalMaxs (Web Queries) </span></nobr></DIV>
<DIV style="position:absolute;top:273;left:690"><nobr><span class="ft6">0.87 s </span></nobr></DIV>
<DIV style="position:absolute;top:273;left:775"><nobr><span class="ft6">0.99 s </span></nobr></DIV>
<DIV style="position:absolute;top:293;left:516"><nobr><span class="ft6">PATtree+LocalMaxs  </span></nobr></DIV>
<DIV style="position:absolute;top:308;left:532"><nobr><span class="ft6">(Web Queries) </span></nobr></DIV>
<DIV style="position:absolute;top:300;left:690"><nobr><span class="ft6">2.30 s </span></nobr></DIV>
<DIV style="position:absolute;top:300;left:775"><nobr><span class="ft6">0.61 s </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:506"><nobr><span class="ft6">LocalMaxs (1,367 docs) </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:686"><nobr><span class="ft6">63.47 s </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:763"><nobr><span class="ft6">4,851.67 s</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:516"><nobr><span class="ft6">PATtree+LocalMaxs  </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:538"><nobr><span class="ft6">(1,367 docs) </span></nobr></DIV>
<DIV style="position:absolute;top:354;left:683"><nobr><span class="ft6">840.90 s </span></nobr></DIV>
<DIV style="position:absolute;top:354;left:772"><nobr><span class="ft6">71.24 s </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:506"><nobr><span class="ft6">LocalMaxs (5,357 docs) </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:674"><nobr><span class="ft6">47,247.55 s </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:757"><nobr><span class="ft6">350,495.65 s</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:516"><nobr><span class="ft6">PATtree+LocalMaxs  </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:538"><nobr><span class="ft6">(5,357 docs) </span></nobr></DIV>
<DIV style="position:absolute;top:408;left:675"><nobr><span class="ft6">11,086.67 s </span></nobr></DIV>
<DIV style="position:absolute;top:408;left:768"><nobr><span class="ft6">759.32 s </span></nobr></DIV>
<DIV style="position:absolute;top:434;left:477"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:459;left:477"><nobr><span class="ft5"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:459;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:459;left:509"><nobr><span class="ft5"><b>TERM TRANSLATION   </b></span></nobr></DIV>
<DIV style="position:absolute;top:479;left:477"><nobr><span class="ft15">In the term translation module, we utilize the co-occurrence <br>relation and the context information between source queries and <br>target translations to estimate their semantic similarity and <br>determine the most promising translations. Several similarity <br>estimation methods were investigated based on co-occurrence <br>analysis. These included mutual information, DICE coefficient, <br>and statistical tests including the chi-square test and the <br>log-likelihood ratio test [17, 20], where the chi-square test and the <br>context vector analysis achieved the best performance. These will <br>be introduced below.   </span></nobr></DIV>
<DIV style="position:absolute;top:644;left:477"><nobr><span class="ft5"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:519"><nobr><span class="ft5"><b>The Chi-Square Test </b></span></nobr></DIV>
<DIV style="position:absolute;top:664;left:477"><nobr><span class="ft6">The chi-square test (</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:604"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:609"><nobr><span class="ft6">) was adopted as the major method of </span></nobr></DIV>
<DIV style="position:absolute;top:680;left:477"><nobr><span class="ft15">co-occurrence analysis in our study. One major reason is that the <br>required parameters for the chi-square test can be effectively <br>computed using the search-result pages, which alleviates the data <br>sparseness problem. It also makes good use of all relations of <br>co-occurrence between the source and target terms, especially the <br>information that they do not co-occur. For source term s and target <br>term t, the conventional chi-square test can be transformed as the <br>similarity measure defined below [6]: </span></nobr></DIV>
<DIV style="position:absolute;top:823;left:477"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:838;left:777"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:742"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:725"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:689"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:671"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:637"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:620"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:585"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:728"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:652"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:565"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:557"><nobr><span class="ft8"> ,</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:543"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:815;left:734"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:536"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:769"><nobr><span class="ft8">d</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:747"><nobr><span class="ft8">c</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:716"><nobr><span class="ft8">d</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:694"><nobr><span class="ft8">b</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:665"><nobr><span class="ft8">c</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:643"><nobr><span class="ft8">a</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:612"><nobr><span class="ft8">b</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:590"><nobr><span class="ft8">a</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:721"><nobr><span class="ft8">c</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:701"><nobr><span class="ft8">b</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:678"><nobr><span class="ft8">d</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:657"><nobr><span class="ft8">a</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:628"><nobr><span class="ft8">N</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:560"><nobr><span class="ft8">t</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:548"><nobr><span class="ft8">s</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:521"><nobr><span class="ft8">S</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:757"><nobr><span class="ft8">+</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:732"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:704"><nobr><span class="ft8">+</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:679"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:653"><nobr><span class="ft8">+</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:627"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:601"><nobr><span class="ft8">+</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:710"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:690"><nobr><span class="ft8">-</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:667"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:642"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:573"><nobr><span class="ft8">=</span></nobr></DIV>
<DIV style="position:absolute;top:834;left:530"><nobr><span class="ft10"></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:785"><nobr><span class="ft8">  </span></nobr></DIV>
<DIV style="position:absolute;top:823;left:824"><nobr><span class="ft8">(4) </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:494"><nobr><span class="ft8">w</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:505"><nobr><span class="ft6">here </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:511"><nobr><span class="ft29">a: the number of pages containing both terms s and t; <br>b: the number of pages containing term s but not t; <br>c: the number of pages containing term t but not s; <br>d: the number of pages containing neither term s nor t; <br>N: the total number of pages, i.e., N= a+b+c+d. </span></nobr></DIV>
<DIV style="position:absolute;top:978;left:477"><nobr><span class="ft15">Since most search engines accept Boolean queries and can report <br>the number of pages matched, the required parameters for the <br>chi-square test can be obtained by submitting Boolean queries <br>such as `st', `~st', `s~t' to search engines and utilizing the <br>returned page counts. On the other hand, it is easy to get number <br>N using some search engines (e.g., Google), which indicates the </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">111</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft30{font-size:4px;font-family:Times;color:#000000;}
	.ft31{font-size:14px;font-family:Times;color:#000000;}
	.ft32{font-size:22px;font-family:Times;color:#000000;}
	.ft33{font-size:23px;font-family:Times;color:#000000;}
	.ft34{font-size:15px;font-family:Times;color:#000000;}
	.ft35{font-size:8px;font-family:Times;color:#000000;}
	.ft36{font-size:16px;font-family:Times;color:#000000;}
	.ft37{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft38{font-size:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201005.png" alt="background image">
<DIV style="position:absolute;top:108;left:81"><nobr><span class="ft15">total number of their collected Web pages. The number d may not <br>be directly available from the search engine, but it can be <br>calculated using the formula N= a+b+c+d, i.e., d = N-a-b-c. </span></nobr></DIV>
<DIV style="position:absolute;top:164;left:81"><nobr><span class="ft5"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:103"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:123"><nobr><span class="ft5"><b>Context Vector Analysis   </b></span></nobr></DIV>
<DIV style="position:absolute;top:184;left:81"><nobr><span class="ft15">Co-occurrence analysis is applicable to higher frequency terms <br>since they are more likely to appear with their translation <br>candidates.    On the other hand, lower frequency terms have little <br>chance of appearing with candidates on the same pages.  The <br>context vector method (CV) is therefore adopted to deal with this <br>problem. As translation equivalents may share similar terms, for <br>each query term, we take the co-occurring feature terms as the <br>feature vector.  The similarity between query terms and <br>translation candidates can be computed based on their feature <br>vectors.  Thus, lower frequency query terms still have a chance <br>to extract correct translations.   </span></nobr></DIV>
<DIV style="position:absolute;top:364;left:81"><nobr><span class="ft15">The context vector-based method has been used to extract <br>translations from comparable corpora, such as the use of Fung et <br>al.'s seed word [5]. In our method, real users' popular query terms <br>are used as the feature set, which should help to avoid many <br>inappropriate feature terms. Like Fung et al.'s vector space model, <br>we also use the TF-IDF weighting scheme to estimate the <br>significance of context features. This is defined as follows: </span></nobr></DIV>
<DIV style="position:absolute;top:490;left:81"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:492;left:359"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:492;left:353"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:339"><nobr><span class="ft8">n</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:305"><nobr><span class="ft8">log(</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:282"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:264"><nobr><span class="ft8">,</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:245"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:190"><nobr><span class="ft8">max</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:259"><nobr><span class="ft8">)</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:241"><nobr><span class="ft8">,</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:225"><nobr><span class="ft8">(</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:169"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:337"><nobr><span class="ft8">N</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:270"><nobr><span class="ft8">d</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:251"><nobr><span class="ft8">t</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:235"><nobr><span class="ft8">f</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:248"><nobr><span class="ft8">d</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:231"><nobr><span class="ft8">t</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:216"><nobr><span class="ft8">f</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:149"><nobr><span class="ft8">w</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:259"><nobr><span class="ft10">j</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:225"><nobr><span class="ft10">j</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:236"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:160"><nobr><span class="ft10">t</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:164"><nobr><span class="ft30"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:292"><nobr><span class="ft8">×</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:174"><nobr><span class="ft8">=</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:366"><nobr><span class="ft8">  </span></nobr></DIV>
<DIV style="position:absolute;top:490;left:428"><nobr><span class="ft8">(5) </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft6">where f(t</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:130"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:133"><nobr><span class="ft6">,d) is the frequency of term t</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:293"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:295"><nobr><span class="ft6"> in search-result page d, N </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:81"><nobr><span class="ft15">is the total number of Web pages in the collection of search <br>engines, and n is the number of pages containing t</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:374"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:377"><nobr><span class="ft6">. Given the </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:81"><nobr><span class="ft15">context vectors of a source query term and each target translation <br>candidate, their similarity is estimated with cosine measure as <br>follows: </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:81"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:649;left:379"><nobr><span class="ft31"> </span></nobr></DIV>
<DIV style="position:absolute;top:668;left:362"><nobr><span class="ft31">)</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:335"><nobr><span class="ft31">(</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:272"><nobr><span class="ft31">)</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:244"><nobr><span class="ft31">(</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:173"><nobr><span class="ft31">)</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:161"><nobr><span class="ft31">,</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:148"><nobr><span class="ft31">(</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:328"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:369"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:238"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:667;left:279"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:267"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:301"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:662;left:210"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:629;left:239"><nobr><span class="ft32"></span></nobr></DIV>
<DIV style="position:absolute;top:680;left:323"><nobr><span class="ft10">=</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:233"><nobr><span class="ft10">=</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:262"><nobr><span class="ft10">=</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:289"><nobr><span class="ft31">×</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:304"><nobr><span class="ft31">×</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:183"><nobr><span class="ft31">=</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:320"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:320"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:353"><nobr><span class="ft10">t</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:229"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:681;left:229"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:262"><nobr><span class="ft10">s</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:328"><nobr><span class="ft10">t</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:292"><nobr><span class="ft10">s</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:258"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:258"><nobr><span class="ft10">i</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:138"><nobr><span class="ft10">cv</span></nobr></DIV>
<DIV style="position:absolute;top:680;left:357"><nobr><span class="ft30"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:680;left:267"><nobr><span class="ft30"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:332"><nobr><span class="ft30"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:297"><nobr><span class="ft30"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:668;left:341"><nobr><span class="ft31">w</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:251"><nobr><span class="ft31">w</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:316"><nobr><span class="ft31">w</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:280"><nobr><span class="ft31">w</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:167"><nobr><span class="ft31">t</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:154"><nobr><span class="ft31">s</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:129"><nobr><span class="ft31">S</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:387"><nobr><span class="ft8">  </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:428"><nobr><span class="ft8">(6) </span></nobr></DIV>
<DIV style="position:absolute;top:695;left:81"><nobr><span class="ft15">It is not difficult to construct context vectors for source query <br>terms and their translation candidates. For a source query term, we <br>can use a fixed number of the top search results to extract <br>translation candidates. The co-occurring feature terms of each <br>query can also be extracted, and their weights calculated, which <br>together form the context vector of the query. The same procedure <br>is used to construct a context vector for each translation candidate.   </span></nobr></DIV>
<DIV style="position:absolute;top:813;left:81"><nobr><span class="ft5"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:813;left:103"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:813;left:123"><nobr><span class="ft5"><b>The Combined Method </b></span></nobr></DIV>
<DIV style="position:absolute;top:833;left:81"><nobr><span class="ft15">Benefiting from real-world search engines, the search-result-based <br>method using the chi-square test can reduce the work of corpus <br>collection, but has difficulty in dealing with low-frequency query <br>terms. Although context vector analysis can deal with difficulties <br>encountered by the chi-square test, it is not difficult to see that the <br>feature selection issue needs to be carefully handled. Intuitively, a <br>more complete solution is to integrate the above two methods. <br>Considering the various ranges of similarity values in the two <br>methods, we use a linear combination weighting scheme to <br>compute the similarity measure as follows: </span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:81"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:253"><nobr><span class="ft33"></span></nobr></DIV>
<DIV style="position:absolute;top:1007;left:240"><nobr><span class="ft34">=</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:258"><nobr><span class="ft35"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:285"><nobr><span class="ft35"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:300"><nobr><span class="ft35"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:311"><nobr><span class="ft34">t</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:299"><nobr><span class="ft34">s</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:275"><nobr><span class="ft34">R</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:224"><nobr><span class="ft34">t</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:211"><nobr><span class="ft34">s</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:188"><nobr><span class="ft34">S</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:197"><nobr><span class="ft26">all</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:317"><nobr><span class="ft34">)</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:306"><nobr><span class="ft34">,</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:293"><nobr><span class="ft34">(</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:230"><nobr><span class="ft34">)</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:218"><nobr><span class="ft34">,</span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:205"><nobr><span class="ft34">(</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:289"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:327"><nobr><span class="ft8">  </span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:428"><nobr><span class="ft8">(7) </span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:81"><nobr><span class="ft6">where </span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:119"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:128"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:134"><nobr><span class="ft6"> is an assigned weight for each similarity measure S</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:432"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:438"><nobr><span class="ft6">, </span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:81"><nobr><span class="ft6">and R</span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:112"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:119"><nobr><span class="ft6">(s,t) - which represents the similarity ranking of each target </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:477"><nobr><span class="ft15">candidate t with respect to source term s - is assigned to be from 1 <br>to  k (the number of candidates) in decreasing order of similarity <br>measure S</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:532"><nobr><span class="ft10">m</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:538"><nobr><span class="ft6">(s,t). </span></nobr></DIV>
<DIV style="position:absolute;top:164;left:477"><nobr><span class="ft5"><b>4.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:519"><nobr><span class="ft5"><b>Experiments on Term Translation   </b></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:477"><nobr><span class="ft31">4.4.1</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:510"><nobr><span class="ft37"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:530"><nobr><span class="ft31">The Test Bed </span></nobr></DIV>
<DIV style="position:absolute;top:208;left:477"><nobr><span class="ft15">To determine the effectiveness of the proposed approach, we <br>conducted several experiments to extract translation pairs for <br>Chinese and English terms in different domains.   </span></nobr></DIV>
<DIV style="position:absolute;top:264;left:477"><nobr><span class="ft38"><b>Web Queries: </b></span></nobr></DIV>
<DIV style="position:absolute;top:265;left:571"><nobr><span class="ft6">We collected query terms and the logs from two </span></nobr></DIV>
<DIV style="position:absolute;top:281;left:477"><nobr><span class="ft15">real-world Chinese search engines in Taiwan, i.e., Dreamer and <br>GAIS. The Dreamer log contained 228,566 unique query terms for <br>a period of over 3 months in 1998, while the GAIS log contained <br>114,182 unique query terms for a period of two weeks in 1999. <br>We prepared two different test query sets based on these logs. The <br>first, called the popular-query set, contained a set of 430 frequent <br>Chinese queries in the logs. These queries were obtained from the <br>Chinese translations of 1,230 English terms out of the most <br>popular 9,709 query terms (with frequencies above 10 in both <br>logs), which co-occurred with their English counterparts in the <br>logs. The popular-query set was further divided into two types: <br>type Dic (the terms covered in the dictionary), consisting of about <br>36% (156/430) of the test queries and type OOV (out of <br>vocabulary; the terms not in the dictionary), consisting of about <br>64% (274/430) of the test queries.   </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:477"><nobr><span class="ft15">The second set, called the random-query set, contained 200 <br>Chinese query terms, which were randomly selected from the top <br>20,000 queries in the Dreamer log, where 165 (about 82.5%) were <br>not included in general-purpose translation dictionaries. </span></nobr></DIV>
<DIV style="position:absolute;top:594;left:477"><nobr><span class="ft38"><b>Proper Names and Technical Terms: </b></span></nobr></DIV>
<DIV style="position:absolute;top:595;left:719"><nobr><span class="ft6">To further investigate </span></nobr></DIV>
<DIV style="position:absolute;top:611;left:477"><nobr><span class="ft15">the translation effectiveness for proper names and technical terms, <br>we prepared two other query sets containing 50 scientists' names <br>and 50 disease names in English. These were randomly selected <br>from the 256 scientists (Science/People) and 664 diseases <br>(Health/Diseases and Conditions) in the Yahoo! Directory. It <br>should be noted that 76% (38/50) of the scientists' names and <br>72% (36/50) of the disease names were not included in the <br>general-purpose translation dictionary, which contained 202,974 <br>entries collected from the Internet. </span></nobr></DIV>
<DIV style="position:absolute;top:760;left:477"><nobr><span class="ft15">To evaluate the search-result-based methods, we obtained <br>search-result pages of the source query terms by submitting them <br>to real-world Chinese search engines, such as Google Chinese and <br>Openfind. Basically, we used only the first 100 retrieved results <br>(snippets) to extract translation candidates. The context vector of <br>each source query and the required parameters (page counts) for <br>the chi-square test were also extracted from the retrieved <br>search-result pages.   </span></nobr></DIV>
<DIV style="position:absolute;top:893;left:477"><nobr><span class="ft15">To evaluate the performance of translation extraction, we used the <br>average top-n inclusion rate as a metric. For a set of test queries, <br>the top-n inclusion rate was defined as the percentage of queries <br>whose translations could be found in the first n extracted <br>translations. Also, we wished to know if the coverage rate of <br>translations, i.e. the percentage of queries whose translations <br>could be found in the whole extracted candidate set, was high <br>enough in the top search-result pages for real queries.   </span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:477"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">112</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="201006.png" alt="background image">
<DIV style="position:absolute;top:806;left:81"><nobr><span class="ft31">4.4.2</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:114"><nobr><span class="ft37"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:806;left:134"><nobr><span class="ft31">Performance  </span></nobr></DIV>
<DIV style="position:absolute;top:825;left:81"><nobr><span class="ft38"><b>Web Queries </b></span></nobr></DIV>
<DIV style="position:absolute;top:842;left:81"><nobr><span class="ft15">We carried out experiments to determine the performance of the <br>proposed approach by extracting translations for the <br>popular-query set. Tables 3 and 4 show the results in terms of top <br>1-5 inclusion rates and coverage rates for Chinese and English <br>queries respectively. In this table, "CV", "</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:331"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:336"><nobr><span class="ft6">" and "Combined" </span></nobr></DIV>
<DIV style="position:absolute;top:919;left:81"><nobr><span class="ft15">represent the context-vector analysis, the chi-square test, and the <br>combined method, respectively. In addition, "Dic", "OOV" and <br>"All" represent the terms covered in a dictionary, the terms not in <br>a dictionary, and the total test query set, respectively. The <br>coverage rates we obtained were promising, which shows that the <br>Web contains rich mixed texts in both languages. The <br>performance of the English query set was not as good as the <br>Chinese query set. The reason for this was that the English queries <br>suffered from more noise in Chinese translation candidates since <br>the search-result pages in the Chinese Web generally contain </span></nobr></DIV>
<DIV style="position:absolute;top:797;left:477"><nobr><span class="ft15">much more Chinese than English content. We also conducted an <br>experiment for random queries. As Table 5 shows, the coverage <br>rates were encouraging.   </span></nobr></DIV>
<DIV style="position:absolute;top:852;left:477"><nobr><span class="ft38"><b>Proper Names, Technical Terms and Common Terms </b></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:477"><nobr><span class="ft15">To further determine the effectiveness of the proposed approach in <br>dealing with the translation of proper names and technical terms, <br>we conducted an experiment on the test sets of scientists' names <br>and medical terms using the combined method. As the results in <br>Table 6 show, the top-1 inclusion rates for the scientists' and <br>disease names were 40% and 44% respectively. Some examples <br>of the extracted correct translations are shown in Table 7. </span></nobr></DIV>
<DIV style="position:absolute;top:987;left:477"><nobr><span class="ft15">Although the achieved performance for real queries looked <br>promising, we wished to know if it was equally effective for <br>common terms. We randomly selected 100 common nouns and <br>100 common verbs from a general-purpose Chinese dictionary. <br>Table 8 shows the results obtained using the combined method. It <br>is easy to see that the proposed approach is less reliable in </span></nobr></DIV>
<DIV style="position:absolute;top:110;left:455"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:128;left:221"><nobr><span class="ft7"><b>Table 3. Coverage and inclusion rates for popular Chinese queries using different </b></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:429"><nobr><span class="ft7"><b>methods.</b></span></nobr></DIV>
<DIV style="position:absolute;top:143;left:481"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:236"><nobr><span class="ft7"><b>Method </b></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:331"><nobr><span class="ft7"><b>Query Type </b></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:431"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:497"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:561"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:630"><nobr><span class="ft7"><b>Coverage </b></span></nobr></DIV>
<DIV style="position:absolute;top:185;left:355"><nobr><span class="ft6">Dic 56.4% </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:496"><nobr><span class="ft6">70.5% </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:560"><nobr><span class="ft6">74.4% </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:640"><nobr><span class="ft6">80.1% </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:350"><nobr><span class="ft6">OOV 56.2% </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:496"><nobr><span class="ft6">66.1% </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:560"><nobr><span class="ft6">69.3% 85.0% </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:249"><nobr><span class="ft6">CV </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:356"><nobr><span class="ft6">All 56.3% </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:496"><nobr><span class="ft6">67.7% </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:560"><nobr><span class="ft6">71.2% </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:640"><nobr><span class="ft6">83.3% </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:355"><nobr><span class="ft6">Dic 40.4% </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:496"><nobr><span class="ft6">61.5% </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:560"><nobr><span class="ft6">67.9% </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:640"><nobr><span class="ft6">80.1% </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:350"><nobr><span class="ft6">OOV 54.7% </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:496"><nobr><span class="ft6">65.0% </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:560"><nobr><span class="ft6">68.2% 85.0% </span></nobr></DIV>
<DIV style="position:absolute;top:258;left:253"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:256;left:259"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:258;left:264"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:356"><nobr><span class="ft6">All 49.5% </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:496"><nobr><span class="ft6">63.7% </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:560"><nobr><span class="ft6">68.1% </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:640"><nobr><span class="ft6">83.3% </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:355"><nobr><span class="ft6">Dic 57.7% </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:496"><nobr><span class="ft6">71.2% </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:560"><nobr><span class="ft6">75.0% </span></nobr></DIV>
<DIV style="position:absolute;top:301;left:640"><nobr><span class="ft6">80.1% </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:350"><nobr><span class="ft6">OOV 56.6% </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:496"><nobr><span class="ft6">67.9% </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:560"><nobr><span class="ft6">70.9% 85.0% </span></nobr></DIV>
<DIV style="position:absolute;top:315;left:230"><nobr><span class="ft6">Combined </span></nobr></DIV>
<DIV style="position:absolute;top:339;left:356"><nobr><span class="ft6">All 57.2% </span></nobr></DIV>
<DIV style="position:absolute;top:339;left:496"><nobr><span class="ft6">68.6% </span></nobr></DIV>
<DIV style="position:absolute;top:339;left:560"><nobr><span class="ft6">72.8% </span></nobr></DIV>
<DIV style="position:absolute;top:339;left:640"><nobr><span class="ft6">83.3% </span></nobr></DIV>
<DIV style="position:absolute;top:357;left:152"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:384;left:252"><nobr><span class="ft7"><b>Table 4. Coverage and inclusion rates for popular English queries using </b></span></nobr></DIV>
<DIV style="position:absolute;top:399;left:406"><nobr><span class="ft7"><b>different methods. </b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:283"><nobr><span class="ft7"><b>Method </b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:386"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:458"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:530"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:423;left:600"><nobr><span class="ft7"><b>Coverage </b></span></nobr></DIV>
<DIV style="position:absolute;top:444;left:296"><nobr><span class="ft6">CV 50.9% </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:457"><nobr><span class="ft6">60.1% </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:529"><nobr><span class="ft6">60.8% </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:610"><nobr><span class="ft6">80.9% </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:300"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:461;left:306"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:311"><nobr><span class="ft6"> 44.6% </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:457"><nobr><span class="ft6">56.1% </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:529"><nobr><span class="ft6">59.2% </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:610"><nobr><span class="ft6">80.9% </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:277"><nobr><span class="ft6">Combined 51.8 </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:411"><nobr><span class="ft6">% </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:457"><nobr><span class="ft6">60.7% </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:529"><nobr><span class="ft6">62.2% 80.9% </span></nobr></DIV>
<DIV style="position:absolute;top:522;left:265"><nobr><span class="ft7"><b>Table 5. Coverage and inclusion rates for random queries using the </b></span></nobr></DIV>
<DIV style="position:absolute;top:538;left:406"><nobr><span class="ft7"><b>different methods.</b></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:512"><nobr><span class="ft36"> </span></nobr></DIV>
<DIV style="position:absolute;top:562;left:283"><nobr><span class="ft7"><b>Method </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:386"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:458"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:529"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:600"><nobr><span class="ft7"><b>Coverage </b></span></nobr></DIV>
<DIV style="position:absolute;top:584;left:296"><nobr><span class="ft6">CV 25.5% </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:457"><nobr><span class="ft6">45.5% </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:529"><nobr><span class="ft6">50.5% </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:610"><nobr><span class="ft6">60.5% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:299"><nobr><span class="ft6">2 26.0% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:457"><nobr><span class="ft6">44.5% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:529"><nobr><span class="ft6">50.5% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:610"><nobr><span class="ft6">60.5% </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:277"><nobr><span class="ft6">Combined 29.5% </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:457"><nobr><span class="ft6">49.5% </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:529"><nobr><span class="ft6">56.5% 60.5% </span></nobr></DIV>
<DIV style="position:absolute;top:645;left:152"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:253"><nobr><span class="ft7"><b>Table 6. Inclusion rates for proper names and technical terms using the </b></span></nobr></DIV>
<DIV style="position:absolute;top:685;left:405"><nobr><span class="ft7"><b>combined method.</b></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:512"><nobr><span class="ft36"> </span></nobr></DIV>
<DIV style="position:absolute;top:709;left:304"><nobr><span class="ft7"><b>Query Type </b></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:458"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:539"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:618"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:731;left:298"><nobr><span class="ft6">Scientist Name </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:458"><nobr><span class="ft6">40.0% </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:539"><nobr><span class="ft6">52.0% </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:618"><nobr><span class="ft6">60.0% </span></nobr></DIV>
<DIV style="position:absolute;top:751;left:300"><nobr><span class="ft6">Disease Name </span></nobr></DIV>
<DIV style="position:absolute;top:751;left:458"><nobr><span class="ft6">44.0% </span></nobr></DIV>
<DIV style="position:absolute;top:751;left:539"><nobr><span class="ft6">60.0% </span></nobr></DIV>
<DIV style="position:absolute;top:751;left:618"><nobr><span class="ft6">70.0% </span></nobr></DIV>
<DIV style="position:absolute;top:771;left:152"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">113</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="201007.png" alt="background image">
<DIV style="position:absolute;top:421;left:81"><nobr><span class="ft15">extracting translations of such common terms. One possible <br>reason is that the usages of common terms are diverse on the Web <br>and the retrieved search results are not highly relevant. It is <br>fortunate that many of these common words can be found in <br>general-purpose translation dictionaries.   </span></nobr></DIV>
<DIV style="position:absolute;top:508;left:108"><nobr><span class="ft7"><b>Table 8. Top 1, 3, 5 inclusion rates obtained using the </b></span></nobr></DIV>
<DIV style="position:absolute;top:523;left:99"><nobr><span class="ft7"><b>combined method for extracting translations of common </b></span></nobr></DIV>
<DIV style="position:absolute;top:539;left:212"><nobr><span class="ft7"><b>nouns and verbs. </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:131"><nobr><span class="ft7"><b>Query Type </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:259"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:323"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:387"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:584;left:109"><nobr><span class="ft6">100 Common Nouns </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:258"><nobr><span class="ft6">23.0% </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:322"><nobr><span class="ft6">33.0% </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:386"><nobr><span class="ft6">43.0% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:111"><nobr><span class="ft6">100 Common Verbs </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:261"><nobr><span class="ft6">6.0% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:325"><nobr><span class="ft6">8.0% </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:386"><nobr><span class="ft6">10.0% </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:81"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:81"><nobr><span class="ft5"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:649;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:650;left:113"><nobr><span class="ft18"><b>BILINGUAL LEXICON <br>CONSTRUCTION  </b></span></nobr></DIV>
<DIV style="position:absolute;top:691;left:81"><nobr><span class="ft5"><b>5.1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:691;left:108"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:691;left:123"><nobr><span class="ft5"><b>The Approach   </b></span></nobr></DIV>
<DIV style="position:absolute;top:711;left:81"><nobr><span class="ft15">To enhance CLIR services in a digital library that only has <br>monolingual document collections, the proposed approach can be <br>used to construct a domain-specific bilingual lexicon. We take the <br>document set in digital libraries into consideration. The document <br>set in the target language is first analyzed and possible key terms <br>that are representative of the document set are extracted, using the <br>proposed term extraction method. These extracted key terms are <br>likely to be similar to terms that users may use in real user queries, <br>since they are relatively more significant than other terms in the <br>documents. The proposed term translation method can then be <br>applied to those key terms not included in common translation <br>dictionaries to obtain the translation of key terms in the source <br>language. Therefore, a bilingual lexicon can then be constructed <br>where the mappings between key terms and relevant terms in the <br>source and target languages are maintained.   </span></nobr></DIV>
<DIV style="position:absolute;top:953;left:81"><nobr><span class="ft15">As we have already indicated, the constructed bilingual lexicon <br>can benefit CLIR services. For a given source query, the similarity <br>with candidate source relevant terms can be calculated using the <br>context vector method presented in Section 4. Also, and the <br>top-ranked relevant terms can be extracted using the constructed <br>bilingual lexicon. After the corresponding translations of relevant <br>terms are obtained, relevant documents in the target language can <br>be retrieved, using these relevant translations. The source query </span></nobr></DIV>
<DIV style="position:absolute;top:421;left:477"><nobr><span class="ft15">can then be expanded with the relevant translations and <br>conventional CLIR methods can be used to retrieve documents in <br>the target language.   </span></nobr></DIV>
<DIV style="position:absolute;top:477;left:477"><nobr><span class="ft5"><b>5.2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:477;left:504"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:477;left:519"><nobr><span class="ft5"><b>An Application   </b></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:477"><nobr><span class="ft6">We tested the STICNET Database</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:680"><nobr><span class="ft10">4</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:686"><nobr><span class="ft6">, which is a government- </span></nobr></DIV>
<DIV style="position:absolute;top:512;left:477"><nobr><span class="ft15">supported Web-accessible digital library system providing a <br>search service for scientific documents collected in Taiwan. The <br>system contained documents in either English or Chinese, but no <br>cross-language search was provided. To test the performance of <br>bilingual lexicon construction, we selected 1,367 Information <br>Engineering documents and 5,357 Medical documents <br>respectively from the STICNET Database for the period 1983 to <br>1997 as the test bed. Using the PAT-tree-based term extraction <br>method, key terms were automatically extracted from each <br>document collection and their relevant translations were extracted <br>by the proposed term translation approach. </span></nobr></DIV>
<DIV style="position:absolute;top:692;left:477"><nobr><span class="ft15">In the collection of Information Engineering documents, 1,330 <br>key terms (with a threshold of 2 to 6-gram character strings, a <br>term frequency&gt;10, and an association value&gt;0.1) were <br>automatically extracted. Meanwhile, 5,708 key terms (with a <br>threshold of 2 to 6-gram character strings and a term <br>frequency&gt;40) were automatically extracted from the Medical <br>document collection. Among the 1,330 auto-extracted key terms <br>from the Information Engineering documents, 32% were not <br>included in KUH Chinese Dictionary</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:683"><nobr><span class="ft10">5</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:687"><nobr><span class="ft6">  (unknown terms) - one of </span></nobr></DIV>
<DIV style="position:absolute;top:832;left:477"><nobr><span class="ft15">the largest Chinese dictionaries with 158,239 term entries - where <br>75% of these unknown terms were found useful. In the case of <br>Medical documents, 71% of the 5,708 auto-extracted key terms <br>were not included in KUH Chinese Dictionary where 36.6% of <br>these unknown terms were found useful. Table 9 shows the <br>accuracy of the extracted translations for these useful unknown <br>terms. The promising result shows the potential of the proposed <br>approach to assist bilingual lexicon construction.   </span></nobr></DIV>
<DIV style="position:absolute;top:965;left:477"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:990;left:477"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:477"><nobr><span class="ft8">                                                           </span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:477"><nobr><span class="ft10">4</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:481"><nobr><span class="ft6"> http://sticnet.stic.gov.tw/ </span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:477"><nobr><span class="ft10">5</span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:481"><nobr><span class="ft6"> http://www.edu.tw/mandr/clc/dict/ </span></nobr></DIV>
<DIV style="position:absolute;top:131;left:207"><nobr><span class="ft7"><b>Table 7. Some examples of the test English proper names and technical terms, and their </b></span></nobr></DIV>
<DIV style="position:absolute;top:146;left:370"><nobr><span class="ft7"><b>extracted Chinese translations.</b></span></nobr></DIV>
<DIV style="position:absolute;top:145;left:548"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:178;left:206"><nobr><span class="ft7"><b>Query Type </b></span></nobr></DIV>
<DIV style="position:absolute;top:178;left:381"><nobr><span class="ft7"><b>English Query </b></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:575"><nobr><span class="ft7"><b>Extracted Translations </b></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:572"><nobr><span class="ft7"><b>(in Traditional Chinese) </b></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:199"><nobr><span class="ft6">Scientist Name </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:301"><nobr><span class="ft20">Galilei, Galileo      (Astronomer) <br>Crick, Francis              (Biologists) <br>Kepler, Johannes     (Mathematician) <br>Dalton, John        (Physicist) <br>Feynman, Richard    (Physicist) </span></nobr></DIV>
<DIV style="position:absolute;top:208;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:589"><nobr><span class="ft6">// </span></nobr></DIV>
<DIV style="position:absolute;top:227;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:226;left:589"><nobr><span class="ft6">/ </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:589"><nobr><span class="ft6">// </span></nobr></DIV>
<DIV style="position:absolute;top:265;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:264;left:589"><nobr><span class="ft6">// </span></nobr></DIV>
<DIV style="position:absolute;top:284;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:283;left:576"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:342;left:201"><nobr><span class="ft6">Disease Name </span></nobr></DIV>
<DIV style="position:absolute;top:304;left:301"><nobr><span class="ft20">Hypoplastic Left Heart Syndrome <br>Legionnaires' Disease <br>Shingles <br>Stockholm Syndrome <br>Sudden Infant Death Syndrome (SIDS)   </span></nobr></DIV>
<DIV style="position:absolute;top:306;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:304;left:670"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:323;left:616"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:342;left:603"><nobr><span class="ft6">/ </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:361;left:657"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:549"><nobr><span class="ft6"></span></nobr></DIV>
<DIV style="position:absolute;top:381;left:616"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:400;left:169"><nobr><span class="ft8"> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">114</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft39{font-size:11px;font-family:Times;color:#000000;}
	.ft40{font-size:11px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="201008.png" alt="background image">
<DIV style="position:absolute;top:108;left:111"><nobr><span class="ft7"><b>Table 9. The top-<i>n</i> inclusion rates of translations for </b></span></nobr></DIV>
<DIV style="position:absolute;top:124;left:152"><nobr><span class="ft7"><b>auto-extracted useful unknown terms. </b></span></nobr></DIV>
<DIV style="position:absolute;top:148;left:131"><nobr><span class="ft7"><b>Query Type </b></span></nobr></DIV>
<DIV style="position:absolute;top:148;left:259"><nobr><span class="ft7"><b>Top-1 </b></span></nobr></DIV>
<DIV style="position:absolute;top:148;left:323"><nobr><span class="ft7"><b>Top-3 </b></span></nobr></DIV>
<DIV style="position:absolute;top:148;left:387"><nobr><span class="ft7"><b>Top-5 </b></span></nobr></DIV>
<DIV style="position:absolute;top:170;left:90"><nobr><span class="ft6">Auto-extracted useful terms </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:91"><nobr><span class="ft6">in Information Engineering </span></nobr></DIV>
<DIV style="position:absolute;top:178;left:258"><nobr><span class="ft6">33.3% 37.5% 50.0% </span></nobr></DIV>
<DIV style="position:absolute;top:206;left:90"><nobr><span class="ft6">Auto-extracted useful terms </span></nobr></DIV>
<DIV style="position:absolute;top:222;left:132"><nobr><span class="ft6">in Medicine </span></nobr></DIV>
<DIV style="position:absolute;top:214;left:258"><nobr><span class="ft6">34.6% 46.2% 50.0% </span></nobr></DIV>
<DIV style="position:absolute;top:241;left:98"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:266;left:81"><nobr><span class="ft5"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:266;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:266;left:113"><nobr><span class="ft5"><b>RELATED WORK </b></span></nobr></DIV>
<DIV style="position:absolute;top:286;left:81"><nobr><span class="ft15">Many effective retrieval models have been developed for CLIR. <br>For example, the Latent Semantic Indexing (LSI) method [4] has <br>been utilized to model inter-term relationships, instead of exact <br>term matching. Other methods include the cross-lingual relevance <br>model [11], which integrates popular techniques of <br>disambiguation and query expansion. However, translation of <br>queries not covered in a bilingual dictionary remains one of the <br>major challenges in practical CLIR services [9].   </span></nobr></DIV>
<DIV style="position:absolute;top:419;left:81"><nobr><span class="ft15">To deal with the translation of out-of-dictionary terms, <br>conventional research on machine translation has generally used <br>statistical techniques to automatically extract translations from <br>domain-specific, sentence-aligned parallel bilingual corpora [20]. <br>However, a large parallel corpus is difficult to obtain. Some work <br>has been done on term translation extraction from comparable <br>texts, such as bilingual newspapers [5], which are easier to obtain. <br>Using a non-parallel corpus is more difficult than a parallel one, <br>due to the lack of alignment correspondence for sentence pairs. <br>On the other hand, research on digital libraries has made the same <br>endeavor. Larson et al. [10] proposed a method for translingual <br>vocabulary mapping using multilingual subject headings of book <br>titles in online library catalogs - a kind of parallel corpus. <br>However, book titles are still limited in coverage, compared to the <br>rich resources on the Web. </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:81"><nobr><span class="ft15">A new potential research direction is to perform query translation <br>directly, through mining the Web's multilingual and wide-range <br>resources [16]. Web mining is a new research area that focuses on <br>finding useful information from large amounts of semi-structured <br>hypertexts and unstructured texts [1]. Chen et al. [2] proposed a <br>dictionary-based approach in which the search results returned <br>from Yahoo China search engine were utilized to extract <br>translations for terms not covered in the dictionary. In their work <br>only an English term appearing (maybe in parenthesis) <br>immediately or closely after a Chinese term was considered a <br>possible translation. In our previous research, we proposed an <br>approach for extracting translations of Web queries through the <br>mining of anchor texts and link structures and obtained very <br>promising results [12, 13]. Previous experiments showed that the <br>anchor-text-based approach can achieve a good precision rate for <br>popular queries. Its major drawback is the very high cost of the <br>hardware and software required to collect sufficient anchor texts <br>from Web pages. Collecting anchor texts requires a powerful Web <br>spider and takes cost of network bandwidth and storage. Because <br>of the practical needs of digital libraries, search-result pages, <br>which are easier to obtain are, therefore, investigated in this paper.   </span></nobr></DIV>
<DIV style="position:absolute;top:997;left:81"><nobr><span class="ft5"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:996;left:94"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:997;left:113"><nobr><span class="ft5"><b>CONCLUSION </b></span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:81"><nobr><span class="ft15">In this paper, we have introduced a Web-based approach for <br>dealing with the translation of unknown query terms for <br>cross-language information retrieval in digital libraries. With the <br>proposed term extraction and translation methods, it is feasible to </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:477"><nobr><span class="ft15">translate unknown terms and construct a bilingual lexicon for key <br>terms extracted from documents in a digital library. With the help <br>of such bilingual lexicons, it would be convenient for users to <br>formulate cross-lingual queries. The simplicity of the approach <br>not only makes it very suitable for digital library systems, but <br>would also facilitate the implementation of CLIR services. </span></nobr></DIV>
<DIV style="position:absolute;top:211;left:477"><nobr><span class="ft5"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:509"><nobr><span class="ft5"><b>REFERENCES </b></span></nobr></DIV>
<DIV style="position:absolute;top:231;left:477"><nobr><span class="ft6">[1]</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:231;left:504"><nobr><span class="ft15">Chakrabarti, S. Mining the Web: Analysis of Hypertext and <br>Semi Structured Data, Morgan Kaufmann, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:477"><nobr><span class="ft6">[2]</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:504"><nobr><span class="ft15">Chen, A., Jiang, H., and Gey, F. Combining Multiple Sources <br>for Short Query Translation in Chinese-English <br>Cross-Language Information Retrieval. In Proceedings of the <br>5th International Workshop on Information Retrieval with <br>Asian Languages (IRAL 2000), 2000, 17-23. </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:477"><nobr><span class="ft6">[3]</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:504"><nobr><span class="ft15">Chien, L.F. PAT-Tree-based Keyword Extraction for <br>Chinese Information Retrieval. In Proceedings of the 20</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:807"><nobr><span class="ft10">th</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:814"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:504"><nobr><span class="ft15">Annual International ACM Conference on Research and <br>Development in Information Retrieval (SIGIR 1997), 1997, <br>50-58. </span></nobr></DIV>
<DIV style="position:absolute;top:417;left:477"><nobr><span class="ft6">[4]</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:417;left:504"><nobr><span class="ft15">Dumais, S. T., Landauer, T. K., and Littman, M. L. <br>Automatic Cross-Linguistic Information Retrieval Using <br>Latent Semantic Indexing. In Proceedings of ACM-SIGIR <br>Workshop on Cross-Linguistic Information Retrieval (SIGIR <br>1996), 1996, 16-24. </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:477"><nobr><span class="ft6">[5]</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:504"><nobr><span class="ft15">Fung, P. and Yee, L. Y. An IR Approach for Translating <br>New Words from Nonparallel, Comparable Texts. In <br>Proceedings of the 36th Annual Conference of the <br>Association for Computational Linguistics (ACL 1998), 1998, <br>414-420. </span></nobr></DIV>
<DIV style="position:absolute;top:573;left:477"><nobr><span class="ft6">[6]</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:573;left:504"><nobr><span class="ft15">Gale, W. A. and Church, K. W. Identifying Word <br>Correspondences in Parallel Texts. In Proceedings of DARPA <br>Speech and Natural Language Workshop, 1991, 152-157. </span></nobr></DIV>
<DIV style="position:absolute;top:619;left:477"><nobr><span class="ft6">[7]</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:619;left:504"><nobr><span class="ft15">Gale, W.A. and Church, K.W. A Program for Aligning <br>Sentences in Bilingual Corpora. Computational Linguistics, <br>19, 1 (1993), 75-102. </span></nobr></DIV>
<DIV style="position:absolute;top:666;left:477"><nobr><span class="ft6">[8]</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:666;left:504"><nobr><span class="ft15">Gonnet, G.H., Baeza-yates, R.A. and Snider, T. New Indices <br>for Text: Pat Trees and Pat Arrays. Information Retrieval <br>Data Structures &amp; Algorithms, Prentice Hall, 1992, 66-82. </span></nobr></DIV>
<DIV style="position:absolute;top:712;left:477"><nobr><span class="ft6">[9]</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:492"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:712;left:504"><nobr><span class="ft15">Kwok, K. L. NTCIR-2 Chinese, Cross Language Retrieval <br>Experiments Using PIRCS. In Proceedings of NTCIR <br>workshop meeting, 2001, 111-118. </span></nobr></DIV>
<DIV style="position:absolute;top:759;left:477"><nobr><span class="ft6">[10]</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:759;left:504"><nobr><span class="ft15">Larson, R. R., Gey, F., and Chen, A. Harvesting Translingual <br>Vocabulary Mappings for Multilingual Digital Libraries. In <br>Proceedings of ACM/IEEE Joint Conference on Digital <br>Libraries (JCDL 2002), 2002, 185-190. </span></nobr></DIV>
<DIV style="position:absolute;top:821;left:477"><nobr><span class="ft6">[11]</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:821;left:504"><nobr><span class="ft15">Lavrenko, V., Choquette, M., and Croft, W. B. Cross-Lingual <br>Relevance Models. In Proceedings of ACM Conference on <br>Research and Development in Information Retrieval (SIGIR <br>2002), 2002, 175-182. </span></nobr></DIV>
<DIV style="position:absolute;top:883;left:477"><nobr><span class="ft6">[12]</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:883;left:504"><nobr><span class="ft15">Lu, W. H., Chien, L. F., and Lee, H. J. Translation of Web <br>Queries using Anchor Text Mining. ACM Transactions on <br>Asian Language Information Processing, 1 (2002), 159-172. </span></nobr></DIV>
<DIV style="position:absolute;top:930;left:477"><nobr><span class="ft6">[13]</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:930;left:504"><nobr><span class="ft15">Lu, W. H., Chien, L. F., and Lee, H. J. Anchor Text Mining <br>for Translation of Web Queries: A Transitive Translation <br>Approach. ACM Transactions on Information Systems, 22 <br>(2004), 1­28. </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:477"><nobr><span class="ft6">[14]</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:504"><nobr><span class="ft15">Manber, U. and Baeza-yates, R. An Algorithm for String <br>Matching with a Sequence of Don't Cares. Information <br>Processing Letters, 37 (1991), 133-136. </span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:477"><nobr><span class="ft6">[15]</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:504"><nobr><span class="ft15">Morrison, D. PATRICIA: Practical Algorithm to Retrieve <br>Information Coded in Alphanumeric. JACM, 1968, 514-534. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">115</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="201009.png" alt="background image">
<DIV style="position:absolute;top:108;left:81"><nobr><span class="ft6">[16]</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:103"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:108"><nobr><span class="ft15">Nie, J. Y., Isabelle, P., Simard, M., and Durand, R. <br>Cross-language Information Retrieval Based on Parallel <br>Texts and Automatic Mining of Parallel Texts from the Web. <br>In Proceedings of ACM Conference on Research and <br>Development in Information Retrieval (SIGIR 1999), 1999, <br>74-81. </span></nobr></DIV>
<DIV style="position:absolute;top:201;left:81"><nobr><span class="ft6">[17]</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:103"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:201;left:108"><nobr><span class="ft15">Rapp, R. Automatic Identification of Word Translations from <br>Unrelated English and German Corpora, In Proceedings of <br>the 37th Annual Conference of the Association for <br>Computational Linguistics (ACL 1999), 1999, 519-526. </span></nobr></DIV>
<DIV style="position:absolute;top:263;left:81"><nobr><span class="ft15"> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:477"><nobr><span class="ft6">[18]</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:108;left:504"><nobr><span class="ft15">Silva, J. F., Dias, G., Guillore, S., and Lopes, G. P. Using <br>LocalMaxs Algorithm for the Extraction of Contiguous and <br>Non-contiguous Multiword Lexical Units. Lecture Notes in <br>Artificial Intelligence, 1695, Springer-Verlag, 1999, 113-132. </span></nobr></DIV>
<DIV style="position:absolute;top:170;left:477"><nobr><span class="ft6">[19]</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:170;left:504"><nobr><span class="ft15">Silva, J. F. and Lopes, G. P. A Local Maxima Method and a <br>Fair Dispersion Normalization for Extracting Multiword <br>Units. In Proceedings of the 6</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:666"><nobr><span class="ft10">th</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:673"><nobr><span class="ft6"> Meeting on the Mathematics </span></nobr></DIV>
<DIV style="position:absolute;top:217;left:504"><nobr><span class="ft6">of Language, 1999, 369-381. </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:477"><nobr><span class="ft6">[20]</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:499"><nobr><span class="ft40"> </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:504"><nobr><span class="ft15">Smadja, F., McKeown, K., and Hatzivassiloglou, V. <br>Translating Collocations for Bilingual Lexicons: A Statistical <br>Approach, Computational Linguistics, 22, 1 (1996), 1-38. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft14">116</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
