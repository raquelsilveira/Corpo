<DOCUMENT>
<SECTION header="ABSTRACT">Localized search engines are small-scale systems that index
a particular community on the web. They offer several benefits
over their large-scale counterparts in that they are relatively
inexpensive to build, and can provide more precise
and complete search capability over their relevant domains.
One disadvantage such systems have over large-scale search
engines is the lack of global PageRank values. Such information
is needed to assess the value of pages in the localized
search domain within the context of the web as a whole.
In this paper, we present well-motivated algorithms to estimate
the global PageRank values of a local domain. The
algorithms are all highly scalable in that, given a local domain
of size n, they use O(n) resources that include computation
time, bandwidth, and storage. We test our methods
across a variety of localized domains, including site-specific
domains and topic-specific domains. We demonstrate that
by crawling as few as n or 2n additional pages, our methods
can give excellent global PageRank estimates.
</SECTION>
<SECTION header="Categories and Subject Descriptors">H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval; G.1.3 [Numerical Analysis]:
Numerical Linear Algebra; G.3 [Probability and Statistics
]: Markov Processes
</SECTION>
<SECTION header="General Terms">PageRank, Markov Chain, Stochastic Complementation

</SECTION>
<SECTION header="INTRODUCTION">Localized search engines are small-scale search engines
that index only a single community of the web. Such communities
can be site-specific domains, such as pages within
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro£t or commercial advantage and that copies
bear this notice and the full citation on the £rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci£c
permission and/or a fee.
KDD'06, August 20­23, 2006, Philadelphia, Pennsylvania, USA.
Copyright 2006 ACM 1-59593-339-5/06/0008 ...
$
5.00.
the cs.utexas.edu domain, or topic-related communities-for
example, political websites. Compared to the web graph
crawled and indexed by large-scale search engines, the size
of such local communities is typically orders of magnitude
smaller. Consequently, the computational resources needed
to build such a search engine are also similarly lighter. By
restricting themselves to smaller, more manageable sections
of the web, localized search engines can also provide more
precise and complete search capabilities over their respective
domains.
One drawback of localized indexes is the lack of global
information needed to compute link-based rankings. The
PageRank algorithm [3], has proven to be an effective such
measure. In general, the PageRank of a given page is dependent
on pages throughout the entire web graph. In the
context of a localized search engine, if the PageRanks are
computed using only the local subgraph, then we would expect
the resulting PageRanks to reflect the perceived popularity
within the local community and not of the web as a
whole. For example, consider a localized search engine that
indexes political pages with conservative views. A person
wishing to research the opinions on global warming within
the conservative political community may encounter numerous
such opinions across various websites. If only local PageRank
values are available, then the search results will reflect
only strongly held beliefs within the community. However, if
global PageRanks are also available, then the results can ad-ditionally
reflect outsiders' views of the conservative community
(those documents that liberals most often access within
the conservative community).
Thus, for many localized search engines, incorporating
global PageRanks can improve the quality of search results.
However, the number of pages a local search engine indexes
is typically orders of magnitude smaller than the number of
pages indexed by their large-scale counterparts. Localized
search engines do not have the bandwidth, storage capacity,
or computational power to crawl, download, and compute
the global PageRanks of the entire web. In this work, we
present a method of approximating the global PageRanks of
a local domain while only using resources of the same order
as those needed to compute the PageRanks of the local
subgraph.
Our proposed method looks for a supergraph of our local
subgraph such that the local PageRanks within this supergraph
are close to the true global PageRanks. We construct
this supergraph by iteratively crawling global pages on the
current web frontier--i.e., global pages with inlinks from
pages that have already been crawled. In order to provide
116
Research Track Paper
a good approximation to the global PageRanks, care must
be taken when choosing which pages to crawl next; in this
paper, we present a well-motivated page selection algorithm
that also performs well empirically. This algorithm is derived
from a well-defined problem objective and has a running
time linear in the number of local nodes.
We experiment across several types of local subgraphs,
including four topic related communities and several site-specific
domains. To evaluate performance, we measure the
difference between the current global PageRank estimate
and the global PageRank, as a function of the number of
pages crawled. We compare our algorithm against several
heuristics and also against a baseline algorithm that chooses
pages at random, and we show that our method outperforms
these other methods. Finally, we empirically demonstrate
that, given a local domain of size n, we can provide good
approximations to the global PageRank values by crawling
at most n or 2n additional pages.
The paper is organized as follows.
Section 2 gives an
overview of localized search engines and outlines their advantages
over global search. Section 3 provides background
on the PageRank algorithm. Section 4 formally defines our
problem, and section 5 presents our page selection criteria
and derives our algorithms. Section 6 provides experimental
results, section 7 gives an overview of related work, and,
finally, conclusions are given in section 8.
</SECTION>
<SECTION header="LOCALIZED SEARCH ENGINES">Localized search engines index a single community of the
web, typically either a site-specific community, or a topic-specific
community. Localized search engines enjoy three
major advantages over their large-scale counterparts: they
are relatively inexpensive to build, they can offer more precise
search capability over their local domain, and they can
provide a more complete index.
The resources needed to build a global search engine are
enormous. A 2003 study by Lyman et al. [13] found that
the `surface web' (publicly available static sites) consists of
8.9 billion pages, and that the average size of these pages is
approximately 18.7 kilobytes. To download a crawl of this
size, approximately 167 terabytes of space is needed. For a
researcher who wishes to build a search engine with access
to a couple of workstations or a small server, storage of this
magnitude is simply not available. However, building a localized
search engine over a web community of a hundred
thousand pages would only require a few gigabytes of storage
. The computational burden required to support search
queries over a database this size is more manageable as well.
We note that, for topic-specific search engines, the relevant
community can be efficiently identified and downloaded by
using a focused crawler [21, 4].
For site-specific domains, the local domain is readily available
on their own web server. This obviates the need for
crawling or spidering, and a complete and up-to-date index
of the domain can thus be guaranteed. This is in contrast
to their large-scale counterparts, which suffer from several
shortcomings. First, crawling dynamically generated
pages--pages in the `hidden web'--has been the subject of
research [20] and is a non-trivial task for an external crawler.
Second, site-specific domains can enable the robots exclusion
policy. This prohibits external search engines' crawlers
from downloading content from the domain, and an external
search engine must instead rely on outside links and anchor
text to index these restricted pages.
By restricting itself to only a specific domain of the internet
, a localized search engine can provide more precise
search results.
Consider the canonical ambiguous search
query, `jaguar', which can refer to either the car manufacturer
or the animal. A scientist trying to research the habitat
and evolutionary history of a jaguar may have better
success using a finely tuned zoology-specific search engine
than querying Google with multiple keyword searches and
wading through irrelevant results. A method to learn better
ranking functions for retrieval was recently proposed by
Radlinski and Joachims [19] and has been applied to various
local domains, including Cornell University's website [8].
</SECTION>
<SECTION header="PAGERANK OVERVIEW">The PageRank algorithm defines the importance of web
pages by analyzing the underlying hyperlink structure of a
web graph. The algorithm works by building a Markov chain
from the link structure of the web graph and computing its
stationary distribution. One way to compute the stationary
distribution of a Markov chain is to find the limiting
distribution of a random walk over the chain. Thus, the
PageRank algorithm uses what is sometimes referred to as
the `random surfer' model. In each step of the random walk,
the `surfer' either follows an outlink from the current page
(i.e. the current node in the chain), or jumps to a random
page on the web.
We now precisely define the PageRank problem. Let U
be an m
× m adjacency matrix for a given web graph such
that U
ji
= 1 if page i links to page j and U
ji
= 0 otherwise.
We define the PageRank matrix P
U
to be:
P
U
= U D
-1
U
+ (1
- )ve
T
,
(1)
where D
U
is the (unique) diagonal matrix such that U D
-1
U
is column stochastic,  is a given scalar such that 0
1,
e is the vector of all ones, and v is a non-negative, L
1
normalized
vector, sometimes called the `random surfer' vector
. Note that the matrix D
-1
U
is well-defined only if each
column of U has at least one non-zero entry--i.e., each page
in the webgraph has at least one outlink. In the presence of
such `dangling nodes' that have no outlinks, one commonly
used solution, proposed by Brin et al. [3], is to replace each
zero column of U by a non-negative, L
1
-normalized vector.
The PageRank vector r is the dominant eigenvector of the
PageRank matrix, r = P
U
r. We will assume, without loss of
generality, that r has an L
1
-norm of one. Computationally,
r can be computed using the power method. This method
first chooses a random starting vector r
(0)
, and iteratively
multiplies the current vector by the PageRank matrix P
U
;
see Algorithm 1. In general, each iteration of the power
method can take O(m
2
) operations when P
U
is a dense matrix
. However, in practice, the number of links in a web
graph will be of the order of the number of pages. By exploiting
the sparsity of the PageRank matrix, the work per
iteration can be reduced to O(km), where k is the average
number of links per web page. It has also been shown that
the total number of iterations needed for convergence is proportional
to  and does not depend on the size of the web
graph [11, 7]. Finally, the total space needed is also O(km),
mainly to store the matrix U .
117
Research Track Paper
Algorithm 1:
A linear time (per iteration) algorithm for
computing PageRank.
ComputePR
(U )
Input:
U : Adjacency matrix.
Output:
r: PageRank vector.
Choose (randomly) an initial non-negative vector r
(0)
such that r
(0) 1
= 1.
i
0
repeat
i
i + 1
U D
-1
U
r
(i-1)
{ is the random surfing probability
}r
(i)
+ (1 - )v {v is the random surfer vector.}
until
r
(i)
- r
(i-1)
&lt;
{ is the convergence threshold.}
r  r
(i)
</SECTION>
<SECTION header="PROBLEM DEFINITION">Given a local domain L, let G be an N
× N adjacency
matrix for the entire connected component of the web that
contains L, such that G
ji
= 1 if page i links to page j
and G
ji
= 0 otherwise. Without loss of generality, we will
partition G as:
G =
L
G
out
L
out
G
within
,
(2)
where L is the n
× n local subgraph corresponding to links
inside the local domain, L
out
is the subgraph that corresponds
to links from the local domain pointing out to the
global domain, G
out
is the subgraph containing links from
the global domain into the local domain, and G
within
contains
links within the global domain. We assume that when
building a localized search engine, only pages inside the local
domain are crawled, and the links between these pages
are represented by the subgraph L. The links in L
out
are
also known, as these point from crawled pages in the local
domain to uncrawled pages in the global domain.
As defined in equation (1), P
G
is the PageRank matrix
formed from the global graph G, and we define the global
PageRank vector of this graph to be g. Let the n-length
vector p

be the L
1
-normalized vector corresponding to the
global PageRank of the pages in the local domain L:
p

=
E
L
g
E
L
g
1
,
where E
L
= [ I
| 0 ] is the restriction matrix that selects
the components from g corresponding to nodes in L. Let p
denote the PageRank vector constructed from the local domain
subgraph L. In practice, the observed local PageRank
p and the global PageRank p

will be quite different. One
would expect that as the size of local matrix L approaches
the size of global matrix G, the global PageRank and the observed
local PageRank will become more similar. Thus, one
approach to estimating the global PageRank is to crawl the
entire global domain, compute its PageRank, and extract
the PageRanks of the local domain.
Typically, however, n
N , i.e., the number of global
pages is much larger than the number of local pages. Therefore
, crawling all global pages will quickly exhaust all local
resources (computational, storage, and bandwidth) available
to create the local search engine. We instead seek a supergraph
^
F of our local subgraph L with size O(n). Our goal
Algorithm 2:
The FindGlobalPR algorithm.
FindGlobalPR
(L, L
out
, T , k)
Input:
L: zero-one adjacency matrix for the local domain
, L
out
: zero-one outlink matrix from L to global
subgraph as in (2), T : number of iterations, k: number of
pages to crawl per iteration.
Output: ^
p: an improved estimate of the global PageRank
of L.
F  L
F
out
L
out
f  ComputePR(F )
for (i = 1 to T )
{Determine which pages to crawl next}
pages
SelectNodes(F , F
out
, f , k)
Crawl pages, augment F and modify F
out
{Update PageRanks for new local domain}
f  ComputePR(F )
end
{Extract PageRanks of original local domain & normalize}
^
p
E
L
f
E
L
f
1
is to find such a supergraph ^
F with PageRank ^
f , so that
^
f when restricted to L is close to p

. Formally, we seek to
minimize
GlobalDif f ( ^
f ) =
E
L
^
f
E
L
^
f
1
- p

1
.
(3)
We choose the L
1
norm for measuring the error as it does
not place excessive weight on outliers (as the L
2
norm does,
for example), and also because it is the most commonly used
distance measure in the literature for comparing PageRank
vectors, as well as for detecting convergence of the algorithm
[3].
We propose a greedy framework, given in Algorithm 2,
for constructing ^
F . Initially, F is set to the local subgraph
L, and the PageRank f of this graph is computed. The algorithm
then proceeds as follows. First, the SelectNodes
algorithm (which we discuss in the next section) is called
and it returns a set of k nodes to crawl next from the set
of nodes in the current crawl frontier, F
out
. These selected
nodes are then crawled to expand the local subgraph, F , and
the PageRanks of this expanded graph are then recomputed.
These steps are repeated for each of T iterations. Finally,
the PageRank vector ^
p, which is restricted to pages within
the original local domain, is returned. Given our computation
, bandwidth, and memory restrictions, we will assume
that the algorithm will crawl at most O(n) pages. Since the
PageRanks are computed in each iteration of the algorithm,
which is an O(n) operation, we will also assume that the
number of iterations T is a constant. Of course, the main
challenge here is in selecting which set of k nodes to crawl
next. In the next section, we formally define the problem
and give efficient algorithms.
</SECTION>
<SECTION header="NODE SELECTION">In this section, we present node selection algorithms that
operate within the greedy framework presented in the previous
section. We first give a well-defined criteria for the
page selection problem and provide experimental evidence
that this criteria can effectively identify pages that optimize
our problem objective (3). We then present our main al-118
Research Track Paper
gorithmic contribution of the paper, a method with linear
running time that is derived from this page selection criteria
. Finally, we give an intuitive analysis of our algorithm in
terms of `leaks' and `flows'. We show that if only the `flow'
is considered, then the resulting method is very similar to a
widely used page selection heuristic [6].
5.1
Formulation
For a given page j in the global domain, we define the
expanded local graph F
j
:
F
j
=
F
s
u
T
j
0
,
(4)
where u
j
is the zero-one vector containing the outlinks from
F into page j, and s contains the inlinks from page j into
the local domain. Note that we do not allow self-links in
this framework. In practice, self-links are often removed, as
they only serve to inflate a given page's PageRank.
Observe that the inlinks into F from node j are not known
until after node j is crawled. Therefore, we estimate this
inlink vector as the expectation over inlink counts among
the set of already crawled pages,
s =
F
T
e
F
T
e
1
.
(5)
In practice, for any given page, this estimate may not reflect
the true inlinks from that page. Furthermore, this expectation
is sampled from the set of links within the crawled
domain, whereas a better estimate would also use links from
the global domain. However, the latter distribution is not
known to a localized search engine, and we contend that the
above estimate will, on average, be a better estimate than
the uniform distribution, for example.
Let the PageRank of F be f . We express the PageRank
f
+
j
of the expanded local graph F
j
as
f
+
j
=
(1
- x
j
)f
j
x
j
,
(6)
where x
j
is the PageRank of the candidate global node j,
and f
j
is the L
1
-normalized PageRank vector restricted to
the pages in F .
Since directly optimizing our problem goal requires knowing
the global PageRank p

, we instead propose to crawl
those nodes that will have the greatest influence on the PageRanks
of pages in the original local domain L:
influence(j)
=
kL
|f
j
[k]
- f[k]|
(7)
=
E
L
(f
j
- f )
1
.
Experimentally, the influence score is a very good predictor
of our problem objective (3). For each candidate global node
j, figure 1(a) shows the objective function value Global Diff(f
j
)
as a function of the influence of page j. The local domain
used here is a crawl of conservative political pages (we will
provide more details about this dataset in section 6); we
observed similar results in other domains. The correlation
is quite strong, implying that the influence criteria can effectively
identify pages that improve the global PageRank
estimate. As a baseline, figure 1(b) compares our objective
with an alternative criteria, outlink count. The outlink
count is defined as the number of outlinks from the local
domain to page j. The correlation here is much weaker.
.00001
.0001
.001
.01
0.26
0.262
0.264
0.266
Influence
Objective
1
10
100
1000
0.266
0.264
0.262
0.26
Outlink Count
Objective
(a)
(b)
Figure 1: (a) The correlation between our
influence
page selection criteria (7) and the actual objective
function (3) value is quite strong. (b) This is in contrast
to other criteria, such as outlink count, which
exhibit a much weaker correlation.
5.2
Computation
As described, for each candidate global page j, the influence
score (7) must be computed.
If f
j
is computed
exactly for each global page j, then the PageRank algorithm
would need to be run for each of the O(n) such global
pages j we consider, resulting in an O(n
2
) computational
cost for the node selection method. Thus, computing the
exact value of f
j
will lead to a quadratic algorithm, and we
must instead turn to methods of approximating this vector.
The algorithm we present works by performing one power
method iteration used by the PageRank algorithm (Algorithm
1). The convergence rate for the PageRank algorithm
has been shown to equal the random surfer probability  [7,
11]. Given a starting vector x
(0)
, if k PageRank iterations
are performed, the current PageRank solution x
(k)
satisfies:
x
(k)
- x
1
= O(
k
x
(0)
- x
1
),
(8)
where x

is the desired PageRank vector. Therefore, if only
one iteration is performed, choosing a good starting vector
is necessary to achieve an accurate approximation.
We partition the PageRank matrix P
F
j
, corresponding to
the
× subgraph F
j
as:
P
F
j
=
~
F
~
s
~
u
T
j
w
,
(9)
where
~
F
=
F (D
F
+ diag(u
j
))
-1
+ (1
- ) e
+ 1 e
T
,
~
s = s + (1 - ) e
+ 1 ,
~
u
j
=
(D
F
+ diag(u
j
))
-1
u
j
+ (1
- ) e
+ 1 ,
w
=
1
- 
+ 1 ,
and diag(u
j
) is the diagonal matrix with the (i, i)
th
entry
equal to one if the i
th
element of u
j
equals one, and is zero
otherwise. We have assumed here that the random surfer
vector is the uniform vector, and that L has no `dangling
links'. These assumptions are not necessary and serve only
to simplify discussion and analysis.
A simple approach for estimating f
j
is the following. First,
estimate the PageRank f
+
j
of F
j
by computing one PageRank
iteration over the matrix P
F
j
, using the starting vector
 =
f
0
. Then, estimate f
j
by removing the last
119
Research Track Paper
component from our estimate of f
+
j
(i.e., the component
corresponding to the added node j), and renormalizing.
The problem with this approach is in the starting vector.
Recall from (6) that x
j
is the PageRank of the added node
j. The difference between the actual PageRank f
+
j
of P
F
j
and the starting vector  is
- f
+
j
1
=
x
j
+ f
- (1 - x
j
)f
j 1
x
j
+
| f
1
- (1 - x
j
) f
j 1
|
=
x
j
+
|x
j
|
=
2x
j
.
Thus, by (8), after one PageRank iteration, we expect our
estimate of f
+
j
to still have an error of about 2x
j
. In particular
, for candidate nodes j with relatively high PageRank
x
j
, this method will yield more inaccurate results. We will
next present a method that eliminates this bias and runs in
O(n) time.
5.2.1
Stochastic Complementation
Since f
+
j
, as given in (6) is the PageRank of the matrix
P
F
j
, we have:
f
j
(1
- x
j
)
x
j
=
~
F
~
s
~
u
T
j
w
f
j
(1
- x
j
)
x
j
=
~
F f
j
(1
- x
j
) + ~
sx
j
~
u
T
j
f
j
(1
- x
j
) + wx
j
.
Solving the above system for f
j
can be shown to yield
f
j
= ( ~
F + (1 - w)
-1
~
s ~
u
T
j
)f
j
.
(10)
The matrix S = ~
F +(1-w)
-1
~
s ~
u
T
j
is known as the stochastic
complement of the column stochastic matrix P
F
j
with respect
to the sub matrix ~
F . The theory of stochastic complementation
is well studied, and it can be shown the stochastic
complement of an irreducible matrix (such as the PageRank
matrix) is unique. Furthermore, the stochastic complement
is also irreducible and therefore has a unique stationary distribution
as well. For an extensive study, see [15].
It can be easily shown that the sub-dominant eigenvalue
of S is at most
+1
, where
is the size of F . For sufficiently
large , this value will be very close to . This is important,
as other properties of the PageRank algorithm, notably the
algorithm's sensitivity, are dependent on this value [11].
In this method, we estimate the length
vector f
j
by
computing one PageRank iteration over the
× stochastic
complement S, starting at the vector f :
f
j
Sf.
(11)
This is in contrast to the simple method outlined in the previous
section, which first iterates over the ( + 1)
× ( + 1)
matrix P
F
j
to estimate f
+
j
, and then removes the last component
from the estimate and renormalizes to approximate
f
j
. The problem with the latter method is in the choice
of the ( + 1) length starting vector, . Consequently, the
PageRank estimate given by the simple method differs from
the true PageRank by at least 2x
j
, where x
j
is the PageRank
of page j. By using the stochastic complement, we
can establish a tight lower bound of zero for this difference.
To see this, consider the case in which a node k is added
to F to form the augmented local subgraph F
k
, and that
the PageRank of this new graph is
(1
- x
k
)f
x
k
. Specifi-cally
, the addition of page k does not change the PageRanks
of the pages in F , and thus f
k
= f . By construction of
the stochastic complement, f
k
= Sf
k
, so the approximation
given in equation (11) will yield the exact solution.
Next, we present the computational details needed to efficiently
compute the quantity f
j
-f
1
over all known global
pages j. We begin by expanding the difference f
j
-f , where
the vector f
j
is estimated as in (11),
f
j
- f  Sf - f
=
F (D
F
+ diag(u
j
))
-1
f + (1 - ) e
+ 1 e
T
f
+(1
- w)
-1
( ~
u
T
j
f )~
s - f.
(12)
Note that the matrix (D
F
+diag(u
j
))
-1
is diagonal. Letting
o[k] be the outlink count for page k in F , we can express
the k
th
diagonal element as:
(D
F
+ diag(u
j
))
-1
[k, k] =
1
o[k]+1
if u
j
[k] = 1
1
o[k]
if u
j
[k] = 0
Noting that (o[k] + 1)
-1
= o[k]
-1
- (o[k](o[k] + 1))
-1
and
rewriting this in matrix form yields
(D
F
+diag(u
j
))
-1
= D
-1
F
-D
-1
F
(D
F
+diag(u
j
))
-1
diag(u
j
).
(13)
We use the same identity to express
e
+ 1 =
e - e
( + 1) .
(14)
Recall that, by definition, we have P
F
= F D
-1
F
+(1
-)
e
.
Substituting (13) and (14) in (12) yields
f
j
- f  (P
F
f - f)
-F D
-1
F
(D
F
+ diag(u
j
))
-1
diag(u
j
)f
-(1 - )
e
( + 1) + (1 - w)
-1
( ~
u
T
j
f )~
s
=
x + y + ( ~
u
T
j
f )z,
(15)
noting that by definition, f = P
F
f , and defining the vectors
x, y, and z to be
x = -F D
-1
F
(D
F
+ diag(u
j
))
-1
diag(u
j
)f
(16)
y = -(1 - )
e
( + 1)
(17)
z = (1 - w)
-1
~
s.
(18)
The first term x is a sparse vector, and takes non-zero values
only for local pages k that are siblings of the global page
j. We define (i, j)
F if and only if F [j, i] = 1 (equiva-lently
, page i links to page j) and express the value of the
component x[k ] as:
x[k ] =
k
:(k,k )F ,u
j
[k]=1
f [k]
o[k](o[k] + 1) ,
(19)
where o[k], as before, is the number of outlinks from page k
in the local domain. Note that the last two terms, y and z
are not dependent on the current global node j. Given the
function h
j
(f ) =
y + ( ~
u
T
j
f )z
1
, the quantity
f
j
- f
1
120
Research Track Paper
can be expressed as
f
j
- f
1
=
k
x[k] + y[k] + ( ~
u
T
j
f )z[k]
=
k:x[k]=0
y[k] + ( ~
u
T
j
f )z[k]
+
k:x[k]=0
x[k] + y[k] + ( ~
u
T
j
f )z[k]
=
h
j
(f )
k
:x[k]=0
y[k] + ( ~
u
T
j
f )z[k]
+
k:x[k]=0
x[k] + y[k] + ( ~
u
T
j
f )z[k] .(20)
If we can compute the function h
j
in linear time, then we
can compute each value of
f
j
- f
1
using an additional
amount of time that is proportional to the number of non-zero
components in x. These optimizations are carried out
in Algorithm 3. Note that (20) computes the difference between
all components of f and f
j
, whereas our node selection
criteria, given in (7), is restricted to the components
corresponding to nodes in the original local domain L.
Let us examine Algorithm 3 in more detail. First, the
algorithm computes the outlink counts for each page in the
local domain. The algorithm then computes the quantity
~
u
T
j
f for each known global page j. This inner product can
be written as
(1
- ) 1
+ 1 +
k:(k,j)F
out
f [k]
o[k] + 1 ,
where the second term sums over the set of local pages that
link to page j. Since the total number of edges in F
out
was
assumed to have size O( ) (recall that
is the number of
pages in F ), the running time of this step is also O( ).
The algorithm then computes the vectors y and z, as
given in (17) and (18), respectively.
The L
1
NormDiff
method is called on the components of these vectors which
correspond to the pages in L, and it estimates the value of
E
L
(y + ( ~
u
T
j
f )z)
1
for each page j. The estimation works
as follows. First, the values of ~
u
T
j
f are discretized uniformly
into c values
{a
1
, ..., a
c
}. The quantity E
L
(y + a
i
z)
1
is
then computed for each discretized value of a
i
and stored in
a table. To evaluate
E
L
(y + az)
1
for some a
[a
1
, a
c
],
the closest discretized value a
i
is determined, and the corresponding
entry in the table is used. The total running time
for this method is linear in
and the discretization parameter
c (which we take to be a constant). We note that if exact
values are desired, we have also developed an algorithm that
runs in O( log ) time that is not described here.
In the main loop, we compute the vector x, as defined
in equation (16). The nested loops iterate over the set of
pages in F that are siblings of page j. Typically, the size
of this set is bounded by a constant. Finally, for each page
j, the scores vector is updated over the set of non-zero
components k of the vector x with k
L. This set has
size equal to the number of local siblings of page j, and is
a subset of the total number of siblings of page j. Thus,
each iteration of the main loop takes constant time, and the
total running time of the main loop is O( ). Since we have
assumed that the size of F will not grow larger than O(n),
the total running time for the algorithm is O(n).
Algorithm 3:
Node Selection via Stochastic
Complementation.
SC-Select
(F , F
out
, f , k)
Input:
F : zero-one adjacency matrix of size corresponding
to the current local subgraph, F
out
: zero-one
outlink matrix from F to global subgraph, f : PageRank
of F , k: number of pages to return
Output: pages: set of k pages to crawl next
{Compute outlink sums for local subgraph}
foreach (page j
F )
o[j]

k:(j,k)F
F [j, k]
end
{Compute scalar ~u
T
j
f for each global node j }
foreach (page j
F
out
)
g[j]
(1 - )
1
+1
foreach (page k : (k, j)
F
out
)
g[j]
g[j] +
f[k]
o[k]+1
end
end
{Compute vectors y and z as in (17) and (18) }
y  -(1 - )
e
( +1)
z  (1 - w)
-1
~
s
{Approximate y + g[j]  z
1
for all values g[j]
}
norm diffs
L
1
NormDiffs
(g, E
L
y, E
L
z)
foreach (page j
F
out
)
{Compute sparse vector x as in (19)}
x  0
foreach (page k : (k, j)
F
out
)
foreach (page k : (k, k )
F ))
x[k ]
x[k ] f
[k]
o[k](o[k]+1)
end
end
x  x
scores[j]
norm diffs[j]
foreach (k : x[k] &gt; 0 and page k
L)
scores[j]
scores[j] - |y[k] + g[j]  z[k]|
+
|x[k]+y[k]+g[j]z[k])|
end
end
Return k pages with highest scores
5.2.2
PageRank Flows
We now present an intuitive analysis of the stochastic
complementation method by decomposing the change in PageRank
in terms of `leaks' and `flows'. This analysis is motivated
by the decomposition given in (15). PageRank `flow' is
the increase in the local PageRanks originating from global
page j. The flows are represented by the non-negative vector
( ~
u
T
j
f )z (equations (15) and (18)). The scalar ~
u
T
j
f can be
thought of as the total amount of PageRank flow that page
j has available to distribute. The vector z dictates how the
flow is allocated to the local domain; the flow that local
page k receives is proportional to (within a constant factor
due to the random surfer vector) the expected number of its
inlinks.
The PageRank `leaks' represent the decrease in PageRank
resulting from the addition of page j.
The leakage can
be quantified in terms of the non-positive vectors x and
y (equations (16) and (17)). For vector x, we can see from
equation (19) that the amount of PageRank leaked by a
local page is proportional to the weighted sum of the Page-121
Research Track Paper
Ranks of its siblings. Thus, pages that have siblings with
higher PageRanks (and low outlink counts) will experience
more leakage. The leakage caused by y is an artifact of the
random surfer vector.
We will next show that if only the `flow' term, ( ~
u
T
j
f )z,
is considered, then the resulting method is very similar to
a heuristic proposed by Cho et al. [6] that has been widely
used for the "Crawling Through URL Ordering" problem.
This heuristic is computationally cheaper, but as we will see
later, not as effective as the Stochastic Complementation
method.
Our node selection strategy chooses global nodes that
have the largest influence (equation (7)). If this influence is
approximated using only `flows', the optimal node j

is:
j

=
argmax
j
E
L
~
u
T
j
f z
1
=
argmax
j
~
u
T
j
f E
L
z
1
=
argmax
j
~
u
T
j
f
=
argmax
j
(D
F
+ diag(u
j
))
-1
u
j
+ (1
- ) e
+ 1 , f
=
argmax
j
f
T
(D
F
+ diag(u
j
))
-1
u
j
.
The resulting page selection score can be expressed as a sum
of the PageRanks of each local page k that links to j, where
each PageRank value is normalized by o[k]+1. Interestingly,
the normalization that arises in our method differs from the
heuristic given in [6], which normalizes by o[k].
The algorithm
PF-Select, which is omitted due to lack of space,
first computes the quantity f
T
(D
F
+diag(u
j
))
-1
u
j
for each
global page j, and then returns the pages with the k largest
scores. To see that the running time for this algorithm is
O(n), note that the computation involved in this method is
a subset of that needed for the SC-Select method (Algorithm
3), which was shown to have a running time of O(n).
</SECTION>
<SECTION header="EXPERIMENTS">In this section, we provide experimental evidence to verify
the effectiveness of our algorithms. We first outline our
experimental methodology and then provide results across
a variety of local domains.
6.1
Methodology
Given the limited resources available at an academic institution
, crawling a section of the web that is of the same
magnitude as that indexed by Google or Yahoo! is clearly
infeasible. Thus, for a given local domain, we approximate
the global graph by crawling a local neighborhood around
the domain that is several orders of magnitude larger than
the local subgraph. Even though such a graph is still orders
of magnitude smaller than the `true' global graph, we contend
that, even if there exist some highly influential pages
that are very far away from our local domain, it is unrealis-tic
for any local node selection algorithm to find them. Such
pages also tend to be highly unrelated to pages within the
local domain.
When explaining our node selection strategies in section
5, we made the simplifying assumption that our local graph
contained no dangling nodes.
This assumption was only
made to ease our analysis. Our implementation efficiently
handles dangling links by replacing each zero column of our
adjacency matrix with the uniform vector. We evaluate the
algorithm using the two node selection strategies given in
Section 5.2, and also against the following baseline methods:
· Random: Nodes are chosen uniformly at random among
the known global nodes.
· OutlinkCount: Global nodes with the highest number
of outlinks from the local domain are chosen.
At each iteration of the FindGlobalPR algorithm, we evaluate
performance by computing the difference between the
current PageRank estimate of the local domain,
E
L
f
E
L
f
1
, and
the global PageRank of the local domain
E
L
g
E
L
g
1
. All PageRank
calculations were performed using the uniform random
surfer vector. Across all experiments, we set the random
surfer parameter , to be .85, and used a convergence
threshold of 10
-6
. We evaluate the difference between the
local and global PageRank vectors using three different metrics
: the L
1
and L

norms, and Kendall's tau. The L
1
norm
measures the sum of the absolute value of the differences between
the two vectors, and the L

norm measures the absolute
value of the largest difference. Kendall's tau metric is
a popular rank correlation measure used to compare PageRanks
[2, 11]. This metric can be computed by counting
the number of pairs of pairs that agree in ranking, and subtracting
from that the number of pairs of pairs that disagree
in ranking. The final value is then normalized by the total
number of
n
2
such pairs, resulting in a [
-1, 1] range, where
a negative score signifies anti-correlation among rankings,
and values near one correspond to strong rank correlation.
6.2
Results
Our experiments are based on two large web crawls and
were downloaded using the web crawler that is part of the
Nutch open source search engine project [18]. All crawls
were restricted to only `http' pages, and to limit the number
of dynamically generated pages that we crawl, we ig-nored
all pages with urls containing any of the characters
`?', `*', `@', or `='. The first crawl, which we will refer to
as the `edu' dataset, was seeded by homepages of the top
100 graduate computer science departments in the USA, as
rated by the US News and World Report [16], and also by
the home pages of their respective institutions. A crawl of
depth 5 was performed, restricted to pages within the `.edu'
domain, resulting in a graph with approximately 4.7 million
pages and 22.9 million links. The second crawl was seeded
by the set of pages under the `politics' hierarchy in the dmoz
open directory project[17]. We crawled all pages up to four
links away, which yielded a graph with 4.4 million pages and
17.3 million links.
Within the `edu' crawl, we identified the five site-specific
domains corresponding to the websites of the top five graduate
computer science departments, as ranked by the US
News and World Report. This yielded local domains of various
sizes, from 10,626 (UIUC) to 59,895 (Berkeley). For each
of these site-specific domains with size n, we performed 50
iterations of the FindGlobalPR algorithm to crawl a total
of 2n additional nodes. Figure 2(a) gives the (L
1
) difference
from the PageRank estimate at each iteration to the global
PageRank, for the Berkeley local domain.
The performance of this dataset was representative of the
typical performance across the five computer science site-specific
local domains. Initially, the L
1
difference between
the global and local PageRanks ranged from .0469 (Stanford
) to .149 (MIT). For the first several iterations, the
122
Research Track Paper
0
5
10
15
20
25
30
35
40
45
50
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0.055
Number of Iterations
Global and Local PageRank Difference (L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
0
10
20
30
40
50
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Number of Iterations
Global and Local PageRank Difference (L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
0
5
10
15
20
25
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
Number of Iterations
Global and Local PageRank Difference (L1)
Stochastic Complement
PageRank Flow
Outlink Count
Random
(a) www.cs.berkeley.edu
(b) www.enterstageright.com
(c) Politics
Figure 2: L
1
difference between the estimated and true global PageRanks for (a) Berkeley's computer science
website, (b) the site-specific domain, www.enterstageright.com, and (c) the `politics' topic-specific domain. The
stochastic complement method outperforms all other methods across various domains.
three link-based methods all outperform the random selection
heuristic.
After these initial iterations, the random
heuristic tended to be more competitive with (or even outperform
, as in the Berkeley local domain) the outlink count
and PageRank flow heuristics. In all tests, the stochastic
complementation method either outperformed, or was competitive
with, the other methods. Table 1 gives the average
difference between the final estimated global PageRanks and
the true global PageRanks for various distance measures.
Algorithm
L
1
L

Kendall
Stoch. Comp.
.0384
.00154
.9257
PR Flow
.0470
.00272
.8946
Outlink
.0419
.00196
.9053
Random
.0407
.00204
.9086
Table 1: Average final performance of various node
selection strategies for the five site-specific computer
science local domains.
Note that Kendall's
Tau measures similarity, while the other metrics are
dissimilarity measures.
Stochastic Complementation
clearly outperforms the other methods in all
metrics.
Within the `politics' dataset, we also performed two site-specific
tests for the largest websites in the crawl: www.adam-smith
.org, the website for the London based Adam Smith
Institute, and www.enterstageright.com, an online conservative
journal. As with the `edu' local domains, we ran our
algorithm for 50 iterations, crawling a total of 2n nodes. Figure
2 (b) plots the results for the www.enterstageright.com
domain. In contrast to the `edu' local domains, the Random
and OutlinkCount methods were not competitive with either
the SC-Select or the PF-Select methods. Among all
datasets and all node selection methods, the stochastic complementation
method was most impressive in this dataset,
realizing a final estimate that differed only .0279 from the
global PageRank, a ten-fold improvement over the initial local
PageRank difference of .299. For the Adam Smith local
domain, the initial difference between the local and global
PageRanks was .148, and the final estimates given by the
SC-Select
, PF-Select, OutlinkCount, and Random
methods were .0208, .0193, .0222, and .0356, respectively.
Within the `politics' dataset, we constructed four topic-specific
local domains.
The first domain consisted of all
pages in the dmoz politics category, and also all pages within
each of these sites up to two links away. This yielded a local
domain of 90,811 pages, and the results are given in figure 2
(c). Because of the larger size of the topic-specific domains,
we ran our algorithm for only 25 iterations to crawl a total
of n nodes.
We also created topic-specific domains from three political
sub-topics: liberalism, conservatism, and socialism. The
pages in these domains were identified by their corresponding
dmoz categories. For each sub-topic, we set the local
domain to be all pages within three links from the corresponding
dmoz category pages.
Table 2 summarizes the
performance of these three topic-specific domains, and also
the larger political domain.
To quantify a global page j's effect on the global PageRank
values of pages in the local domain, we define page
j's impact to be its PageRank value, g[j], normalized by the
fraction of its outlinks pointing to the local domain:
impact(j) = o
L
[j]
o[j] · g[j],
where, o
L
[j] is the number of outlinks from page j to pages
in the local domain L, and o[j] is the total number of j's
outlinks. In terms of the random surfer model, the impact
of page j is the probability that the random surfer (1) is
currently at global page j in her random walk and (2) takes
an outlink to a local page, given that she has already decided
not to jump to a random page.
For the politics local domain, we found that many of the
pages with high impact were in fact political pages that
should have been included in the dmoz politics topic, but
were not. For example, the two most influential global pages
were the political search engine www.askhenry.com, and the
home page of the online political magazine, www.policy-review
.com. Among non-political pages, the home page of
the journal "Education Next" was most influential.
The
journal is freely available online and contains articles regarding
various aspect of K-12 education in America. To provide
some anecdotal evidence for the effectiveness of our page selection
methods, we note that the SC-Select method chose
11 pages within the www.educationnext.org domain, the
PF-Select
method discovered 7 such pages, while the OutlinkCount
and Random methods found only 6 pages each.
For the conservative political local domain, the socialist
website www.ornery.org had a very high impact score. This
123
Research Track Paper
All Politics:
Algorithm
L
1
L
2
Kendall
Stoch. Comp.
.1253
.000700
.8671
PR Flow
.1446
.000710
.8518
Outlink
.1470
.00225
.8642
Random
.2055
.00203
.8271
Conservativism:
Algorithm
L
1
L
2
Kendall
Stoch. Comp.
.0496
.000990
.9158
PR Flow
.0554
.000939
.9028
Outlink
.0602
.00527
.9144
Random
.1197
.00102
.8843
Liberalism:
Algorithm
L
1
L
2
Kendall
Stoch. Comp.
.0622
.001360
.8848
PR Flow
.0799
.001378
.8669
Outlink
.0763
.001379
.8844
Random
.1127
.001899
.8372
Socialism:
Algorithm
L
1
L

Kendall
Stoch. Comp.
.04318
.00439
.9604
PR Flow
.0450
.004251
.9559
Outlink
.04282
.00344
.9591
Random
.0631
.005123
.9350
Table 2: Final performance among node selection
strategies for the four political topic-specific crawls.
Note that Kendall's Tau measures similarity, while
the other metrics are dissimilarity measures.
was largely due to a link from the front page of this site
to an article regarding global warming published by the
National Center for Public Policy Research, a conservative
research group in Washington, DC. Not surprisingly, the
global PageRank of this article (which happens to be on the
home page of the NCCPR, www.nationalresearch.com),
was approximately .002, whereas the local PageRank of this
page was only .00158. The SC-Select method yielded a
global PageRank estimate of approximately .00182, the PF-Select
method estimated a value of .00167, and the Random
and OutlinkCount methods yielded values of .01522
and .00171, respectively.
</SECTION>
<SECTION header="RELATED WORK">The node selection framework we have proposed is similar
to the url ordering for crawling problem proposed by Cho
et al. in [6]. Whereas our framework seeks to minimize the
difference between the global and local PageRank, the objective
used in [6] is to crawl the most highly (globally) ranked
pages first. They propose several node selection algorithms,
including the outlink count heuristic, as well as a variant of
our PF-Select algorithm which they refer to as the `Page-Rank
ordering metric'. They found this method to be most
effective in optimizing their objective, as did a recent survey
of these methods by Baeza-Yates et al. [1]. Boldi et al. also
experiment within a similar crawling framework in [2], but
quantify their results by comparing Kendall's rank correlation
between the PageRanks of the current set of crawled
pages and those of the entire global graph. They found that
node selection strategies that crawled pages with the highest
global PageRank first actually performed worse (with
respect to Kendall's Tau correlation between the local and
global PageRanks) than basic depth first or breadth first
strategies. However, their experiments differ from our work
in that our node selection algorithms do not use (or have
access to) global PageRank values.
Many algorithmic improvements for computing exact PageRank
values have been proposed [9, 10, 14]. If such algorithms
are used to compute the global PageRanks of our
local domain, they would all require O(N ) computation,
storage, and bandwidth, where N is the size of the global
domain. This is in contrast to our method, which approximates
the global PageRank and scales linearly with the size
of the local domain.
Wang and Dewitt [22] propose a system where the set of
web servers that comprise the global domain communicate
with each other to compute their respective global PageRanks
. For a given web server hosting n pages, the computational
, bandwidth, and storage requirements are also
linear in n. One drawback of this system is that the number
of distinct web servers that comprise the global domain
can be very large. For example, our `edu' dataset contains
websites from over 3,200 different universities; coordinating
such a system among a large number of sites can be very
difficult.
Gan, Chen, and Suel propose a method for estimating the
PageRank of a single page [5] which uses only constant bandwidth
, computation, and space. Their approach relies on the
availability of a remote connectivity server that can supply
the set of inlinks to a given page, an assumption not used in
our framework. They experimentally show that a reasonable
estimate of the node's PageRank can be obtained by visiting
at most a few hundred nodes. Using their algorithm for our
problem would require that either the entire global domain
first be downloaded or a connectivity server be used, both
of which would lead to very large web graphs.
</SECTION>
<SECTION header="CONCLUSIONS AND FUTURE WORK">The internet is growing exponentially, and in order to navigate
such a large repository as the web, global search engines
have established themselves as a necessity. Along with
the ubiquity of these large-scale search engines comes an increase
in search users' expectations. By providing complete
and isolated coverage of a particular web domain, localized
search engines are an effective outlet to quickly locate content
that could otherwise be difficult to find. In this work,
we contend that the use of global PageRank in a localized
search engine can improve performance.
To estimate the global PageRank, we have proposed an
iterative node selection framework where we select which
pages from the global frontier to crawl next. Our primary
contribution is our stochastic complementation page selection
algorithm. This method crawls nodes that will most
significantly impact the local domain and has running time
linear in the number of nodes in the local domain. Experimentally
, we validate these methods across a diverse set of
local domains, including seven site-specific domains and four
topic-specific domains. We conclude that by crawling an additional
n or 2n pages, our methods find an estimate of the
global PageRanks that is up to ten times better than just
using the local PageRanks. Furthermore, we demonstrate
that our algorithm consistently outperforms other existing
heuristics.
124
Research Track Paper
Often times, topic-specific domains are discovered using
a focused web crawler which considers a page's content in
conjunction with link anchor text to decide which pages to
crawl next [4]. Although such crawlers have proven to be
quite effective in discovering topic-related content, many irrelevant
pages are also crawled in the process. Typically,
these pages are deleted and not indexed by the localized
search engine. These pages can of course provide valuable
information regarding the global PageRank of the local domain
. One way to integrate these pages into our framework
is to start the FindGlobalPR algorithm with the current
subgraph F equal to the set of pages that were crawled by
the focused crawler.
The global PageRank estimation framework, along with
the node selection algorithms presented, all require O(n)
computation per iteration and bandwidth proportional to
the number of pages crawled, T k. If the number of iterations
T is relatively small compared to the number of pages
crawled per iteration, k, then the bottleneck of the algorithm
will be the crawling phase. However, as the number of iterations
increases (relative to k), the bottleneck will reside in
the node selection computation. In this case, our algorithms
would benefit from constant factor optimizations. Recall
that the FindGlobalPR algorithm (Algorithm 2) requires
that the PageRanks of the current expanded local domain be
recomputed in each iteration. Recent work by Langville and
Meyer [12] gives an algorithm to quickly recompute PageRanks
of a given webgraph if a small number of nodes are
added. This algorithm was shown to give speedup of five to
ten times on some datasets. We plan to investigate this and
other such optimizations as future work.
In this paper, we have objectively evaluated our methods
by measuring how close our global PageRank estimates are
to the actual global PageRanks. To determine the benefit
of using global PageRanks in a localized search engine,
we suggest a user study in which users are asked to rate
the quality of search results for various search queries. For
some queries, only the local PageRanks are used in ranking
, and for the remaining queries, local PageRanks and the
approximate global PageRanks, as computed by our algorithms
, are used. The results of such a study can then be
analyzed to determine the added benefit of using the global
PageRanks computed by our methods, over just using the
local PageRanks.
Acknowledgements. This research was supported by NSF
grant CCF-0431257, NSF Career Award ACI-0093404, and
a grant from Sabre, Inc.
</SECTION>
<SECTION header="REFERENCES">[1] R. Baeza-Yates, M. Marin, C. Castillo, and
A. Rodriguez. Crawling a country: better strategies
than breadth-first for web page ordering. World-Wide
Web Conference, 2005.
[2] P. Boldi, M. Santini, and S. Vigna. Do your worst to
make the best: paradoxical effects in pagerank
incremental computations. Workshop on Web Graphs,
3243:168­180, 2004.
[3] S. Brin and L. Page. The anatomy of a large-scale
hypertextual web search engine. Computer Networks
and ISDN Systems, 33(1­7):107­117, 1998.
[4] S. Chakrabarti, M. van den Berg, and B. Dom.
Focused crawling: a new approach to topic-specific
web resource discovery. World-Wide Web Conference,
1999.
[5] Y. Chen, Q. Gan, and T. Suel. Local methods for
estimating pagerank values. Conference on
Information and Knowledge Management, 2004.
[6] J. Cho, H. Garcia-Molina, and L. Page. Efficient
crawling through url ordering. World-Wide Web
Conference, 1998.
[7] T. H. Haveliwala and S. D. Kamvar. The second
eigenvalue of the Google matrix. Technical report,
Stanford University, 2003.
[8] T. Joachims, F. Radlinski, L. Granka, A. Cheng,
C. Tillekeratne, and A. Patel. Learning retrieval
functions from implicit feedback.
http://www.cs.cornell.edu/People/tj/career.
[9] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and
G. H. Golub. Exploiting the block structure of the
web for computing pagerank. World-Wide Web
Conference, 2003.
[10] S. D. Kamvar, T. H. Haveliwala, C. D. Manning, and
G. H. Golub. Extrapolation methods for accelerating
pagerank computation. World-Wide Web Conference,
2003.
[11] A. N. Langville and C. D. Meyer. Deeper inside
pagerank. Internet Mathematics, 2004.
[12] A. N. Langville and C. D. Meyer. Updating the
stationary vector of an irreducible markov chain with
an eye on Google's pagerank. SIAM Journal on
Matrix Analysis, 2005.
[13] P. Lyman, H. R. Varian, K. Swearingen, P. Charles,
N. Good, L. L. Jordan, and J. Pal. How much
information 2003? School of Information Management
and System, University of California at Berkely, 2003.
[14] F. McSherry. A uniform approach to accelerated
pagerank computation. World-Wide Web Conference,
2005.
[15] C. D. Meyer. Stochastic complementation, uncoupling
markov chains, and the theory of nearly reducible
systems. SIAM Review, 31:240­272, 1989.
[16] US News and World Report. http://www.usnews.com.
[17] Dmoz open directory project. http://www.dmoz.org.
[18] Nutch open source search engine.
http://www.nutch.org.
[19] F. Radlinski and T. Joachims. Query chains: learning
to rank from implicit feedback. ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2005.
[20] S. Raghavan and H. Garcia-Molina. Crawling the
hidden web. In Proceedings of the Twenty-seventh
International Conference on Very Large Databases,
2001.
[21] T. Tin Tang, D. Hawking, N. Craswell, and
K. Griffiths. Focused crawling for both topical
relevance and quality of medical information.
Conference on Information and Knowledge
Management, 2005.
[22] Y. Wang and D. J. DeWitt. Computing pagerank in a
distributed internet search system. Proceedings of the
30th VLDB Conference, 2004.
125
Research Track Paper

</SECTION>
</DOCUMENT>