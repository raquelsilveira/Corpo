<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - AUIC Tactons paper final.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="stephen">
<META name="date" content="2004-03-24T07:17:23+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
	.ft0{font-size:22px;font-family:Times;color:#000000;}
	.ft1{font-size:16px;font-family:Times;color:#000000;}
	.ft2{font-size:13px;font-family:Times;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:7px;font-family:Times;color:#000000;}
	.ft5{font-size:14px;font-family:Times;color:#000000;}
	.ft6{font-size:6px;font-family:Times;color:#000000;}
	.ft7{font-size:10px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Courier;color:#000000;}
	.ft9{font-size:16px;font-family:Times;color:#000000;}
	.ft10{font-size:13px;line-height:17px;font-family:Times;color:#000000;}
	.ft11{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="892" height="1263" src="185001.png" alt="background image">
<DIV style="position:absolute;top:92;left:122"><nobr><span class="ft0"><b>Tactons: Structured Tactile Messages for Non-Visual  </b></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:307"><nobr><span class="ft0"><b> Information </b></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:500"><nobr><span class="ft0"><b>Display </b></span></nobr></DIV>
<DIV style="position:absolute;top:169;left:294"><nobr><span class="ft1"><b>Stephen Brewster and Lorna M. Brown </b></span></nobr></DIV>
<DIV style="position:absolute;top:194;left:338"><nobr><span class="ft2">Glasgow Interactive Systems Group  </span></nobr></DIV>
<DIV style="position:absolute;top:211;left:342"><nobr><span class="ft2">Department of Computing Science  </span></nobr></DIV>
<DIV style="position:absolute;top:229;left:378"><nobr><span class="ft2">University of Glasgow </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:372"><nobr><span class="ft2">Glasgow, G12 8QQ, UK </span></nobr></DIV>
<DIV style="position:absolute;top:274;left:195"><nobr><span class="ft3">{stephen, lorna}@dcs.gla.ac.uk      www.dcs.gla.ac.uk/~stephen </span></nobr></DIV>
<DIV style="position:absolute;top:297;left:446"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:313;left:86"><nobr><span class="ft5"><b>Abstract </b></span></nobr></DIV>
<DIV style="position:absolute;top:342;left:86"><nobr><span class="ft10">Tactile displays are now becoming available in a form <br>that can be easily used in a user interface. This paper de-<br>scribes a new form of tactile output. Tactons, or tactile <br>icons, are structured, abstract messages that can be used <br>to communicate messages non-visually. A range of differ-<br>ent parameters can be used for Tacton construction in-<br>cluding: frequency, amplitude and duration of a tactile <br>pulse, plus other parameters such as rhythm and location. <br>Tactons have the potential to improve interaction in a <br>range of different areas, particularly where the visual dis-<br>play is overloaded, limited in size or not available, such as <br>interfaces for blind people or in mobile and wearable de-<br>vices.</span></nobr></DIV>
<DIV style="position:absolute;top:547;left:121"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:123"><nobr><span class="ft2">This paper describes Tactons, the parameters used </span></nobr></DIV>
<DIV style="position:absolute;top:567;left:86"><nobr><span class="ft10">to construct them and some possible ways to design them. <br>Examples of where Tactons might prove useful in user <br>interfaces are given.  </span></nobr></DIV>
<DIV style="position:absolute;top:627;left:86"><nobr><span class="ft10">Keywords:  Tactons, tactile displays, multimodal interac-<br>tion, non-visual cues.</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:215"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:680;left:86"><nobr><span class="ft5">1 </span></nobr></DIV>
<DIV style="position:absolute;top:680;left:119"><nobr><span class="ft5">Introduction </span></nobr></DIV>
<DIV style="position:absolute;top:708;left:86"><nobr><span class="ft10">The area of haptic (touch-based) human computer inter-<br>action (HCI) has grown rapidly over the last few years. A <br>range of new applications has become possible now that <br>touch can be used as an interaction technique (Wall et al., <br>2002). However, most current haptic devices have scant <br>provision for tactile stimulation, being primarily pro-<br>grammable, constrained motion force-feedback devices <br>for kinaesthetic display. The cutaneous (skin-based) <br>component is ignored even though it is a key part of our <br>experience of touch (van Erp, 2002). It is, for example, <br>important for recognising texture, and detecting slip, <br>compliance and direction of edges. As Tan (1997) says <br>"In the general area of human-computer interfaces ... the <br>tactual sense is still underutilised compared with vision <br>and audition". One reason for this is that, until recently, <br>the technology for tactile displays was limited.  </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:86"><nobr><span class="ft10">Tactile displays are not new but they have not received <br>much attention from HCI researchers as they are often <br>engineering prototypes or designed for very specific ap-</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:86"><nobr><span class="ft2">                                                           </span></nobr></DIV>
<DIV style="position:absolute;top:1079;left:86"><nobr><span class="ft11">Copyright © 2004, Australian Computer Society, Inc.  This <br>paper appeared at the 5th Australasian User Interface Confer-<br>ence (AUIC2004), Dunedin.  Conferences in Research and Prac-<br>tice in Information Technology, Vol. 28. A. Cockburn, Ed. Re-<br>production for academic, not-for profit purposes permitted pro-<br>vided this text is included. </span></nobr></DIV>
<DIV style="position:absolute;top:308;left:457"><nobr><span class="ft10">plications (Kaczmarek et al., 1991). They have been used <br>in areas such as tele-operation or displays for blind peo-<br>ple to provide sensory substitution ­ where one sense is <br>used to receive information normally received by another <br>(Kaczmarek et al.). Most of the development of these <br>devices has taken place in robotics or engineering labs <br>and has focused on the challenges inherent in building <br>low cost, high-resolution devices with realistic size, <br>power and safety performance. Little research has gone <br>into how they might actually be used at the user interface. <br>Devices are now available that allow the use of tactile <br>displays so the time is right to think about how they <br>might be used to improve interaction.  </span></nobr></DIV>
<DIV style="position:absolute;top:541;left:457"><nobr><span class="ft10">In this paper the concept of Tactons, or tactile icons, is <br>introduced as a new communication method to comple-<br>ment graphical and auditory feedback at the user inter-<br>face. Tactons are structured, abstract messages that can be <br>used to communicate messages non-visually. Conveying <br>structured messages through touch will be very useful in <br>areas such as wearable computing where screens are lim-<br>ited. The paper gives some background to the perception  <br>and use of tactile stimuli and then describes the design of <br>Tactons. It finishes with examples of potential uses for <br>Tactons. </span></nobr></DIV>
<DIV style="position:absolute;top:749;left:457"><nobr><span class="ft5">2 </span></nobr></DIV>
<DIV style="position:absolute;top:749;left:490"><nobr><span class="ft5">Background and previous work </span></nobr></DIV>
<DIV style="position:absolute;top:777;left:457"><nobr><span class="ft2">The skin is the largest organ in the body, about 2 m</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:786"><nobr><span class="ft6">2</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:790"><nobr><span class="ft7"> </span></nobr></DIV>
<DIV style="position:absolute;top:777;left:795"><nobr><span class="ft2">in </span></nobr></DIV>
<DIV style="position:absolute;top:794;left:457"><nobr><span class="ft10">the average male (Montagu, 1971). Little direct use is <br>made of it for displaying information in human-computer <br>interfaces (Tan and Pentland, 1997, van Erp, 2002), yet a <br>touch on the hand or other parts of the body is a very rich <br>experience. The skin can therefore potentially be used as <br>a medium to communicate information. As a receiving <br>instrument the skin combines important aspects of the eye <br>and the ear, with high acuity in both space and time <br>(Gunther, 2001) giving it good potential as a communica-<br>tion medium. </span></nobr></DIV>
<DIV style="position:absolute;top:976;left:457"><nobr><span class="ft10">The human sense of touch can be roughly split in to two <br>parts: kinaesthetic and cutaneous. "Kinaesthetic" is often <br>used as catch-all term to describe the information arising <br>from forces and positions sensed by the muscles and <br>joints. Force-feedback haptic devices (such as the <br>PHANToM from SensAble) are used to present informa-<br>tion to the kinaesthetic sense. Cutaneous perception refers <br>to the mechanoreceptors contained within the skin, and <br>includes the sensations of vibration, temperature, pain <br>and indentation. Tactile devices are used to present feed-<br>back to the cutaneous sense. </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">15</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185002.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">Current haptic devices use force-feedback to present kin-<br>aesthetic stimuli. This works well for some aspects of <br>touch (e.g. identifying the geometric properties of ob-<br>jects) but is poor for features such as texture (normally <br>perceived cutaneously). Oakley et al. (2000) found that <br>trying to use texture in a user interface with a force-<br>feedback device actually reduced user performance. One <br>reason for this is that the textures had to be made large so <br>that they could be perceived kinaesthetically, but they <br>then perturbed users' movements. The use of a tactile <br>haptic device to present texture would not have this prob-<br>lem as small indentations in the fingertip would not affect <br>hand movements. At present, however, there are no hap-<br>tic devices that do a good job of presenting both tactile <br>and force-feedback cues to users. </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:86"><nobr><span class="ft10">Current force-feedback devices use a point interaction <br>model; the user is represented by a single point of contact <br>corresponding to the tip of a stylus. This is analogous to <br>exploring the world by remote contact through a stick <br>thus depriving the user of the rich, spatially varying cuta-<br>neous cues that arise on the finger pad when contacting a <br>real object (Wall and Harwin, 2001). Users must integrate <br>temporally varying cues as they traverse the structure of <br>virtual objects with the single point of contact, which <br>places considerable demands on short-term memory <br>(Jansson and Larsson, 2002). Even when exploring sim-<br>ple geometric primitives, performance is greatly reduced <br>compared to natural touch. Lederman and Klatzky (1999) <br>have shown that such removal of cutaneous input to the <br>fingertip impedes perception of edge direction, which is <br>an essential component of understanding haptic objects. It <br>can therefore be seen that tactile feedback and cutaneous <br>perception are key parts of touch that must be incorpo-<br>rated into haptic displays if they are to be effective and <br>usable. </span></nobr></DIV>
<DIV style="position:absolute;top:718;left:86"><nobr><span class="ft5">2.1  Vibrotactile actuators </span></nobr></DIV>
<DIV style="position:absolute;top:745;left:86"><nobr><span class="ft10">There are two basic types of vibrotactile display device. <br>These evoke tactile sensations using mechanical vibration <br>of the skin (usually in the range 10-500Hz) (Kaczmarek <br>et al., 1991). This is commonly done by vibrating a small <br>plate pressed against the skin or via a pin or array of pins <br>on the fingertip. These are very easy to control from stan-<br>dard PC hardware. Other types of actuator technology  <br>are available, including pneumatic and electrotactile <br>(Stone, 2000), but these tend to be bulkier and harder to <br>control so are less useful in many situations.  </span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:385"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:1107;left:103"><nobr><span class="ft2">Figure 1: The pins arrays on the VirTouch tactile </span></nobr></DIV>
<DIV style="position:absolute;top:1124;left:171"><nobr><span class="ft2">mouse (www.virtouch.com).  </span></nobr></DIV>
<DIV style="position:absolute;top:1155;left:86"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft10">The first type of vibrotactile display uses a pin or array of <br>small pins (e.g. the VirTouch mouse in Figure 1 or those <br>produced by Summers et al. (2001)) to stimulate the fin-<br>gertip. Such devices can present very fine cues for surface <br>texture, edges, lines, etc. The second type uses larger <br>point-contact stimulators (e.g. Figure 2 or alternatively <br>small loudspeaker cones playing tones, or other simple <br>vibrating actuators placed against the skin as used by Tan <br>(1997) and in devices such as the CyberTouch glove ­ <br>www.immersion.com). The cues here are much lower <br>resolution but can exert more force; they can also be dis-<br>tributed over the body to allow multiple simultaneous <br>cues (often mounted in a vest on the user's back or in a <br>belt around the waist). These devices are both easy to <br>control and use. For a full review see Kaczmarek et al. <br>(1991).  </span></nobr></DIV>
<DIV style="position:absolute;top:510;left:745"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:537;left:471"><nobr><span class="ft2">Figure 2: Audiological Engineering Corp. VBW32 </span></nobr></DIV>
<DIV style="position:absolute;top:554;left:530"><nobr><span class="ft2">transducers (www.tactaid.com). </span></nobr></DIV>
<DIV style="position:absolute;top:594;left:457"><nobr><span class="ft5">2.2    Previous work on tactile display </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:457"><nobr><span class="ft10">One common form of tactile output is Braille, and dy-<br>namic Braille cells are available. A display is made up of <br>a line of `soft' cells (often 40 or 80), each with 6 or 8 pins <br>that move up and down to represent the dots of a Braille <br>cell. The user can read a line of Braille cells by touching <br>the pins of each cell as they pop up (for more information <br>see www.tiresias.org). The focus of the work reported <br>here is not on Braille as it tends to be used mainly for <br>representing text (although other notations are used, e.g. <br>music) and the cells are very low resolution (8 pins <br>maximum). These displays are also very expensive with <br>an 80 cell display costing around £4000. There have been <br>many other tactile devices for blind people, such as the <br>Optacon (TeleSensory Inc.), which used an array of 144 <br>pins to display the input from a camera to the fingertip, <br>but again these are mainly used for reading text. Pin ar-<br>rays produce Braille but can do much more, especially the <br>higher resolution displays such as shown in Figure 1. </span></nobr></DIV>
<DIV style="position:absolute;top:941;left:457"><nobr><span class="ft10">Our research also builds on the work that has been done <br>on tactile graphics for blind people (this mainly takes the <br>form of raised lines and dots on special `swell' paper). <br>Kurze (1997, 1998) and Challis (2001) have developed <br>guidelines which allow images and objects to be pre-<br>sented that are understandable through touch by blind <br>users.  </span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:457"><nobr><span class="ft10">Two other examples show that the cutaneous sense is <br>very effective for communication. Firstly, Tadoma is a <br>tactile language used by deaf/blind people. The transmit-<br>ter speaks normally and the receiver puts a hand on the <br>face of the speaker, covering the mouth and neck (Tan <br>and Pentland, 2001). Tadoma users can listen at very high </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">16</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185003.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">speeds (normal speaking speed for experts) and pick up <br>subtleties of the speech such as accent. In the second ex-<br>ample, Geldard (1957) taught participants a simple tactile <br>language of 45 symbols, using three intensities, three <br>durations and five locations on the chest. Participants <br>were able to learn the alphabet quickly and could recog-<br>nise up to 38 words per minute in some cases. Other sen-<br>sory substitution systems convert sound into vibration for <br>hearing-impaired people (e.g. the TactAid system from <br>Audiological Engineering). Again this shows that cutane-<br>ous perception is very powerful and if we can make use <br>of it at the user interfaces we will have a rich new way to <br>present information to users. </span></nobr></DIV>
<DIV style="position:absolute;top:320;left:86"><nobr><span class="ft10">Research and existing applications have shown that the <br>cutaneous sense is a very powerful method of receiving <br>information. Other work has shown that it can be used in <br>user interfaces and wearable computers (Gemperle et al., <br>1998). Tan has begun to investigate the use of tactile dis-<br>plays on wearable computers (Tan and Pentland, 1997). <br>She used a 3x3 grid of stimulators on a user's back to <br>provide navigation information. Informal results sug-<br>gested it was useful but no formal evaluation has taken <br>place. Other relevant work has taken place in aircraft <br>cockpits to provide pilots with navigation information <br>(van Veen and van Erp, 2001, Rupert, 2000). In these <br>examples only simple tactile cues for direction have been <br>provided. For example, an actuator maybe vibrated on <br>one side of the body to indicate the direction to turn. <br>More sophisticated cues could be used to provide much <br>more information to users without them needing to use <br>their eyes. </span></nobr></DIV>
<DIV style="position:absolute;top:639;left:86"><nobr><span class="ft10">Gunther et al. have used tactile cues to present `musical' <br>compositions to users (Gunther, 2001, Gunther et al., <br>2002). They say: "The approach taken ... views haptic <br>technologies ­ in particular the vibrotactile stimulator ­ <br>as independent output devices to be used in conjunction <br>with the composition and perception of music. Vibrotac-<br>tile stimuli are viewed not as signals carrying information <br>per se, but as aesthetic artifacts themselves".  He used an <br>array of 13 transducers across the body of a `listener' so <br>that he/she could experience the combined sonic/tactile <br>presentation. Gunther created a series of compositions <br>played to listeners who appeared to enjoy them. This <br>work was artistic in nature so no formal usability assess-<br>ments were made but the listeners all liked the experi-<br>ence. </span></nobr></DIV>
<DIV style="position:absolute;top:907;left:86"><nobr><span class="ft10">In order to create a tactile composition (the same is true <br>for the Tactons described below) a good understanding of <br>the experience of touch is needed. However, as Gunther <br>et al. suggest: "It is indeed premature to hammer out the <br>details of a language for tactile composition. It seems <br>more productive at this point in time to identify the un-<br>derpinnings of such a language, specifically those dimen-<br>sions of tactile stimuli that can be manipulated to form <br>the basic vocabulary elements of a compositional lan-<br>guage". Research is needed to gain a more systematic <br>understanding of cutaneous perception for use in the <br>presentation of such messages.  </span></nobr></DIV>
<DIV style="position:absolute;top:1123;left:86"><nobr><span class="ft10">Enriquez and MacLean (2003) recently proposed `haptic <br>icons', which they define as "brief programmed forces <br>applied to a user through a haptic interface, with the role </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft10">of communicating a simple idea in a manner similar to <br>visual or auditory icons". The problem they are trying to <br>address is different to that of Tactons, as they say "With <br>the introduction of "active" haptic interfaces, a single <br>handle ­ e.g. a knob or a joystick ­ can control several <br>different and perhaps unrelated functions. These multi-<br>function controllers can no longer be differentiated from <br>one another by position, shape or texture... Active haptic <br>icons, or "hapticons", may be able to solve this problem <br>by rendering haptically distinct and meaningful sensa-<br>tions for the different functions". These use one degree-<br>of-freedom force-feedback devices, rather than tactile <br>displays, so encode information very differently to Tac-<br>tons. They report the construction of a tool to allow a user <br>to create and edit haptic icons. This is early work and <br>they do not report results from the use of hapticons in any <br>interfaces.  Their results, however, will be directly rele-<br>vant to Tactons. </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:457"><nobr><span class="ft5">3 </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:490"><nobr><span class="ft5">Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:443;left:457"><nobr><span class="ft10">Given that the cutaneous sense is rich and a powerful <br>communication medium currently little utilised in HCI, <br>how can we make effective use of it? One approach is to <br>use it to render objects from the real world more realisti-<br>cally in virtual environments, for example in improving <br>the presentation of texture in haptic devices. It could also <br>be used to improve targeting in desktop interactions along <br>the lines suggested by Oakley et al. (2000). In this paper <br>it is suggested that it can additionally be used to present <br>structured informational messages to users. </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:457"><nobr><span class="ft10">Tactons are structured, abstract messages that can be used <br>to communicate complex concepts to users non-visually. <br>Shneiderman (1998) defines an icon as "an image, picture <br>or symbol representing a concept". Tactons can represent <br>complex interface concepts, objects and actions very con-<br>cisely.  Visual icons and their auditory equivalent earcons <br>(Blattner et al., 1989, Brewster et al., 1994) are very <br>powerful ways of displaying information but there is cur-<br>rently no tactile equivalent. In the visual domain there is <br>text and its counterpart the icon, the same is true in sound <br>with synthetic speech and the earcon. In the tactile do-<br>main there is Braille but it has no `iconic' counterpart. <br>Tactons fill this gap. Icons/Earcons/Tactons form a sim-<br>ple, efficient language to represent concepts at the user <br>interface. </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:457"><nobr><span class="ft10">Tactons are similar to Braille in the same way that visual <br>icons are similar to text, or earcons are similar to syn-<br>thetic speech. For example, visual icons can convey com-<br>plex information in a very small amount of screen space, <br>much smaller than for a textual description. Earcons con-<br>vey information in a small amount of time as compared to <br>synthetic speech. Tactons can convey information in a <br>smaller amount of space and time than Braille. Research <br>will also show which form of iconic display is most suit-<br>able for which type of information. Visual icons are good <br>for spatial information, earcons for temporal. One prop-<br>erty of Tactons is that they operate both spatially and <br>temporally so they can complement both icons and ear-<br>cons. Further research is needed to understand how these <br>different types of feedback work together. </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">17</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185004.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">Using speech as an example from the auditory domain: <br>presenting information in speech is slow because of its <br>serial nature; to assimilate information the user must hear <br>a spoken message from beginning to end and many words <br>may have to be comprehended before the message can be <br>understood. With earcons the messages are shorter and <br>therefore more rapidly heard, speeding up interactions. <br>The same is true of Tactons when compared to Braille. <br>Speech suffers from many of the same problems as <br>graphical text in text-based computer systems, as this is <br>also a serial medium. Barker &amp; Manji (1989) claim that <br>an important limitation of text is its lack of expressive <br>capability: It may take many words to describe a fairly <br>simple concept. Graphical iconic displays were intro-<br>duced that speeded up interactions as users could see a <br>picture of the thing they wanted instead of having to read <br>its name from a list (Barker and Manji, 1989). In the <br>same way, an encoded tactile message may be able to <br>communicate its information in fewer symbols. The user <br>feels the Tacton then recalls its meaning rather than hav-<br>ing the meaning described in Braille (or speech or text). <br>The icon is also (in principle) universal: it means the <br>same thing in different languages and the Tacton would <br>have similar universality. </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:86"><nobr><span class="ft5">4 </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:119"><nobr><span class="ft5">Designing with Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:547;left:86"><nobr><span class="ft10">Tactons are created by encoding information using the <br>parameters of cutaneous perception. The encoding is <br>similar to that of earcons in sound (Blattner et al., 1989, <br>Brewster et al., 1994) where each of the musical parame-<br>ters (e.g. timbre, frequency, amplitude) is varied to en-<br>code information. Similar parameters can be used for <br>Tactons (although their relative importance is different). <br>As suggested by Blattner, short motifs could be used to <br>represent simple objects or actions and these can then be <br>combined in different ways to represent more complex <br>messages and concepts. As Tactons are abstract the map-<br>ping between the Tacton and what it represents must be <br>learned, but work on earcons has shown that learning can <br>take place quickly (Brewster, 1998b). </span></nobr></DIV>
<DIV style="position:absolute;top:797;left:86"><nobr><span class="ft10">The properties that can be manipulated for Tactons are <br>similar to those used in the creation of earcons. The pa-<br>rameters for manipulation also vary depending on the <br>type of transducer used; not all transducers allow all types <br>of parameters. The general basic parameters are: </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:86"><nobr><span class="ft10">Frequency: A range of frequencies can be used to differ-<br>entiate Tactons. The range of 20 ­ 1000 Hz is perceivable <br>but maximum sensitivity occurs around 250 Hz (Gunther <br>et al., 2002). The number of discrete values that can be <br>differentiated is not well understood, but Gill (2003) sug-<br>gests that a maximum of nine different levels can be used. <br>As in audition, a change in amplitude leads to a change in <br>the perception of frequency so this has an impact on the <br>use of frequency as a cue. The number of levels of fre-<br>quency that can be discriminated also depends on whether <br>the cues are presented in a relative or absolute way. Mak-<br>ing relative comparisons between stimuli is much easier <br>than absolute identification, which will lead to much <br>fewer discriminable values, as shown in the work on ear-<br>con design (Brewster et al., 1994). </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft10">Amplitude: Intensity of stimulation can be used to encode <br>values to present information to the user. Gunther (2002) <br>reports that the intensity range extends to 55 dB above the <br>threshold of detection; above this pain occurs. Craig and <br>Sherrick (1982) indicate that perception deteriorates <br>above 28 dB so this would seem to be a useful maximum. <br>Gunther (2001) reports that various values, ranging from <br>0.4dB to 3.2dB, have been reported for the just noticeable <br>difference (JND) value for intensity. Gill states that that <br>no more than four different intensities should be used <br>(Gill, 2003). Again the number of useful discriminable <br>values will depend on absolute or relative presentation of <br>stimuli. Due to the interactions between this and fre-<br>quency several researchers have suggested that they be <br>combined into a single parameter to simplify design </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:457"><nobr><span class="ft10">Waveform: The perception of wave shape is much more <br>limited than with the perception of timbre in sound. Users <br>can differentiate sine waves and square waves but more <br>subtle differences are more difficult (Gunther, 2001). <br>This limits the number of different values that can be en-<br>coded and makes this a much less important variable than <br>it is in earcon design (where it is one of the key vari-<br>ables). </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:457"><nobr><span class="ft10">Duration: Pulses of different durations can encode infor-<br>mation. Gunther (2001) investigated a range of subjective <br>responses to pulses of different durations. He found that <br>stimuli lasting less than 0.1 seconds were perceived as <br>taps or jabs whereas stimuli of longer duration, when <br>combined with gradual attacks and decays, may be per-<br>ceived as smoothly flowing tactile phrases. He suggests <br>combining duration with alterations in the envelope of a <br>vibration, e.g. an abrupt attack feels like a tap against the <br>skin, a gradual attack feels like something rising up out of <br>the skin. </span></nobr></DIV>
<DIV style="position:absolute;top:700;left:457"><nobr><span class="ft10">Rhythm: Building on from duration, groups of pulses of <br>different durations can be composed into rhythmic units. <br>This is a very powerful cue in both sound and touch. <br>Gunther (2001) suggests that differences in duration can <br>be used to group events when multiple events occur on <br>the same area of skin. </span></nobr></DIV>
<DIV style="position:absolute;top:813;left:457"><nobr><span class="ft10">Specific transducer types allow other parameters to be <br>used: </span></nobr></DIV>
<DIV style="position:absolute;top:856;left:457"><nobr><span class="ft10">Body location: Spatially distributed transducers can en-<br>code information in the position of stimulation across the <br>body. The choice of body location for vibrotactile display <br>is important, as different locations have different levels of <br>sensitivity and spatial acuity. A display may make use of <br>several body locations, so that the location can be used as <br>another parameter, or can be used to group tactile stimuli. </span></nobr></DIV>
<DIV style="position:absolute;top:986;left:457"><nobr><span class="ft10">The fingers are often used for vibrotactile displays be-<br>cause of their high sensitivity to small amplitudes and <br>their high spatial acuity (Craig and Sherrick, 1982). How-<br>ever, the fingers are often required for other tasks, so <br>other body locations may be more suitable. Craig and <br>Sherrick suggest the back, thigh and abdomen as other <br>suitable body locations. They report that, once subjects <br>have been trained in vibrotactile pattern recognition on <br>the back, they can almost immediately recognise the same <br>patterns when they are presented to the thigh or abdomen. <br>This transfer also occurs to some extent when patterns are </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">18</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185005.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">presented to different fingers after training on one finger, <br>but is not so immediate. </span></nobr></DIV>
<DIV style="position:absolute;top:130;left:86"><nobr><span class="ft10">Certain body locations are particularly suitable, or par-<br>ticularly unsuitable, for certain types of vibrotactile dis-<br>plays. For example, transducers should not be placed on <br>or near the head, as this can cause leakage of vibrations <br>into the ears, resulting in unwanted sounds (Gunther et <br>al., 2002). An example of a suitable body location is in <br>Gunther's Skinscape display, where he positions low fre-<br>quency transducers on the torso as this is where low fre-<br>quencies are felt when loud music is heard. </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:86"><nobr><span class="ft10">The method of attaching the transducers to a user's body <br>is also important. The pressure of the transducer against <br>the body has a significant effect on the user's perception <br>of the vibrations. Transducers should rest lightly on the <br>skin, allowing the user to feel the vibration against the <br>skin, and to isolate the location of the vibration with ease. <br>Exerting too much pressure with the transducer against <br>the user's body will cause the vibrations to be felt in the <br>bone structure, making them less isolated due to skeletal <br>conduction. In addition, tightening the straps holding the <br>transducer to achieve this level of pressure may impede <br>circulation (Gunther, 2001). </span></nobr></DIV>
<DIV style="position:absolute;top:510;left:86"><nobr><span class="ft10">Rupert (2000) suggests using the full torso for displaying <br>3D information, with 128 transducers distributed over the <br>body. His system displays information to pilots about the <br>location of objects around them in 3D space, by stimulat-<br>ing the transducers at the part of their body corresponding <br>to the location of the object in 3D space around them. <br>This could be used to indicate horizons, borders, targets, <br>or other aircraft.  </span></nobr></DIV>
<DIV style="position:absolute;top:657;left:86"><nobr><span class="ft10">Spatiotemporal patterns: Related to position and rhythm, <br>spatial patterns can also be "drawn" on the user's body. <br>For example, if a user has a 3x3 array of stimulators lo-<br>cated on his/her back, lines and geometric shapes can be <br>"drawn" on the back, by stimulating, in turn, the stimula-<br>tors that make up that shape. In Figure 3, an `L' shaped <br>gesture can be drawn by activating the stimulators: 1-4-7-<br>8-9 in turn. Patterns can move about the body, varying in <br>time and location to encode information. Cholewiak <br>(1996) and Sherrick (1985) have also looked at low-level <br>perception of distributed tactile cues. </span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:163"><nobr><span class="ft2">.</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:358"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:126"><nobr><span class="ft2">Figure 3: "Drawing" an L-shaped gesture.  </span></nobr></DIV>
<DIV style="position:absolute;top:1095;left:86"><nobr><span class="ft10">Now that the basic parameters for Tactons have been de-<br>scribed, we will give some examples of how they might <br>be designed to convey information. The fundamental de-<br>sign of Tactons is similar to that of earcons. </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft5">4.1  Compound Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:115;left:457"><nobr><span class="ft10">A simple set of Tactons could be created as in Figure 4. A <br>high-frequency pulse that increases in intensity could <br>represent `Create', a lower frequency pulse that decreases <br>in intensity could represent `Delete'. A two note falling <br>Tacton could represent a file and a two rising notes a <br>folder. The mapping is abstract; there is no intuitive link <br>between what the user feels and what it represents. </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:457"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:272;left:496"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:337;left:511"><nobr><span class="ft6">Create</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:595"><nobr><span class="ft6">Delete</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:521"><nobr><span class="ft6">File</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:596"><nobr><span class="ft6">Folder</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:698"><nobr><span class="ft6">Create File </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:698"><nobr><span class="ft6">Delete Folder</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:767"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:466;left:470"><nobr><span class="ft2">Figure 4: Compound Tactons (after Blattner et al., </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:612"><nobr><span class="ft2">1989). </span></nobr></DIV>
<DIV style="position:absolute;top:514;left:457"><nobr><span class="ft10">These Tactons can then be combined to create compound <br>messages. For example, `create file' or `delete folder'. <br>The set of basic elements could be extended and a simple <br>language of tactile elements created to provide feedback <br>in a user interface. </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:457"><nobr><span class="ft5">4.2  Hierarchical Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:646;left:457"><nobr><span class="ft10">Tactons could also be combined in a hierarchical way, as <br>shown in Figure 5. Each Tacton is a node in a tree and <br>inherits properties from the levels above it. Figure 5 <br>shows a hierarchy of Tactons representing a hypothetical <br>family of errors. The top of the tree is a family Tacton <br>which has a basic rhythm played using a sinewave (a dif-<br>ferent family of errors would use a different rhythm so <br>that they are not confused). The rhythmic structure of <br>Level 2 inherits the Tacton from Level 1 and adds to it. In <br>this case a second, higher frequency Tacton played with a <br>squarewave. At Level 3 the tempo of the two Tactons is <br>changed. In this way a hierarchical structure can be pre-<br>sented. The other parameters discussed above could be <br>used to add further levels. </span></nobr></DIV>
<DIV style="position:absolute;top:906;left:457"><nobr><span class="ft5">4.3  Transformational Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:933;left:457"><nobr><span class="ft10">A third type of Tacton is the Transformational Tacton. <br>These have several properties, each represented by a dif-<br>ferent tactile parameter. For example, if Transformational <br>Tactons were used to represent files in a computer inter-<br>face, the file type could be represented by rhythm, size by <br>frequency, and creation date by body location. Each file <br>type would be mapped to a unique rhythm. Therefore, <br>two files of the same type, and same size, but different <br>creation date would share the same rhythm and fre-<br>quency, but would be presented to a different body loca-<br>tion. If two files were of different types but the same size <br>they would be represented by different rhythms with the <br>same frequency.  </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">19</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185006.png" alt="background image">
<DIV style="position:absolute;top:560;left:86"><nobr><span class="ft5">5 </span></nobr></DIV>
<DIV style="position:absolute;top:560;left:119"><nobr><span class="ft5">Uses for Tactons </span></nobr></DIV>
<DIV style="position:absolute;top:587;left:86"><nobr><span class="ft10">We are interested in three areas of use for Tactons, al-<br>though there are many others where they have potential to <br>improve usability.  </span></nobr></DIV>
<DIV style="position:absolute;top:657;left:86"><nobr><span class="ft5">5.1  Enhancements of desktop interfaces </span></nobr></DIV>
<DIV style="position:absolute;top:685;left:86"><nobr><span class="ft10">The first, and simplest, area of interest is in the addition <br>of Tactons to desktop graphical interfaces. The addition <br>of earcons to desktops has shown many advantages in <br>terms of reduced errors, reduced times to complete tasks <br>and lowered workload (Brewster, 1998a). One problem <br>with audio is that users believe that it may be annoying to <br>use (although no research has actually shown this to be <br>the case) and it has the potential to annoy others nearby <br>(for a discussion see (Brewster, 2002)). The addition of <br>Tactons to widgets has the same potential to indicate us-<br>ability problems but without the potential to annoy. </span></nobr></DIV>
<DIV style="position:absolute;top:884;left:86"><nobr><span class="ft10">One reason for enhancing standard desktop interfaces is <br>that users can become overloaded with visual information <br>on large, high-resolution displays. In highly complex <br>graphical displays users must concentrate on one part of <br>the display to perceive the visual feedback, so that feed-<br>back from another part may be missed. This becomes <br>very important in situations where users must notice and <br>deal with large amounts of dynamic data or output from <br>multiple applications or tasks. If information about sec-<br>ondary tasks was presented through touch then users <br>could concentrate their visual attention on the primary <br>one but feel information about the others. </span></nobr></DIV>
<DIV style="position:absolute;top:1100;left:86"><nobr><span class="ft10">As a simple example, the display of a progress bar widget <br>could be presented tactually. Two sets of tactile pulses <br>could be used to indicate the current and end points of a <br>download. The time between the two pulses would indi-</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:457"><nobr><span class="ft10">cate the amount of time remaining, the closer the two <br>pulses the nearer the download is to finishing. The two <br>pulses could use different waveforms to ensure they were <br>not confused. Different rhythms for each pulse could be <br>used to indicate different types of downloads. If a more <br>sophisticated set of transducers on a belt around the waist <br>was available then the position of a pulse moving around <br>the body in a clockwise direction (starting from the front) <br>would give information about progress: when the pulse <br>was at the right side of the body the download would be <br>25% of the way through, when it was on the left hand <br>side 75%, and when it got back around to the front it <br>would be finished. There would be no need for any visual <br>presentation of the progress bar, allowing users to focus <br>their visual attention on the main task they are involved <br>with. </span></nobr></DIV>
<DIV style="position:absolute;top:831;left:457"><nobr><span class="ft10">Tactons could also be used to enhance interactions with <br>buttons, scrollbars, menus, etc. to indicate when users are <br>on targets and when certain types of errors occur. Others <br>have shown that basic tactile feedback can improve  <br>pointing and steering type interactions (Akamatsu et al., <br>1995, Campbell et al., 1999). There are some commercial <br>systems that give simple tactile feedback in desktop user <br>interfaces, e.g. the software that comes with the Logitech <br>iFeel mouse (www.logitech.com). This provides basic <br>targeting: a brief pulse is played, for example, when a <br>user moves over a target. We believe there is much more <br>that can be presented with tactile feedback. </span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:457"><nobr><span class="ft5">5.2  Visually impaired users </span></nobr></DIV>
<DIV style="position:absolute;top:1084;left:457"><nobr><span class="ft10">Tactons will be able to work alongside Braille in tactile <br>displays for blind and visually impaired users, in the same <br>way as earcons work alongside synthetic speech. They <br>will allow information to be delivered more efficiently. In <br>addition, hierarchical Tactons could help users navigate </span></nobr></DIV>
<DIV style="position:absolute;top:190;left:386"><nobr><span class="ft3">Sine</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:312"><nobr><span class="ft3">Sine </span></nobr></DIV>
<DIV style="position:absolute;top:306;left:375"><nobr><span class="ft3">Square</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:381"><nobr><span class="ft3">Error </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:285"><nobr><span class="ft3">Operating system error </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:555"><nobr><span class="ft3">Execution error </span></nobr></DIV>
<DIV style="position:absolute;top:303;left:566"><nobr><span class="ft3">Sine</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:629"><nobr><span class="ft3">Square </span></nobr></DIV>
<DIV style="position:absolute;top:364;left:379"><nobr><span class="ft3">Overflow </span></nobr></DIV>
<DIV style="position:absolute;top:443;left:368"><nobr><span class="ft3">Sine</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:431"><nobr><span class="ft3">Square</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:610"><nobr><span class="ft3">Underflow </span></nobr></DIV>
<DIV style="position:absolute;top:443;left:599"><nobr><span class="ft3">Sine</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:662"><nobr><span class="ft3">Square </span></nobr></DIV>
<DIV style="position:absolute;top:465;left:386"><nobr><span class="ft3">Fast tempo </span></nobr></DIV>
<DIV style="position:absolute;top:465;left:612"><nobr><span class="ft3">Slow tempo </span></nobr></DIV>
<DIV style="position:absolute;top:507;left:397"><nobr><span class="ft2">Figure 5: Hierarchical Tacton composition. </span></nobr></DIV>
<DIV style="position:absolute;top:115;left:169"><nobr><span class="ft2">Level 1 </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:169"><nobr><span class="ft2">Level 2 </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:169"><nobr><span class="ft2">Level 3 </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">20</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185007.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">around Braille media by providing navigation information <br>(Brewster, 1998b). </span></nobr></DIV>
<DIV style="position:absolute;top:130;left:86"><nobr><span class="ft10">One of our main interests is in using Tactons to improve <br>access to graphical information non-visually. Text can be <br>rendered in a relatively straightforward manner by speech <br>or Braille, but graphics are more problematic. One area <br>that we and others have focused on is visualisation for <br>blind people. Understanding and manipulating informa-<br>tion using visualisations such as graphs, tables, bar charts <br>and 3D plots is very common for sighted people. The <br>skills needed are learned early in school and then used <br>throughout life, for example, in analysing information or <br>managing home finances. The basic skills needed for cre-<br>ating and manipulating graphs are necessary for all parts <br>of education and employment. Blind people have very <br>restricted access to information presented in these visual <br>ways (Edwards, 1995). As Wise et al. (2001) say "Inac-<br>cessibility of instructional materials, media, and tech-<br>nologies used in science, engineering, and mathematics <br>education severely restricts the ability of students with <br>little or no sight to excel in these disciplines". To allow <br>blind people to gain the skills needed for the workplace <br>new technologies are necessary to make visualisations <br>usable. Tactons provide another route through which in-<br>formation can be presented. </span></nobr></DIV>
<DIV style="position:absolute;top:536;left:86"><nobr><span class="ft10">Research has shown that using haptic devices is an effec-<br>tive way of presenting graphical information non-visually <br>(Yu and Brewster, 2003, Wies et al., 2001, Van Scoy et <br>al., 2000). The most common approach has been to use <br>haptic devices to present graphs, tables or 3D plots that <br>users can feel kinaesthetically by tracing a line or shape <br>with a finger using a device like the PHANToM <br>(www.sensable.com). Lederman and Klatzky  (1999) <br>have shown that removal of cutaneous input to the finger-<br>tip impedes perception of edge direction, which is an es-<br>sential component of tracing a haptic line graph. This lack <br>of cutaneous stimulation leads to problems with naviga-<br>tion (exploring using a single point of contact means it is <br>difficult to locate items as there is no context, which can <br>be given in a tactile display), exploring small scale fea-<br>tures (these would be perceived cutaneously on the finger <br>pad in real life), and information overload (all haptic in-<br>formation is perceived kinaesthetically rather than being <br>shared with cutaneous perception). Incorporating a tactile <br>display into a force-feedback device will alleviate many <br>of these problems and potentially increase user efficiency <br>and comprehension of visualisations.  </span></nobr></DIV>
<DIV style="position:absolute;top:924;left:86"><nobr><span class="ft10">Tactons could be presented as the user moves the force-<br>feedback device over the visualisation. Dimensions of the <br>data can be encoded into a Tacton to give information <br>about the current point, using the parameters described in <br>Section 4. This would allow more data to be presented <br>more efficiently. For example, with multidimensional <br>data one dimension might be mapped to the frequency of <br>a pulse in a Tacton, another might map to rhythm and <br>another to body locatoin. As the user moves about the <br>data he/she would feel the different parameters. In addi-<br>tion to the finger pad, we can also include tactile displays <br>to other parts of the body (e.g. to the back) using spatially <br>distributed transducers to provide even more display area. <br>As long as this is done in a comprehensible manner users </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft10">will be able to gain access to their data in a much more <br>effective way than with current force-feedback only visu-<br>alisation tools. </span></nobr></DIV>
<DIV style="position:absolute;top:157;left:457"><nobr><span class="ft5">5.3  Mobile and wearable devices </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:457"><nobr><span class="ft10">Our other main application area is mobile and wearable <br>device displays (for both sighted and blind people). Mo-<br>bile telephones and handheld computers are currently one <br>of the fastest growth areas of computing and this growth <br>will extend into more sophisticated, fully wearable com-<br>puters in the future. One problem with these devices is <br>their limited output capabilities. Their small displays eas-<br>ily become cluttered with information and widgets and <br>this makes interface design difficult. In addition, users are <br>not always looking at the display of a device as they must <br>walk or navigate through their environment which re-<br>quires visual attention. One way to solve this problem is <br>to use other display modalities and so reduce demands on <br>visual display, or replace it if not available. Work has <br>gone into using speech and non-speech sounds to over-<br>come the display bottleneck. Tactile displays have great <br>potential here too but are much less well investigated.  </span></nobr></DIV>
<DIV style="position:absolute;top:487;left:457"><nobr><span class="ft10">Sound has many advantages but it can be problematic; in <br>loud environments it can be impossible to hear auditory <br>output from a device, in quiet places the audio may be <br>disturbing to others nearby. Blind people often do not like <br>to wear headphones when outdoors as they mask impor-<br>tant environmental sounds. Tactile displays do not suffer <br>from these problems (although there may be other prob-<br>lems for example, perceiving tactile stimuli whilst run-<br>ning due to the difficulties of keeping the transducers in <br>contact with the skin). Mobile telephones commonly have <br>a very simple point-contact tactile stimulator built-in that <br>can alert the user to a call. These are often only able to <br>produce pulses of different durations.  A pin array would <br>be possible on such a device as the user will be holding it <br>in a hand when in use. Such a sophisticated tactile display <br>could do much more, e.g. it could give information on the <br>caller, replace or enhance items on the display (like icons, <br>progress indicators, games) or aid in the navigation of the <br>devices' menus so that the user does not need to look at <br>the screen.  </span></nobr></DIV>
<DIV style="position:absolute;top:841;left:457"><nobr><span class="ft10">In a wearable device users could have body mounted <br>transducers so that information can be displayed over <br>their body. In the simplest case this could be used to give <br>directional information by vibrating one side of the body <br>or other to indicate which way to turn (Tan and Pentland, <br>1997). A belt of transducers around the waist could give a <br>compass-like display of direction; a pulse could be played <br>continuously at north so the user can maintain orientation <br>after turning (useful when navigating in the dark) or at the <br>position around the waist corresponding to the direction <br>in which to head. A more sophisticated display might <br>give information about the user's context. For example, <br>presenting Tactons describing information such as the <br>type of building (shop, bank, office-block, house), the <br>type of shop (clothes, phones, food, furniture) the price-<br>bracket of a shop (budget, mid-range, expensive), or in-<br>formation more related to the concerns of visually im-<br>paired people, such as the number of stairs leading up to <br>the entrance (for firefighters, whose vision is impaired </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">21</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185008.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft10">due to smoke and flames, a tactile display could also pro-<br>vide information on the location of rooms and exits in a <br>burning building). A tactile display could also present <br>information on stock market data (building on from the <br>work on tactile visualisation in the section above) so that <br>users could keep track of trades whilst away from the <br>office. Such tactile displays could also work alongside <br>auditory or visual ones.  </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:86"><nobr><span class="ft5">6 </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:119"><nobr><span class="ft5">Future work and conclusions </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:86"><nobr><span class="ft10">This paper has laid out some of the foundations of infor-<br>mation display through Tactons. There is still much work <br>to be done to fully understand how they should be de-<br>signed and used. There are many lower level perceptual <br>questions to be addressed before higher level design is-<br>sues can be investigated. Many of the parameters of touch <br>described in Section 4 are not fully understood and the <br>full usable ranges of the parameters are not known. Stud-<br>ies need to be undertaken to explore the parameter space <br>so that the relative importance of the different parameters <br>can be discovered. </span></nobr></DIV>
<DIV style="position:absolute;top:469;left:86"><nobr><span class="ft10">Once the range of parameters is understood then the con-<br>struction of Tactons can be examined. Basic studies are <br>needed to understand how the parameters can be com-<br>bined to construct Tactons. Parameters which work well <br>alone may not work well when combined with others into <br>a Tacton. For example, one parameter may mask another. <br>When the basic design of Tactons is understood the com-<br>position of simple Tactons into more complex messages, <br>encoding hierarchical information into Tactons, and their <br>learnability and memorability can be investigated. The <br>concurrent presentation of multiple Tactons must also be <br>studied. These studies will answer some of the main ques-<br>tions regarding the usability of Tactons and a good under-<br>standing of their design and usability will have been a-<br>chieved.   </span></nobr></DIV>
<DIV style="position:absolute;top:737;left:86"><nobr><span class="ft10">Another important task is to investigate the strong rela-<br>tionships between hearing and touch by examining cross-<br>modal uses of audio and tactile multimodal displays <br>(Spence and Driver, 1997), e.g. combined audio and tac-<br>tile cues, redundant tactile and audio cues, and moving <br>from an audio to a tactile presentation of the same infor-<br>mation (and vice versa). This is important in a mo-<br>bile/wearable context because at different times different <br>display techniques might be appropriate. For example, <br>audio might be inappropriate in a very noisy environ-<br>ment, or tactile cues might be masked when the user is <br>running. One important issue is to identify the types of <br>information best presented in sound and those best pre-<br>sented tactually. For example, the range of the vibrotac-<br>tile frequency response is roughly 20 times less than that <br>of the auditory system. Such discrepancies must be ac-<br>counted for when performing cross-modal mappings from <br>hearing to touch. </span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:86"><nobr><span class="ft10">In conclusion, this paper has proposed a new form of tac-<br>tile output called Tactons. These are structured tactile <br>messages that can be used to communicate information. <br>Tactile output is underused in current interfaces and Tac-<br>tons provide a way of addressing this problem. The basic <br>parameters have been described and design issues dis-</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft10">cussed. A technique is now available to allow tactile dis-<br>play to form a significant part of the set of interaction and <br>display techniques that can be used to communicate with <br>users at the interface. </span></nobr></DIV>
<DIV style="position:absolute;top:174;left:457"><nobr><span class="ft5">7 </span></nobr></DIV>
<DIV style="position:absolute;top:174;left:490"><nobr><span class="ft5">Acknowledgements </span></nobr></DIV>
<DIV style="position:absolute;top:202;left:457"><nobr><span class="ft10">This research was conducted when Brewster was on sab-<br>batical in the Department of Computer Science at the <br>University of Canterbury, Christchurch, New Zealand. <br>Thanks to Andy Cockburn for his thoughts and comments <br>on this work. The sabbatical was funded by an Erskine <br>Fellowship from the University of Canterbury. The work <br>was part funded by EPSRC grant GR/S53244. Brown is <br>funded by an EPSRC studentship. </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:457"><nobr><span class="ft5">8 </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:490"><nobr><span class="ft5">References </span></nobr></DIV>
<DIV style="position:absolute;top:386;left:457"><nobr><span class="ft2">Akamatsu, M., MacKenzie, I. S. and Hasbrouq, T. </span></nobr></DIV>
<DIV style="position:absolute;top:403;left:478"><nobr><span class="ft10">(1995): A comparison of tactile, auditory, and visual <br>feedback in a pointing task using a mouse-type de-<br>vice. Ergonomics, 38, 816-827. </span></nobr></DIV>
<DIV style="position:absolute;top:459;left:457"><nobr><span class="ft2">Barker, P. G. and Manji, K. A. (1989): Pictorial dialogue </span></nobr></DIV>
<DIV style="position:absolute;top:476;left:478"><nobr><span class="ft10">methods. International Journal of Man-Machine Stud-<br>ies, 31, 323-347. </span></nobr></DIV>
<DIV style="position:absolute;top:515;left:457"><nobr><span class="ft2">Blattner, M., Sumikawa, D. and Greenberg, R. (1989): </span></nobr></DIV>
<DIV style="position:absolute;top:533;left:478"><nobr><span class="ft10">Earcons and icons: Their structure and common de-<br>sign principles. Human Computer Interaction,  4, 11-<br>44. </span></nobr></DIV>
<DIV style="position:absolute;top:589;left:457"><nobr><span class="ft2">Brewster, S. A. (1998a): The design of sonically-</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:478"><nobr><span class="ft10">enhanced widgets. Interacting with Computers,  11, <br>211-235. </span></nobr></DIV>
<DIV style="position:absolute;top:645;left:457"><nobr><span class="ft2">Brewster, S. A. (1998b): Using Non-Speech Sounds to </span></nobr></DIV>
<DIV style="position:absolute;top:662;left:478"><nobr><span class="ft10">Provide Navigation Cues. ACM Transactions on <br>Computer-Human Interaction, 5, 224-259. </span></nobr></DIV>
<DIV style="position:absolute;top:701;left:457"><nobr><span class="ft2">Brewster, S. A. (2002): Chapter 12: Non-speech auditory </span></nobr></DIV>
<DIV style="position:absolute;top:719;left:478"><nobr><span class="ft10">output. In The Human Computer Interaction Hand-<br>book (Eds, Jacko, J. and Sears, A.) Lawrence Erlbaum <br>Associates, pp. 220-239. </span></nobr></DIV>
<DIV style="position:absolute;top:775;left:457"><nobr><span class="ft2">Brewster, S. A., Wright, P. C. and Edwards, A. D. N. </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:478"><nobr><span class="ft10">(1994): A detailed investigation into the effectiveness <br>of earcons. In Auditory Display (Ed, Kramer, G.) Ad-<br>dison-Wesley, Reading, MA, pp. 471-498. </span></nobr></DIV>
<DIV style="position:absolute;top:848;left:457"><nobr><span class="ft2">Campbell, C., Zhai, S., May, K. and Maglio, P. (1999): </span></nobr></DIV>
<DIV style="position:absolute;top:866;left:478"><nobr><span class="ft10">What You Feel Must Be What You See: Adding Tac-<br>tile Feedback to the Trackpoint. Proceedings of IFIP <br>INTERACT'99, Edinburgh, UK, 383-390, IOS Press </span></nobr></DIV>
<DIV style="position:absolute;top:922;left:457"><nobr><span class="ft2">Challis, B. and Edwards, A. D. N. (2001): Design princi-</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:478"><nobr><span class="ft10">ples for tactile interaction. In Haptic Human-<br>Computer Interaction, Vol. 2058 (Eds, Brewster, S. <br>A. and Murray-Smith, R.) Springer LNCS, Berlin, <br>Germany, pp. 17-24. </span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:457"><nobr><span class="ft2">Cholewiak, R. W. and Collins, A. (1996): Vibrotactile </span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:478"><nobr><span class="ft10">pattern discrimination and communality at several <br>body sites. Perception and Psychophysics,  57, 724-<br>737. </span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:457"><nobr><span class="ft2">Craig, J. C. and Sherrick, C. E. (1982): Dynamic Tactile </span></nobr></DIV>
<DIV style="position:absolute;top:1103;left:478"><nobr><span class="ft10">Displays. In Tactual Perception: A Sourcebook (Ed, <br>Foulke, E.) Cambridge University Press, pp. 209-233. </span></nobr></DIV>
<DIV style="position:absolute;top:1216;left:437"><nobr><span class="ft9">22</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:892;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="892" height="1263" src="185009.png" alt="background image">
<DIV style="position:absolute;top:87;left:86"><nobr><span class="ft2">Edwards, A. D. N. (Ed.) (1995) Extra-Ordinary Human-</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:107"><nobr><span class="ft10">Computer Interaction, Cambridge University Press, <br>Cambridge, UK. </span></nobr></DIV>
<DIV style="position:absolute;top:143;left:86"><nobr><span class="ft2">Enriquez, M. J. and Maclean, K. (2003): The Hapticon </span></nobr></DIV>
<DIV style="position:absolute;top:160;left:107"><nobr><span class="ft10">editor: A tool in support of haptic communication re-<br>search.  Haptics Symposium 2003, Los Angeles, CA, <br>356-362, IEEE Press </span></nobr></DIV>
<DIV style="position:absolute;top:217;left:86"><nobr><span class="ft2">Geldard, F. A. (1957): Adventures in tactile literacy. The </span></nobr></DIV>
<DIV style="position:absolute;top:234;left:107"><nobr><span class="ft2">American Psychologist, 12, 115-124. </span></nobr></DIV>
<DIV style="position:absolute;top:256;left:86"><nobr><span class="ft2">Gemperle, F., Kasabach, C., Stivoric, J., Bauer, M. and </span></nobr></DIV>
<DIV style="position:absolute;top:273;left:107"><nobr><span class="ft10">Martin, R. (1998): Design for wearability. Proceed-<br>ings of Second International Symposium on Wearable <br>Computers, Los Alamitos, CA, 116-122, IEEE Com-<br>puter Society </span></nobr></DIV>
<DIV style="position:absolute;top:346;left:86"><nobr><span class="ft2">Gill, J. (2003), Vol. 2003 Royal National Institute of the </span></nobr></DIV>
<DIV style="position:absolute;top:364;left:107"><nobr><span class="ft2">Blind, UK. </span></nobr></DIV>
<DIV style="position:absolute;top:385;left:86"><nobr><span class="ft2">Gunther, E. (2001): Skinscape: A Tool for Composition in </span></nobr></DIV>
<DIV style="position:absolute;top:402;left:107"><nobr><span class="ft10">the Tactile Modality. Massachusetts Institute of Tech-<br>nology. Masters of Engineering. </span></nobr></DIV>
<DIV style="position:absolute;top:441;left:86"><nobr><span class="ft2">Gunther, E., Davenport, G. and O'Modhrain, S. (2002): </span></nobr></DIV>
<DIV style="position:absolute;top:459;left:107"><nobr><span class="ft10">Cutaneous Grooves: Composing for the Sense of <br>Touch.  Proceedings of Conference on New Instru-<br>ments for Musical Expression, Dublin, IR, 1-6,  </span></nobr></DIV>
<DIV style="position:absolute;top:515;left:86"><nobr><span class="ft2">Jansson, G. and Larsson, K. (2002): Identification of </span></nobr></DIV>
<DIV style="position:absolute;top:532;left:107"><nobr><span class="ft10">Haptic Virtual Objects with Differing Degrees of <br>Complexity. Proceedings of Eurohaptics 2002, Edin-<br>burgh, UK, 57-60, Edinburgh University </span></nobr></DIV>
<DIV style="position:absolute;top:589;left:86"><nobr><span class="ft2">Kaczmarek, K., Webster, J., Bach-y-Rita, P. and Tomp-</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:107"><nobr><span class="ft10">kins, W. (1991): Electrotacile and vibrotactile dis-<br>plays for sensory substitution systems. IEEE Transac-<br>tion on Biomedical Engineering, 38, 1-16. </span></nobr></DIV>
<DIV style="position:absolute;top:662;left:86"><nobr><span class="ft2">Kurze, M. (1997): Rendering drawings for interactive </span></nobr></DIV>
<DIV style="position:absolute;top:679;left:107"><nobr><span class="ft10">haptic perception. Proceedings of ACM CHI'97, At-<br>lanta, GA, 423-430, ACM Press, Addison-Wesley </span></nobr></DIV>
<DIV style="position:absolute;top:718;left:86"><nobr><span class="ft2">Kurze, M. (1998): TGuide: a guidance system for tactile </span></nobr></DIV>
<DIV style="position:absolute;top:735;left:107"><nobr><span class="ft10">image exploration. Proceedings of ACM ASSETS '98, <br>Marina del Rey, CA, ACM Press </span></nobr></DIV>
<DIV style="position:absolute;top:774;left:86"><nobr><span class="ft2">Lederman, S. J. and Klatzky, R. L. (1999): Sensing and </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:107"><nobr><span class="ft10">Displaying Spatially Distributed Fingertip Forces in <br>Haptic Interfaces for Teleoperator and Virtual Envi-<br>ronment Systems. Presence: Teleoperators and Vir-<br>tual Environments, 8, 86-103. </span></nobr></DIV>
<DIV style="position:absolute;top:865;left:86"><nobr><span class="ft2">Montagu, A. (1971): Touching: The Human Significance </span></nobr></DIV>
<DIV style="position:absolute;top:882;left:107"><nobr><span class="ft2">of the Skin, Columbia University Press, New York. </span></nobr></DIV>
<DIV style="position:absolute;top:904;left:86"><nobr><span class="ft2">Oakley, I., McGee, M., Brewster, S. A. and Gray, P. D. </span></nobr></DIV>
<DIV style="position:absolute;top:921;left:107"><nobr><span class="ft10">(2000): Putting the feel in look and feel. Proceedings <br>of ACM CHI 2000, The Hague, Netherlands, 415-422, <br>ACM Press, Addison-Wesley </span></nobr></DIV>
<DIV style="position:absolute;top:978;left:86"><nobr><span class="ft2">Rupert, A. (2000): Tactile situation awareness system: </span></nobr></DIV>
<DIV style="position:absolute;top:995;left:107"><nobr><span class="ft10">proprioceptive prostheses for sensory deficiencies. <br>Aviation, Space and Environmental Medicine, 71, 92-<br>99. </span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:86"><nobr><span class="ft2">Sherrick, C. (1985): A scale for rate of tactual vibration. </span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:107"><nobr><span class="ft2">Journal of the Acoustical Society of America, 78. </span></nobr></DIV>
<DIV style="position:absolute;top:87;left:457"><nobr><span class="ft2">Shneiderman, B. (1998): Designing the user interface, 3</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:798"><nobr><span class="ft4">rd</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:806"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:104;left:478"><nobr><span class="ft2">Ed. Addison-Wesley, Reading (MA). </span></nobr></DIV>
<DIV style="position:absolute;top:126;left:457"><nobr><span class="ft2">Spence, C. and Driver, J. (1997): Cross-modal links in </span></nobr></DIV>
<DIV style="position:absolute;top:143;left:478"><nobr><span class="ft10">attention between audition, vision and touch: implica-<br>tions for interface design. International Journal of <br>Cognitive Ergonomics, 1, 351-373. </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:457"><nobr><span class="ft2">Stone, R. (2000): Haptic feedback: A potted history, from </span></nobr></DIV>
<DIV style="position:absolute;top:217;left:478"><nobr><span class="ft10">telepresence to virtual reality. The First International <br>Workshop on Haptic Human-Computer Interaction, <br>Glasgow, UK, 1-7, Springer-Verlag Lecture Notes in <br>Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:290;left:457"><nobr><span class="ft2">Summers, I. R., Chanter, C. M., Southall, A. L. and </span></nobr></DIV>
<DIV style="position:absolute;top:307;left:478"><nobr><span class="ft10">Brady, A. C. (2001): Results from a Tactile Array on <br>the Fingertip. Proceedings of Eurohaptics 2001, Bir-<br>mingham, UK, 26-28, University of Birmingham </span></nobr></DIV>
<DIV style="position:absolute;top:364;left:457"><nobr><span class="ft2">Tan, H. Z. and Pentland, A. (1997): Tactual Displays for </span></nobr></DIV>
<DIV style="position:absolute;top:381;left:478"><nobr><span class="ft10">Wearable Computing. Proceedings of the First Inter-<br>national Symposium on Wearable Computers, IEEE </span></nobr></DIV>
<DIV style="position:absolute;top:420;left:457"><nobr><span class="ft2">Tan, H. Z. and Pentland, A. (2001): Chapter 18: Tactual </span></nobr></DIV>
<DIV style="position:absolute;top:437;left:478"><nobr><span class="ft10">displays for sensory substitution and wearable com-<br>puters. In Fundamentals of wearable computers and <br>augmented reality (Eds, Barfield, W. and Caudell, T.) <br>Lawrence Erlbaum Associates, Mahwah, New Jersey, <br>pp. 579-598. </span></nobr></DIV>
<DIV style="position:absolute;top:528;left:457"><nobr><span class="ft2">van Erp, J. B. F. (2002): Guidelines for the use of active </span></nobr></DIV>
<DIV style="position:absolute;top:545;left:478"><nobr><span class="ft10">vibro-tactile displays in human-computer interaction. <br>Proceedings of Eurohaptics 2002, Edinburgh, UK, <br>18-22, University of Edinburgh </span></nobr></DIV>
<DIV style="position:absolute;top:601;left:457"><nobr><span class="ft2">Van Scoy, F., Kawai, T., Darrah, M. and Rash, C. (2000): </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:478"><nobr><span class="ft10">Haptic Display of Mathematical Functions for Teach-<br>ing Mathematics to Students with Vision Disabilities: <br>Design and Proof of Concept. Proceedings of the First <br>Workshop on Haptic Human-Computer Interaction, <br>Glasgow, UK, University of Glasgow </span></nobr></DIV>
<DIV style="position:absolute;top:709;left:457"><nobr><span class="ft2">van Veen, H. and van Erp, J. B. F. (2001): Tactile infor-</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:478"><nobr><span class="ft10">mation presentation in the cockpit. In Haptic Human-<br>Computer Interaction (LNCS2058), Vol. 2058 (Eds, <br>Brewster, S. A. and Murray-Smith, R.) Springer, Ber-<br>lin, Germany, pp. 174-181. </span></nobr></DIV>
<DIV style="position:absolute;top:800;left:457"><nobr><span class="ft2">Wall, S. A. and Harwin, W. S. (2001): A High Bandwidth </span></nobr></DIV>
<DIV style="position:absolute;top:817;left:478"><nobr><span class="ft10">Interface for Haptic Human Computer Interaction. <br>Mechatronics. The Science of Intelligent Machines. <br>An International Journal, 11, 371-387. </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:457"><nobr><span class="ft2">Wall, S. A., Riedel, B., Crossan, A. and McGee, M. R. </span></nobr></DIV>
<DIV style="position:absolute;top:891;left:478"><nobr><span class="ft10">(Eds.) (2002) Eurohaptics 2002 Conference Proceed-<br>ings, University of Edinburgh, Edinburgh, Scotland. </span></nobr></DIV>
<DIV style="position:absolute;top:930;left:457"><nobr><span class="ft2">Wies, E., Gardner, J., O'Modhrain, S., Hasser, C. and </span></nobr></DIV>
<DIV style="position:absolute;top:947;left:478"><nobr><span class="ft10">Bulatov, V. (2001): Web-based touch display for ac-<br>cessible science education. In Haptic Human-<br>Computer Interaction, Vol. 2058 (Eds, Brewster, S. <br>A. and Murray-Smith, R.) Springer LNCS, Berlin, pp. <br>52-60. </span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:457"><nobr><span class="ft2">Yu, W. and Brewster, S. A. (2003): Evaluation of multi-</span></n