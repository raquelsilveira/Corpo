<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>p108-cho.dvi</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2002-04-23T08:52:36+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:15px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:12px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:15px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:6px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft8{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146001.png" alt="background image">
<DIV style="position:absolute;top:109;left:350"><nobr><span class="ft0"><b>Parallel Crawlers</b></span></nobr></DIV>
<DIV style="position:absolute;top:177;left:235"><nobr><span class="ft1">Junghoo Cho</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:168"><nobr><span class="ft2">University of California, Los Angeles</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:219"><nobr><span class="ft1">cho@cs.ucla.edu</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:542"><nobr><span class="ft1">Hector Garcia-Molina</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:564"><nobr><span class="ft2">Stanford University</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:542"><nobr><span class="ft1">cho@cs.stanford.edu</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:81"><nobr><span class="ft3"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:337;left:81"><nobr><span class="ft9">In this paper we study how we can design an effective parallel<br>crawler. As the size of the Web grows, it becomes imperative<br>to parallelize a crawling process, in order to finish download-<br>ing pages in a reasonable amount of time. We first propose<br>multiple architectures for a parallel crawler and identify fun-<br>damental issues related to parallel crawling. Based on this<br>understanding, we then propose metrics to evaluate a par-<br>allel crawler, and compare the proposed architectures using<br>40 million pages collected from the Web. Our results clarify<br>the relative merits of each architecture and provide a good<br>guideline on when to adopt which architecture.</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:81"><nobr><span class="ft3"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:551;left:81"><nobr><span class="ft9">H.5.4 [Hypertext/Hypermedia]: Architectures;<br>H.3.4 [Systems and Software]: Distributed Systems</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:81"><nobr><span class="ft3"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:624;left:81"><nobr><span class="ft4">Management, Performance, Design</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:81"><nobr><span class="ft3"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:81"><nobr><span class="ft4">Web Crawler, Web Spider, Parallelization</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:81"><nobr><span class="ft3"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:714;left:112"><nobr><span class="ft3"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:736;left:94"><nobr><span class="ft4">A crawler is a program that downloads and stores Web</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:81"><nobr><span class="ft9">pages, often for a Web search engine. Roughly, a crawler<br>starts off by placing an initial set of URLs, S</span></nobr></DIV>
<DIV style="position:absolute;top:770;left:357"><nobr><span class="ft5">0</span></nobr></DIV>
<DIV style="position:absolute;top:768;left:363"><nobr><span class="ft4">, in a queue,</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:81"><nobr><span class="ft9">where all URLs to be retrieved are kept and prioritized.<br>From this queue, the crawler gets a URL (in some order),<br>downloads the page, extracts any URLs in the downloaded<br>page, and puts the new URLs in the queue. This process is<br>repeated until the crawler decides to stop. Collected pages<br>are later used for other applications, such as a Web search<br>engine or a Web cache.</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:94"><nobr><span class="ft4">As the size of the Web grows, it becomes more difficult to</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:81"><nobr><span class="ft9">retrieve the whole or a significant portion of the Web using<br>a single process. Therefore, many search engines often run<br>multiple processes in parallel to perform the above task, so<br>that download rate is maximized. We refer to this type of<br>crawler as a parallel crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:94"><nobr><span class="ft4">In this paper we study how we should design a paral-</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:81"><nobr><span class="ft4">lel crawler, so that we can maximize its performance (e.g.,</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:81"><nobr><span class="ft6">Copyright is held by the author/owner(s).</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:81"><nobr><span class="ft4">WWW2002</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:150"><nobr><span class="ft6">, May 7­11, 2002, Honolulu, Hawaii, USA.</span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:81"><nobr><span class="ft6">ACM 1-58113-449-5/02/0005.</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:475"><nobr><span class="ft9">download rate) while minimizing the overhead from paral-<br>lelization. We believe many existing search engines already<br>use some sort of parallelization, but there has been little<br>scientific research conducted on this topic. Thus, little has<br>been known on the tradeoffs among various design choices<br>for a parallel crawler. In particular, we believe the following<br>issues make the study of a parallel crawler challenging and<br>interesting:</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:495"><nobr><span class="ft4">· Overlap: When multiple processes run in parallel to</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:509"><nobr><span class="ft9">download pages, it is possible that different processes<br>download the same page multiple times. One process<br>may not be aware that another process has already<br>downloaded the page. Clearly, such multiple down-<br>loads should be minimized to save network bandwidth<br>and increase the crawler's effectiveness. Then how can<br>we coordinate the processes to prevent overlap?</span></nobr></DIV>
<DIV style="position:absolute;top:584;left:495"><nobr><span class="ft4">· Quality: Often, a crawler wants to download "impor-</span></nobr></DIV>
<DIV style="position:absolute;top:603;left:509"><nobr><span class="ft9">tant" pages first, in order to maximize the "quality"<br>of the downloaded collection. However, in a parallel<br>crawler, each process may not be aware of the whole<br>image of the Web that they have collectively down-<br>loaded so far. For this reason, each process may make<br>a crawling decision solely based on its own image of<br>the Web (that itself has downloaded) and thus make<br>a poor crawling decision. Then how can we make sure<br>that the quality of the downloaded pages is as good for<br>a parallel crawler as for a centralized one?</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:495"><nobr><span class="ft4">· Communication bandwidth: In order to prevent over-</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:509"><nobr><span class="ft9">lap, or to improve the quality of the downloaded pages,<br>crawling processes need to periodically communicate<br>to coordinate with each other. However, this commu-<br>nication may grow significantly as the number of crawl-<br>ing processes increases. Exactly what do they need to<br>communicate and how significant would this overhead<br>be? Can we minimize this communication overhead<br>while maintaining the effectiveness of the crawler?</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:489"><nobr><span class="ft4">While challenging to implement, we believe that a paral-</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:475"><nobr><span class="ft9">lel crawler has many important advantages, compared to a<br>single-process crawler:</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:495"><nobr><span class="ft4">· Scalability: Due to enormous size of the Web, it is often</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:509"><nobr><span class="ft9">imperative to run a parallel crawler. A single-process<br>crawler simply cannot achieve the required download<br>rate in certain cases.</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:495"><nobr><span class="ft4">· Network-load dispersion: Multiple crawling processes</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:509"><nobr><span class="ft4">of a parallel crawler may run at geographically distant</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">124</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="146002.png" alt="background image">
<DIV style="position:absolute;top:86;left:114"><nobr><span class="ft9">locations, each downloading "geographically-adjacent"<br>pages. For example, a process in Germany may down-<br>load all European pages, while another one in Japan<br>crawls all Asian pages. In this way, we can disperse<br>the network load to multiple regions. In particular,<br>this dispersion might be necessary when a single net-<br>work cannot handle the heavy load from a large-scale<br>crawl.</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:101"><nobr><span class="ft4">· Network-load reduction: In addition to the dispersing</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:114"><nobr><span class="ft9">load, a parallel crawler may actually reduce the net-<br>work load.</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:193"><nobr><span class="ft4">For example, assume that a crawler in</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:114"><nobr><span class="ft9">North America retrieves a page from Europe. To be<br>downloaded by the crawler, the page first has to go<br>through the network in Europe, then the Europe-to-<br>North America inter-continental network and finally<br>the network in North America. Instead, if a crawl-<br>ing process in Europe collects all European pages, and<br>if another process in North America crawls all North<br>American pages, the overall network load will be re-<br>duced, because pages go through only "local" networks.</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:114"><nobr><span class="ft9">Note that the downloaded pages may need to be trans-<br>ferred later to a central location, so that a central index<br>can be built. However, even in that case, we believe<br>that the transfer can be significantly smaller than the<br>original page download traffic, by using some of the<br>following methods:</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:129"><nobr><span class="ft4">­</span></nobr></DIV>
<DIV style="position:absolute;top:519;left:144"><nobr><span class="ft9">Compression: Once the pages are collected and<br>stored, it is easy to compress the data before send-<br>ing them to a central location.</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:129"><nobr><span class="ft4">­</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:144"><nobr><span class="ft9">Difference: Instead of sending the entire image<br>with all downloaded pages, we may first take dif-<br>ference between previous image and the current<br>one and send only this difference. Since many<br>pages are static and do not change very often,<br>this scheme can significantly reduce the network<br>traffic.</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:129"><nobr><span class="ft4">­</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:144"><nobr><span class="ft9">Summarization: In certain cases, we may need<br>only a central index, not the original pages them-<br>selves. In this case, we may extract the necessary<br>information for the index construction (e.g., post-<br>ings list) and transfer this data only.</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:94"><nobr><span class="ft4">To build an effective web crawler, we clearly need to ad-</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:81"><nobr><span class="ft9">dress many more challenges than just parallelization. For<br>example, a crawler needs to figure out how often a page<br>changes and how often it would revisit the page in order to<br>maintain the page up to date [7, 10]. Also, it has to make<br>sure that a particular Web site is not flooded with its HTTP<br>requests during a crawl [17, 12, 24]. In addition, it has to<br>carefully select what page to download and store in its lim-<br>ited storage space in order to make the best use of its stored<br>collection of pages [9, 5, 11]. While all of these issues are<br>important, we focus on the crawler parallelization in this<br>paper, because this problem has been paid significantly less<br>attention than the others.</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:94"><nobr><span class="ft4">In summary, we believe a parallel crawler has many ad-</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:81"><nobr><span class="ft9">vantages and poses interesting challenges. In particular, we<br>believe our paper makes the following contributions:</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:101"><nobr><span class="ft4">· We identify major issues and problems related to a</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:114"><nobr><span class="ft9">parallel crawler and discuss how we can solve these<br>problems.</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:495"><nobr><span class="ft4">· We present multiple techniques for a parallel crawler</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:509"><nobr><span class="ft9">and discuss their advantages and disadvantages. As far<br>as we know most of these techniques have not been de-<br>scribed in open literature. (Very little is known about<br>the internals of crawlers, as they are closely guarded<br>secrets.)</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:495"><nobr><span class="ft4">· Using a large dataset (40M web pages) collected from</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:509"><nobr><span class="ft9">the Web, we experimentally compare the design choices<br>and study their tradeoffs quantitatively.</span></nobr></DIV>
<DIV style="position:absolute;top:238;left:495"><nobr><span class="ft4">· We propose various optimization techniques that can</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:509"><nobr><span class="ft9">minimize the coordination effort between crawling pro-<br>cesses, so that they can operate more independently<br>while maximizing their effectiveness.</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:475"><nobr><span class="ft3"><b>1.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:516"><nobr><span class="ft3"><b>Related work</b></span></nobr></DIV>
<DIV style="position:absolute;top:337;left:489"><nobr><span class="ft4">Web crawlers have been studied since the advent of the</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:475"><nobr><span class="ft9">Web [18, 23, 4, 22, 14, 6, 19, 12, 9, 5, 11, 10, 7]. These<br>studies can be roughly categorized into one of the following<br>topics:</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:495"><nobr><span class="ft4">· General architecture [22, 14, 6, 19, 12]: The work in</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:509"><nobr><span class="ft9">this category describes the general architecture of a<br>Web crawler and studies how a crawler works. For ex-<br>ample, Reference [14] describes the architecture of the<br>Compaq SRC crawler and its major design goals. Some<br>of these studies briefly describe how the crawling task<br>is parallelized. For instance, Reference [22] describes<br>a crawler that distributes individual URLs to multi-<br>ple machines, which download Web pages in parallel.<br>The downloaded pages are then sent to a central ma-<br>chine, on which links are extracted and sent back to<br>the crawling machines. However, these studies do not<br>carefully compare various issues related to a parallel<br>crawler and how design choices affect performance. In<br>this paper, we first identify multiple techniques for a<br>parallel crawler and compare their relative merits us-<br>ing real Web data.</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:495"><nobr><span class="ft4">· Page selection [9, 5, 11]: Since many crawlers can</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:509"><nobr><span class="ft9">download only a small subset of the Web, crawlers need<br>to carefully decide what page to download. By retriev-<br>ing "important" or "relevant" pages early, a crawler<br>may improve the "quality" of the downloaded pages.<br>The studies in this category explore how a crawler can<br>discover and identify "important" pages early, and pro-<br>pose various algorithms to achieve this goal. In our pa-<br>per, we study how parallelization affects some of these<br>techniques and explain how we can fix the problems<br>introduced by parallelization.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:495"><nobr><span class="ft4">· Page update [10, 7]: Web crawlers need to update the</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:509"><nobr><span class="ft9">downloaded pages periodically, in order to maintain<br>the pages up to date. The studies in this category<br>discuss various page revisit policies to maximize the<br>"freshness" of the downloaded pages.</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:754"><nobr><span class="ft4">For example,</span></nobr></DIV>
<DIV style="position:absolute;top:944;left:509"><nobr><span class="ft9">Reference [7] studies how a crawler should adjust re-<br>visit frequencies for pages when the pages change at<br>different rates. We believe these studies are orthogo-<br>nal to what we discuss in this paper.</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:489"><nobr><span class="ft4">There also exists a significant body of literature study-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft9">ing the general problem of parallel and distributed comput-<br>ing [21, 25]. Some of these studies focus on the design of ef-<br>ficient parallel algorithms. For example, References [20, 16]</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">125</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft10{font-size:6px;font-family:Helvetica;color:#000000;}
	.ft11{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft12{font-size:7px;font-family:Times;color:#000000;}
	.ft13{font-size:12px;font-family:Times;color:#000000;}
	.ft14{font-size:16px;font-family:Times;color:#000000;}
	.ft15{font-size:13px;font-family:Times;color:#000000;}
	.ft16{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
	.ft17{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
	.ft18{font-size:11px;line-height:12px;font-family:Times;color:#000000;}
	.ft19{font-size:11px;line-height:18px;font-family:Times;color:#000000;}
	.ft20{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146003.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft9">present various architectures for parallel computing, pro-<br>pose algorithms that solve various problems (e.g., finding<br>maximum cliques) under the architecture, and study the<br>complexity of the proposed algorithms. While the general<br>principles described are being used in our work,</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:381"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:394"><nobr><span class="ft4">none of</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:81"><nobr><span class="ft9">the existing solutions can be directly applied to the crawl-<br>ing problem.</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:94"><nobr><span class="ft4">Another body of literature designs and implements dis-</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft9">tributed operating systems, where a process can use dis-<br>tributed resources transparently (e.g., distributed memory,<br>distributed file systems) [25, 1]. Clearly, such OS-level sup-<br>port makes it easy to build a general distributed applica-<br>tion, but we believe that we cannot simply run a centralized<br>crawler on a distributed OS to achieve parallelism. A web<br>crawler contacts millions of web sites in a short period of<br>time and consumes extremely large network, storage and<br>memory resources. Since these loads push the limit of ex-<br>isting hardwares, the task should be carefully partitioned<br>among processes and they should be carefully coordinated.<br>Therefore, a general-purpose distributed operating system<br>that does not understand the semantics of web crawling will<br>lead to unacceptably poor performance.</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:81"><nobr><span class="ft3"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:449;left:112"><nobr><span class="ft16"><b>ARCHITECTURE OF A PARALLEL<br>CRAWLER</b></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:94"><nobr><span class="ft4">In Figure 1 we illustrate the general architecture of a par-</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:81"><nobr><span class="ft9">allel crawler. A parallel crawler consists of multiple crawling<br>processes, which we refer to as</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:276"><nobr><span class="ft4">C-proc's. Each C-proc per-</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:81"><nobr><span class="ft9">forms the basic tasks that a single-process crawler conducts.<br>It downloads pages from the Web, stores the pages locally,<br>extracts URLs from the downloaded pages and follows links.<br>Depending on how the</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:232"><nobr><span class="ft4">C-proc's split the download task,</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft4">some of the extracted links may be sent to other</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:388"><nobr><span class="ft4">C-proc's.</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:81"><nobr><span class="ft4">The</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:109"><nobr><span class="ft4">C-proc's performing these tasks may be distributed ei-</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:81"><nobr><span class="ft9">ther on the same local network or at geographically distant<br>locations.</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:101"><nobr><span class="ft4">· Intra-site parallel crawler: When all C-proc's run on</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:114"><nobr><span class="ft9">the same local network and communicate through a<br>high speed interconnect (such as LAN), we call it an<br>intra-site parallel crawler. In Figure 1, this scenario<br>corresponds to the case where all</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:318"><nobr><span class="ft4">C-proc's run only on</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:114"><nobr><span class="ft4">the local network on the top. In this case, all</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:392"><nobr><span class="ft4">C-proc's</span></nobr></DIV>
<DIV style="position:absolute;top:770;left:114"><nobr><span class="ft9">use the same local network when they download pages<br>from remote Web sites. Therefore, the network load<br>from</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:147"><nobr><span class="ft4">C-proc's is centralized at a single location where</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:114"><nobr><span class="ft4">they operate.</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:101"><nobr><span class="ft4">· Distributed crawler: When C-proc's run at geograph-</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:114"><nobr><span class="ft9">ically distant locations connected by the Internet (or<br>a wide area network), we call it a distributed crawler.<br>For example, one</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:223"><nobr><span class="ft4">C-proc may run in the US, crawling</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:114"><nobr><span class="ft4">all US pages, and another</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:279"><nobr><span class="ft4">C-proc may run in France,</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:114"><nobr><span class="ft9">crawling all European pages. As we discussed in the<br>introduction, a distributed crawler can disperse and<br>even reduce the load on the overall network.</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:114"><nobr><span class="ft4">When</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:155"><nobr><span class="ft4">C-proc's run at distant locations and communi-</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:114"><nobr><span class="ft9">cate through the Internet, it becomes important how<br>often and how much</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:249"><nobr><span class="ft4">C-proc's need to communicate.</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:114"><nobr><span class="ft4">The bandwidth between</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:267"><nobr><span class="ft4">C-proc's may be limited and</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:88"><nobr><span class="ft4">For example, we may consider that our proposed solution</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:81"><nobr><span class="ft17">is a variation of "divide and conquer" approach, since we<br>partition and assign the Web to multiple processes.</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:710"><nobr><span class="ft10">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:675"><nobr><span class="ft11">. . .</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:612"><nobr><span class="ft10">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:651"><nobr><span class="ft12">Local connect</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:710"><nobr><span class="ft10">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:594"><nobr><span class="ft12">collected pages</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:675"><nobr><span class="ft12">queues of URLs to visit</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:675"><nobr><span class="ft11">. . .</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:612"><nobr><span class="ft10">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:651"><nobr><span class="ft12">Local connect</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:565"><nobr><span class="ft12">NET</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:561"><nobr><span class="ft12">INTER</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:476"><nobr><span class="ft4">Figure 1: General architecture of a parallel crawler</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:565"><nobr><span class="ft13"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:552"><nobr><span class="ft14"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:680"><nobr><span class="ft13"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:665"><nobr><span class="ft14"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:600"><nobr><span class="ft13"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:713"><nobr><span class="ft13"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:582"><nobr><span class="ft14"><i>(C  )</i></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:696"><nobr><span class="ft14"><i>(C  )</i></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:575"><nobr><span class="ft15"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:378;left:597"><nobr><span class="ft15"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:619"><nobr><span class="ft15"><i>c</i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:618"><nobr><span class="ft15"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:499;left:594"><nobr><span class="ft15"><i>e</i></span></nobr></DIV>
<DIV style="position:absolute;top:382;left:706"><nobr><span class="ft15"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:419;left:694"><nobr><span class="ft15"><i>g</i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:683"><nobr><span class="ft15"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:475;left:735"><nobr><span class="ft15"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:552;left:475"><nobr><span class="ft4">Figure 2: Site S</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:595"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:608"><nobr><span class="ft4">is crawled by C</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:722"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:736"><nobr><span class="ft4">and site S</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:810"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:823"><nobr><span class="ft4">is</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:475"><nobr><span class="ft4">crawled by C</span></nobr></DIV>
<DIV style="position:absolute;top:571;left:567"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:509"><nobr><span class="ft9">sometimes unavailable, as is often the case with the<br>Internet.</span></nobr></DIV>
<DIV style="position:absolute;top:639;left:489"><nobr><span class="ft4">When multiple</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:581"><nobr><span class="ft4">C-proc's download pages in parallel, differ-</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:475"><nobr><span class="ft4">ent</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:498"><nobr><span class="ft4">C-proc's may download the same page multiple times. In</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:475"><nobr><span class="ft4">order to avoid this overlap,</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:640"><nobr><span class="ft4">C-proc's need to coordinate with</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:475"><nobr><span class="ft9">each other on what pages to download. This coordination<br>can be done in one of the following ways:</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:495"><nobr><span class="ft4">· Independent: At one extreme, C-proc's may download</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:509"><nobr><span class="ft9">pages totally independently without any coordination.<br>That is, each</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:598"><nobr><span class="ft4">C-proc starts with its own set of seed</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:509"><nobr><span class="ft9">URLs and follows links without consulting with other<br>C-proc's. In this scenario, downloaded pages may over-<br>lap, but we may hope that this overlap will not be sig-<br>nificant, if all</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:594"><nobr><span class="ft4">C-proc's start from different seed URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:509"><nobr><span class="ft9">While this scheme has minimal coordination overhead<br>and can be very scalable, we do not directly study<br>this option due to its overlap problem. Later we will<br>consider an improved version of this option, which sig-<br>nificantly reduces overlap.</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:495"><nobr><span class="ft4">· Dynamic assignment: When there exists a central co-</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:509"><nobr><span class="ft9">ordinator that logically divides the Web into small par-<br>titions (using a certain partitioning function) and dy-<br>namically assigns each partition to a</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:734"><nobr><span class="ft4">C-proc for down-</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:509"><nobr><span class="ft4">load, we call it dynamic assignment.</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:509"><nobr><span class="ft20">For example, assume that a central coordinator par-<br>titions the Web by the site name of a URL. That<br>is, pages in the same site (e.g., http://cnn.com/top.<br>html and http://cnn.com/content.html) belong to</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">126</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="146004.png" alt="background image">
<DIV style="position:absolute;top:86;left:114"><nobr><span class="ft9">the same partition, while pages in different sites be-<br>long to different partitions. Then during a crawl, the<br>central coordinator constantly decides on what parti-<br>tion to crawl next (e.g., the site cnn.com) and sends<br>URLs within this partition (that have been discovered<br>so far) to a</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:188"><nobr><span class="ft4">C-proc as seed URLs. Given this request,</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:114"><nobr><span class="ft4">the</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:137"><nobr><span class="ft4">C-proc downloads the pages and extracts links from</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:114"><nobr><span class="ft9">them. When the extracted links point to pages in the<br>same partition (e.g., http://cnn.com/article.html),<br>the</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:139"><nobr><span class="ft4">C-proc follows the links, but if a link points to a</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:114"><nobr><span class="ft20">page in another partition (e.g., http://nytimes.com/<br>index.html), the</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:225"><nobr><span class="ft4">C-proc reports the link to the cen-</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:114"><nobr><span class="ft9">tral coordinator. The central coordinator later uses<br>this link as a seed URL for the appropriate partition.</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:114"><nobr><span class="ft9">Note that the Web can be partitioned at various gran-<br>ularities. At one extreme, the central coordinator may<br>consider every page as a separate partition and assign<br>individual URLs to</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:232"><nobr><span class="ft4">C-proc's for download. In this case,</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:114"><nobr><span class="ft4">a</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:126"><nobr><span class="ft4">C-proc does not follow links, because different pages</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:114"><nobr><span class="ft4">belong to separate partitions.</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:312"><nobr><span class="ft4">It simply reports all</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:114"><nobr><span class="ft9">extracted URLs back to the coordinator. Therefore,<br>the communication between a</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:302"><nobr><span class="ft4">C-proc and the central</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:114"><nobr><span class="ft9">coordinator may vary dramatically, depending on the<br>granularity of the partitioning function.</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:101"><nobr><span class="ft4">· Static assignment: When the Web is partitioned and</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:114"><nobr><span class="ft4">assigned to each</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:218"><nobr><span class="ft4">C-proc before they start to crawl, we</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:114"><nobr><span class="ft4">call it static assignment. In this case, every</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:401"><nobr><span class="ft4">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:114"><nobr><span class="ft4">knows which</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:195"><nobr><span class="ft4">C-proc is responsible for which page dur-</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:114"><nobr><span class="ft9">ing a crawl, and the crawler does not need a central<br>coordinator. We will shortly discuss in more detail<br>how</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:143"><nobr><span class="ft4">C-proc's operate under this scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:94"><nobr><span class="ft4">In this paper, we mainly focus on static assignment be-</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:81"><nobr><span class="ft9">cause of its simplicity and scalability, and defer the study of<br>dynamic assignment to future work. Note that in dynamic<br>assignment, the central coordinator may become the major<br>bottleneck, because it has to maintain a large number of<br>URLs reported from all</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:230"><nobr><span class="ft4">C-proc's and has to constantly co-</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:81"><nobr><span class="ft4">ordinate all</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:156"><nobr><span class="ft4">C-proc's. Thus the coordinator itself may also</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:81"><nobr><span class="ft4">need to be parallelized.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:81"><nobr><span class="ft3"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:742;left:112"><nobr><span class="ft16"><b>CRAWLING MODES FOR STATIC<br>ASSIGNMENT</b></span></nobr></DIV>
<DIV style="position:absolute;top:785;left:94"><nobr><span class="ft4">Under static assignment, each</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:289"><nobr><span class="ft4">C-proc is responsible for</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:81"><nobr><span class="ft9">a certain partition of the Web and has to download pages<br>within the partition. However, some pages in the partition<br>may have links to pages in another partition. We refer to<br>this type of link as an inter-partition link. To illustrate how<br>a</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:91"><nobr><span class="ft4">C-proc may handle inter-partition links, we use Figure 2 as</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:81"><nobr><span class="ft4">our example. In the figure, we assume two</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:341"><nobr><span class="ft4">C-proc's, C</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:406"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:417"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:81"><nobr><span class="ft4">C</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:91"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:97"><nobr><span class="ft4">, are responsible for sites S</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:269"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:281"><nobr><span class="ft4">and S</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:318"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:324"><nobr><span class="ft4">, respectively. For</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:81"><nobr><span class="ft9">now, we assume that the Web is partitioned by sites and<br>that the Web has only S</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:237"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:249"><nobr><span class="ft4">and S</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:286"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:292"><nobr><span class="ft4">. Also, we assume that</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:81"><nobr><span class="ft4">each</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:112"><nobr><span class="ft4">C-proc starts its crawl from the root page of each site,</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:81"><nobr><span class="ft4">a and f.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:97"><nobr><span class="ft4">1. Firewall mode: In this mode, each</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:335"><nobr><span class="ft4">C-proc downloads</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:114"><nobr><span class="ft9">only the pages within its partition and does not follow<br>any inter-partition link. All inter-partition links are<br>ignored and thrown away. For example, the links a</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:426"><nobr><span class="ft4"></span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:114"><nobr><span class="ft4">g, c</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:139"><nobr><span class="ft4"> g and h  d in Figure 2 are ignored and thrown</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:114"><nobr><span class="ft4">away by C</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:178"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:189"><nobr><span class="ft4">and C</span></nobr></DIV>
<DIV style="position:absolute;top:1069;left:226"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:232"><nobr><span class="ft4">.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:509"><nobr><span class="ft9">In this mode, the overall crawler does not have any<br>overlap in the downloaded pages, because a page can<br>be downloaded by only one</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:681"><nobr><span class="ft4">C-proc, if ever. However,</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:509"><nobr><span class="ft9">the overall crawler may not download all pages that it<br>has to download, because some pages may be reach-<br>able only through inter-partition links. For example,<br>in Figure 2, C</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:592"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:602"><nobr><span class="ft4">can download a, b and c, but not d and</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:509"><nobr><span class="ft4">e, because they can be reached only through h</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:807"><nobr><span class="ft4"> d</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:509"><nobr><span class="ft4">link. However,</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:606"><nobr><span class="ft4">C-proc's can run quite independently</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:509"><nobr><span class="ft9">in this mode, because they do not conduct any run-<br>time coordination or URL exchanges.</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:491"><nobr><span class="ft4">2. Cross-over mode: Primarily, each</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:729"><nobr><span class="ft4">C-proc downloads</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:509"><nobr><span class="ft9">pages within its partition, but when it runs out of<br>pages in its partition, it also follows inter-partition<br>links. For example, consider C</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:697"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:709"><nobr><span class="ft4">in Figure 2. Process</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:509"><nobr><span class="ft4">C</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:519"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:530"><nobr><span class="ft4">first downloads pages a, b and c by following links</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:509"><nobr><span class="ft4">from a. At this point, C</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:659"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:670"><nobr><span class="ft4">runs out of pages in S</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:806"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:812"><nobr><span class="ft4">, so</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:509"><nobr><span class="ft4">it follows a link to g and starts exploring S</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:782"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:788"><nobr><span class="ft4">. After</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:509"><nobr><span class="ft4">downloading g and h, it discovers a link to d in S</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:807"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:813"><nobr><span class="ft4">, so</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:509"><nobr><span class="ft4">it comes back to S</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:621"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:632"><nobr><span class="ft4">and downloads pages d and e.</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:509"><nobr><span class="ft9">In this mode, downloaded pages may clearly overlap<br>(pages g and h are downloaded twice), but the over-<br>all crawler can download more pages than the firewall<br>mode (C</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:562"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:573"><nobr><span class="ft4">downloads d and e in this mode). Also, as</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:509"><nobr><span class="ft4">in the firewall mode,</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:639"><nobr><span class="ft4">C-proc's do not need to commu-</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:509"><nobr><span class="ft9">nicate with each other, because they follow only the<br>links discovered by themselves.</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:491"><nobr><span class="ft4">3. Exchange mode: When</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:679"><nobr><span class="ft4">C-proc's periodically and</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:509"><nobr><span class="ft9">incrementally exchange inter-partition URLs, we say<br>that they operate in an exchange mode. Processes do<br>not follow inter-partition links.</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:509"><nobr><span class="ft4">For example, C</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:600"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:610"><nobr><span class="ft4">in Figure 2 informs C</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:739"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:749"><nobr><span class="ft4">of page g after</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:509"><nobr><span class="ft4">it downloads page a (and c) and C</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:715"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:725"><nobr><span class="ft4">transfers the URL</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:509"><nobr><span class="ft4">of page d to C</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:597"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:608"><nobr><span class="ft4">after it downloads page h. Note that</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:509"><nobr><span class="ft4">C</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:519"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:531"><nobr><span class="ft4">does not follow links to page g. It only transfers</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:509"><nobr><span class="ft4">the links to C</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:592"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:598"><nobr><span class="ft4">, so that C</span></nobr></DIV>
<DIV style="position:absolute;top:662;left:663"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:673"><nobr><span class="ft4">can download the page. In</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:509"><nobr><span class="ft9">this way, the overall crawler can avoid overlap, while<br>maximizing coverage.</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:489"><nobr><span class="ft4">Note that the firewall and the cross-over modes give</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:820"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:734;left:475"><nobr><span class="ft9">proc's much independence (C-proc's do not need to commu-<br>nicate with each other), but they may download the same<br>page multiple times, or may not download some pages. In<br>contrast, the exchange mode avoids these problems but re-<br>quires constant URL exchange between</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:718"><nobr><span class="ft4">C-proc's.</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:475"><nobr><span class="ft3"><b>3.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:825;left:516"><nobr><span class="ft3"><b>URL exchange minimization</b></span></nobr></DIV>
<DIV style="position:absolute;top:848;left:489"><nobr><span class="ft4">To reduce URL exchange, a crawler based on the exchange</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:475"><nobr><span class="ft4">mode may use some of the following techniques:</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:491"><nobr><span class="ft4">1. Batch communication: Instead of transferring an</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:509"><nobr><span class="ft9">inter-partition URL immediately after it is discovered,<br>a</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:520"><nobr><span class="ft4">C-proc may wait for a while, to collect a set of URLs</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:509"><nobr><span class="ft9">and send them in a batch. That is, with batching, a<br>C-proc collects all inter-partition URLs until it down-<br>loads k pages. Then it partitions the collected URLs<br>and sends them to an appropriate</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:719"><nobr><span class="ft4">C-proc. Once these</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:509"><nobr><span class="ft4">URLs are transferred, the</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:678"><nobr><span class="ft4">C-proc then purges them</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:509"><nobr><span class="ft9">and starts to collect a new set of URLs from the next<br>downloaded pages. Note that a</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:702"><nobr><span class="ft4">C-proc does not main-</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:509"><nobr><span class="ft9">tain the list of all inter-partition URLs discovered so<br>far. It only maintains the list of inter-partition links</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">127</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft21{font-size:11px;line-height:25px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146005.png" alt="background image">
<DIV style="position:absolute;top:86;left:114"><nobr><span class="ft9">in the current batch, in order to minimize the memory<br>overhead for URL storage.</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:114"><nobr><span class="ft9">This batch communication has various advantages over<br>incremental communication. First, it incurs less com-<br>munication overhead, because a set of URLs can be<br>sent in a batch, instead of sending one URL per mes-<br>sage. Second, the absolute number of exchanged URLs<br>will also decrease. For example, consider C</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:385"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:398"><nobr><span class="ft4">in Fig-</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:114"><nobr><span class="ft9">ure 2. The link to page g appears twice, in page a and<br>in page c. Therefore, if C</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:267"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:277"><nobr><span class="ft4">transfers the link to g after</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:114"><nobr><span class="ft9">downloading page a, it needs to send the same URL<br>again after downloading page c.</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:313"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:331"><nobr><span class="ft4">In contrast, if C</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:433"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:114"><nobr><span class="ft9">waits until page c and sends URLs in batch, it needs<br>to send the URL for g only once.</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:97"><nobr><span class="ft4">2. Replication: It is known that the number of incom-</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:114"><nobr><span class="ft9">ing links to pages on the Web follows a Zipfian dis-<br>tribution [3, 2, 26]. That is, a small number of Web<br>pages have an extremely large number of links pointing<br>to them, while a majority of pages have only a small<br>number of incoming links.</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:114"><nobr><span class="ft9">Thus, we may significantly reduce URL exchanges, if<br>we replicate the most "popular" URLs at each</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:401"><nobr><span class="ft4">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:114"><nobr><span class="ft9">(by most popular, we mean the URLs with most in-<br>coming links) and stop transferring them between</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:426"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:114"><nobr><span class="ft9">proc's. That is, before we start crawling pages, we<br>identify the most popular k URLs based on the im-<br>age of the Web collected in a previous crawl. Then<br>we replicate these URLs at each</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:322"><nobr><span class="ft4">C-proc, so that the</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:114"><nobr><span class="ft9">C-proc's do not exchange them during a crawl. Since<br>a small number of Web pages have a large number of<br>incoming links, this scheme may significantly reduce<br>URL exchanges between</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:267"><nobr><span class="ft4">C-proc's, even if we replicate</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:114"><nobr><span class="ft4">a small number of URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:630;left:114"><nobr><span class="ft9">Note that some of the replicated URLs may be used<br>as the seed URLs for a</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:257"><nobr><span class="ft4">C-proc. That is, if some URLs</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:114"><nobr><span class="ft9">in the replicated set belong to the same partition that<br>a</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:127"><nobr><span class="ft4">C-proc is responsible for, the C-proc may use those</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:114"><nobr><span class="ft4">URLs as its seeds rather than starting from other pages.</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:114"><nobr><span class="ft4">Also note that it is possible that each</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:352"><nobr><span class="ft4">C-proc tries to</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:114"><nobr><span class="ft9">discover popular URLs on the fly during a crawl, in-<br>stead of identifying them based on the previous im-<br>age. For example, each</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:270"><nobr><span class="ft4">C-proc may keep a "cache"</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:114"><nobr><span class="ft9">of recently seen URL entries. This cache may pick<br>up "popular" URLs automatically, because the pop-<br>ular URLs show up repeatedly. However, we believe<br>that the popular URLs from a previous crawl will be a<br>good approximation for the popular URLs in the cur-<br>rent Web; Most popular Web pages (such as Yahoo!)<br>maintain their popularity for a relatively long period<br>of time, even if their exact popularity may change<br>slightly.</span></nobr></DIV>
<DIV style="position:absolute;top:930;left:81"><nobr><span class="ft3"><b>3.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:930;left:121"><nobr><span class="ft3"><b>Partitioning function</b></span></nobr></DIV>
<DIV style="position:absolute;top:953;left:94"><nobr><span class="ft4">So far, we have mainly assumed that the Web pages are</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:81"><nobr><span class="ft9">partitioned by Web sites. Clearly, there exists a multitude<br>of ways to partition the Web, including the following:</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:97"><nobr><span class="ft4">1. URL-hash based: Based on the hash value of the</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:114"><nobr><span class="ft4">URL of a page, we assign the page to a</span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:374"><nobr><span class="ft4">C-proc. In</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:81"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:88"><nobr><span class="ft4">When it downloads page c, it does not remember whether</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">the link to g has been already sent.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:509"><nobr><span class="ft9">this scheme, pages in the same site can be assigned<br>to different</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:584"><nobr><span class="ft4">C-proc's. Therefore, the locality of link</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:509"><nobr><span class="ft4">structure</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:564"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:575"><nobr><span class="ft4">is not reflected in the partition, and there</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:509"><nobr><span class="ft4">will be many inter-partition links.</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:491"><nobr><span class="ft4">2. Site-hash based: Instead of computing the hash value</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:509"><nobr><span class="ft20">on an entire URL, we compute the hash value only<br>on the site name of a URL (e.g., cnn.com in http:<br>//cnn.com/index.html) and assign the page to a</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:820"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:509"><nobr><span class="ft9">proc.<br>In this scheme, note that the pages in the same site<br>will be allocated to the same partition. Therefore, only<br>some of the inter-site links will be inter-partition links,<br>and thus we can reduce the number of inter-partition<br>links quite significantly compared to the URL-hash<br>based scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:491"><nobr><span class="ft4">3. Hierarchical: Instead of using a hash-value, we may</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:509"><nobr><span class="ft19">partition the Web hierarchically based on the URLs<br>of pages. For example, we may divide the Web into<br>three partitions (the pages in the .com domain, .net<br>domain and all other pages) and allocate them to three<br>C-proc's. Even further, we may decompose the Web by<br>language or country (e.g., .mx for Mexico).</span></nobr></DIV>
<DIV style="position:absolute;top:457;left:509"><nobr><span class="ft9">Because pages hosted in the same domain or country<br>may be more likely to link to pages in the same domain,<br>scheme may have even fewer inter-partition links than<br>the site-hash based scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:475"><nobr><span class="ft9">In this paper, we do not consider the URL-hash based scheme,<br>because it generates a large number of inter-partition links.<br>When the crawler uses URL-hash based scheme,</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:786"><nobr><span class="ft4">C-proc's</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:475"><nobr><span class="ft9">need to exchange much larger number of URLs (exchange<br>mode), and the coverage of the overall crawler can be much<br>lower (firewall mode).</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:489"><nobr><span class="ft4">In addition, in our later experiments, we will mainly use</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:475"><nobr><span class="ft9">the site-hash based scheme as our partitioning function. We<br>chose this option because it is much simpler to implement,<br>and because it captures the core issues that we want to<br>study.</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:524"><nobr><span class="ft4">For example, under the hierarchical scheme, it is</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:475"><nobr><span class="ft9">not easy to divide the Web into equal size partitions, while<br>it is relatively straightforward under the site-hash based<br>scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:522"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:534"><nobr><span class="ft4">Also, we believe we can interpret the results from</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:475"><nobr><span class="ft9">the site-hash based scheme as the upper/lower bound for<br>the hierarchical scheme. For instance, assuming Web pages<br>link to more pages in the same domain, the number of inter-<br>partition links will be lower in the hierarchical scheme than<br>in the site-hash based scheme (although we could not con-<br>firm this trend in our experiments).</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:489"><nobr><span class="ft4">In Figure 3, we summarize the options that we have dis-</span></nobr></DIV>
<DIV style="position:absolute;top:862;left:475"><nobr><span class="ft4">cussed so far.</span></nobr></DIV>
<DIV style="position:absolute;top:862;left:574"><nobr><span class="ft4">The right-hand table in the figure shows</span></nobr></DIV>
<DIV style="position:absolute;top:877;left:475"><nobr><span class="ft9">more detailed view on the static coordination scheme. In<br>the diagram, we highlight the main focus of our paper with<br>dark grey. That is, we mainly study the static coordination<br>scheme (the third column in the left-hand table) and we use<br>the site-hash based partitioning for our experiments (the<br>second row in the second table). However, during our dis-<br>cussion, we will also briefly explore the implications of other</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:476"><nobr><span class="ft5">3</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:482"><nobr><span class="ft4">According to our experiments, about 90% of the links in a</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:475"><nobr><span class="ft4">page point to pages in the same site on average.</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:476"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:482"><nobr><span class="ft4">While the sizes of individual Web sites vary, the sizes of</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:475"><nobr><span class="ft17">partitions are similar, because each partition contains many<br>Web sites and their average sizes are similar among parti-<br>tions.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">128</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft22{font-size:11px;font-family:Times;color:#000000;}
	.ft23{font-size:11px;font-family:Times;color:#000000;}
	.ft24{font-size:5px;font-family:Times;color:#000000;}
	.ft25{font-size:6px;line-height:10px;font-family:Times;color:#000000;}
	.ft26{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146006.png" alt="background image">
<DIV style="position:absolute;top:148;left:81"><nobr><span class="ft22"><b>Type</b></span></nobr></DIV>
<DIV style="position:absolute;top:118;left:765"><nobr><span class="ft23">URL-hash</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:765"><nobr><span class="ft23">Site-hash</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:765"><nobr><span class="ft23">Hierarchical</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:123"><nobr><span class="ft23">Distributed</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:123"><nobr><span class="ft23">Intra-site</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:195"><nobr><span class="ft23">Independent</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:291"><nobr><span class="ft23">Dynamic</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:375"><nobr><span class="ft23">Static</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:627"><nobr><span class="ft23">Batch Replication None</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:291"><nobr><span class="ft23">Main focus</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:399"><nobr><span class="ft23"> Also discussed</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:765"><nobr><span class="ft22"><b>Partitioning</b></span></nobr></DIV>
<DIV style="position:absolute;top:226;left:669"><nobr><span class="ft23">Exchange</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:270"><nobr><span class="ft22"><b>Coordination</b></span></nobr></DIV>
<DIV style="position:absolute;top:208;left:477"><nobr><span class="ft23">Firewall</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:549"><nobr><span class="ft23">Cross-over</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:585"><nobr><span class="ft22"><b>Mode</b></span></nobr></DIV>
<DIV style="position:absolute;top:280;left:519"><nobr><span class="ft23">Not discussed further</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:303"><nobr><span class="ft4">Figure 3: Summary of the options discussed</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:81"><nobr><span class="ft9">options. For instance, the firewall mode is an "improved"<br>version of the independent coordination scheme (in the first<br>table), so our study on the firewall mode will show the impli-<br>cations of the independent coordination scheme. Also, we<br>roughly estimate the performance of the URL-hash based<br>scheme (first row in the second table) when we discuss the<br>results from the site-hash based scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:94"><nobr><span class="ft4">Given our table of crawler design space, it would be very</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:81"><nobr><span class="ft9">interesting to see what options existing search engines se-<br>lected for their own crawlers. Unfortunately, this informa-<br>tion is impossible to obtain in most cases because compa-<br>nies consider their technologies proprietary and want to keep<br>them secret. The only two crawler designs that we know of<br>are the prototype Google crawler [22] (when it was devel-<br>oped at Stanford) and the Mercator crawler [15] at Compaq.<br>The prototype google crawler used the intra-site, static and<br>site-hash based scheme and ran in exchange mode [22]. The<br>Mercator crawler uses the site-based hashing scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:661;left:81"><nobr><span class="ft3"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:661;left:112"><nobr><span class="ft3"><b>EVALUATION MODELS</b></span></nobr></DIV>
<DIV style="position:absolute;top:683;left:94"><nobr><span class="ft4">In this section, we define metrics that will let us quantify</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:81"><nobr><span class="ft9">the advantages or disadvantages of different parallel crawl-<br>ing schemes. These metrics will be used later in our experi-<br>ments.</span></nobr></DIV>
<DIV style="position:absolute;top:754;left:97"><nobr><span class="ft4">1. Overlap: When multiple</span></nobr></DIV>
<DIV style="position:absolute;top:751;left:284"><nobr><span class="ft4">C-proc's are downloading</span></nobr></DIV>
<DIV style="position:absolute;top:770;left:114"><nobr><span class="ft9">Web pages simultaneously, it is possible that differ-<br>ent</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:139"><nobr><span class="ft4">C-proc's download the same page multiple times.</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:114"><nobr><span class="ft9">Multiple downloads of the same page are clearly unde-<br>sirable.</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:114"><nobr><span class="ft9">More precisely, we define the overlap of downloaded<br>pages as</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:170"><nobr><span class="ft5">N-I</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:179"><nobr><span class="ft5">I</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:196"><nobr><span class="ft4">. Here, N represents the total number of</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:114"><nobr><span class="ft9">pages downloaded by the overall crawler, and I repre-<br>sents the number of unique pages downloaded, again,<br>by the overall crawler. Thus, the goal of a parallel<br>crawler is to minimize the overlap.</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:114"><nobr><span class="ft9">Note that a parallel crawler does not have an overlap<br>problem, if it is based on the firewall mode (Section 3,<br>Item 1) or the exchange mode (Section 3, Item 3). In<br>these modes, a</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:206"><nobr><span class="ft4">C-proc downloads pages only within its</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:114"><nobr><span class="ft4">own partition, so the overlap is always zero.</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:97"><nobr><span class="ft4">2. Coverage: When multiple</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:279"><nobr><span class="ft4">C-proc's run independently,</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:114"><nobr><span class="ft9">it is possible that they may not download all pages that<br>they have to. In particular, a crawler based on the fire-<br>wall mode (Section 3, Item 1) may have this problem,</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:509"><nobr><span class="ft4">because its</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:579"><nobr><span class="ft4">C-proc's do not follow inter-partition links</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:509"><nobr><span class="ft4">nor exchange the links with others.</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:509"><nobr><span class="ft9">To formalize this notion, we define the coverage of<br>downloaded pages as</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:643"><nobr><span class="ft5">I</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:641"><nobr><span class="ft5">U</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:652"><nobr><span class="ft4">, where U represents the total</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:509"><nobr><span class="ft9">number of pages that the overall crawler has to down-<br>load, and I is the number of unique pages downloaded<br>by the overall crawler. For example, in Figure 2, if C</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:828"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:509"><nobr><span class="ft4">downloaded pages a, b and c, and if C</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:751"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:763"><nobr><span class="ft4">downloaded</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:509"><nobr><span class="ft9">pages f through i, the coverage of the overall crawler<br>is</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:525"><nobr><span class="ft25">7<br>9</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:536"><nobr><span class="ft4">= 0.77, because it downloaded 7 pages out of 9.</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:491"><nobr><span class="ft4">3. Quality: Often, crawlers cannot download the whole</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:509"><nobr><span class="ft9">Web, and thus they try to download an "important" or<br>"relevant" section of the Web. For example, a crawler<br>may have storage space only for 1 million pages and<br>may want to download the "most important" 1 mil-<br>lion pages. To implement this policy, a crawler needs<br>a notion of "importance" of pages, often called an im-<br>portance metric [9].</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:509"><nobr><span class="ft9">For example, let us assume that the crawler uses back-<br>link count as its importance metric. That is, the crawler<br>considers a page p important when a lot of other pages<br>point to it. Then the goal of the crawler is to download<br>the most highly-linked 1 million pages. To achieve this<br>goal, a single-process crawler may use the following<br>method [9]: The crawler constantly keeps track of how<br>many backlinks each page has from the pages that it<br>has already downloaded, and first visits the page with<br>the highest backlink count. Clearly, the pages down-<br>loaded in this way may not be the top 1 million pages,<br>because the page selection is not based on the entire<br>Web, only on what has been seen so far. Thus, we may<br>formalize the notion of "quality" of downloaded pages<br>as follows [9]:</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:509"><nobr><span class="ft9">First, we assume a hypothetical oracle crawler, which<br>knows the exact importance of every page under a cer-<br>tain importance metric. We assume that the oracle<br>crawler downloads the most important N pages in to-<br>tal, and use P</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:594"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:609"><nobr><span class="ft4">to represent that set of N pages. We</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:509"><nobr><span class="ft4">also use A</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:569"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:583"><nobr><span class="ft4">to represent the set of N pages that an ac-</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:509"><nobr><span class="ft26">tual crawler would download, which would not be nec-<br>essarily the same as P</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:644"><nobr><span class="ft5">N</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:655"><nobr><span class="ft4">. Then we define</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:766"><nobr><span class="ft5">|A</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:778"><nobr><span class="ft24">N</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:787"><nobr><span class="ft5">P</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:802"><nobr><span class="ft24">N</span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:811"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:779"><nobr><span class="ft5">|P</span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:789"><nobr><span class="ft24">N</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:798"><nobr><span class="ft5">|</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:822"><nobr><span class="ft4">as</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:509"><nobr><span class="ft9">the quality of downloaded pages by the actual crawler.<br>Under this definition, the quality represents the frac-<br>tion of the true top N pages that are downloaded by</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">129</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft27{font-size:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146007.png" alt="background image">
<DIV style="position:absolute;top:86;left:114"><nobr><span class="ft4">the crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:108;left:114"><nobr><span class="ft9">Note that the quality of a parallel crawler may be<br>worse than that of a single-process crawler, because<br>many importance metrics depend on the global struc-<br>ture of the Web (e.g., backlink count). That is, each</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:426"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:114"><nobr><span class="ft9">proc in a parallel crawler may know only the pages that<br>are downloaded by itself, and thus have less informa-<br>tion on page importance than a single-process crawler<br>does.</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:159"><nobr><span class="ft4">On the other hand, a single-process crawler</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:114"><nobr><span class="ft19">knows all pages it has downloaded so far. Therefore, a<br>C-proc in a parallel crawler may make a worse crawling<br>decision than a single-process crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:114"><nobr><span class="ft4">In order to avoid this quality problem,</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:345"><nobr><span class="ft4">C-proc's need to</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:114"><nobr><span class="ft9">periodically exchange information on page importance.<br>For example, if the backlink count is the importance<br>metric, a</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:171"><nobr><span class="ft4">C-proc may periodically notify other C-proc's</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:114"><nobr><span class="ft9">of how many pages in its partition have links to pages<br>in other partitions.</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:114"><nobr><span class="ft9">Note that this backlink exchange can be naturally in-<br>corporated in an exchange mode crawler (Section 3,<br>Item 3). In this mode, crawling processes exchange<br>inter-partition URLs periodically, so a</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:347"><nobr><span class="ft4">C-proc can sim-</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:114"><nobr><span class="ft9">ply count how many inter-partition links it receives<br>from other</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:184"><nobr><span class="ft4">C-proc's, to count backlinks originating in</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:114"><nobr><span class="ft9">other partitions. More precisely, if the crawler uses the<br>batch communication technique (Section 3.1, Item 1),<br>process C</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:174"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:186"><nobr><span class="ft4">would send a message like [http://cnn.</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:114"><nobr><span class="ft4">com/index.html, 3] to C</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:271"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:278"><nobr><span class="ft4">, to notify that C</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:380"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:390"><nobr><span class="ft4">has seen</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:114"><nobr><span class="ft4">3 links to the page in the current batch.</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:350"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:362"><nobr><span class="ft4">On receipt of</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:114"><nobr><span class="ft4">this message, C</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:208"><nobr><span class="ft5">2</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:218"><nobr><span class="ft4">then increases the backlink count for</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:114"><nobr><span class="ft9">the page by 3 to reflect the inter-partition links. By<br>incorporating this scheme, we believe that the quality<br>of the exchange mode will be better than that of the<br>firewall mode or the cross-over mode.</span></nobr></DIV>
<DIV style="position:absolute;top:646;left:114"><nobr><span class="ft9">However, note that the quality of an exchange mode<br>crawler may vary depending on how often it exchanges<br>backlink messages. For instance, if crawling processes<br>exchange backlink messages after every page down-<br>load, they will have essentially the same backlink infor-<br>mation as a single-process crawler does. (They know<br>backlink counts from all pages that have been down-<br>loaded.)</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:177"><nobr><span class="ft4">Therefore, the quality of the downloaded</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:114"><nobr><span class="ft9">pages would be virtually the same as that of a single-<br>process crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:227"><nobr><span class="ft4">In contrast, if</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:324"><nobr><span class="ft4">C-proc's rarely ex-</span></nobr></DIV>
<DIV style="position:absolute;top:803;left:114"><nobr><span class="ft9">change backlink messages, they do not have "accurate"<br>backlink counts from downloaded pages, so they may<br>make poor crawling decisions, resulting in poor qual-<br>ity. Later, we will study how often</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:346"><nobr><span class="ft4">C-proc's should</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:114"><nobr><span class="ft9">exchange backlink messages in order to maximize the<br>quality.</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:97"><nobr><span class="ft4">4. Communication overhead: The</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:333"><nobr><span class="ft4">C-proc's in a par-</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:114"><nobr><span class="ft9">allel crawler need to exchange messages to coordinate<br>their work. In particular,</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:281"><nobr><span class="ft4">C-proc's based on the ex-</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:114"><nobr><span class="ft9">change mode (Section 3, Item 3) swap their inter-<br>partition URLs periodically. To quantify how much<br>communication is required for this exchange, we de-<br>fine communication overhead as the average number of<br>inter-partition URLs exchanged per downloaded page.</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft5">5</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:88"><nobr><span class="ft4">If the</span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:125"><nobr><span class="ft4">C-proc's send inter-partition URLs incrementally af-</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:81"><nobr><span class="ft4">ter every page, the</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:206"><nobr><span class="ft4">C-proc's can send the URL only, and</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">other</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:117"><nobr><span class="ft4">C-proc's can simply count these URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:489"><nobr><span class="ft27">Mode</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:562"><nobr><span class="ft27">Coverage</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:625"><nobr><span class="ft27">Overlap</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:681"><nobr><span class="ft27">Quality</span></nobr></DIV>
<DIV style="position:absolute;top:84;left:734"><nobr><span class="ft27">Communication</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:489"><nobr><span class="ft27">Firewall</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:576"><nobr><span class="ft27">Bad</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:631"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:690"><nobr><span class="ft27">Bad</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:762"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:489"><nobr><span class="ft27">Cross-over</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:572"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:635"><nobr><span class="ft27">Bad</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:690"><nobr><span class="ft27">Bad</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:762"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:489"><nobr><span class="ft27">Exchange</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:572"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:631"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:686"><nobr><span class="ft27">Good</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:766"><nobr><span class="ft27">Bad</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:495"><nobr><span class="ft4">Table 1: Comparison of three crawling modes</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:509"><nobr><span class="ft9">For example, if a parallel crawler has downloaded 1,000<br>pages in total and if its</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:652"><nobr><span class="ft4">C-proc's have exchanged 3,000</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:509"><nobr><span class="ft9">inter-partition URLs, its communication overhead is<br>3,000/1,000 = 3. Note that a crawler based on the the<br>firewall and the cross-over mode do not have any com-<br>munication overhead, because they do not exchange<br>any inter-partition URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:489"><nobr><span class="ft4">In Table 1, we compare the relative merits of the three</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:475"><nobr><span class="ft9">crawling modes (Section 3, Items 1­3). In the table, "Good"<br>means that the mode is expected to perform relatively well<br>for that metric, and "Bad" means that it may perform worse<br>compared to other modes. For instance, the firewall mode<br>does not exchange any inter-partition URLs (Communica-<br>tion: Good) and downloads pages only once (Overlap: Good),<br>but it may not download every page (Coverage: Bad). Also,<br>because</span></nobr></DIV>
<DIV style="position:absolute;top:442;left:526"><nobr><span class="ft4">C-proc's do not exchange inter-partition URLs, the</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:475"><nobr><span class="ft9">downloaded pages may be of lower quality than those of an<br>exchange mode crawler. Later, we will examine these issues<br>more quantitatively through experiments based on real Web<br>data.</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:475"><nobr><span class="ft3"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:542;left:507"><nobr><span class="ft3"><b>DESCRIPTION OF DATASET</b></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:489"><nobr><span class="ft4">We have discussed various issues related to a parallel</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:475"><nobr><span class="ft9">crawler and identified multiple alternatives for its architec-<br>ture. In the remainder of this paper, we quantitatively study<br>these issues through experiments conducted on real Web<br>data.</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:489"><nobr><span class="ft4">In all of the following experiments, we used 40 million</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:475"><nobr><span class="ft9">Web pages in our Stanford WebBase repository. Because<br>the property of this dataset may significantly impact the<br>result of our experiments, readers might be interested in<br>how we collected these pages.</span></nobr></DIV>
<DIV style="position:absolute;top:721;left:489"><nobr><span class="ft4">We downloaded the pages using our Stanford WebBase</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:475"><nobr><span class="ft4">crawler in December 1999 in the period of 2 weeks.</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:821"><nobr><span class="ft4">In</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft9">downloading the pages, the WebBase crawler started with<br>the URLs listed in Open Directory (http://www.dmoz.org),<br>and followed links. We decided to use the Open Directory<br>URLs as seed URLs, because these pages are the ones that<br>are considered "important" by its maintainers. In addition,<br>some of our local WebBase users were keenly interested in<br>the Open Directory pages and explicitly requested that we<br>cover them. The total number of URLs in the Open Direc-<br>tory was around 1 million at that time. Then conceptually,<br>the WebBase crawler downloaded all these pages, extracted<br>URLs within the downloaded pages, and followed links in a<br>breadth-first manner. (The WebBase crawler uses various<br>techniques to expedite and prioritize crawling process, but<br>we believe these optimizations do not affect the final dataset<br>significantly.)</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:489"><nobr><span class="ft4">Our dataset is relatively "small" (40 million pages) com-</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:475"><nobr><span class="ft9">pared to the full Web, but keep in mind that using a sig-<br>nificantly larger dataset would have made many of our ex-<br>periments prohibitively expensive. As we will see, each of<br>the graphs we present study multiple configurations, and<br>for each configuration, multiple crawler runs were made to</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">130</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft28{font-size:9px;font-family:Times;color:#000000;}
	.ft29{font-size:10px;font-family:Times;color:#000000;}
	.ft30{font-size:10px;font-family:Helvetica;color:#000000;}
	.ft31{font-size:8px;font-family:Times;color:#000000;}
	.ft32{font-size:8px;font-family:Times;color:#000000;}
	.ft33{font-size:11px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146008.png" alt="background image">
<DIV style="position:absolute;top:252;left:178"><nobr><span class="ft28">2</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:219"><nobr><span class="ft28">4</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:259"><nobr><span class="ft28">8</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:296"><nobr><span class="ft28">16</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:337"><nobr><span class="ft28">32</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:378"><nobr><span class="ft28">64</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:117"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:117"><nobr><span class="ft28">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:117"><nobr><span class="ft28">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:117"><nobr><span class="ft28">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:132"><nobr><span class="ft28">1</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:265"><nobr><span class="ft29">Number of </span></nobr></DIV>
<DIV style="position:absolute;top:264;left:325"><nobr><span class="ft30">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:362"><nobr><span class="ft29">'s</span></nobr></DIV>
<DIV style="position:absolute;top:81;left:125"><nobr><span class="ft29">Coverage</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:395"><nobr><span class="ft15"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:298;left:106"><nobr><span class="ft4">Figure 4: Number of processes vs. Coverage</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:98"><nobr><span class="ft31">64</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:119"><nobr><span class="ft31">4096</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:159"><nobr><span class="ft31">10000</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:230"><nobr><span class="ft31">20000</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:302"><nobr><span class="ft31">30000</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:82"><nobr><span class="ft31">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:82"><nobr><span class="ft31">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:81"><nobr><span class="ft31">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:82"><nobr><span class="ft31">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:95"><nobr><span class="ft31">1</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:382"><nobr><span class="ft31">64</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:383"><nobr><span class="ft31">32</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:383"><nobr><span class="ft31">8</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:383"><nobr><span class="ft31">2</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:392"><nobr><span class="ft12">processes</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:393"><nobr><span class="ft12">processes</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:398"><nobr><span class="ft12">processes</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:399"><nobr><span class="ft12">processes</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:91"><nobr><span class="ft32">Coverage</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:277"><nobr><span class="ft32">Number of Seeds</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:345"><nobr><span class="ft33"><i>s</i></span></nobr></DIV>
<DIV style="position:absolute;top:526;left:100"><nobr><span class="ft4">Figure 5: Number of seed URLs vs. Coverage</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:81"><nobr><span class="ft9">obtain statistically valid data points. Each run involves sim-<br>ulating how one or more</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:233"><nobr><span class="ft4">C-proc's would visit the 40 million</span></nobr></DIV>
<DIV style="position:absolute;top:586;left:81"><nobr><span class="ft9">pages. Such detailed simulations are inherently very time<br>consuming.</span></nobr></DIV>
<DIV style="position:absolute;top:617;left:94"><nobr><span class="ft4">It is clearly difficult to predict what would happen for a</span></nobr></DIV>
<DIV style="position:absolute;top:633;left:81"><nobr><span class="ft9">larger dataset. In the extended version of this paper [8],<br>we examine this data size issue a bit more carefully and<br>discuss whether a larger dataset would have changed our<br>conclusions.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft3"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:715;left:112"><nobr><span class="ft3"><b>FIREWALL MODE AND COVERAGE</b></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:94"><nobr><span class="ft4">A firewall mode crawler (Section 3, Item 1) has mini-</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:81"><nobr><span class="ft9">mal communication overhead, but it may have coverage and<br>quality problems (Section 4). In this section, we quanti-<br>tatively study the effectiveness of a firewall mode crawler<br>using the 40 million pages in our repository. In particular,<br>we estimate the coverage (Section 4, Item 2) of a firewall<br>mode crawler when it employs n</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:287"><nobr><span class="ft4">C-proc's in parallel. (We</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft4">discuss the quality issue of a parallel crawler later.)</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:94"><nobr><span class="ft4">In our experiments, we considered the 40 million pages</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft9">within our WebBase repository as the entire Web, and we<br>used site-hash based partitioning (Section 3.2, Item 2). As<br>the seed URLs, each</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:207"><nobr><span class="ft4">C-proc was given 5 random URLs from</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:81"><nobr><span class="ft9">its own partition, so 5n seed URLs were used in total by the<br>overall crawler. (We discuss the effect of the number of seed<br>URLs shortly.) Since the crawler ran in firewall mode,</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:426"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:81"><nobr><span class="ft19">proc's followed only intra-partition links, not inter-partition<br>links. Under these settings, we let the</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:330"><nobr><span class="ft4">C-proc's run until</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:81"><nobr><span class="ft9">they ran out of URLs. After this simulated crawling, we<br>measured the overall coverage of the crawler. We performed<br>these experiments with 5n random seed URLS and repeated<br>the experiments multiple times with different seed URLs. In<br>all of the runs, the results were essentially the same.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:489"><nobr><span class="ft4">In Figure 4, we summarize the results from the exper-</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:475"><nobr><span class="ft9">iments. The horizontal axis represents n, the number of<br>parallel</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:524"><nobr><span class="ft4">C-proc's, and the vertical axis shows the coverage of</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:475"><nobr><span class="ft9">the overall crawler for the given experiment. Note that the<br>coverage is only 0.9 even when n = 1 (a single-process). This<br>result is because the crawler in our experiment started with<br>only 5 URLs, while the actual dataset was collected with 1<br>million seed URLs. Thus, some of the 40 million pages were<br>unreachable from the 5 seed URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:489"><nobr><span class="ft4">From the figure it is clear that the coverage decreases as</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:475"><nobr><span class="ft9">the number of processes increases. This trend is because the<br>number of inter-partition links increases as the Web is split<br>into smaller partitions, and thus more pages are reachable<br>only through inter-partition links.</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:489"><nobr><span class="ft4">From this result we can see that we may run a crawler in a</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:475"><nobr><span class="ft9">firewall mode without much decrease in coverage with fewer<br>than 4</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:522"><nobr><span class="ft4">C-proc's. For example, for the 4-process case, the</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:475"><nobr><span class="ft9">coverage decreases only 10% from the single-process case.<br>At the same time, we can also see that the firewall mode<br>crawler becomes quite ineffective with a large number of</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:821"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:475"><nobr><span class="ft19">proc's. Less than 10% of the Web can be downloaded when<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:494"><nobr><span class="ft4">C-proc's run together, each starting with 5 seed URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:431;left:489"><nobr><span class="ft4">Clearly, coverage may depend on the number of seed URLs</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:475"><nobr><span class="ft4">that each</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:538"><nobr><span class="ft4">C-proc starts with. To study this issue, we also</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:475"><nobr><span class="ft9">ran experiments varying the number of seed URLs, s, and<br>we show the results in Figure 5.</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:694"><nobr><span class="ft4">The horizontal axis in</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:475"><nobr><span class="ft9">the graph represents s, the total number of seed URLs that<br>the overall crawler used, and the vertical axis shows the<br>coverage for that experiment. For example, when s = 128,<br>the overall crawler used 128 total seed URLs, each</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:796"><nobr><span class="ft4">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:475"><nobr><span class="ft4">starting with 2 seed URLs when 64</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:693"><nobr><span class="ft4">C-proc's ran in parallel.</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:475"><nobr><span class="ft4">We performed the experiments for 2, 8, 32, 64</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:761"><nobr><span class="ft4">C-proc cases</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:475"><nobr><span class="ft9">and plotted their coverage values. From this figure, we can<br>observe the following trends:</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:495"><nobr><span class="ft4">· When a large number of C-proc's run in parallel (e.g.,</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:509"><nobr><span class="ft9">32 or 64), the total number of seed URLs affects the<br>coverage very significantly. For example, when 64 pro-<br>cesses run in parallel the coverage value jumps from<br>0.4% to 10% if the number of seed URLs increases<br>from 64 to 1024.</span></nobr></DIV>
<DIV style="position:absolute;top:725;left:495"><nobr><span class="ft4">· When only a small number of processes run in parallel</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:509"><nobr><span class="ft9">(e.g., 2 or 8), coverage is not significantly affected by<br>the number of seed URLs. While coverage increases<br>slightly as s increases, the improvement is marginal.</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:475"><nobr><span class="ft4">Based on these results, we draw the following conclusions:</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:491"><nobr><span class="ft4">1. When a relatively small number of</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:715"><nobr><span class="ft4">C-proc's are running</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:509"><nobr><span class="ft9">in parallel, a crawler using the firewall mode provides<br>good coverage. In this case, the crawler may start with<br>only a small number of seed URLs, because coverage<br>is not much affected by the number of seed URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:491"><nobr><span class="ft4">2. The firewall mode is not a good choice if the crawler</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:509"><nobr><span class="ft9">wants to download every single page on the Web. The<br>crawler may miss some portion of the Web, particu-<br>larly when it runs many</span></nobr></DIV>
<DIV style="position:absolute;top:952;left:659"><nobr><span class="ft4">C-proc's in parallel.</span></nobr></DIV>
<DIV style="position:absolute;top:980;left:490"><nobr><span class="ft4">Example 1. (Generic search engine) To illustrate how</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:475"><nobr><span class="ft9">our results could guide the design of a parallel crawler, con-<br>sider the following example. Assume that to operate a Web<br>search engine, we need to download 1 billion pages</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:784"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:795"><nobr><span class="ft4">in one</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:476"><nobr><span class="ft5">6</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:482"><nobr><span class="ft4">Currently the Web is estimated to have around 1 billion</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">pages.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">131</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft34{font-size:12px;font-family:Times;color:#000000;}
	.ft35{font-size:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146009.png" alt="background image">
<DIV style="position:absolute;top:253;left:187"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:236"><nobr><span class="ft28">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:284"><nobr><span class="ft28">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:332"><nobr><span class="ft28">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:387"><nobr><span class="ft28">1</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:125"><nobr><span class="ft28">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:140"><nobr><span class="ft28">1</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:125"><nobr><span class="ft28">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:140"><nobr><span class="ft28">2</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:125"><nobr><span class="ft28">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:140"><nobr><span class="ft28">3</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:127"><nobr><span class="ft34">overlap</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:335"><nobr><span class="ft34">coverage</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:271"><nobr><span class="ft35"><i>n=64</i></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:297"><nobr><span class="ft35"><i>n=32</i></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:180"><nobr><span class="ft29">: number of </span></nobr></DIV>
<DIV style="position:absolute;top:110;left:169"><nobr><span class="ft35"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:283"><nobr><span class="ft29">'s</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:244"><nobr><span class="ft30">C-proc</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:367"><nobr><span class="ft35"><i>n=4</i></span></nobr></DIV>
<DIV style="position:absolute;top:222;left:353"><nobr><span class="ft35"><i>n=8</i></span></nobr></DIV>
<DIV style="position:absolute;top:236;left:381"><nobr><span class="ft35"><i>n=2</i></span></nobr></DIV>
<DIV style="position:absolute;top:225;left:317"><nobr><span class="ft35"><i>n=16</i></span></nobr></DIV>
<DIV style="position:absolute;top:298;left:81"><nobr><span class="ft4">Figure 6:</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:161"><nobr><span class="ft4">Coverage vs. Overlap for a cross-over</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:81"><nobr><span class="ft4">mode crawler</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:81"><nobr><span class="ft4">month. Each machine that we plan to run our</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:371"><nobr><span class="ft4">C-proc's on</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:81"><nobr><span class="ft9">has 10 Mbps link to the Internet, and we can use as many<br>machines as we want.</span></nobr></DIV>
<DIV style="position:absolute;top:387;left:94"><nobr><span class="ft4">Given that the average size of a web page is around 10K</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:81"><nobr><span class="ft4">bytes, we roughly need to download 10</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:310"><nobr><span class="ft5">4</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:321"><nobr><span class="ft4">× 10</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:351"><nobr><span class="ft5">9</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:362"><nobr><span class="ft4">= 10</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:392"><nobr><span class="ft5">13</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:410"><nobr><span class="ft4">bytes</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:81"><nobr><span class="ft9">in one month. This download rate corresponds to 34 Mbps,<br>and we need 4 machines (thus 4</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:274"><nobr><span class="ft4">C-proc's) to obtain the rate.</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:81"><nobr><span class="ft9">Given the results of our experiment (Figure 4), we may es-<br>timate that the coverage will be at least 0.8 with 4</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:387"><nobr><span class="ft4">C-proc's.</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:81"><nobr><span class="ft9">Therefore, in this scenario, the firewall mode may be good<br>enough, unless it is very important to download the "entire"<br>Web.</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:96"><nobr><span class="ft4">Example 2. (High freshness) As a second example, let us</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:81"><nobr><span class="ft9">now assume that we have strong "freshness" requirement on<br>the 1 billion pages and need to revisit every page once every<br>week, not once every month.</span></nobr></DIV>
<DIV style="position:absolute;top:588;left:273"><nobr><span class="ft4">This new scenario requires</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:81"><nobr><span class="ft9">approximately 140 Mbps for page download, and we need to<br>run 14</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:129"><nobr><span class="ft4">C-proc's. In this case, the coverage of the overall</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:81"><nobr><span class="ft9">crawler decreases to less than 0.5 according to Figure 4. Of<br>course, the coverage could be larger than our conservative<br>estimate, but to be safe one would probably want to consider<br>using a crawler mode different than the firewall mode.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft3"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:715;left:112"><nobr><span class="ft3"><b>CROSS-OVER MODE AND OVERLAP</b></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:94"><nobr><span class="ft4">In this section, we study the effectiveness of a cross-over</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:81"><nobr><span class="ft9">mode crawler (Section 3, Item 2). A cross-over crawler may<br>yield improved coverage of the Web, since it follows inter-<br>partition links when a</span></nobr></DIV>
<DIV style="position:absolute;top:781;left:221"><nobr><span class="ft4">C-proc runs out of URLs in its own</span></nobr></DIV>
<DIV style="position:absolute;top:800;left:81"><nobr><span class="ft9">partition. However, this mode incurs overlap in downloaded<br>pages (Section 4, Item 1), because a page can be downloaded<br>by multiple</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:158"><nobr><span class="ft4">C-proc's. Therefore, the crawler increases its</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft4">coverage at the expense of overlap in the downloaded pages.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:94"><nobr><span class="ft4">In Figure 6, we show the relationship between the coverage</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft9">and the overlap of a cross-over mode crawler obtained from<br>the following experiments. We partitioned the 40M pages<br>using site-hash partitioning and assigned them to n</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:388"><nobr><span class="ft4">C-proc's.</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:81"><nobr><span class="ft4">Each of the n</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:167"><nobr><span class="ft4">C-proc's then was given 5 random seed URLs</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:81"><nobr><span class="ft9">from its partition and followed links in the cross-over mode.<br>During this experiment, we measured how much overlap the<br>overall crawler incurred when its coverage reached various<br>points. The horizontal axis in the graph shows the coverage<br>at a particular time and the vertical axis shows the overlap<br>at the given coverage. We performed the experiments for<br>n = 2, 4, . . . , 64.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:94"><nobr><span class="ft4">Note that in most cases the overlap stays at zero until the</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">coverage becomes relatively large. For example, when n =</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:594"><nobr><span class="ft27">2</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:626"><nobr><span class="ft27">4</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:658"><nobr><span class="ft27">8</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:684"><nobr><span class="ft27">16</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:717"><nobr><span class="ft27">32</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:749"><nobr><span class="ft27">64</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:541"><nobr><span class="ft27">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:556"><nobr><span class="ft27">1</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:541"><nobr><span class="ft27">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:556"><nobr><span class="ft27">2</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:541"><nobr><span class="ft27">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:556"><nobr><span class="ft27">3</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:672"><nobr><span class="ft4">URL Hash</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:672"><nobr><span class="ft4">Site Hash</span></nobr></DIV>
<DIV style="position:absolute;top:80;left:547"><nobr><span class="ft29">Communication overhead</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:773"><nobr><span class="ft13"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:264;left:663"><nobr><span class="ft29">Number of C-proc's</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:475"><nobr><span class="ft9">Figure 7: Number of crawling processes vs. Number<br>of URLs exchanged per page</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:475"><nobr><span class="ft9">16, the overlap is zero until coverage reaches 0.5. We can un-<br>derstand this result by looking at the graph in Figure 4. Ac-<br>cording to that graph, a crawler with 16</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:725"><nobr><span class="ft4">C-proc's can cover</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:475"><nobr><span class="ft9">around 50% of the Web by following only intra-partition<br>links. Therefore, even a cross-over mode crawler will fol-<br>low only intra-partition links until its coverage reaches that<br>point. Only after that, each</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:658"><nobr><span class="ft4">C-proc starts to follow inter-</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:475"><nobr><span class="ft19">partition links, thus increasing the overlap. For this reason,<br>we believe that the overlap would have been much worse in<br>the beginning of the crawl, if we adopted the independent<br>model (Section 2). By applying the partitioning scheme to<br>C-proc's, we make each C-proc stay in its own partition in<br>the beginning and suppress the overlap as long as possible.</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:489"><nobr><span class="ft4">While the crawler in the cross-over mode is much better</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:475"><nobr><span class="ft9">than one based on the independent model, it is clear that the<br>cross-over crawler still incurs quite significant overlap. For<br>example, when 4</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:585"><nobr><span class="ft4">C-proc's run in parallel in the cross-over</span></nobr></DIV>
<DIV style="position:absolute;top:608;left:475"><nobr><span class="ft9">mode, the overlap becomes almost 2.5 to obtain coverage<br>close to 1. For this reason, we do not recommend the cross-<br>over mode, unless it is absolutely necessary to download<br>every page without any communication between</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:771"><nobr><span class="ft4">C-proc's.</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:475"><nobr><span class="ft3"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:694;left:507"><nobr><span class="ft16"><b>EXCHANGE MODE AND COMMUNICA-<br>TION</b></span></nobr></DIV>
<DIV style="position:absolute;top:737;left:489"><nobr><span class="ft4">To avoid the overlap and coverage problems, an exchange</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft9">mode crawler (Section 3, Item 3) constantly exchanges inter-<br>partition URLs between</span></nobr></DIV>
<DIV style="position:absolute;top:765;left:626"><nobr><span class="ft4">C-proc's. In this section, we study</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:475"><nobr><span class="ft9">the communication overhead (Section 4, Item 4) of an ex-<br>change mode crawler and how much we can reduce it by<br>replicating the most popular k URLs. For now, let us assume<br>that a</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:521"><nobr><span class="ft4">C-proc immediately transfers inter-partition URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:847;left:475"><nobr><span class="ft9">(We will discuss batch communication later when we dis-<br>cuss the quality of a parallel crawler.)</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:489"><nobr><span class="ft4">In the experiments, again, we split the 40 million pages</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:475"><nobr><span class="ft4">into n partitions based on site-hash values and ran n</span></nobr></DIV>
<DIV style="position:absolute;top:891;left:820"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:475"><nobr><span class="ft9">proc's in the exchange mode. At the end of the crawl, we<br>measured how many URLs had been exchanged during the<br>crawl. We show the results in Figure 7. In the figure, the<br>horizontal axis represents the number of parallel</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:782"><nobr><span class="ft4">C-proc's,</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:475"><nobr><span class="ft9">n, and the vertical axis shows the communication overhead<br>(the average number of URLs transferred per page). For<br>comparison purposes, the figure also shows the overhead for<br>a URL-hash based scheme, although the curve is clipped at<br>the top because of its large overhead values.</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:489"><nobr><span class="ft4">To explain the graph, we first note that an average page</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">has 10 out-links, and about 9 of them point to pages in</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">132</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft36{font-size:10px;line-height:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146010.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft9">the same site. Therefore, the 9 links are internally followed<br>by a</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:112"><nobr><span class="ft4">C-proc under site-hash partitioning. Only the remain-</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:81"><nobr><span class="ft9">ing 1 link points to a page in a different site and may be<br>exchanged between processes. Figure 7 indicates that this<br>URL exchange increases with the number of processes. For<br>example, the</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:165"><nobr><span class="ft4">C-proc's exchanged 0.4 URLs per page when</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:81"><nobr><span class="ft9">2 processes ran, while they exchanged 0.8 URLs per page<br>when 16 processes ran. Based on the graph, we draw the<br>following conclusions:</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:101"><nobr><span class="ft4">· The site-hash based partitioning scheme significantly</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:114"><nobr><span class="ft9">reduces communication overhead, compared to the URL-<br>hash based scheme. We need to transfer only up to<br>one link per page (or 10% of the links), which is sig-<br>nificantly smaller than the URL-hash based scheme.<br>For example, when we ran 2</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:292"><nobr><span class="ft4">C-proc's using the URL-</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:114"><nobr><span class="ft9">hash based scheme the crawler exchanged 5 links per<br>page under the URL-hash based scheme, which was<br>significantly larger than 0.5 links per page under the<br>site-hash based scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:101"><nobr><span class="ft4">· The network bandwidth used for the URL exchange is</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:114"><nobr><span class="ft9">relatively small, compared to the actual page down-<br>load bandwidth.</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:222"><nobr><span class="ft4">Under the site-hash based scheme,</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:114"><nobr><span class="ft9">at most 1 URL will be exchanged per page, which is<br>about 40 bytes.</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:207"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:219"><nobr><span class="ft4">Given that the average size of a Web</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:114"><nobr><span class="ft9">page is 10 KB, the URL exchange consumes less than<br>40/10K = 0.4% of the total network bandwidth.</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:101"><nobr><span class="ft4">· However, the overhead of the URL exchange on the</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:114"><nobr><span class="ft9">overall system can be quite significant. The processes<br>need to exchange up to one message per page, and the<br>message has to go through the TCP/IP network stack<br>at the sender and the receiver. Thus it is copied to and<br>from kernel space twice, incurring two context switches<br>between the kernel and the user mode. Since these op-<br>erations pose significant overhead even if the message<br>size is small, the overall overhead can be important if<br>the processes exchange one message per every down-<br>loaded page.</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:94"><nobr><span class="ft4">In the extended version of this paper [8], we also study how</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:81"><nobr><span class="ft9">much we can reduce this overhead by replication. In short,<br>our results indicate that we can get significant reduction in<br>communication cost (between 40% ­ 50% reduction) when<br>we replication the most popular 10,000 ­ 100,000 URLs<br>in each</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:130"><nobr><span class="ft4">C-proc. When we replicated more URLs, the cost</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:81"><nobr><span class="ft9">reduction was not as dramatic as the first 100,000 URLs.<br>Thus, we recommend replicating 10,000 ­ 100,000 URLs.</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft3"><b>9.</b></span></nobr></DIV>
<DIV style="position:absolute;top:845;left:112"><nobr><span class="ft16"><b>QUALITY AND BATCH COMMUNICA-<br>TION</b></span></nobr></DIV>
<DIV style="position:absolute;top:889;left:94"><nobr><span class="ft4">As we discussed, the quality (Section 4, Item 3) of a</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:81"><nobr><span class="ft9">parallel crawler can be worse than that of a single-process<br>crawler, because each</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:218"><nobr><span class="ft4">C-proc may make crawling decisions</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:81"><nobr><span class="ft9">solely based on the information collected within its own par-<br>tition. We now study this quality issue. In the discussion we<br>also study the impact of the batch communication technique<br>(Section 3.1, Item 1) on quality.</span></nobr></DIV>
<DIV style="position:absolute;top:998;left:94"><nobr><span class="ft4">Throughout the experiments in this section, we assume</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:81"><nobr><span class="ft9">that the crawler uses the number of backlinks to page p as<br>the importance of p, or I(p). That is, if 1000 pages on the</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:81"><nobr><span class="ft5">7</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:88"><nobr><span class="ft4">In our estimation, an average URL was about 40 bytes</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:81"><nobr><span class="ft4">long.</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:556"><nobr><span class="ft28">0</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:577"><nobr><span class="ft28">1 2</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:606"><nobr><span class="ft28">4</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:627"><nobr><span class="ft28">10 20</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:675"><nobr><span class="ft28">50 100</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:742"><nobr><span class="ft28">500 1000</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:522"><nobr><span class="ft28">0.025</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:529"><nobr><span class="ft28">0.05</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:522"><nobr><span class="ft28">0.075</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:537"><nobr><span class="ft28">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:522"><nobr><span class="ft28">0.125</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:529"><nobr><span class="ft28">0.15</span></nobr></DIV>
<DIV style="position:absolute;top:117;left:522"><nobr><span class="ft28">0.175</span></nobr></DIV>
<DIV style="position:absolute;top:100;left:536"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:552"><nobr><span class="ft29">Quality</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:647"><nobr><span class="ft29">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:178;left:733"><nobr><span class="ft36">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:701"><nobr><span class="ft29">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:781"><nobr><span class="ft13"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:267;left:563"><nobr><span class="ft4">(a) URL exchange vs. Quality</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:543"><nobr><span class="ft28">0</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:580"><nobr><span class="ft28">1</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:602"><nobr><span class="ft28">2</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:630"><nobr><span class="ft28">4</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:668"><nobr><span class="ft28">10</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:704"><nobr><span class="ft28">20</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:752"><nobr><span class="ft28">50</span></nobr></DIV>
<DIV style="position:absolute;top:413;left:522"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:522"><nobr><span class="ft28">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:522"><nobr><span class="ft28">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:522"><nobr><span class="ft28">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:522"><nobr><span class="ft28">1.0</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:649"><nobr><span class="ft29">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:605"><nobr><span class="ft36">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:572"><nobr><span class="ft29">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:435;left:784"><nobr><span class="ft15"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:286;left:525"><nobr><span class="ft29">Communication overhead</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:537"><nobr><span class="ft4">(b) URL exchange vs. Communication</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:475"><nobr><span class="ft9">Figure 8: Crawlers downloaded 500K pages (1.2%<br>of 40M)</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:475"><nobr><span class="ft9">Web have links to page p, the importance of p is I(p) = 1000.<br>Clearly, there exist many other ways to define the impor-<br>tance of a page, but we use this metric because it (or its<br>variations) is being used by some existing search engines [22,<br>13]. Also, note that this metric depends on the global struc-<br>ture of the Web. If we use an importance metric that solely<br>depends on a page itself, not on the global structure of the<br>Web, the quality of a parallel crawler will be essentially the<br>same as that of a single crawler, because each</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:766"><nobr><span class="ft4">C-proc in a</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:475"><nobr><span class="ft9">parallel crawler can make good decisions based on the pages<br>that it has downloaded.</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:489"><nobr><span class="ft4">Under the backlink metric, each</span></nobr></DIV>
<DIV style="position:absolute;top:716;left:681"><nobr><span class="ft4">C-proc in our experiments</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:475"><nobr><span class="ft9">counted how many backlinks a page has from the down-<br>loaded pages and visited the page with the most backlinks<br>first. Remember that the</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:637"><nobr><span class="ft4">C-proc's need to periodically ex-</span></nobr></DIV>
<DIV style="position:absolute;top:782;left:475"><nobr><span class="ft9">change messages to inform others of the inter-partition back-<br>links. Depending on how often they exchange messages, the<br>quality of the downloaded pages will differ. For example,<br>if</span></nobr></DIV>
<DIV style="position:absolute;top:826;left:488"><nobr><span class="ft4">C-proc's never exchange messages, the quality will be the</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:475"><nobr><span class="ft9">same as that of a firewall mode crawler, and if they exchange<br>messages after every downloaded page, the quality will be<br>similar to that of a single-process crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:489"><nobr><span class="ft4">To study these issues, we compared the quality of the</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:475"><nobr><span class="ft4">downloaded pages when</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:628"><nobr><span class="ft4">C-proc's exchanged backlink mes-</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:475"><nobr><span class="ft9">sages at various intervals and we show the results in Fig-<br>ures 8(a), 9(a) and 10(a). Each graph shows the quality<br>achieved by the overall crawler when it downloaded a total of<br>500K, 2M, and 8M pages, respectively. The horizontal axis<br>in the graphs represents the total number of URL exchanges<br>during a crawl, x, and the vertical axis shows the quality<br>for the given experiment. For example, when x = 1, the<br>C-proc's exchanged backlink count information only once in<br>the middle of the crawl. Therefore, the case when x = 0 rep-<br>resents the quality of a firewall mode crawler, and the case</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">133</span></nobr></DIV>
</DIV>
<!-- Page 11 -->
<a name="11"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft37{font-size:14px;font-family:Times;color:#000000;}
	.ft38{font-size:11px;line-height:11px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="146011.png" alt="background image">
<DIV style="position:absolute;top:244;left:154"><nobr><span class="ft28">0</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:177"><nobr><span class="ft28">1 2</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:206"><nobr><span class="ft28">4</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:228"><nobr><span class="ft28">10 20</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:278"><nobr><span class="ft28">50 100</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:348"><nobr><span class="ft28">500 1000</span></nobr></DIV>
<DIV style="position:absolute;top:211;left:127"><nobr><span class="ft28">0.05</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:135"><nobr><span class="ft28">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:127"><nobr><span class="ft28">0.15</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:134"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:100;left:127"><nobr><span class="ft28">0.25</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:250"><nobr><span class="ft29">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:148"><nobr><span class="ft29">Quality</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:323"><nobr><span class="ft36">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:291"><nobr><span class="ft29">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:387"><nobr><span class="ft13"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:269;left:168"><nobr><span class="ft4">(a) URL exchange vs. Quality</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:159"><nobr><span class="ft27">0</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:181"><nobr><span class="ft27">1 2 4</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:230"><nobr><span class="ft27">10 20</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:279"><nobr><span class="ft27">50 100</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:346"><nobr><span class="ft27">5001000</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:127"><nobr><span class="ft27">0.25</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:133"><nobr><span class="ft27">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:127"><nobr><span class="ft27">0.75</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:150"><nobr><span class="ft27">1</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:224"><nobr><span class="ft38">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:288;left:139"><nobr><span class="ft23">Communication overhead</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:386"><nobr><span class="ft37"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:461;left:237"><nobr><span class="ft23">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:185"><nobr><span class="ft23">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:143"><nobr><span class="ft4">(b) URL exchange vs. Communication</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:81"><nobr><span class="ft9">Figure 9: Crawlers downloaded 2M pages (5% of<br>40M)</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:81"><nobr><span class="ft4">when x</span></nobr></DIV>
<DIV style="position:absolute;top:546;left:129"><nobr><span class="ft4">  shows the quality of a single-process crawler.</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:81"><nobr><span class="ft9">In Figures 8(b), 9(b) and 10(b), we also show the commu-<br>nication overhead (Section 4, Item 4); that is, the average<br>number of [URL, backlink count] pairs exchanged per a<br>downloaded page.</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:94"><nobr><span class="ft4">From these figures, we can observe the following trends:</span></nobr></DIV>
<DIV style="position:absolute;top:647;left:101"><nobr><span class="ft4">· As the number of crawling processes increases, the qual-</span></nobr></DIV>
<DIV style="position:absolute;top:666;left:114"><nobr><span class="ft9">ity of downloaded pages becomes worse, unless they ex-<br>change backlink messages often. For example, in Fig-<br>ure 8(a), the quality achieved by a 2-process crawler<br>(0.12) is significantly higher than that of a 64-process<br>crawler (0.025) in the firewall mode (x = 0). Again,<br>this result is because each</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:288"><nobr><span class="ft4">C-proc learns less about</span></nobr></DIV>
<DIV style="position:absolute;top:760;left:114"><nobr><span class="ft9">the global backlink counts when the Web is split into<br>smaller parts.</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:101"><nobr><span class="ft4">· The quality of the firewall mode crawler (x = 0 ) is sig-</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:114"><nobr><span class="ft9">nificantly worse than that of the single-process crawler<br>(x</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:132"><nobr><span class="ft4"> ) when the crawler downloads a relatively small</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:114"><nobr><span class="ft9">fraction of the pages (Figures 8(a) and 9(a)). How-<br>ever, the difference is not very significant when the<br>crawler</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:174"><nobr><span class="ft4">downloads</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:253"><nobr><span class="ft4">a</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:276"><nobr><span class="ft4">relatively</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:348"><nobr><span class="ft4">large</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:393"><nobr><span class="ft4">fraction</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:114"><nobr><span class="ft9">(Figure 10(a)). In other experiments, when the crawler<br>downloaded more than 50% of the pages, the difference<br>was almost negligible in any case. (Due to space lim-<br>itations, we do not show the graphs.) Intuitively, this<br>result makes sense because quality is an important is-<br>sue only when the crawler downloads a small portion<br>of the Web. (If the crawler will visit all pages anyway,<br>quality is not relevant.)</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:101"><nobr><span class="ft4">· The communication overhead does not increase lin-</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:114"><nobr><span class="ft9">early as the number of URL exchange increases. The<br>graphs in Figures 8(b), 9(b) and 10(b) are not straight<br>lines. This result is because a popular URL appears</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:542"><nobr><span class="ft28">0</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:565"><nobr><span class="ft28">1 2</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:596"><nobr><span class="ft28">4</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:618"><nobr><span class="ft28">10 20</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:669"><nobr><span class="ft28">50 100</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:742"><nobr><span class="ft28">500 1000</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:523"><nobr><span class="ft28">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:522"><nobr><span class="ft28">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:522"><nobr><span class="ft28">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:523"><nobr><span class="ft28">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:522"><nobr><span class="ft28">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:522"><nobr><span class="ft28">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:640"><nobr><span class="ft29">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:534"><nobr><span class="ft29">Quality</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:714"><nobr><span class="ft36">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:682"><nobr><span class="ft29">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:781"><nobr><span class="ft13"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:271;left:563"><nobr><span class="ft4">(a) URL exchange vs. Quality</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:545"><nobr><span class="ft27">0</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:567"><nobr><span class="ft27">1 2 4</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:619"><nobr><span class="ft27">10 20</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:669"><nobr><span class="ft27">50 100</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:739"><nobr><span class="ft27">500 1000</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:522"><nobr><span class="ft27">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:522"><nobr><span class="ft27">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:522"><nobr><span class="ft27">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:522"><nobr><span class="ft27">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:522"><nobr><span class="ft27">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:606"><nobr><span class="ft38">2<br>8<br>64</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:532"><nobr><span class="ft23">Communication overhead</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:571"><nobr><span class="ft23">Processes</span></nobr></DIV>
<DIV style="position:absolute;top:438;left:779"><nobr><span class="ft37"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:464;left:637"><nobr><span class="ft23">Number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:537"><nobr><span class="ft4">(b) URL exchange vs. Communication</span></nobr></DIV>
<DIV style="position:absolute;top:509;left:475"><nobr><span class="ft9">Figure 10: Crawlers downloaded 8M pages (20% of<br>40M)</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:509"><nobr><span class="ft4">multiple times between backlink exchanges.</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:794"><nobr><span class="ft4">There-</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:509"><nobr><span class="ft9">fore, a popular URL can be transferred as one entry<br>(URL and its backlink count) in the exchange, even if<br>it appeared multiple times. This reduction increases<br>as</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:525"><nobr><span class="ft4">C-proc's exchange backlink messages less frequently.</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:495"><nobr><span class="ft4">· One does not need a large number of URL exchanges</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:509"><nobr><span class="ft9">to achieve high quality. Through multiple experiments,<br>we tried to identify how often</span></nobr></DIV>
<DIV style="position:absolute;top:665;left:686"><nobr><span class="ft4">C-proc's should exchange</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:509"><nobr><span class="ft9">backlink messages to achieve the highest quality value.<br>From these experiments, we found that a parallel<br>crawler can get the highest quality values even if the<br>processes communicate less than 100 times during a<br>crawl.</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:489"><nobr><span class="ft4">We use the following example to illustrate how one can</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:475"><nobr><span class="ft4">use the results of our experiments.</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:490"><nobr><span class="ft4">Example 3. (Medium-Scale Search Engine) Say we plan</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:475"><nobr><span class="ft9">to operate a medium-scale search engine, and we want to<br>maintain about 20% of the Web (200 M pages) in our index.<br>Our plan is to refresh the index once a month. The machines<br>that we can use have individual T1 links (1.5 Mbps) to the<br>Internet.</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:489"><nobr><span class="ft4">In order to update the index once a month, we need about</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:475"><nobr><span class="ft9">6.2 Mbps download bandwidth, so we have to run at least<br>5</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:488"><nobr><span class="ft4">C-proc's on 5 machines. According to Figure 10(a) (20%</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:475"><nobr><span class="ft4">download case), we can achieve the highest quality if the</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:820"><nobr><span class="ft4">C-</span></nobr></DIV>
<DIV style="position:absolute;top:969;left:475"><nobr><span class="ft19">proc's exchange backlink messages 10 times during a crawl<br>when 8 processes run in parallel.</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:694"><nobr><span class="ft4">(We use the 8 process</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:475"><nobr><span class="ft9">case because it is the closest number to 5.) Also, from Fig-<br>ure 10(b), we can see that when</span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:669"><nobr><span class="ft4">C-proc's exchange messages</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:475"><nobr><span class="ft9">10 times during a crawl they need to exchange fewer than<br>0 .17</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:507"><nobr><span class="ft4">× 200M = 34M [URL, backlink count] pairs in to-</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:475"><nobr><span class="ft4">tal. Therefore, the total network bandwidth used by the back-</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">134</span></nobr></DIV>
</DIV>
<!-- Page 12 -->
<a name="12"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="146012.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft4">link exchange is only (34M</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:247"><nobr><span class="ft4">· 40 )/(200M · 10K )  0 .06 % of</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:81"><nobr><span class="ft9">the bandwidth used by actual page downloads. Also, since<br>the exchange happens only 10 times during a crawl, the<br>context-switch overhead for message transfers (discussed in<br>Section 8) is minimal.</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:94"><nobr><span class="ft4">Note that in this scenario we need to exchange 10 back-</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:81"><nobr><span class="ft9">link messages in one month or one message every three days.<br>Therefore, even if the connection between</span></nobr></DIV>
<DIV style="position:absolute;top:193;left:333"><nobr><span class="ft4">C-proc's is unreli-</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:81"><nobr><span class="ft9">able or sporadic, we can still use the exchange mode without<br>any problem.</span></nobr></DIV>
<DIV style="position:absolute;top:258;left:81"><nobr><span class="ft3"><b>10.</b></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:121"><nobr><span class="ft3"><b>CONCLUSION</b></span></nobr></DIV>
<DIV style="position:absolute;top:280;left:94"><nobr><span class="ft4">Crawlers are being used more and more often to collect</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:81"><nobr><span class="ft9">Web data for search engine, caches, and data mining. As<br>the size of the Web grows, it becomes increasingly impor-<br>tant to use parallel crawlers. Unfortunately, almost nothing<br>is known (at least in the open literature) about options for<br>parallelizing crawlers and their performance. Our paper ad-<br>dresses this shortcoming by presenting several architectures<br>and strategies for parallel crawlers, and by studying their<br>performance. We believe that our paper offers some useful<br>guidelines for crawler designers, helping them, for example,<br>select the right number of crawling processes, or select the<br>proper inter-process coordination scheme.</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:94"><nobr><span class="ft4">In summary, the main conclusions of our study were the</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:81"><nobr><span class="ft4">following:</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:101"><nobr><span class="ft4">· When a small number of crawling processes run in par-</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:114"><nobr><span class="ft9">allel (in our experiment, 4 or fewer), the firewall mode<br>provides good coverage.</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:279"><nobr><span class="ft4">Given that firewall mode</span></nobr></DIV>
<DIV style="position:absolute;top:553;left:114"><nobr><span class="ft9">crawlers can run totally independently and are easy<br>to implement, we believe that it is a good option to<br>consider. The cases when the firewall mode might not<br>be appropriate are:</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:126"><nobr><span class="ft4">1. when we need to run more than 4 crawling pro-</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:144"><nobr><span class="ft4">cesses or</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:126"><nobr><span class="ft4">2. when we download only a small subset of the Web</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:144"><nobr><span class="ft9">and the quality of the downloaded pages is impor-<br>tant.</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:101"><nobr><span class="ft4">· A crawler based on the exchange mode consumes small</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:114"><nobr><span class="ft9">network bandwidth for URL exchanges (less than 1%<br>of the network bandwidth). It can also minimize other<br>overheads by adopting the batch communication tech-<br>nique. In our experiments, the crawler could maximize<br>the quality of the downloaded pages, even if it ex-<br>changed backlink messages fewer than 100 times dur-<br>ing a crawl.</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:101"><nobr><span class="ft4">· By replicating between 10,000 and 100,000 popular</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:114"><nobr><span class="ft9">URLs, we can reduce the communication overhead by<br>roughly 40%. Replicating more URLs does not signif-<br>icantly reduce the overhead.</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:81"><nobr><span class="ft3"><b>11.</b></span></nobr></DIV>
<DIV style="position:absolute;top:921;left:121"><nobr><span class="ft3"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:938;left:88"><nobr><span class="ft4">[1] T. E. Anderson, M. D. Dahlin, J. M. Neefe, D. A.</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:109"><nobr><span class="ft9">Patterson, D. S. Roselli, and R. Y. Wang. Serverless<br>network file systems. In Proc. of SOSP, 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:88"><nobr><span class="ft4">[2] A. Barabasi and R. Albert. Emergence of scaling in</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:109"><nobr><span class="ft4">random networks. Science, 286(509), 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:88"><nobr><span class="ft4">[3] A. Z. Broder, S. R. Kumar, F. Maghoul, P. Raghavan,</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:109"><nobr><span class="ft9">S. Rajagopalan, R. Stata, A. Tomkins, and J. L.<br>Wiener. Graph structure in the web. In Proc. of<br>WWW Conf., 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:482"><nobr><span class="ft4">[4] M. Burner. Crawling towards eterneity: Building an</span></nobr></DIV>
<DIV style="position:absolute;top:102;left:503"><nobr><span class="ft9">archive of the world wide web. Web Techniques<br>Magazine, 2(5), May 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:482"><nobr><span class="ft4">[5] S. Chakrabarti, M. van den Berg, and B. Dom.</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:503"><nobr><span class="ft9">Focused crawling: A new approach to topic-specific<br>web resource discovery. In Proc. of WWW Conf., 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:482"><nobr><span class="ft4">[6] J. Cho and H. Garcia-Molina. The evolution of the</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:503"><nobr><span class="ft9">web and implications for an incremental crawler. In<br>Proc. of VLDB Conf., 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:482"><nobr><span class="ft4">[7] J. Cho and H. Garcia-Molina. Synchronizing a</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:503"><nobr><span class="ft9">database to improve freshness. In Proc. of SIGMOD<br>Conf., 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:482"><nobr><span class="ft4">[8] J. Cho and H. Garcia-Molina. Parallel crawlers.</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:503"><nobr><span class="ft4">Technical report, UCLA Computer Science, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:482"><nobr><span class="ft4">[9] J. Cho, H. Garcia-Molina, and L. Page. Efficient</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:503"><nobr><span class="ft9">crawling through URL ordering. In Proc. of WWW<br>Conf., 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:475"><nobr><span class="ft4">[10] E. Coffman, Jr., Z. Liu, and R. R. Weber. Optimal</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:503"><nobr><span class="ft9">robot scheduling for web search engines. Technical<br>report, INRIA, 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:475"><nobr><span class="ft4">[11] M. Diligenti, F. M. Coetzee, S. Lawrence, C. L. Giles,</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:503"><nobr><span class="ft9">and M. Gori. Focused crawling using context graphs.<br>In Proc. of VLDB Conf., 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:475"><nobr><span class="ft4">[12] D. Eichmann. The RBSE spider: Balancing effective</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:503"><nobr><span class="ft9">search against web load. In Proc. of WWW Conf.,<br>1994.</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:475"><nobr><span class="ft26">[13] Google Inc. http://www.google.com.<br>[14] A. Heydon and M. Najork. Mercator: A scalable,</span></nobr></DIV>
<DIV style="position:absolute;top:541;left:503"><nobr><span class="ft9">extensible web crawler. Word Wide Web,<br>2(4):219­229, December 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:475"><nobr><span class="ft4">[15] A. Heydon and M. Najork. High-performance web</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:503"><nobr><span class="ft9">crawling. Technical report, SRC Research Report, 173,<br>Compaq Systems Research Center, September 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:475"><nobr><span class="ft4">[16] D. Hirschberg. Parallel algorithms for the transitive</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:503"><nobr><span class="ft9">closure and the connected component problem. In<br>Proc. of STOC Conf., 1976.</span></nobr></DIV>
<DIV style="position:absolute;top:671;left:475"><nobr><span class="ft4">[17] M. Koster. Robots in the web: threat or treat?</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:503"><nobr><span class="ft4">ConneXions, 4(4), April 1995.</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:475"><nobr><span class="ft4">[18] O. A. McBryan. GENVL and WWWW: Tools for</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:503"><nobr><span class="ft4">taming the web. In Proc. of WWW Conf., 1994.</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:475"><nobr><span class="ft4">[19] R. C. Miller and K. Bharat. SPHINX: a framework for</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:503"><nobr><span class="ft9">creating personal, site-specific web crawlers. In Proc.<br>of WWW Conf., 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:475"><nobr><span class="ft4">[20] D. Nassimi and S. Sahni. Parallel permutation and</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:503"><nobr><span class="ft9">sorting algorithms and a new generalized connection<br>network. Journal of ACM, 29:642­667, July 1982.</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:475"><nobr><span class="ft4">[21] M. T. Ozsu and P. Valduriez. Principles of Distributed</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:503"><nobr><span class="ft4">Database Systems. Prentice Hall, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:475"><nobr><span class="ft4">[22] L. Page and S. Brin. The anatomy of a large-scale</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:503"><nobr><span class="ft9">hypertextual web search engine. In Proc. of WWW<br>Conf., 1998.</span></nobr></DIV>
<DIV style="position:absolute;top:915;left:475"><nobr><span class="ft4">[23] B. Pinkerton. Finding what people want: Experiences</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:503"><nobr><span class="ft4">with the web crawler. In Proc. of WWW Conf., 1994.</span></nobr></DIV>
<DIV style="position:absolute;top:948;left:475"><nobr><span class="ft4">[24] Robots exclusion protocol. http://info.webcrawler.</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:503"><nobr><span class="ft4">com/mak/projects/robots/exclusion.html.</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:475"><nobr><span class="ft4">[25] A. S. Tanenbaum and R. V. Renesse. Distributed</span></nobr></DIV>
<DIV style="position:absolute;top:996;left:503"><nobr><span class="ft9">operating systems. ACM Computing Surveys, 17(4),<br>December 1985.</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:475"><nobr><span class="ft4">[26] G. K. Zipf. Human Behaviour and the Principle of</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:503"><nobr><span class="ft9">Least Effort: an Introduction to Human Ecology.<br>Addison-Wesley, 1949.</span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:447"><nobr><span class="ft8">135</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
