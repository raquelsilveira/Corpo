<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - sp41-lai.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="Rose">
<META name="date" content="2006-11-15T08:37:04+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Times;color:#000000;}
	.ft1{font-size:16px;font-family:Times;color:#000000;}
	.ft2{font-size:13px;font-family:Times;color:#000000;}
	.ft3{font-size:11px;font-family:Times;color:#000000;}
	.ft4{font-size:8px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Courier;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft10{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="197001.png" alt="background image">
<DIV style="position:absolute;top:109;left:176"><nobr><span class="ft0"><b>Towards Content-Based Relevance Ranking  </b></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:352"><nobr><span class="ft0"><b>for Video Search</b></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:273"><nobr><span class="ft1">Wei Lai </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:381"><nobr><span class="ft1">Xian-Sheng Hua </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:543"><nobr><span class="ft1">Wei-Ying Ma </span></nobr></DIV>
<DIV style="position:absolute;top:209;left:377"><nobr><span class="ft2">Microsoft Research Asia </span></nobr></DIV>
<DIV style="position:absolute;top:227;left:268"><nobr><span class="ft2">No.49, Zhichun Road, Haidian District, Beijing, P.R.China </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:333"><nobr><span class="ft2">{weilai, xshua, wyma}@microsoft.com </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:287;left:81"><nobr><span class="ft1">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:179"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:308;left:81"><nobr><span class="ft8">Most existing web video search engines index videos by file <br>names, URLs, and surrounding texts. These types of video <br>metadata roughly describe the whole video in an abstract level <br>without taking the rich content, such as semantic content <br>descriptions and speech within the video, into consideration. <br>Therefore the relevance ranking of the video search results is not <br>satisfactory as the details of video contents are ignored.  In this <br>paper we propose a novel relevance ranking approach for Web-<br>based video search using both video metadata and the rich content <br>contained in the videos. To leverage real content into ranking, the <br>videos are segmented into shots, which are smaller and more <br>semantic-meaningful retrievable units, and then more detailed <br>information of video content such as semantic descriptions and <br>speech of each shots are used to improve the retrieval and ranking <br>performance. With video metadata and content information of <br>shots, we developed an integrated ranking approach, which <br>achieves improved ranking performance. We also introduce <br>machine learning into the ranking system, and compare them with <br>IR­model (information retrieval model) based method. The <br>evaluation results demonstrate the effectiveness of the proposed <br>ranking methods. </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:81"><nobr><span class="ft1">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:353"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:671;left:81"><nobr><span class="ft9">H.3.3 [Information Storage and Retrieval]: Information Search <br>and Retrieval - Search process </span></nobr></DIV>
<DIV style="position:absolute;top:712;left:81"><nobr><span class="ft1">General Terms: </span></nobr></DIV>
<DIV style="position:absolute;top:716;left:210"><nobr><span class="ft3">Algorithms, Performance, Experimentation. </span></nobr></DIV>
<DIV style="position:absolute;top:742;left:81"><nobr><span class="ft1">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:81"><nobr><span class="ft3">Video search, Relevance ranking, Content-based ranking </span></nobr></DIV>
<DIV style="position:absolute;top:797;left:81"><nobr><span class="ft1">1.  INTRODUCTION </span></nobr></DIV>
<DIV style="position:absolute;top:818;left:81"><nobr><span class="ft8">Multimedia search has become an active research field due to the <br>rapid increase of online-available content and new practical <br>applications. Search technology is considered the key to <br>navigating the Internet's growing media (video, audio and image) <br>collections. Google Yahoo, Blinkx and other search companies <br>have provided elementary video search engines. However, <br>existing video search engines are all based on the text information <br>related to the video which can be retrieved from web pages, such <br>as file names, URLs, and surrounding texts. These types of textual <br>information can be considered as "metadata" of the video since <br>they only roughly describe the video. There is no doubt that text <br>searching is the most efficient way to retrieve information (even <br>when searching for videos), because it well matches the manner of </span></nobr></DIV>
<DIV style="position:absolute;top:287;left:475"><nobr><span class="ft8">human thinking. However, only using metadata is far form <br>people's expectation in video searching, because even the best <br>case scenario, the metadata is only the highly concentrated <br>overview of a video, with many losses on details. </span></nobr></DIV>
<DIV style="position:absolute;top:359;left:488"><nobr><span class="ft3">In general, a video consists of many shots and sub-events with a </span></nobr></DIV>
<DIV style="position:absolute;top:375;left:475"><nobr><span class="ft8">temporal main thread. The video should be segmented into smaller <br>retrievable units that are directly related to what users perceive as <br>meaningful. Much research has concentrated on segmenting video <br>streams into "shots" using low level visual features [1]. Each <br>segment has its own scenes and meanings. In many cases, when <br>users query a video, they intend to find some desired clips in the <br>video instead of viewing it thoroughly. However, this can seldom <br>be achieved by searching the surrounding text which is related to <br>the whole video.  </span></nobr></DIV>
<DIV style="position:absolute;top:526;left:488"><nobr><span class="ft3">Much content information can be used to search videos and </span></nobr></DIV>
<DIV style="position:absolute;top:542;left:475"><nobr><span class="ft8">shots. In content-based video retrieval systems, video shots can be <br>classified into or annotated by several semantic concepts. The <br>most substantial works in this field are presented in the TREC <br>Video Retrieval Evaluation (TRECVID) community [2]. In <br>addition, speech is also significant information which has close <br>connection to video contents. Some videos are associated with <br>transcripts/closed captions which are provided by content provider. <br>Using ASR (automatically speech recognition) to generate speech <br>text is another practical solution. </span></nobr></DIV>
<DIV style="position:absolute;top:694;left:488"><nobr><span class="ft3">In this paper, with the video metadata and content information </span></nobr></DIV>
<DIV style="position:absolute;top:710;left:475"><nobr><span class="ft8">of video shots, we index and rank the videos in a way similar to <br>general text-based web page search engines. The IR-model, which <br>is widely employed in text information retrieval and web page <br>search, will be applied to rank the search results by examining <br>relevance between query and indexed information (including both <br>metadata and content information). To fully utilize the content <br>information and get a better ranking performance, we integrate the <br>"shot relevance" into "video relevance". That is, the ranking is <br>decided not only by the relevance of video (metadata of the entire <br>video), but also by all the relevant shots within the video. </span></nobr></DIV>
<DIV style="position:absolute;top:877;left:488"><nobr><span class="ft3">We also apply learning based method to rank the search results </span></nobr></DIV>
<DIV style="position:absolute;top:893;left:475"><nobr><span class="ft8">based on a set of features extracted from the corresponding query, <br>video metadata, and content information.  </span></nobr></DIV>
<DIV style="position:absolute;top:933;left:488"><nobr><span class="ft3">The rest of this paper is organized as follows. Section 2 </span></nobr></DIV>
<DIV style="position:absolute;top:949;left:475"><nobr><span class="ft8">introduces the IR-model based ranking, including extraction of <br>video metadata and content information, and a ranking method <br>integrating these two types of information. In section 3, a learning <br>based ranking approach is presented. Section 4 compares the <br>ranking performance evolution results, and Section 5 concludes <br>the paper. </span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:87"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:87"><nobr><span class="ft10">Copyright is held by the author/owner(s). <br>MM'06, October 23?7, 2006, Santa Barbara, California, USA. <br>ACM 1-59593-447-2/06/0010. <br> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">627</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft11{font-size:18px;font-family:Times;color:#000000;}
	.ft12{font-size:6px;font-family:Times;color:#000000;}
	.ft13{font-size:7px;font-family:Times;color:#000000;}
	.ft14{font-size:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="197002.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft1">2.  IR-MODEL BASED RANKING </span></nobr></DIV>
<DIV style="position:absolute;top:139;left:81"><nobr><span class="ft1">2.1  Relevance Evaluation of Text Search </span></nobr></DIV>
<DIV style="position:absolute;top:160;left:81"><nobr><span class="ft8">In the traditional text retrieval and web page search, IR <br>(Information Retrieval) models are usually used to evaluate the <br>relevance between a query and a document. BM25 [3] is one of <br>the frequently used evaluation methods. Given a query Q and a <br>document D, the relevance between Q and D can be modeled as <br>the summation of the relevance between each query term (word) t <br>in Q and D: </span></nobr></DIV>
<DIV style="position:absolute;top:275;left:250"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:297;left:253"><nobr><span class="ft12"></span></nobr></DIV>
<DIV style="position:absolute;top:279;left:239"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:258"><nobr><span class="ft12">Q</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:251"><nobr><span class="ft12">t</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:296"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:281"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:267"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:221"><nobr><span class="ft3">Q</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:206"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:192"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:301"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:292"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:276"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:231"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:216"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:200"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:308"><nobr><span class="ft3">                                   (1) </span></nobr></DIV>
<DIV style="position:absolute;top:315;left:81"><nobr><span class="ft3">where: </span></nobr></DIV>
<DIV style="position:absolute;top:345;left:383"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:374"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:351"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:335"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:326"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:298"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:292"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:273"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:239"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:213"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:205"><nobr><span class="ft3">((</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:310"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:294"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:285"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:262"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:256"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:228"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:166"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:156"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:141"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:192"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:239"><nobr><span class="ft12">1</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:379"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:365"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:341"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:331"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:315"><nobr><span class="ft3">tf</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:272"><nobr><span class="ft3">avdl</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:279"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:256"><nobr><span class="ft3">b</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:231"><nobr><span class="ft3">b</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:186"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:299"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:289"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:274"><nobr><span class="ft3">tf</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:233"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:161"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:146"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:132"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:359"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:362;left:305"><nobr><span class="ft3">+</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:265"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:362;left:246"><nobr><span class="ft3">+</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:222"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:200"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:335;left:268"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:335;left:247"><nobr><span class="ft3">+</span></nobr></DIV>
<DIV style="position:absolute;top:344;left:174"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:390"><nobr><span class="ft3">           (2) </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:290"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:281"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:315"><nobr><span class="ft3">5</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:311"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:304"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:287"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:278"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:219"><nobr><span class="ft3">log</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:200"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:191"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:285"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:266"><nobr><span class="ft3">df</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:282"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:264"><nobr><span class="ft3">df</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:240"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:195"><nobr><span class="ft3">t</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:182"><nobr><span class="ft3">w</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:294"><nobr><span class="ft3">+</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:253"><nobr><span class="ft3">-</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:208"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:399;left:325"><nobr><span class="ft3">                              (3) </span></nobr></DIV>
<DIV style="position:absolute;top:437;left:95"><nobr><span class="ft3">Here k</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:131"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:136"><nobr><span class="ft3"> and b are parameters. tf(t,D) is term frequency, means </span></nobr></DIV>
<DIV style="position:absolute;top:453;left:81"><nobr><span class="ft8">the frequency of term t appears in document D. df(t) is document <br>frequency, means the frequency of document which contains term <br>t within all documents. |D| stands for the length of document D. </span></nobr></DIV>
<DIV style="position:absolute;top:505;left:94"><nobr><span class="ft3">The basic idea of this IR model can be explained as, if the </span></nobr></DIV>
<DIV style="position:absolute;top:521;left:81"><nobr><span class="ft8">query term appears in document more frequently (higher tf), and <br>the query term is more unique that less documents contain it <br>(lower df), the query will be more relevant to the document. </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:81"><nobr><span class="ft1">2.2  Index and Rank the Video Information </span></nobr></DIV>
<DIV style="position:absolute;top:603;left:81"><nobr><span class="ft8">The video data used in our experimental system are from MSN <br>Video (http://video.msn.com/), which contains 7230 videos.  </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:81"><nobr><span class="ft14"><i>2.2.1  Metadata of the video </i></span></nobr></DIV>
<DIV style="position:absolute;top:669;left:81"><nobr><span class="ft8">Because the videos in our data set are made by professional <br>content provider, there is rich meta information that describes <br>each entire video with brief text. Each video has the following <br>metadata fields: headline, caption, keywords, source, video URL, <br>thumbnail image URL, related web page anchor text, page URL, <br>etc. Besides these types of textual information, some format <br>information of the video, such as video length, frame size, bit rate, <br>and creation date are also extracted. Some selected information <br>fields of video metadata are listed in Table 1. </span></nobr></DIV>
<DIV style="position:absolute;top:820;left:191"><nobr><span class="ft3">Table 1. Video metadata </span></nobr></DIV>
<DIV style="position:absolute;top:843;left:91"><nobr><span class="ft3">Field </span></nobr></DIV>
<DIV style="position:absolute;top:844;left:176"><nobr><span class="ft5">Example Value </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:91"><nobr><span class="ft3">Headline </span></nobr></DIV>
<DIV style="position:absolute;top:860;left:176"><nobr><span class="ft5">Discovery launches </span></nobr></DIV>
<DIV style="position:absolute;top:876;left:91"><nobr><span class="ft3">Caption </span></nobr></DIV>
<DIV style="position:absolute;top:877;left:176"><nobr><span class="ft5">July 26: Watch the entire launch of space shuttle D... </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:91"><nobr><span class="ft3">Source </span></nobr></DIV>
<DIV style="position:absolute;top:893;left:176"><nobr><span class="ft5">MSNBC </span></nobr></DIV>
<DIV style="position:absolute;top:909;left:91"><nobr><span class="ft3">Keywords </span></nobr></DIV>
<DIV style="position:absolute;top:910;left:176"><nobr><span class="ft5">Technology, science, Space, Partner Codes ... </span></nobr></DIV>
<DIV style="position:absolute;top:925;left:91"><nobr><span class="ft3">Video URL </span></nobr></DIV>
<DIV style="position:absolute;top:927;left:176"><nobr><span class="ft5">http://www.msnbc.msn.com/default.cdnx/id/871313... </span></nobr></DIV>
<DIV style="position:absolute;top:942;left:91"><nobr><span class="ft3">Link anchor </span></nobr></DIV>
<DIV style="position:absolute;top:943;left:176"><nobr><span class="ft5">MSNBC.com's Technology and Science front </span></nobr></DIV>
<DIV style="position:absolute;top:958;left:91"><nobr><span class="ft3">Link URL </span></nobr></DIV>
<DIV style="position:absolute;top:960;left:176"><nobr><span class="ft5">http://www.msnbc.msn.com/id/3032118 </span></nobr></DIV>
<DIV style="position:absolute;top:975;left:91"><nobr><span class="ft3">date </span></nobr></DIV>
<DIV style="position:absolute;top:976;left:176"><nobr><span class="ft5">7/26/2005 4:40:48 PM </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:91"><nobr><span class="ft3">video length </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:176"><nobr><span class="ft5">609.72 seconds </span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:91"><nobr><span class="ft3">Frame size </span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:176"><nobr><span class="ft5">320 x 240 </span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:91"><nobr><span class="ft3">Bit rate </span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:176"><nobr><span class="ft5">180 Kbps </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:488"><nobr><span class="ft3">For the videos contained in general web pages, some attributes </span></nobr></DIV>
<DIV style="position:absolute;top:124;left:475"><nobr><span class="ft8">mentioned above may not be obtained directly, but the <br>surrounding texts, URL, filename can be extracted as the metadata <br>fields of the video. </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:488"><nobr><span class="ft3">These information fields correspond to document D in Section </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:475"><nobr><span class="ft3">2.1. Different fields can be represented by different type of D (D</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:834"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:837"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:475"><nobr><span class="ft8">in Equation 4). The overall relevance can be calculated by the <br>weighted summation of the relevance of all fields. The weight of <br>the fields (DW</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:564"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:567"><nobr><span class="ft3"> in Equation 4) can be determined by their </span></nobr></DIV>
<DIV style="position:absolute;top:260;left:475"><nobr><span class="ft3">importance, significance, and representativeness to the video. </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:632"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:286;left:673"><nobr><span class="ft3"></span></nobr></DIV>
<DIV style="position:absolute;top:286;left:621"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:719"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:705"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:687"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:613"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:599"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:565"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:709"><nobr><span class="ft3">Q</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:692"><nobr><span class="ft3">D</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:678"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:648"><nobr><span class="ft3">DW</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:603"><nobr><span class="ft3">Q</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:568"><nobr><span class="ft3">Video</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:556"><nobr><span class="ft3">R</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:701"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:668"><nobr><span class="ft12">i</span></nobr></DIV>
<DIV style="position:absolute;top:285;left:725"><nobr><span class="ft3">                            (4) </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:488"><nobr><span class="ft3">In our system, four major information fields from video </span></nobr></DIV>
<DIV style="position:absolute;top:333;left:475"><nobr><span class="ft8">metadata are selected to be indexed: headline, caption, keywords, <br>and source. Headline is a highly representative description of the <br>video content. Keywords are also good recapitulative terms. For <br>these two fields, higher weights are set. Caption is a more general <br>and detail depicts for the video; Source provided a higher level <br>and less relevant information, they will be set lower weights for <br>ranking. Table 2 gives out the weights of fields in our <br>experimental system. </span></nobr></DIV>
<DIV style="position:absolute;top:469;left:536"><nobr><span class="ft3">Table 2. Weights for relevance evaluation </span></nobr></DIV>
<DIV style="position:absolute;top:491;left:595"><nobr><span class="ft3">Fields weight </span></nobr></DIV>
<DIV style="position:absolute;top:507;left:588"><nobr><span class="ft3">Headline </span></nobr></DIV>
<DIV style="position:absolute;top:508;left:696"><nobr><span class="ft5">10 </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:584"><nobr><span class="ft3">Keywords </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:696"><nobr><span class="ft5">10 </span></nobr></DIV>
<DIV style="position:absolute;top:539;left:591"><nobr><span class="ft3">Caption </span></nobr></DIV>
<DIV style="position:absolute;top:541;left:698"><nobr><span class="ft5">5 </span></nobr></DIV>
<DIV style="position:absolute;top:556;left:595"><nobr><span class="ft3">source </span></nobr></DIV>
<DIV style="position:absolute;top:557;left:698"><nobr><span class="ft5">1 </span></nobr></DIV>
<DIV style="position:absolute;top:581;left:475"><nobr><span class="ft14"><i>2.2.2  Content information of the video shots </i></span></nobr></DIV>
<DIV style="position:absolute;top:600;left:475"><nobr><span class="ft8">There is plenty of information in the visual/audio content of the <br>video sequence, which can not be sufficiently presented by the <br>aforementioned textual video metadata. We can build a set of <br>models that can be applied to automatically detect a corresponding <br>set of concepts such that each video shot can be annotated with a <br>detection confidence score for each concept. Successful concept <br>modeling and detection approaches have been introduced in <br>TRECVID, relying predominantly on visual/aural analysis and <br>statistical machine learning methods [4]. The LSCOM-lite <br>Lexicon [5] designed for the TRECVID 2005 Benchmark consists <br>of more than 40 concepts spread across multiple concept-types <br>such as object, events, site etc. Though the size of the lexicon is <br>still far from practical application for general Web-based video <br>search, this semantic information is promising to enable real <br>content-based video search, and therefore it is applied in our <br>ranking system.  </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:488"><nobr><span class="ft3">Besides visual contents, information from audio channel, </span></nobr></DIV>
<DIV style="position:absolute;top:875;left:475"><nobr><span class="ft8">especially the speech, is also very useful for searching videos.. In <br>our experimental system, we use Microsoft speech recognition <br>engine (with a large vocabulary). This engine gives recognized <br>words with a start timestamp, length, and a recognition confidence <br>value, which are very useful for later indexing and ranking. The <br>speech texts are allotted and assigned into video shots, according <br>to the timestamp of words and video shots. </span></nobr></DIV>
<DIV style="position:absolute;top:995;left:488"><nobr><span class="ft3">The content information is associated with individual video shot, </span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:475"><nobr><span class="ft8">which consist of semantic keywords (with corresponding detection <br>confidences), and speech words (with recognition confidences). <br>The confidences of words will act as weights of term frequencies <br>tf to calculate the relevance in Equation (2). </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">628</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft15{font-size:11px;font-family:Times;color:#999999;}
	.ft16{font-size:16px;line-height:21px;font-family:Times;color:#000000;}
	.ft17{font-size:11px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="197003.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft16">2.3  Integrated Ranking with Metadata and <br>Content Information </span></nobr></DIV>
<DIV style="position:absolute;top:151;left:81"><nobr><span class="ft8">To combine metadata and content to rank the videos, we index the <br>videos by metadata and index the video shots by content <br>information separately, and then integrated these two rank lists, <br>named  video list and shot list, to form a final ranking. The <br>integrated ranking returns search result by video, but taking all the <br>relevance shots within this video into consideration. </span></nobr></DIV>
<DIV style="position:absolute;top:255;left:95"><nobr><span class="ft3">For  video list, each item is a video. Let item</span></nobr></DIV>
<DIV style="position:absolute;top:261;left:348"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:255;left:355"><nobr><span class="ft3">.vid denotes the </span></nobr></DIV>
<DIV style="position:absolute;top:271;left:81"><nobr><span class="ft3">video ID of the i</span></nobr></DIV>
<DIV style="position:absolute;top:269;left:175"><nobr><span class="ft13">th</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:182"><nobr><span class="ft3"> item, item</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:242"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:248"><nobr><span class="ft3">.score denotes the ranking score of </span></nobr></DIV>
<DIV style="position:absolute;top:286;left:81"><nobr><span class="ft3">the  i</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:107"><nobr><span class="ft13">th</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:114"><nobr><span class="ft3"> item. For shot list, each item is a shot from a video. Let </span></nobr></DIV>
<DIV style="position:absolute;top:302;left:81"><nobr><span class="ft3">item</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:104"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:110"><nobr><span class="ft3">.vid,  item</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:163"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:169"><nobr><span class="ft3">.sid,  item</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:221"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:227"><nobr><span class="ft3">.score donote the video ID which the </span></nobr></DIV>
<DIV style="position:absolute;top:318;left:81"><nobr><span class="ft8">shot belong to, the shot ID within the video, and the ranking score <br>of the i</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:119"><nobr><span class="ft13">th</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:126"><nobr><span class="ft3"> item respectively. </span></nobr></DIV>
<DIV style="position:absolute;top:359;left:94"><nobr><span class="ft3">The integrating process is presented in Algorithm 1. The basic </span></nobr></DIV>
<DIV style="position:absolute;top:375;left:81"><nobr><span class="ft8">idea is that, all the ranking score of the relevance shots within the <br>video are accumulated to the ranking score of the video, with <br>corresponding weights. The relevant shots in the video will be <br>highlight when displaying the video as search result. </span></nobr></DIV>
<DIV style="position:absolute;top:448;left:89"><nobr><span class="ft3">new a integrated result list (item denotes as item</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:350"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:356"><nobr><span class="ft3">) </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:89"><nobr><span class="ft3">for each item</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:160"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:167"><nobr><span class="ft3"> in video list{ </span></nobr></DIV>
<DIV style="position:absolute;top:479;left:103"><nobr><span class="ft3">new item</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:152"><nobr><span class="ft13">ic</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:158"><nobr><span class="ft3">; </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:103"><nobr><span class="ft3">item</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:126"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:132"><nobr><span class="ft3">.vid = item</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:189"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:195"><nobr><span class="ft3">.vid; </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:103"><nobr><span class="ft3">item</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:126"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:132"><nobr><span class="ft3"> score = item</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:202"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:209"><nobr><span class="ft3">. score * Weight_v; </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:103"><nobr><span class="ft3">for each item item</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:201"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:207"><nobr><span class="ft3"> in shot list{ </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:116"><nobr><span class="ft3">if(item</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:152"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:159"><nobr><span class="ft3">.vid == item</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:224"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:543;left:230"><nobr><span class="ft3">.vid) { </span></nobr></DIV>
<DIV style="position:absolute;top:559;left:130"><nobr><span class="ft3">item</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:153"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:159"><nobr><span class="ft3"> addshot(item</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:232"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:239"><nobr><span class="ft3">.sid); </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:130"><nobr><span class="ft3">item</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:153"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:159"><nobr><span class="ft3"> score += item</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:237"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:574;left:243"><nobr><span class="ft3">. score * Weight_s; </span></nobr></DIV>
<DIV style="position:absolute;top:590;left:130"><nobr><span class="ft3">remove item</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:197"><nobr><span class="ft13">is</span></nobr></DIV>
<DIV style="position:absolute;top:590;left:203"><nobr><span class="ft3"> from shot list </span></nobr></DIV>
<DIV style="position:absolute;top:606;left:116"><nobr><span class="ft3">} </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:103"><nobr><span class="ft8">} <br>remove item</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:170"><nobr><span class="ft13">iv</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:176"><nobr><span class="ft3"> from video list </span></nobr></DIV>
<DIV style="position:absolute;top:654;left:103"><nobr><span class="ft3">add item</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:149"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:155"><nobr><span class="ft3"> to integrated list </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:89"><nobr><span class="ft8">} <br>add the remaining video list and shot list into the integrated list <br>sort the integrated list by item</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:251"><nobr><span class="ft13">iI</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:256"><nobr><span class="ft3">.score. </span></nobr></DIV>
<DIV style="position:absolute;top:726;left:89"><nobr><span class="ft15">// Weight_v and Weight_s are weights for score accumulating </span></nobr></DIV>
<DIV style="position:absolute;top:752;left:138"><nobr><span class="ft3">Algorithm 1. Generate integrated rank list. </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:81"><nobr><span class="ft1">3.  LEARNING BASED RANKING </span></nobr></DIV>
<DIV style="position:absolute;top:822;left:81"><nobr><span class="ft1">3.1  Extracted Features </span></nobr></DIV>
<DIV style="position:absolute;top:843;left:81"><nobr><span class="ft8">IR-model based ranking just consider some basic features such as <br>term frequency tf, document frequency df, and document length, <br>etc. In the learning based approach, more features are extracted <br>from the query, metadata, and content information. To be clear, <br>suppose the query contains three terms "a b c", we compute the <br>following features from each document field:  </span></nobr></DIV>
<DIV style="position:absolute;top:947;left:81"><nobr><span class="ft8">Ordered match: the frequency that both "a" and "b" appeared in <br>the indexed text, and "b" appears after "a". </span></nobr></DIV>
<DIV style="position:absolute;top:988;left:81"><nobr><span class="ft8">Partly exact match: the frequency that "a b" or "b c" appeared in <br>the indexed text. </span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:81"><nobr><span class="ft8">Exact match: the frequency that "a b c" appeared in the indexed <br>text. </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:475"><nobr><span class="ft3">Query length: number of query terms  </span></nobr></DIV>
<DIV style="position:absolute;top:129;left:488"><nobr><span class="ft3">For the content information, each word has a confidence value, </span></nobr></DIV>
<DIV style="position:absolute;top:145;left:475"><nobr><span class="ft8">we also consider: <br>Weighted tf: Term frequency with confidence weighted, <br>High confident match: query term match with words with high <br>confidence. <br>High confident words: words with high confidence in the <br>indexed text. </span></nobr></DIV>
<DIV style="position:absolute;top:259;left:488"><nobr><span class="ft3">Some non-textual, query-independent features, such as shot </span></nobr></DIV>
<DIV style="position:absolute;top:275;left:475"><nobr><span class="ft8">length, video length, frame size, bit rate, etc, are also taken into <br>account. </span></nobr></DIV>
<DIV style="position:absolute;top:311;left:488"><nobr><span class="ft3">By counting in the combinations of several document fields and </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:475"><nobr><span class="ft8">query terms (or part of query), we have about 50 dimensional <br>features in total for a query and search result to form a sample.. <br>The GroundTruth of sample is the relevance judgments between a <br>query and a result, which is collected by a user labeling system <br>introduced in the next section.  </span></nobr></DIV>
<DIV style="position:absolute;top:420;left:475"><nobr><span class="ft1">3.2  Neutral Network based Ranking </span></nobr></DIV>
<DIV style="position:absolute;top:440;left:475"><nobr><span class="ft8">Traditionally the learning algorithms are used for classification <br>problems. For ranking problems, one possible way is organize the <br>samples by pairs. Pair sample (x</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:653"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:658"><nobr><span class="ft3">, x</span></nobr></DIV>
<DIV style="position:absolute;top:478;left:672"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:472;left:676"><nobr><span class="ft3">) are considered as a positive </span></nobr></DIV>
<DIV style="position:absolute;top:488;left:475"><nobr><span class="ft3">sample, if x</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:544"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:548"><nobr><span class="ft3"> are ranked ahead of x</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:682"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:686"><nobr><span class="ft3">, and vice versa. The loss </span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft8">function is also formulated in pair wise. RankNET [6] is used in <br>our implementation to train the ranking model and to validate the <br>performance.  </span></nobr></DIV>
<DIV style="position:absolute;top:560;left:488"><nobr><span class="ft3">About half of the labeled data are used in training, and the </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:475"><nobr><span class="ft3">second half are used for validation. </span></nobr></DIV>
<DIV style="position:absolute;top:611;left:475"><nobr><span class="ft1">4.  EVALUATIONS </span></nobr></DIV>
<DIV style="position:absolute;top:641;left:475"><nobr><span class="ft1">4.1  Data Preparation </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:475"><nobr><span class="ft8">To evaluate the ranking performance of our proposed methods, we <br>developed a user labeling tool to collect some query-result <br>relevance judgments. </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:488"><nobr><span class="ft3">The video data set we used in our experiment includes news </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:475"><nobr><span class="ft8">video, TV programs, movie trailers, advertisements, etc. <br>According to the characteristics of the content of these videos, we <br>selected some news related queries, such as hot events, hot place, <br>and hot person names, to evaluate the ranking performance. For <br>each query, we use the IR-model based ranking describe in <br>Section 2 to generate a result list, and randomly select some <br>results form the list to label. Considering the labeling workload, <br>for each labeler and each query, 9 results are select from the list. <br>To make the selected query-result samples have a good uniformity <br>on distribution, 3 results are randomly selected from the first 1/3 <br>part of the list, 3 are from the second 1/3 part, and the other 3 are <br>from the last 1/3 part. The order of these 9 selected results is <br>shuffled and then provided to users to do relevance labeling. </span></nobr></DIV>
<DIV style="position:absolute;top:943;left:488"><nobr><span class="ft3">In the labeling tool, for a query and a result, user can see all the </span></nobr></DIV>
<DIV style="position:absolute;top:958;left:475"><nobr><span class="ft8">information of the result, including the file format information <br>(frame size, bit rate, video length, etc), description (headline, <br>caption, keywords), video thumbnail, video (in a video player), <br>thumbnails of video shots, the speech text of the relevant shots. <br>The words matched with query terms are highlighted. See Figure <br>1. If there are relevant shots in the result, the thumbnails of them <br>are displayed with doubled size. The shot number, time </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">629</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft18{font-size:11px;font-family:Times;color:#ff9900;}
	.ft19{font-size:11px;font-family:Times;color:#ff0000;}
-->
</STYLE>
<IMG width="918" height="1188" src="197004.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft8">information, and the speech are also shown in the interface. Users <br>are asked to read the displayed information, browse the <br>thumbnails, and play the video and shots (a tool button is provided <br>to play from one shot) to give a relevance judgment from 1, 2, 3, 4, <br>and 5, which represent bad,  fair,  good,  excellent, and perfect, <br>respectively. </span></nobr></DIV>
<DIV style="position:absolute;top:540;left:419"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:558;left:168"><nobr><span class="ft3">Figure 1. Relevance labeling tool </span></nobr></DIV>
<DIV style="position:absolute;top:580;left:81"><nobr><span class="ft8">In our experiment, ten users are invited to do labeling, and about <br>2,000 relevance judgments of query-result samples are collected.  </span></nobr></DIV>
<DIV style="position:absolute;top:627;left:81"><nobr><span class="ft1">4.2  Precision Performance of Ranking </span></nobr></DIV>
<DIV style="position:absolute;top:648;left:81"><nobr><span class="ft17">We have conducted a comparison between the 4 approaches listed <br>below: <br>MR: Ranking only based on video metadata (Section 2.2.1). <br>CR: Ranking only by content information (Section 2.2.2). <br>RI: Integrated Ranking described in Section 2.3 <br>RN: RankNET based ranking described in Section 3.2. </span></nobr></DIV>
<DIV style="position:absolute;top:766;left:94"><nobr><span class="ft3">The precision in top N of the rank lists of all the labeled queries </span></nobr></DIV>
<DIV style="position:absolute;top:782;left:81"><nobr><span class="ft3">is used to evaluate the performance of ranking method. </span></nobr></DIV>
<DIV style="position:absolute;top:821;left:354"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:332"><nobr><span class="ft3">top</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:318"><nobr><span class="ft3">in</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:274"><nobr><span class="ft3">results</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:226"><nobr><span class="ft3">labeled</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:193"><nobr><span class="ft3">total</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:362"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:344"><nobr><span class="ft3">top</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:329"><nobr><span class="ft3">in</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:285"><nobr><span class="ft3">results</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:236"><nobr><span class="ft3">labeled</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:184"><nobr><span class="ft3">relevant</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:158"><nobr><span class="ft3">N</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:90"><nobr><span class="ft3">Precision</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:172"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:143"><nobr><span class="ft3">@</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:376"><nobr><span class="ft3">               (5) </span></nobr></DIV>
<DIV style="position:absolute;top:845;left:95"><nobr><span class="ft3">In our implementation, the judgment Perfect or Excellent are </span></nobr></DIV>
<DIV style="position:absolute;top:861;left:81"><nobr><span class="ft8">considered as relevant results, while other judgments are treated as <br>irrelevant results.  The Presicion@N (N=1 to 5) of the 4 ranking <br>methods are shown in Table 3.  </span></nobr></DIV>
<DIV style="position:absolute;top:913;left:95"><nobr><span class="ft3">From the results, we can see that: </span></nobr></DIV>
<DIV style="position:absolute;top:933;left:81"><nobr><span class="ft3">1)  Precisions of MR are very low. Only using video metadata </span></nobr></DIV>
<DIV style="position:absolute;top:949;left:108"><nobr><span class="ft8">will result in a poor performance, since details of the video <br>content are ignored. The content information is more <br>effective to search and rank video than metadata, as <br>precisions of CR are higher than that of MR. </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:81"><nobr><span class="ft3">2)  Precisions of RI are much higher than that of MR and CR. </span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:108"><nobr><span class="ft8">By combining video metadata and content information, the <br>performance is significantly improved and reaches an </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:502"><nobr><span class="ft8">acceptable level, which shows that content-based relevance <br>ranking is a promising approach.</span></nobr></DIV>
<DIV style="position:absolute;top:124;left:680"><nobr><span class="ft18"> </span></nobr></DIV>
<DIV style="position:absolute;top:149;left:475"><nobr><span class="ft3">3)  RN has a good performance, even better than RI. Comparing </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:502"><nobr><span class="ft8">to IR-model based ranking, more features are included to <br>learning the relevance. The result implies that the learning <br>method can organize the information for ranking in a more <br>effective way.  </span></nobr></DIV>
<DIV style="position:absolute;top:238;left:527"><nobr><span class="ft3">Table 3. Precision of the ranking approaches </span></nobr></DIV>
<DIV style="position:absolute;top:260;left:482"><nobr><span class="ft3">Precision@?</span></nobr></DIV>
<DIV style="position:absolute;top:260;left:582"><nobr><span class="ft3">1 2 3 4 5 </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:508"><nobr><span class="ft3">MR </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:570"><nobr><span class="ft3">0.305 0.326 0.299 0.259 0.228 </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:509"><nobr><span class="ft3">CR </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:570"><nobr><span class="ft3">0.544 0.526 0.571 0.550 0.522 </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:511"><nobr><span class="ft3">RI </span></nobr></DIV>
<DIV style="position:absolute;top:308;left:570"><nobr><span class="ft3">0.796 0.727 0.684 0.634 0.606 </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:509"><nobr><span class="ft3">RN </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:570"><nobr><span class="ft3">0.805 0.746 0.763 0.717 0.669 </span></nobr></DIV>
<DIV style="position:absolute;top:359;left:475"><nobr><span class="ft1">5.  DISCUSSIONS AND CONCLUSION </span></nobr></DIV>
<DIV style="position:absolute;top:380;left:475"><nobr><span class="ft8">We have presented a novel content-based approach to rank video <br>search results</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:551"><nobr><span class="ft19">.</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:554"><nobr><span class="ft3"> In addition to the video metadata, more detailed </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:475"><nobr><span class="ft8">content information in the video is used to improve the relevance <br>ranking of video search results. The videos are segmented into <br>shots, which can carry rich content information such as semantic <br>concept keywords and speech. With the video metadata and <br>content information, we proposed an IR-model based ranking <br>method and a learning-based ranking method. Evaluation of the <br>top ranked results shows that the proposed ranking methods have <br>significantly improved performance comparing to the approach <br>use video metadata only, which is frequently used in existing web <br>video search engines. </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:488"><nobr><span class="ft3">In future work, more types of content information can be </span></nobr></DIV>
<DIV style="position:absolute;top:592;left:475"><nobr><span class="ft8">integrated into our ranking scheme, such as content-based quality <br>metric, user comments and rating for videos shared in web <br>communities. Moreover, how to define effective semantic <br>concepts, i.e., video semantic ontology, that facilitate video <br>searching and ranking is also a challenging problem., which is <br>also one of our future works. </span></nobr></DIV>
<DIV style="position:absolute;top:696;left:475"><nobr><span class="ft1">6.  REFERENCES </span></nobr></DIV>
<DIV style="position:absolute;top:717;left:475"><nobr><span class="ft3">[1]  Hong-Jiang Zhang, A. Kankanhalli, and S. Smoliar, </span></nobr></DIV>
<DIV style="position:absolute;top:733;left:502"><nobr><span class="ft8">"Automatic Partitioning of Full-motion Video," A Guided <br>Tour of Multimedia Systems and Applications, IEEE <br>Computer Society Press, 1995. </span></nobr></DIV>
<DIV style="position:absolute;top:785;left:475"><nobr><span class="ft17">[2]  http://www-nlpir.nist.gov/projects/trecvid <br>[3]  S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at </span></nobr></DIV>
<DIV style="position:absolute;top:821;left:502"><nobr><span class="ft8">TREC­7: automatic ad hoc, filtering, VLC and filtering <br>tracks. In Proceedings of TREC'99. </span></nobr></DIV>
<DIV style="position:absolute;top:857;left:475"><nobr><span class="ft3">[4]  M. Naphade, J.R. Smith, F. Souvannavong, "On the </span></nobr></DIV>
<DIV style="position:absolute;top:873;left:502"><nobr><span class="ft8">Detection of Semantic Concepts at TRECVID," ACM <br>Multimedia, ACM Press, New York, NY, pp. 660-667, Oct. <br>10-16, 2004 </span></nobr></DIV>
<DIV style="position:absolute;top:925;left:475"><nobr><span class="ft3">[5]  M. Naphade, L. Kennedy, J.R. Kender, S.F. Chang, J.R. </span></nobr></DIV>
<DIV style="position:absolute;top:941;left:502"><nobr><span class="ft8">Smith, P. Over, A. Hauptmann, "LSCOM-lite: A Light Scale <br>Concept Ontology for Multimedia Understanding for <br>TRECVID 2005," IBM Research Tech. Report, RC23612 <br>(W0505-104), May, 2005. </span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:475"><nobr><span class="ft3">[6]  Chris Burges, et.al, "Learning to Rank using Gradient </span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:502"><nobr><span class="ft8">Descent", ICML 2005, Bonn, Germany, pp.89-96, August 7-<br>11, 2005. </span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:656"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">630</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
