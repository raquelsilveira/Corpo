<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\123</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-11-15T08:36:45+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Times;color:#000000;}
	.ft1{font-size:11px;font-family:Times;color:#000000;}
	.ft2{font-size:15px;font-family:Times;color:#000000;}
	.ft3{font-size:12px;font-family:Times;color:#000000;}
	.ft4{font-size:9px;font-family:Times;color:#000000;}
	.ft5{font-size:16px;font-family:Courier;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft8{font-size:11px;line-height:22px;font-family:Times;color:#000000;}
	.ft9{font-size:11px;line-height:13px;font-family:Times;color:#000000;}
	.ft10{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123001.png" alt="background image">
<DIV style="position:absolute;top:102;left:81"><nobr><span class="ft0"><b>Learning Concepts from Large Scale Imbalanced Data Sets</b></span></nobr></DIV>
<DIV style="position:absolute;top:132;left:242"><nobr><span class="ft0"><b>Using Support Cluster Machines</b></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:658"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:302"><nobr><span class="ft2">Jinhui Yuan, Jianmin Li and Bo Zhang</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:260"><nobr><span class="ft3">State Key Laboratory of Intelligent Technology and Systems</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:292"><nobr><span class="ft3">Department of Computer Science and Technology</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:296"><nobr><span class="ft3">Tsinghua University, Beijing, 100084, P. R. China</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:322"><nobr><span class="ft2">yuan-jh03@mails.tsinghua.edu.cn</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:298"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:305"><nobr><span class="ft2">lijianmin, dcszb</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:427"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:434"><nobr><span class="ft2">@mail.tsinghua.edu.cn</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:81"><nobr><span class="ft2">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:81"><nobr><span class="ft7">This paper considers the problem of using Support Vector<br>Machines (SVMs) to learn concepts from large scale imbal-<br>anced data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:195"><nobr><span class="ft1">The objective of this paper is twofold.</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:81"><nobr><span class="ft7">Firstly, we investigate the effects of large scale and im-<br>balance on SVMs.</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:210"><nobr><span class="ft1">We highlight the role of linear non-</span></nobr></DIV>
<DIV style="position:absolute;top:439;left:81"><nobr><span class="ft7">separability in this problem. Secondly, we develop a both<br>practical and theoretical guaranteed meta-algorithm to han-<br>dle the trouble of scale and imbalance.</span></nobr></DIV>
<DIV style="position:absolute;top:471;left:338"><nobr><span class="ft1">The approach is</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:81"><nobr><span class="ft7">named Support Cluster Machines (SCMs). It incorporates<br>the informative and the representative under-sampling mech-<br>anisms to speedup the training procedure. The SCMs differs<br>from the previous similar ideas in two ways, (a) the theo-<br>retical foundation has been provided, and (b) the clustering<br>is performed in the feature space rather than in the input<br>space. The theoretical analysis not only provides justifica-<br>tion, but also guides the technical choices of the proposed<br>approach. Finally, experiments on both the synthetic and<br>the TRECVID data are carried out. The results support<br>the previous analysis and show that the SCMs are efficient<br>and effective while dealing with large scale imbalanced data<br>sets.</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:81"><nobr><span class="ft2">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:81"><nobr><span class="ft7">H.3.1 [Information Storage and Retrieval]: Content<br>Analysis and Indexing--Abstracting methods,Indexing meth-<br>ods; I.5.2 [Pattern Recognition]: Design Methodology--<br>Classifier design and evaluation</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:81"><nobr><span class="ft2">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:81"><nobr><span class="ft1">Algorithms, Theory, Experimentation</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:81"><nobr><span class="ft9">Supported by National Natural Science Foundation of<br>China (60135010, 60321002) and Chinese National Key<br>Foundation Research &amp; Development Plan (2004CB318108).</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:81"><nobr><span class="ft10">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that copies<br>bear this notice and the full citation on the first page. To copy otherwise, to<br>republish, to post on servers or to redistribute to lists, requires prior specific<br>permission and/or a fee.<br>MM'06, October 23­27, 2006, Santa Barbara, California, USA.<br>Copyright 2006 ACM 1-59593-447-2/06/0010 ...</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:315"><nobr><span class="ft1">$</span></nobr></DIV>
<DIV style="position:absolute;top:1064;left:322"><nobr><span class="ft4">5.00.</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:475"><nobr><span class="ft2">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:475"><nobr><span class="ft7">Support Vector Machines, Kernel k-means, Clustering, Con-<br>cept Modelling, Large Scale, Imbalance</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:475"><nobr><span class="ft2">1.</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:507"><nobr><span class="ft2">INTRODUCTION</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:489"><nobr><span class="ft1">In the context of concept modelling, this paper considers</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:475"><nobr><span class="ft7">the problem of how to make full use of the large scale an-<br>notated data sets. In particular, we study the behaviors of<br>Support Vector Machines (SVMs) on large scale imbalanced<br>data sets, not only because its solid theoretical foundations<br>but also for its empirical success in various applications.</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:475"><nobr><span class="ft2">1.1</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:516"><nobr><span class="ft2">Motivation</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:489"><nobr><span class="ft1">Bridging the semantic gap has been becoming the most</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:475"><nobr><span class="ft7">challenging problem of Multimedia Information Retrieval<br>(MIR). Currently, there are mainly two types of methods<br>to bridge the gap [8]. The first one is relevance feedback<br>which attempts to capture the user's precise needs through<br>iterative feedback and query refinement. Another promis-<br>ing direction is concept modelling. As noted by Hauptmann<br>[14], this splits the semantic gap between low level features<br>and user information needs into two, hopefully smaller gaps:<br>(a) mapping the low-level features into the intermediate se-<br>mantic concepts and (b) mapping these concepts into user<br>needs. The automated image annotation methods for CBIR<br>and the high level feature extraction methods in CBVR are<br>all the efforts to model the first mapping. Of these methods,<br>supervised learning is one of the most successful ones. An<br>early difficulty of supervised learning is the lack of anno-<br>tated training data. Currently, however, it seems no longer<br>a problem. This is due to both the techniques developed to<br>leverage surrounding texts of web images and the large scale<br>collaborative annotation. Actually, there is an underway ef-<br>fort named Large Scale Concept Ontology for Multimedia<br>Understanding (LSCOM), which intends to annotate 1000<br>concepts in broadcast news video [13]. The initial fruits of<br>this effort have been harvested in the practice of TRECVID<br>hosted by National Institute of Standards and Technology<br>(NIST) [1]. In TRECVID 2005, 39 concepts are annotated<br>by multiple participants through web collaboration, and ten<br>of them are used in the evaluation.</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:489"><nobr><span class="ft1">The available large amount of annotated data is undoubt-</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:475"><nobr><span class="ft7">edly beneficial to supervised learning. However, it also brings<br>out a novel challenge, that is, how to make full use of the<br>data while training the classifiers. On the one hand, the an-<br>notated data sets are usually in rather large scale. The de-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">441</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft11{font-size:6px;font-family:Times;color:#000000;}
	.ft12{font-size:-1px;font-family:Times;color:#000000;}
	.ft13{font-size:5px;font-family:Times;color:#000000;}
	.ft14{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
	.ft15{font-size:11px;line-height:12px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123002.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft7">velopment set of TRECVID 2005 includes 74523 keyframes.<br>The data set of LSCOM with over 1000 annotated concepts<br>might be even larger. With all the data, the training of<br>SVMs will be rather slow. On the other hand, each con-<br>cept will be the minority class under one-against-all strat-<br>egy. Only a small portion of the data belong to the concept,<br>while all the others are not (In our case, the minority class<br>always refers to the positive class). The ratio of the positive<br>examples and the negative ones is typically below 1 : 100<br>in TRECVID data. These novel challenges have spurred<br>great interest in the communities of data mining and ma-<br>chine learning[2, 6, 21, 22, 29]. Our first motivation is to<br>investigate the effects of large scale and imbalance on SVMs.<br>This is critical for correct technical choices and development.<br>The second objective of this paper is to provide a practical<br>as well as theoretical guaranteed approach to addressing the<br>problem.</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:81"><nobr><span class="ft2">1.2</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:121"><nobr><span class="ft2">Our Results</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:94"><nobr><span class="ft1">The major contribution of this paper can be summarized</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:81"><nobr><span class="ft1">as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:97"><nobr><span class="ft1">1. We investigate the effects of large scale and imbal-</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:114"><nobr><span class="ft7">ance on SVMs and highlight the role of linear non-<br>separability of the data sets. We find that SVMs has<br>no difficulties with linear separable large scale imbal-<br>anced data.</span></nobr></DIV>
<DIV style="position:absolute;top:499;left:97"><nobr><span class="ft1">2. We establish the relations between the SVMs trained</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:114"><nobr><span class="ft7">on the centroids of the clusters and the SVMs obtained<br>on the original data set. We show that the difference<br>between their optimal solutions are bounded by the<br>perturbation of the kernel matrix. We also prove the<br>optimal criteria for approximating the original optimal<br>solutions.</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:97"><nobr><span class="ft1">3. We develop a meta-algorithm named Support Cluster</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:114"><nobr><span class="ft1">Machines (SCMs).</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:240"><nobr><span class="ft1">A fast kernel k-means approach</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:114"><nobr><span class="ft7">has been employed to partition the data in the feature<br>space rather than in the input space.</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:81"><nobr><span class="ft7">Experiments on both the synthetic data and the TRECVID<br>data are carried out. The results support the previous anal-<br>ysis and show that the SCMs are efficient and effective while<br>dealing with large scale imbalanced data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:81"><nobr><span class="ft2">1.3</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:121"><nobr><span class="ft2">Organization</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:94"><nobr><span class="ft1">The structure of this paper is as follows. In Section 2 we</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:81"><nobr><span class="ft7">give a brief review of SVMs and kernel k-means. We dis-<br>cuss the effects of the large scale imbalanced data on SVMs<br>in Section 3. We develop the theoretical foundations and<br>present the detailed SCMs approach in Section 4. In Sec-<br>tion 5 we carry out experiments on both the synthetic and<br>the TRECVID data sets. Finally, we conclude the paper in<br>Section 6.</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:81"><nobr><span class="ft2">2.</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:112"><nobr><span class="ft2">PRELIMINARIES</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:81"><nobr><span class="ft2">2.1</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:121"><nobr><span class="ft2">Support Vector Machines</span></nobr></DIV>
<DIV style="position:absolute;top:987;left:94"><nobr><span class="ft1">Here, we present a sketch introduction to the soft-margin</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:81"><nobr><span class="ft7">SVMs for the convenience of the deduction in Section 4. For<br>a binary classification problem, given a training data set</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:428"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:81"><nobr><span class="ft1">of size n</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:148"><nobr><span class="ft1">D = {(x</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:198"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:203"><nobr><span class="ft1">, y</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:216"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:221"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:226"><nobr><span class="ft1">|x</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:238"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:247"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:272"><nobr><span class="ft11">N</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:282"><nobr><span class="ft1">, y</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:295"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:304"><nobr><span class="ft1"> {1, -1}},</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:475"><nobr><span class="ft1">where x</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:523"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:532"><nobr><span class="ft1">indicates the training vector of the ith sample and</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:475"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:482"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:492"><nobr><span class="ft1">indicates its target value, and i = 1, . . . , n. The classifi-</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:475"><nobr><span class="ft1">cation hyperplane is defined as</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:605"><nobr><span class="ft1">w, (x) + b = 0,</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:475"><nobr><span class="ft1">where (</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:531"><nobr><span class="ft1">·) is a mapping from R</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:673"><nobr><span class="ft11">N</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:688"><nobr><span class="ft1">to a (usually) higher di-</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:475"><nobr><span class="ft1">mension Hilbert space</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:613"><nobr><span class="ft1">H, and ·, · denotes the dot product</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:475"><nobr><span class="ft1">in</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:491"><nobr><span class="ft1">H. Thus, the decision function f(x) is</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:571"><nobr><span class="ft1">f (x) = sign( w, (x) + b).</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:475"><nobr><span class="ft7">The SVMs aims to find the hyperplane with the maximum<br>margin between the two classes, i.e., the optimal hyperplane.<br>This can be obtained by solving the following quadratic op-<br>timization problem</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:564"><nobr><span class="ft1">min</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:563"><nobr><span class="ft11">w,b,</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:617"><nobr><span class="ft14">1<br>2 w</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:651"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:661"><nobr><span class="ft1">+ C</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:694"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:689"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:710"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:333;left:716"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:528"><nobr><span class="ft1">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:616"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:622"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:627"><nobr><span class="ft1">( w, (x</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:680"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:684"><nobr><span class="ft1">) + b)</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:732"><nobr><span class="ft1"> 1 - </span></nobr></DIV>
<DIV style="position:absolute;top:366;left:776"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:816"><nobr><span class="ft1">(1)</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:616"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:386;left:622"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:630"><nobr><span class="ft1"> 0, i = 1, . . . , n.</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:475"><nobr><span class="ft7">With the help of Lagrange multipliers, the dual of the above<br>problem is</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:573"><nobr><span class="ft1">min</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:581"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:623"><nobr><span class="ft1">G() = 1</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:674"><nobr><span class="ft1">2 </span></nobr></DIV>
<DIV style="position:absolute;top:453;left:691"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:700"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:455;left:724"><nobr><span class="ft1">- e</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:745"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:754"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:484;left:536"><nobr><span class="ft1">subject to</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:623"><nobr><span class="ft1">0</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:634"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:486;left:657"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:666"><nobr><span class="ft1"> C, i = 1, . . . , n</span></nobr></DIV>
<DIV style="position:absolute;top:484;left:816"><nobr><span class="ft1">(2)</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:623"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:500;left:632"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:641"><nobr><span class="ft1">y = 0,</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:475"><nobr><span class="ft1">where  is a vector with components </span></nobr></DIV>
<DIV style="position:absolute;top:534;left:722"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:733"><nobr><span class="ft1">that are the La-</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:475"><nobr><span class="ft7">grange multipliers, C is the upper bound, e is a vector of<br>all ones, and Q is an n</span></nobr></DIV>
<DIV style="position:absolute;top:560;left:626"><nobr><span class="ft1">× n positive semi-definite matrix,</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:475"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:486"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:500"><nobr><span class="ft1">= y</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:522"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:527"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:534"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:545"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:569"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:574"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:581;left:609"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:615"><nobr><span class="ft1">) . Since the mapping (</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:770"><nobr><span class="ft1">·) only ap-</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:475"><nobr><span class="ft7">pears in the dot product, therefore, we need not know its<br>explicit form. Instead, we define a kernel K(</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:741"><nobr><span class="ft1">·, ·) to calculate</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:475"><nobr><span class="ft1">the dot product, i.e., K(x</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:634"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:639"><nobr><span class="ft1">, x</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:653"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:659"><nobr><span class="ft1">) = (x</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:714"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:719"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:754"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:760"><nobr><span class="ft1">) . The ma-</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:475"><nobr><span class="ft1">trix K with components K(x</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:656"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:661"><nobr><span class="ft1">, x</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:676"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:681"><nobr><span class="ft1">) is named Gram Matrix</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:475"><nobr><span class="ft1">(or kernel matrix). With kernel K</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:699"><nobr><span class="ft1">·, · , we can implicitly</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:475"><nobr><span class="ft15">map the training data from input space to a feature space<br>H.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:475"><nobr><span class="ft2">2.2</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:515"><nobr><span class="ft2">Kernel</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:572"><nobr><span class="ft1">k</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:580"><nobr><span class="ft2">-means and Graph Partitioning</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:489"><nobr><span class="ft1">Given a set of vectors x</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:638"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:644"><nobr><span class="ft1">, . . . , x</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:683"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:691"><nobr><span class="ft1">, the standard k-means</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft1">algorithm aims to find clusters </span></nobr></DIV>
<DIV style="position:absolute;top:755;left:671"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:677"><nobr><span class="ft1">, . . . , </span></nobr></DIV>
<DIV style="position:absolute;top:756;left:716"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:726"><nobr><span class="ft1">that minimize the</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:475"><nobr><span class="ft1">objective function</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:548"><nobr><span class="ft1">J(</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:562"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:577"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:583"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:802;left:590"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:590"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:609"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:640"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:633"><nobr><span class="ft11">c=1 x</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:662"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:667"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:827;left:681"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:695"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:704"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:712"><nobr><span class="ft1">- m</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:739"><nobr><span class="ft11">c 2</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:757"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:816"><nobr><span class="ft1">(3)</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:475"><nobr><span class="ft1">where</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:516"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:530"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:536"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:543"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:543"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:567"><nobr><span class="ft1">denotes the partitioning of the data set and</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:475"><nobr><span class="ft1">m</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:489"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:500"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:519"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:529"><nobr><span class="ft13">xic</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:562"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:568"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:536"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:546"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:552"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:581"><nobr><span class="ft1">is the centroid of the cluster </span></nobr></DIV>
<DIV style="position:absolute;top:874;left:771"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:777"><nobr><span class="ft1">. Similar</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:475"><nobr><span class="ft7">to the idea of nonlinear SVMs, the k-means can also be<br>performed in the feature space with the help of a nonlinear<br>mapping (</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:547"><nobr><span class="ft1">·), which results in the so-called kernel k-means</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:537"><nobr><span class="ft1">J(</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:552"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:566"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:572"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:956;left:579"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:579"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:599"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:629"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:623"><nobr><span class="ft11">c=1 x</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:651"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:978;left:656"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:980;left:670"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:685"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:709"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:714"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:722"><nobr><span class="ft1">- m</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:749"><nobr><span class="ft11">c 2</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:768"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:816"><nobr><span class="ft1">(4)</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:475"><nobr><span class="ft1">where m</span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:529"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:539"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:556"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:567"><nobr><span class="ft13">xic</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:599"><nobr><span class="ft11">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:618"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:623"><nobr><span class="ft11">)</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:582"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:592"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:598"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:629"><nobr><span class="ft1">. If we expand the Euclidean dis-</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:475"><nobr><span class="ft1">tance (x</span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:543"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:548"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:556"><nobr><span class="ft1">- m</span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:584"><nobr><span class="ft11">c 2</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:607"><nobr><span class="ft1">in the objective function, we can find</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:475"><nobr><span class="ft1">that the image of x</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:595"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:604"><nobr><span class="ft1">only appears in the form of dot prod-</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:475"><nobr><span class="ft1">uct. Thus, given a kernel matrix K with the same meaning</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">442</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft16{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123003.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft7">in SVMs, we can compute the distance between points and<br>centroids without knowing explicit representation of (x</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:424"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:429"><nobr><span class="ft1">).</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:94"><nobr><span class="ft1">Recently, an appealing alternative, i.e., the graph clus-</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:81"><nobr><span class="ft7">tering has attracted great interest. It treats clustering as<br>a graph partition problem. Given a graph G = (</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:388"><nobr><span class="ft1">V, E, A),</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:81"><nobr><span class="ft1">which consists of a set of vertices</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:279"><nobr><span class="ft1">V and a set of edges E such</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:81"><nobr><span class="ft7">that an edge between two vertices represents their similarity.<br>The affinity matrix A is</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:227"><nobr><span class="ft1">|V|×|V| whose entries represent the</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:81"><nobr><span class="ft1">weights of the edges. Let links(</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:281"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:290"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:296"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:302"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:311"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:317"><nobr><span class="ft1">) be the sum of the</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:81"><nobr><span class="ft1">edge weights between the nodes in</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:294"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:302"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:313"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:340"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:348"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:355"><nobr><span class="ft1">, that is</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:172"><nobr><span class="ft1">links(</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:209"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:217"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:224"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:245;left:230"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:238"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:245"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:268"><nobr><span class="ft11">iV</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:287"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:293"><nobr><span class="ft11">,jV</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:316"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:324"><nobr><span class="ft1">A</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:334"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:344"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:81"><nobr><span class="ft7">Ratio association is a type of graph partitioning objective<br>which aims to maximize within-cluster association relative<br>to the size of the cluster</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:141"><nobr><span class="ft1">RAssoc(G) =</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:236"><nobr><span class="ft1">max</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:227"><nobr><span class="ft11">V</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:234"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:240"><nobr><span class="ft11">,...,V</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:264"><nobr><span class="ft13">k</span></nobr></DIV>
<DIV style="position:absolute;top:338;left:280"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:273"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:297"><nobr><span class="ft1">links(</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:334"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:342"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:348"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:354"><nobr><span class="ft1">V</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:363"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:368"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:324"><nobr><span class="ft1">|V</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:337"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:343"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:376"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:422"><nobr><span class="ft1">(5)</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:81"><nobr><span class="ft7">The following theorem establishes the relation between ker-<br>nel k-means and graph clustering [10].</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:335"><nobr><span class="ft1">With this result,</span></nobr></DIV>
<DIV style="position:absolute;top:422;left:81"><nobr><span class="ft7">we can develop some techniques to handle the difficulty of<br>storing the large kernel matrix for kernel k-means.</span></nobr></DIV>
<DIV style="position:absolute;top:461;left:96"><nobr><span class="ft1">Theorem 1. Given a data set, we can construct a weighted</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:81"><nobr><span class="ft1">graph G = (</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:153"><nobr><span class="ft1">V, E, A), by treating each sample as a node and</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:81"><nobr><span class="ft7">linking an edge between each other. If we define the edge<br>weight A</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:134"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:149"><nobr><span class="ft1">= K(x</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:192"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:196"><nobr><span class="ft1">, x</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:211"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:217"><nobr><span class="ft1">), that is, A = K, the minimization</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:81"><nobr><span class="ft1">of (4) is equivalent to the maximization of (5).</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft2">3.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:112"><nobr><span class="ft16">THE EFFECTS OF LARGE SCALE IM-<br>BALANCED DATA ON SVMS</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:81"><nobr><span class="ft2">3.1</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:121"><nobr><span class="ft2">The Large Scale Data</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:94"><nobr><span class="ft1">There are two obstacles yielded by large scale. The first</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:81"><nobr><span class="ft7">one is the kernel evaluation, which has been intensively dis-<br>cussed in the previous work. The computational cost scales<br>quadratically with the data size. Furthermore, it is impos-<br>sible to store the whole kernel matrix in the memory for<br>common computers. The decomposition algorithms (e.g.,<br>SMO) have been developed to solve the problem [20, 22].<br>The SMO-like algorithms actually transform the space load<br>to the time cost, i.e., numerous iterations until convergence.<br>To reduce or avoid the kernel reevaluations, various efficient<br>caching techniques are also proposed [16]. Another obsta-<br>cle caused by large scale is the increased classification diffi-<br>culty, that is, the more probable data overlapping. We can<br>not prove it is inevitable but it typically happens. Assume<br>we will draw n randomly chosen numbers between 1 to 100<br>from a uniform distribution, our chances of drawing a num-<br>ber close to 100 would improve with increasing values of n,<br>even though the expected mean of the draws is invariant [2].<br>The checkerboard experiment in [29] is an intuitive exam-<br>ple. This is true especially for the real world data, either<br>because of the weak features (we mean features that are<br>less discriminative) or because of the noises. With the large<br>scale data, the samples in the overlapping area might be<br>so many that the samples violating KKT conditions become<br>abundant. This means the SMO algorithm might need more<br>iterations to converge.</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:94"><nobr><span class="ft1">Generally, the existing algorithmic approaches have not</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:81"><nobr><span class="ft1">been able to tackle the very large data set. Whereas, the</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:475"><nobr><span class="ft7">under-sampling method, e.g., active learning, is possible.<br>With unlabelled data, active learning selects a well-chosen<br>subset of data to label so that reduce the labor of manual an-<br>notations [24]. With large scale labelled data, active learn-<br>ing can also be used to reduce the scale of training data [21].<br>The key issue of active learning is how to choose the most<br>"valuable" samples. The informative sampling is a popular<br>criterion. That is, the samples closest to the boundary or<br>maximally violating the KKT conditions (the misclassified<br>samples) are preferred [24, 26]. Active learning is usually<br>in an iterative style. It requires an initial (usually random<br>selected) data set to obtain the estimation of the boundary.<br>The samples selected in the following iterations depend on<br>this initial boundary. In addition, active learning can not<br>work like the decomposition approach which stops until all<br>the samples satisfy the KKT conditions. This imply a poten-<br>tial danger, that is, if the initial data are selected improperly,<br>the algorithm might not be able to find the suitable hyper-<br>plane. Thus, another criterion, i.e., representative, must be<br>considered. Here, "representative" refers to the ability to<br>characterize the data distribution. Nguyen et al. [19] show<br>that the active learning method considering the represen-<br>tative criterion will achieve better results. Specifically for<br>SVMs, pre-clustering is proposed to estimate the data dis-<br>tribution before the under-sampling [31, 3, 30]. Similar ideas<br>of representative sampling appear in [5, 12].</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft2">3.2</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:516"><nobr><span class="ft2">The Imbalanced Data</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:489"><nobr><span class="ft1">The reason why general machine learning systems suffer</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:475"><nobr><span class="ft7">performance loss with imbalanced data is not yet clear [23,<br>28], but the analysis on SVMs seems relatively straightfor-<br>ward. Akbani et al. have summarized three possible causes<br>for SVMs [2].</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:573"><nobr><span class="ft1">They are, (a) positive samples lie further</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:475"><nobr><span class="ft7">from the ideal boundary, (b) the weakness of the soft-margin<br>SVMs, and (c) the imbalanced support vector ratio. Of these<br>causes, in our opinion, what really matters is the second one.<br>The first cause is pointed out by Wu et al. [29]. This sit-<br>uation occurs when the data are linearly separable and the<br>imbalance is caused by the insufficient sampling of the mi-<br>nority class. Only in this case does the "ideal" boundary<br>make sense.</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:560"><nobr><span class="ft1">As for the third cause, Akbani et al.</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:807"><nobr><span class="ft1">have</span></nobr></DIV>
<DIV style="position:absolute;top:730;left:475"><nobr><span class="ft7">pointed out that it plays a minor role because of the con-<br>straint </span></nobr></DIV>
<DIV style="position:absolute;top:741;left:528"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:537"><nobr><span class="ft1">y = 0 on Lagrange multipliers [2].</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:489"><nobr><span class="ft1">The second cause states that the soft-margin SVMs has in-</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:475"><nobr><span class="ft7">herent weakness for handling imbalanced data. We find that<br>it depends on the linear separability of the data whether the<br>imbalance has negative effects on SVMs. For linearly sep-<br>arable data, the imbalance will have tiny effects on SVMs,<br>since all the slack variables  of (1) tend to be zeros (, unless<br>the C is so small that the maximization of the margin dom-<br>inates the objective). In the result, there is no contradiction<br>between the capacity of the SVMs and the empirical er-<br>ror. Unfortunately, linear non-separable data often occurs.<br>The SVMs has to achieve a tradeoff between maximizing<br>the margin and minimizing the empirical error. For imbal-<br>anced data, the majority class outnumbers the minority one<br>in the overlapping area. To reduce the overwhelming errors<br>of misclassifying the majority class, the optimal hyperplane<br>will inevitably be skew to the minority. In the extreme, if C<br>is not very large, SVMs simply learns to classify everything<br>as negative because that makes the "margin" the largest,<br>with zero cumulative error on the abundant negative exam-<br>ples. The only tradeoff is the small amount of cumulative</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">443</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft17{font-size:6px;line-height:8px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123004.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft7">error on the few positive examples, which does not count for<br>much.</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:94"><nobr><span class="ft1">Several variants of SVMs have been adopted to solve the</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:81"><nobr><span class="ft7">problem of imbalance. One choice is the so-called one-class<br>SVMs, which uses only positive examples for training. With-<br>out using the information of the negative samples, it is usu-<br>ally difficult to achieve as good result as that of binary SVMs<br>classifier [18]. Using different penalty constants C</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:380"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:393"><nobr><span class="ft1">and C</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:430"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:81"><nobr><span class="ft7">for the positive and negative examples have been reported to<br>be effective [27, 17]. However, Wu et al. point out that the<br>effectiveness of this method is limited [29]. The explanation<br>of Wu is based on the KKT condition </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:326"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:335"><nobr><span class="ft1">y = 0, which im-</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:81"><nobr><span class="ft14">poses an equal total influence from the positive and negative<br>support vectors. We evaluate this method and the result<br>shows that tuning</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:201"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:208"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:200"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:208"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:225"><nobr><span class="ft1">does work (details refer to Section</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:81"><nobr><span class="ft14">5). We find this also depends on the linear separability of<br>the data whether this method works. For linearly separable<br>data, tuning</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:159"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:350;left:167"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:159"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:167"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:181"><nobr><span class="ft1">has little effects, since the penalty constants</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:81"><nobr><span class="ft14">are useless with the zero-valued slack variables. However, if<br>the data are linearly non-separable, tuning</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:345"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:352"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:344"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:352"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:390;left:367"><nobr><span class="ft1">does change</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:81"><nobr><span class="ft7">the position of separating hyperplane. The method to mod-<br>ify the kernel matrix is also proposed to improve SVMs for<br>imbalanced data [29]. A possible drawback of this type ap-<br>proach is its high computational costs.</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:81"><nobr><span class="ft2">4.</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:112"><nobr><span class="ft2">OVERALL APPROACH</span></nobr></DIV>
<DIV style="position:absolute;top:516;left:94"><nobr><span class="ft1">The proposed approach is named Support Cluster Ma-</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:81"><nobr><span class="ft7">chines (SCMs). We first partition the negative samples into<br>disjoint clusters, then train an initial SVMs model using<br>the positive samples and the representatives of the negative<br>clusters. With the global picture of the initial SVMs, we can<br>approximately identify the support vectors and non-support<br>vectors. A shrinking technique is then used to remove the<br>samples which are most probably not support vectors. This<br>procedure of clustering and shrinking are performed itera-<br>tively several times until some stop criteria satisfied. With<br>such a from coarse-to-fine procedure, the representative and<br>informative mechanisms are incorporated. There are four<br>key issues in the meta-algorithm of SCMs: (a) How to get<br>the partition of the training data, (b) How to get the rep-<br>resentative for each cluster, (c) How to safely remove the<br>non-support vector samples, (d) When to stop the itera-<br>tion procedure. Though similar ideas have been proposed<br>to speed-up SVMs in [30, 3, 31], no theoretical analysis of<br>this idea has been provided. In the following, we present an<br>in-depth analysis for this type of approaches and attempt to<br>improve the algorithm under the theoretical guide.</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:81"><nobr><span class="ft2">4.1</span></nobr></DIV>
<DIV style="position:absolute;top:857;left:121"><nobr><span class="ft2">Theoretical Analysis</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:94"><nobr><span class="ft1">Suppose</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:148"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:163"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:168"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:175"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:175"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:199"><nobr><span class="ft1">is a partition of the training set that the</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:81"><nobr><span class="ft7">samples within the same cluster have the same class label.<br>If we construct a representative u</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:289"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:301"><nobr><span class="ft1">for each cluster </span></nobr></DIV>
<DIV style="position:absolute;top:913;left:408"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:911;left:414"><nobr><span class="ft1">, we</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:81"><nobr><span class="ft1">can obtain two novel models of SVMs.</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:94"><nobr><span class="ft1">The first one is named Support Cluster Machines (SCMs).</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:81"><nobr><span class="ft7">It treats each representative as a sample, thus the data size<br>is reduced from n to k. This equals to the classification of<br>the clusters. That is where the name SCMs come from. The<br>new training set is</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:104"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:115"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:126"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:141"><nobr><span class="ft1">{(u</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:162"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:168"><nobr><span class="ft1">, y</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:181"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:186"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:192"><nobr><span class="ft1">|u</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:204"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:214"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:239"><nobr><span class="ft11">N</span></nobr></DIV>
<DIV style="position:absolute;top:1032;left:249"><nobr><span class="ft1">, y</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:262"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:272"><nobr><span class="ft1"> {1, -1}, c = 1, . . . , k},</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:81"><nobr><span class="ft1">in which y</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:144"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:154"><nobr><span class="ft1">equals the labels of the samples within </span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:403"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:409"><nobr><span class="ft1">. We</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:475"><nobr><span class="ft1">define the dual problem of support cluster machines as</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:557"><nobr><span class="ft1">min</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:561"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:120;left:569"><nobr><span class="ft13"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:607"><nobr><span class="ft1">G</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:618"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:626"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:640"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:647"><nobr><span class="ft1">) = 1</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:673"><nobr><span class="ft1">2 </span></nobr></DIV>
<DIV style="position:absolute;top:106;left:690"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:690"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:699"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:711"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:719"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:728"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:109;left:738"><nobr><span class="ft1">- e</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:760"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:115;left:760"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:112;left:768"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:777"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:138;left:525"><nobr><span class="ft1">subjectto</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:607"><nobr><span class="ft1">0</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:618"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:140;left:641"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:657"><nobr><span class="ft1"> |</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:683"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:688"><nobr><span class="ft1">|C, i = 1, . . . , k</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:816"><nobr><span class="ft1">(6)</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:607"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:154;left:616"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:163;left:616"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:160;left:625"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:633"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:160;left:645"><nobr><span class="ft1">= 0,</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:475"><nobr><span class="ft1">where </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:525"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:187;left:538"><nobr><span class="ft1">is a vector of size k with components </span></nobr></DIV>
<DIV style="position:absolute;top:189;left:782"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:799"><nobr><span class="ft1">corre-</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:475"><nobr><span class="ft1">sponding to u</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:562"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:567"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:577"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:589"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:593"><nobr><span class="ft1">|C is the upper bound for </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:770"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:781"><nobr><span class="ft1">, e</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:798"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:202;left:812"><nobr><span class="ft1">is a</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:475"><nobr><span class="ft1">k dimension vector of all ones, and Q</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:704"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:218;left:717"><nobr><span class="ft1">is an k</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:761"><nobr><span class="ft1">× k positive</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:475"><nobr><span class="ft1">semi-definite matrix, Q</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:616"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:636"><nobr><span class="ft1">= y</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:658"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:662"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:669"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:680"><nobr><span class="ft1">(u</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:705"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:709"><nobr><span class="ft1">), (u</span></nobr></DIV>
<DIV style="position:absolute;top:236;left:745"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:751"><nobr><span class="ft1">) .</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:489"><nobr><span class="ft1">Another one is named Duplicate Support Vector Machines</span></nobr></DIV>
<DIV style="position:absolute;top:265;left:475"><nobr><span class="ft7">(DSVMs). Different from SCMs, it does not reduce the size<br>of training set. Instead, it replace each sample x</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:773"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:783"><nobr><span class="ft1">with the</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:475"><nobr><span class="ft1">representative of the cluster that x</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:690"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:700"><nobr><span class="ft1">belongs to. Thus, the</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:475"><nobr><span class="ft7">samples within the same cluster are duplicate. That is why<br>it is named DSVMs. The training set is</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:490"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:487"><nobr><span class="ft1">D = {(~x</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:537"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:542"><nobr><span class="ft1">, ~</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:548"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:555"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:560"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:565"><nobr><span class="ft1">|x</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:585"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:594"><nobr><span class="ft1"> D, if x</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:646"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:654"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:357;left:676"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:681"><nobr><span class="ft1">, ~</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:687"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:696"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:704"><nobr><span class="ft1">= u</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:728"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:738"><nobr><span class="ft1">and ~</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:765"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:772"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:355;left:780"><nobr><span class="ft1">= y</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:802"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:811"><nobr><span class="ft1">},</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:475"><nobr><span class="ft1">and the corresponding dual problem is defined as</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:571"><nobr><span class="ft1">min</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:579"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:624"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:621"><nobr><span class="ft1">G() = 1</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:671"><nobr><span class="ft1">2 </span></nobr></DIV>
<DIV style="position:absolute;top:408;left:689"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:700"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:698"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:722"><nobr><span class="ft1">- e</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:743"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:752"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:440;left:538"><nobr><span class="ft1">subjectto</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:621"><nobr><span class="ft1">0</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:631"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:442;left:655"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:663"><nobr><span class="ft1"> C, i = 1, . . . , n</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:816"><nobr><span class="ft1">(7)</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:621"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:456;left:629"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:638"><nobr><span class="ft1">y = 0,</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:475"><nobr><span class="ft1">where ~</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:515"><nobr><span class="ft1">Q is is an n</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:591"><nobr><span class="ft1">× n positive semi-definite matrix, ~</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:798"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:809"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:823"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:476"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:482"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:488"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:487"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:494"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:505"><nobr><span class="ft1">(~</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:520"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:529"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:533"><nobr><span class="ft1">), (~</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:560"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:569"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:574"><nobr><span class="ft1">) .</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:603"><nobr><span class="ft1">We have the following theorem that</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:475"><nobr><span class="ft1">states (6) is somehow equivalent to (7):</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:490"><nobr><span class="ft1">Theorem 2. With the above definitions of the SCMs and</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:475"><nobr><span class="ft1">the DSVMs, if </span></nobr></DIV>
<DIV style="position:absolute;top:559;left:574"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:586"><nobr><span class="ft1">and </span></nobr></DIV>
<DIV style="position:absolute;top:559;left:621"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:632"><nobr><span class="ft1">are their optimal solutions respec-</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:475"><nobr><span class="ft1">tively, the relation G</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:606"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:580;left:614"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:628"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:580;left:635"><nobr><span class="ft1">) = ~</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:665"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:690"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:580;left:697"><nobr><span class="ft1">) holds. Furthermore,</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:475"><nobr><span class="ft1">any </span></nobr></DIV>
<DIV style="position:absolute;top:598;left:511"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:523"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:548"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:560"><nobr><span class="ft1">satisfying</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:621"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:637"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:654"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:669"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:683"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:690"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:695"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:604;left:709"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:717"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:726"><nobr><span class="ft17"><br>i</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:733"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:592;left:749"><nobr><span class="ft1">c = 1, . . . , k}</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:475"><nobr><span class="ft1">is the optimal solution of SCMs. Inversely, any </span></nobr></DIV>
<DIV style="position:absolute;top:608;left:775"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:800"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:612;left:812"><nobr><span class="ft1">sat-</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:475"><nobr><span class="ft1">isfying</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:518"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:626;left:525"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:540"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:546"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:632;left:551"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:635;left:565"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:574"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:629;left:582"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:591"><nobr><span class="ft1">= </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:615"><nobr><span class="ft17"><br>c</span></nobr></DIV>
<DIV style="position:absolute;top:627;left:627"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:633"><nobr><span class="ft1">c = 1, . . . , k} and the constraints</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:475"><nobr><span class="ft1">of (7) is the optimal solution of DSVMs.</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:475"><nobr><span class="ft7">The proof is in Appendix A. Theorem 2 shows that solving<br>the SCMs is equivalent to solving a quadratic programming<br>problem of the same scale as that of the SVMs in (2).</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:489"><nobr><span class="ft1">Comparing (2) and (7), we can find that only the Hessian</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:475"><nobr><span class="ft7">matrix is different. Thus, to estimate the approximation<br>from SCMs of (6) to SVMs of (2), we only need to ana-<br>lyze the stability of the quadratic programming model in<br>(2) when the Hessian matrix varies from Q to ~</span></nobr></DIV>
<DIV style="position:absolute;top:783;left:769"><nobr><span class="ft1">Q. Daniel</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:475"><nobr><span class="ft7">has presented a study on the stability of the solution of def-<br>inite quadratic programming, which requires that both Q<br>and ~</span></nobr></DIV>
<DIV style="position:absolute;top:830;left:503"><nobr><span class="ft1">Q are positive definite [7]. However, in our situation,</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:475"><nobr><span class="ft1">Q is usually positive definite and ~</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:684"><nobr><span class="ft1">Q is not (because of the</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:475"><nobr><span class="ft1">duplications). We develop a novel theorem for this case.</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:489"><nobr><span class="ft1">If define  = Q</span></nobr></DIV>
<DIV style="position:absolute;top:877;left:590"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:604"><nobr><span class="ft1">Q , where</span></nobr></DIV>
<DIV style="position:absolute;top:877;left:682"><nobr><span class="ft1">· denotes the Frobenius</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:475"><nobr><span class="ft7">norm of a matrix, the value of  measure the size of the<br>perturbations between Q and ~</span></nobr></DIV>
<DIV style="position:absolute;top:912;left:667"><nobr><span class="ft1">Q.</span></nobr></DIV>
<DIV style="position:absolute;top:912;left:695"><nobr><span class="ft1">We have the following</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:475"><nobr><span class="ft1">theorem:</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:490"><nobr><span class="ft1">Theorem 3. If Q is positive definite and  = Q - ~</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:811"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:830"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:475"><nobr><span class="ft1">let </span></nobr></DIV>
<DIV style="position:absolute;top:968;left:503"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:972;left:514"><nobr><span class="ft1">and ~</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:540"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:968;left:549"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:972;left:560"><nobr><span class="ft1">be the optimal solutions to (2) and (7) respec-</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:475"><nobr><span class="ft1">tively, we have</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:591"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:590"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:599"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:608"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:631"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:649"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:666"><nobr><span class="ft1">mC</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:676"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:553"><nobr><span class="ft1">G( ~</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:570"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:579"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:585"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:594"><nobr><span class="ft1">- G(</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:632"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:639"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:649"><nobr><span class="ft1"> (m</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:683"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:693"><nobr><span class="ft1">+ ~</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:707"><nobr><span class="ft1">m</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:719"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:725"><nobr><span class="ft1">)C</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:741"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:747"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:706"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">444</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft18{font-size:0px;font-family:Times;color:#000000;}
	.ft19{font-size:1px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123005.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft1">where  is the minimum eigenvalue of Q, m and ~</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:376"><nobr><span class="ft1">m indicate</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:81"><nobr><span class="ft7">the numbers of the support vectors for (2) and (7) respec-<br>tively.</span></nobr></DIV>
<DIV style="position:absolute;top:141;left:81"><nobr><span class="ft7">The proof is in Appendix B. This theorem shows that the<br>approximation from (2) to (7) is bounded by . Note that<br>this does not mean that with minimal  we are sure to get<br>the best approximate solution. For example, adopting the<br>support vectors of (1) to construct ~</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:302"><nobr><span class="ft1">Q will yield the exact</span></nobr></DIV>
<DIV style="position:absolute;top:220;left:81"><nobr><span class="ft7">optimal solution of (2) but the corresponding  are not nec-<br>essarily minimum. However, we do not know which samples<br>are support vectors beforehand. What we can do is to mini-<br>mize the potential maximal distortion between the solutions<br>between (2) and (7).</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:94"><nobr><span class="ft1">Now we consider the next problem, that is, given the par-</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:81"><nobr><span class="ft1">tition</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:120"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:135"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:141"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:147"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:147"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:167"><nobr><span class="ft1">, what are the best representatives</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:391"><nobr><span class="ft1">{u</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:407"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:413"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:420"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:317;left:420"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:81"><nobr><span class="ft7">for the clusters in the sense of approximating Q? In fact,<br>we have the following theorem:</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:96"><nobr><span class="ft1">Theorem 4. Given the partition {</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:322"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:328"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:335"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:335"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:354"><nobr><span class="ft1">, the</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:391"><nobr><span class="ft1">{u</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:407"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:413"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:420"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:420"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:81"><nobr><span class="ft1">satisfying</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:152"><nobr><span class="ft1">(u</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:176"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:182"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:208"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:222"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:229"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:234"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:426;left:248"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:256"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:280"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:285"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:238"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:250"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:256"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:292"><nobr><span class="ft1">, c = 1, . . . , k</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:422"><nobr><span class="ft1">(8)</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft1">will make  = Q</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:190"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:203"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:227"><nobr><span class="ft1">minimum.</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:81"><nobr><span class="ft7">The proof is in Appendix C. This theorem shows that, given<br>the partition, (u</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:194"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:200"><nobr><span class="ft1">) = m</span></nobr></DIV>
<DIV style="position:absolute;top:518;left:242"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:254"><nobr><span class="ft1">yields the best approximation</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:81"><nobr><span class="ft1">between ~</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:134"><nobr><span class="ft1">Q and Q.</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:94"><nobr><span class="ft1">Here we come to the last question, i.e., what partition</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:95"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:101"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:108"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:108"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:134"><nobr><span class="ft1">will make  =</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:235"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:251"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:265"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:290"><nobr><span class="ft1">minimum. To make the</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:81"><nobr><span class="ft1">problem more clearly, we expand </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:291"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:302"><nobr><span class="ft1">as</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:87"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:100"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:112"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:131"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:141"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:163"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:156"><nobr><span class="ft11">h=1</span></nobr></DIV>
<DIV style="position:absolute;top:606;left:185"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:179"><nobr><span class="ft11">l=1 x</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:207"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:212"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:226"><nobr><span class="ft13">h</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:235"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:241"><nobr><span class="ft13">j</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:247"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:261"><nobr><span class="ft13">l</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:265"><nobr><span class="ft1">( (x</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:300"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:305"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:340"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:346"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:357"><nobr><span class="ft1">- m</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:388"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:395"><nobr><span class="ft1">, m</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:414"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:424"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:429"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:622;left:436"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:422"><nobr><span class="ft1">(9)</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:81"><nobr><span class="ft1">There are approximately k</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:246"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:253"><nobr><span class="ft1">/k! types of such partitions of</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:81"><nobr><span class="ft7">the data set. An exhaustive search for the best partition is<br>impossible. Recalling that (9) is similar to (4), we have the<br>following theorem which states their relaxed equivalence.</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:96"><nobr><span class="ft1">Theorem 5. The relaxed optimal solution of minimizing</span></nobr></DIV>
<DIV style="position:absolute;top:762;left:81"><nobr><span class="ft7">(9) and the relaxed optimal solution of minimizing (4) are<br>equivalent.</span></nobr></DIV>
<DIV style="position:absolute;top:809;left:81"><nobr><span class="ft7">The proof can be found in Appendix D. Minimizing  amounts<br>to find a low-rank matrix approximating Q. Ding et al. have<br>pointed out the relaxed equivalence between kernel PCA and<br>kernel k-means in [11]. Note that minimizing (9) is different<br>from kernel PCA in that it is with an additional block-wise<br>constant constraint. That is, the value of ~</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:342"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:889;left:353"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:368"><nobr><span class="ft1">must be in-</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:81"><nobr><span class="ft1">variant with respect to the cluster </span></nobr></DIV>
<DIV style="position:absolute;top:905;left:298"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:309"><nobr><span class="ft1">containing ~</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:376"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:385"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:394"><nobr><span class="ft1">and the</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:81"><nobr><span class="ft1">cluster </span></nobr></DIV>
<DIV style="position:absolute;top:921;left:135"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:146"><nobr><span class="ft1">containing ~</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:214"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:223"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:229"><nobr><span class="ft1">. With Theorem 5 we know that</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:81"><nobr><span class="ft7">kernel k-means is a suitable method to obtain the partition<br>of data.</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:94"><nobr><span class="ft1">According to the above results, the SCMs essentially finds</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:81"><nobr><span class="ft7">an approximate solution to the original SVMs by smoothing<br>the kernel matrix K (or Hessian matrix Q). Fig.1 illustrates<br>the procedure of smoothing the kernel matrix via clustering.<br>Hence, by solving a smaller quadratic programming prob-<br>lem, the position of separating hyperplane can be roughly<br>determined.</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:483"><nobr><span class="ft18">-5</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:502"><nobr><span class="ft18">0</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:520"><nobr><span class="ft18">5</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:537"><nobr><span class="ft18">10</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:555"><nobr><span class="ft18">15</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:476"><nobr><span class="ft18">-4</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:476"><nobr><span class="ft18">-2</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:478"><nobr><span class="ft18">0</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:478"><nobr><span class="ft18">2</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:478"><nobr><span class="ft18">4</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:478"><nobr><span class="ft18">6</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:478"><nobr><span class="ft18">8</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:476"><nobr><span class="ft18">10</span></nobr></DIV>
<DIV style="position:absolute;top:92;left:476"><nobr><span class="ft18">12</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:476"><nobr><span class="ft18">14</span></nobr></DIV>
<DIV style="position:absolute;top:78;left:476"><nobr><span class="ft18">16</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:590"><nobr><span class="ft19">50</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:608"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:627"><nobr><span class="ft19">150</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:647"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:568"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:568"><nobr><span class="ft19">40</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:568"><nobr><span class="ft19">60</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:568"><nobr><span class="ft19">80</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:566"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:566"><nobr><span class="ft19">120</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:566"><nobr><span class="ft19">140</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:566"><nobr><span class="ft19">160</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:566"><nobr><span class="ft19">180</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:566"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:680"><nobr><span class="ft18">50</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:699"><nobr><span class="ft18">100</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:718"><nobr><span class="ft18">150</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:737"><nobr><span class="ft18">200</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:659"><nobr><span class="ft18">20</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:659"><nobr><span class="ft18">40</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:659"><nobr><span class="ft18">60</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:659"><nobr><span class="ft18">80</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:657"><nobr><span class="ft18">100</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:657"><nobr><span class="ft18">120</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:657"><nobr><span class="ft18">140</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:657"><nobr><span class="ft18">160</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:657"><nobr><span class="ft18">180</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:657"><nobr><span class="ft18">200</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:771"><nobr><span class="ft19">50</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:790"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:809"><nobr><span class="ft19">150</span></nobr></DIV>
<DIV style="position:absolute;top:155;left:828"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:83;left:750"><nobr><span class="ft19">20</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:750"><nobr><span class="ft19">40</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:750"><nobr><span class="ft19">60</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:750"><nobr><span class="ft19">80</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:748"><nobr><span class="ft19">100</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:748"><nobr><span class="ft19">120</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:748"><nobr><span class="ft19">140</span></nobr></DIV>
<DIV style="position:absolute;top:137;left:748"><nobr><span class="ft19">160</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:748"><nobr><span class="ft19">180</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:748"><nobr><span class="ft19">200</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:514"><nobr><span class="ft13">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:605"><nobr><span class="ft13">(b)</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:696"><nobr><span class="ft13">(c )</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:786"><nobr><span class="ft13">(d)</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:475"><nobr><span class="ft14">Figure 1: (a) 2D data distribution, (b) the visual-<br>ization of the kernel matrix Q, (c) the kernel matrix<br>Q by re-ordering the entries so that the samples be-<br>longing to the same cluster come together, (d) the<br>approximate kernel matrix ~</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:666"><nobr><span class="ft1">Q obtained by replacing</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:475"><nobr><span class="ft1">each sample with the corresponding centroid.</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:475"><nobr><span class="ft2">4.2</span></nobr></DIV>
<DIV style="position:absolute;top:300;left:516"><nobr><span class="ft2">Kernel-based Graph Clustering</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:489"><nobr><span class="ft1">In the previous work, k-means [30], BIRCH [31] and PDDP</span></nobr></DIV>
<DIV style="position:absolute;top:338;left:475"><nobr><span class="ft7">[3] have been used to obtain the partition of the data. None<br>of them performs clustering in the feature space, though the<br>SVMs works in the feature space. This is somewhat unnat-<br>ural. Firstly, recalling that the kernel K(</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:721"><nobr><span class="ft1">·, ·) usually implies</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:475"><nobr><span class="ft7">an implicitly nonlinear mapping from the input space to the<br>feature space, the optimal partition of input space is not nec-<br>essarily the optimal one of feature space. Take k-means as<br>an example, due to the fact that the squared Euclidean dis-<br>tance is used as the distortion measure, the clusters must be<br>separated by piece-wise hyperplanes (i.e., voronoi diagram).<br>However, these separating hyperplanes are no longer hyper-<br>planes in the feature space with nonlinear mapping (</span></nobr></DIV>
<DIV style="position:absolute;top:507;left:821"><nobr><span class="ft1">·).</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:475"><nobr><span class="ft7">Secondly, the k-means approach can not capture the com-<br>plex structure of data. As shown in Fig.2, the negative class<br>is in a ring-shape in the input space. If the k-means is used,<br>the centroids of positive and negative class might overlap.<br>Whereas in the feature space, the kernel k-means might get<br>separable centroids.</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:489"><nobr><span class="ft1">Several factors limit the application of kernel k-means to</span></nobr></DIV>
<DIV style="position:absolute;top:636;left:475"><nobr><span class="ft7">large scale data. Firstly, it is almost impossible to store the<br>whole kernel matrix K in the memory, e.g., for n = 100 000,<br>we still need 20 gigabytes memory taking the symmetry into<br>account. Secondly, the kernel k-means relies heavily on an<br>effective initialization to achieve good results, and we do not<br>have such a sound method yet. Finally, the computational<br>cost of the kernel k-means might exceeds that of SVMs, and<br>therefore, we lose the benefits of under-sampling. Dhillon et<br>al. recently propose a multilevel kernel k-means method [9],<br>which seems to cater to our requirements. The approach<br>is based on the equivalence between graph clustering and<br>kernel k-means. It incorporates the coarsening and initial<br>partitioning phases to obtain a good initial clustering. Most<br>importantly, the approach is extremely efficient. It can han-<br>dle a graph with 28,294 nodes and 1,007,284 edges in several<br>seconds. Therefore, here we adopt this approach. The de-<br>tailed description can be found in [9]. In the following, we<br>focus on how to address the difficulty of storing large scale<br>kernel matrix.</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:489"><nobr><span class="ft1">Theorem 1 states that kernel k-means is equivalent to a</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:475"><nobr><span class="ft7">type of graph clustering. Kernel k-means focuses on group-<br>ing data so that their average distance from the centroid is<br>minimum ,while graph clustering aims to minimizing the av-<br>erage pair-wise distance among the data. Central grouping<br>and pair-wise grouping are two different views of the same<br>approach. From the perspective of pair-wise grouping, we<br>can expect that two samples with large distance will not<br>belong to the same cluster in the optimal solution. Thus,</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">445</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft20{font-size:4px;font-family:Times;color:#000000;}
	.ft21{font-size:10px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123006.png" alt="background image">
<DIV style="position:absolute;top:134;left:215"><nobr><span class="ft20">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:209"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:81;left:161"><nobr><span class="ft20">2</span></nobr></DIV>
<DIV style="position:absolute;top:74;left:156"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:172;left:422"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:418"><nobr><span class="ft21"><i>z</i></span></nobr></DIV>
<DIV style="position:absolute;top:83;left:340"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:74;left:334"><nobr><span class="ft1">z</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:320"><nobr><span class="ft13">3</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:314"><nobr><span class="ft21"><i>z</i></span></nobr></DIV>
<DIV style="position:absolute;top:213;left:171"><nobr><span class="ft13">Positive Class</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:253"><nobr><span class="ft13">Negative Class</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:81"><nobr><span class="ft7">Figure 2: The left and right figures show the data<br>distribution of input space and feature space respec-<br>tively. The two classes are indicated by squares and<br>circles. Each class is grouped into one cluster, and<br>the solid mark indicates the centroid of the class.</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:81"><nobr><span class="ft7">we add the constraint that two samples with distance large<br>enough are not linked by an edge, that is, transforming the<br>dense graph to a sparse graph. This procedure is the com-<br>mon practice in spectral clustering or manifold embedding.<br>Usually, two methods have been widely used for this pur-<br>pose, i.e., k-nearest neighbor and -ball. Here, we adopt the</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:86"><nobr><span class="ft1">-ball approach. Concretely, the edges with weight A</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:404"><nobr><span class="ft11">ij</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:419"><nobr><span class="ft1">&lt;</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:81"><nobr><span class="ft1">is removed from the original graph, in which the parameter</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:93"><nobr><span class="ft1">is pre-determined. By transforming a dense graph into</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:81"><nobr><span class="ft7">a sparse graph, we only need store the sparse affinity ma-<br>trix instead of the original kernel matrix. Nevertheless, we<br>have to point out that the time complexity of constructing<br>sparse graph is O(n</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:202"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:208"><nobr><span class="ft1">) for data set with n examples, which</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:81"><nobr><span class="ft7">is the efficiency bottleneck of the current implementation.<br>With the sparse graph, each iteration of the multilevel ker-<br>nel k-means costs O(ln</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:220"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:229"><nobr><span class="ft1">) time, where ln</span></nobr></DIV>
<DIV style="position:absolute;top:570;left:327"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:341"><nobr><span class="ft1">is the number of</span></nobr></DIV>
<DIV style="position:absolute;top:591;left:81"><nobr><span class="ft1">nonzero entries in the kernel matrix.</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:81"><nobr><span class="ft2">4.3</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:121"><nobr><span class="ft2">Support Cluster Machines</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:94"><nobr><span class="ft1">According to Theorem 4, choosing the centroid of each</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:81"><nobr><span class="ft7">cluster as representative will yield the best approximation.<br>However, the explicit form of (</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:284"><nobr><span class="ft1">·) is unknown. We don't</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:81"><nobr><span class="ft1">know the exact pre-images of</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:259"><nobr><span class="ft1">{m</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:280"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:285"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:682;left:292"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:292"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:312"><nobr><span class="ft1">, what we can get are</span></nobr></DIV>
<DIV style="position:absolute;top:703;left:81"><nobr><span class="ft1">the dot products between the centroids by</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:127"><nobr><span class="ft1">m</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:140"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:148"><nobr><span class="ft1">, m</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:167"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:181"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:727;left:215"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:197"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:209"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:216"><nobr><span class="ft1">||</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:231"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:236"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:244"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:250"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:255"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:756;left:269"><nobr><span class="ft13">h</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:278"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:285"><nobr><span class="ft13">j</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:291"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:756;left:305"><nobr><span class="ft13">l</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:314"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:338"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:343"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:378"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:384"><nobr><span class="ft1">) ,</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:81"><nobr><span class="ft1">which requires O(n</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:197"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:204"><nobr><span class="ft1">) costs. Then the pre-computed kernel</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft7">SVMs can be used. The pre-computed kernel SVMs takes<br>the kernel matrix K</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:203"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:812;left:215"><nobr><span class="ft1">as input, and save the indices of sup-</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:81"><nobr><span class="ft1">port vectors in the model [15].</span></nobr></DIV>
<DIV style="position:absolute;top:827;left:287"><nobr><span class="ft1">To classify the incoming</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:81"><nobr><span class="ft7">sample x, we have to calculate the dot product between x<br>and all the samples in the support clusters, e.g., </span></nobr></DIV>
<DIV style="position:absolute;top:861;left:378"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:388"><nobr><span class="ft1">(If m</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:420"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:859;left:430"><nobr><span class="ft1">is</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:81"><nobr><span class="ft1">a support vector, we define the cluster </span></nobr></DIV>
<DIV style="position:absolute;top:877;left:318"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:327"><nobr><span class="ft1">as support cluster.)</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:184"><nobr><span class="ft1">x, m</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:212"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:227"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:899;left:250"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:243"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:255"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:260"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:268"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:275"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:280"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:927;left:294"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:305"><nobr><span class="ft1">x, x</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:328"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:338"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:950;left:81"><nobr><span class="ft7">We need another O(nm) costs to predict all the training<br>samples if there are m samples in support clusters. This<br>is unacceptable for large scale data. To reduce the kernel<br>reevaluation, we adopt the same method as [3], i.e., selecting<br>a pseudo-center for each cluster as the representative</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:132"><nobr><span class="ft1">u</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:141"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:150"><nobr><span class="ft1">= arg min</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:186"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:193"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:198"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:212"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:224"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:248"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:253"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:261"><nobr><span class="ft1">- 1</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:277"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:289"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:294"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:302"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:309"><nobr><span class="ft13">j</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:314"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:329"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:336"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:360"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:366"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:378"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1045;left:384"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:630"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:623"><nobr><span class="ft21"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:219;left:516"><nobr><span class="ft20">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:587"><nobr><span class="ft20">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:498"><nobr><span class="ft20">2</span></nobr></DIV>
<DIV style="position:absolute;top:77;left:492"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:817"><nobr><span class="ft13">1</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:809"><nobr><span class="ft21"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:219;left:705"><nobr><span class="ft20">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:775"><nobr><span class="ft20">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:685"><nobr><span class="ft20">2</span></nobr></DIV>
<DIV style="position:absolute;top:78;left:680"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:553"><nobr><span class="ft1">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:229;left:740"><nobr><span class="ft3">(b)</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:475"><nobr><span class="ft7">Figure 3: (a) Each class is grouped into one cluster,<br>(b) each class is grouped into two clusters. The solid<br>mark represents the centroid of the corresponding<br>class.</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:524"><nobr><span class="ft1">The solid lines indicate the support hyper-</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:475"><nobr><span class="ft7">planes yielded by SCMs and the dot lines indicate<br>the true support hyperplanes.</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:475"><nobr><span class="ft1">which can be directly obtained by</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:550"><nobr><span class="ft1">u</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:559"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:568"><nobr><span class="ft1">= arg max</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:605"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:611"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:415;left:616"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:418;left:630"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:638"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:644"><nobr><span class="ft13">j</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:650"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:426;left:664"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:675"><nobr><span class="ft1">(x</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:699"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:703"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:739"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:745"><nobr><span class="ft1">) .</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:809"><nobr><span class="ft1">(10)</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:475"><nobr><span class="ft7">Thus, the kernel evaluation within training procedure re-<br>quires O(</span></nobr></DIV>
<DIV style="position:absolute;top:464;left:533"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:548"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:548"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:570"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:467;left:581"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:587"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:591"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:601"><nobr><span class="ft1">+ k</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:624"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:630"><nobr><span class="ft1">) time, which be further reduced</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:475"><nobr><span class="ft7">by probabilistic speedup proposed by Smola [25]. The ker-<br>nel evaluation of predicting the training samples is reduced<br>from O(nm) to O(ns), where s indicates the number of sup-<br>port clusters.</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:475"><nobr><span class="ft2">4.4</span></nobr></DIV>
<DIV style="position:absolute;top:555;left:516"><nobr><span class="ft2">Shrinking Techniques</span></nobr></DIV>
<DIV style="position:absolute;top:578;left:489"><nobr><span class="ft1">With the initial SCMs, we can remove the samples that</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:475"><nobr><span class="ft7">are not likely support vectors. However, there is no theoret-<br>ical guarantee for the security of the shrinking. In Fig. 3,<br>we give a simple example to show that the shrinking might<br>not be safe. In the example, if the samples outside the mar-<br>gin between support hyperplanes are to be removed, the<br>case (a) will remove the true support vectors while the case<br>(b) will not. The example shows that the security depends<br>on whether the hyperplane of SCMs is parallel to the true<br>separating hyperplane. However, we do not know the direc-<br>tion of true separating hyperplane before the classification.<br>Therefore, what we can do is to adopt sufficient initial clus-<br>ter numbers so that the solution of SCMs can approximate<br>the original optimal solution enough. Specifically for large<br>scale imbalanced data, the samples satisfying the following<br>condition will be removed from the training set:</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:595"><nobr><span class="ft1">| w, (x) + b| &gt; ,</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:809"><nobr><span class="ft1">(11)</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:475"><nobr><span class="ft1">where  is a predefined parameter.</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:475"><nobr><span class="ft2">4.5</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:516"><nobr><span class="ft2">The Algorithm</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:489"><nobr><span class="ft1">Yu [31] and Boley [3] have adopted different stop crite-</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:475"><nobr><span class="ft7">ria. In Yu et al.'s approach, the algorithm stops when each<br>cluster has only one sample. Whereas, Boley et al. limit<br>the maximum iterations by a fixed parameter.</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:779"><nobr><span class="ft1">Here, we</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:475"><nobr><span class="ft7">propose two novel criteria especially suitable for imbalanced<br>data. The first one is to stop whenever the ratio of posi-<br>tive and negative samples is relatively imbalanced. Another<br>choice is the Neyman-Pearson criterion, that is, minimizing<br>the total error rate subject to a constraint that the miss rate<br>of positive class is less than some threshold. Thus, once the</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">446</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft22{font-size:8px;font-family:Times;color:#000000;}
	.ft23{font-size:14px;font-family:Times;color:#000000;}
	.ft24{font-size:2px;font-family:Times;color:#000000;}
	.ft25{font-size:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123007.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft7">miss rate of positive class exceeds some threshold, we stop<br>the algorithm.</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:94"><nobr><span class="ft1">The overall approach is illustrated in Algorithm 1. With</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:81"><nobr><span class="ft7">large scale balanced data, we carry out the data clustering<br>for both classes separately. Whereas with imbalanced data,<br>the clustering and shrinking will only be conducted on the<br>majority class. The computation complexity is dominated<br>by kernel evaluation. Therefore, it will not exceed O((n</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:407"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:417"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:422"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:429"><nobr><span class="ft1">+</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:81"><nobr><span class="ft1">(n</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:94"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:104"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:109"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:115"><nobr><span class="ft1">), where n</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:179"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:194"><nobr><span class="ft1">and n</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:230"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:244"><nobr><span class="ft1">indicate the number of negative</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:81"><nobr><span class="ft1">and positive examples respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:94"><nobr><span class="ft1">Algorithm 1: Support Cluster Machines</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:108"><nobr><span class="ft1">Input</span></nobr></DIV>
<DIV style="position:absolute;top:278;left:159"><nobr><span class="ft1">: Training data set</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:276"><nobr><span class="ft1">D = D</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:316"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:329"><nobr><span class="ft1"> D</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:352"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:108"><nobr><span class="ft1">Output: Decision function f</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:108"><nobr><span class="ft1">repeat</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:94"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:129"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:144"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:143"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:153"><nobr><span class="ft1">, m</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:173"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:173"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:182"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:188"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:195"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:188"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:212"><nobr><span class="ft1">=KernelKMeans(</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:314"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:325"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:335"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:94"><nobr><span class="ft22">2</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:129"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:144"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:143"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:153"><nobr><span class="ft1">, m</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:173"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:173"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:182"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:189"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:195"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:189"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:213"><nobr><span class="ft1">=KernelKMeans(</span></nobr></DIV>
<DIV style="position:absolute;top:353;left:315"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:326"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:357;left:336"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:94"><nobr><span class="ft22">3</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:129"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:139"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:377;left:151"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:165"><nobr><span class="ft1">{m</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:186"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:186"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:195"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:202"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:208"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:202"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:224"><nobr><span class="ft1"> {m</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:257"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:257"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:374;left:266"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:273"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:279"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:273"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:94"><nobr><span class="ft22">4</span></nobr></DIV>
<DIV style="position:absolute;top:393;left:129"><nobr><span class="ft1">f</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:135"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:393;left:147"><nobr><span class="ft1">=SVMTrain(</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:221"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:231"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:394;left:239"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:94"><nobr><span class="ft22">5</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:129"><nobr><span class="ft1">f (</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:142"><nobr><span class="ft1">D) =SVMPredict(f</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:257"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:408;left:265"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:405;left:271"><nobr><span class="ft1">D)</span></nobr></DIV>
<DIV style="position:absolute;top:408;left:94"><nobr><span class="ft22">6</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:129"><nobr><span class="ft1">D = D</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:169"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:181"><nobr><span class="ft1"> D</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:205"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:218"><nobr><span class="ft1">=Shrinking(f (</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:312"><nobr><span class="ft1">D));</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:94"><nobr><span class="ft22">7</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:108"><nobr><span class="ft1">until</span></nobr></DIV>
<DIV style="position:absolute;top:445;left:145"><nobr><span class="ft1">stop criterion is true</span></nobr></DIV>
<DIV style="position:absolute;top:444;left:94"><nobr><span class="ft22">8</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:81"><nobr><span class="ft2">5.</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:112"><nobr><span class="ft2">EXPERIMENTS</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:94"><nobr><span class="ft1">The experiments on both the synthetic and the TRECVID</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:81"><nobr><span class="ft7">data are carried out. The experiments on synthetic data<br>are used to analyze the effects of large scale and imbalance<br>on SVMs and the experiments on TRECVID data serve to<br>evaluate the effectiveness and efficiency of SCMs. The multi-<br>level kernel graph partitioning code graclus [9] is adopted for<br>data clustering and the well-known LibSVM software [15] is<br>used in our experiments. All our experiments are done in a<br>Pentium 4 3.00GHz machine with 1G memory.</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:81"><nobr><span class="ft2">5.1</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:121"><nobr><span class="ft2">Synthetic Data Set</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:94"><nobr><span class="ft1">We generate two-dimensional data for the convenience</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:81"><nobr><span class="ft7">of observation. Let x is a random variable uniformly dis-<br>tributed in [0, ]. The data are generated by</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:86"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:97"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:752;left:107"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:117"><nobr><span class="ft1">{(x, y)|y =sin(x)-+0.7×[rand(0, 1)-1], x  [0, ]}</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:86"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:97"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:107"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:769;left:117"><nobr><span class="ft1">{(x, y)|y =- sin(x)+1+0.7×rand(0, 1), x  [0, ]},</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:81"><nobr><span class="ft7">where rand(0, 1) generates the random numbers uniformly<br>distributed between 0 and 1, and  is a parameter control-<br>ling the overlapping ratio of the two classes. Fig. 4 and<br>Fig. 5 show some examples of the synthetic data. We use<br>the linear kernel function in all the experiments on synthetic<br>data.</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:85"><nobr><span class="ft23"><i>5.1.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:900;left:134"><nobr><span class="ft23"><i>The Effects of Scale</i></span></nobr></DIV>
<DIV style="position:absolute;top:921;left:94"><nobr><span class="ft1">We generate two types of balanced data, i.e., n</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:387"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:402"><nobr><span class="ft1">= n</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:426"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:436"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:81"><nobr><span class="ft1">but one (</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:140"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:150"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:937;left:163"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:933;left:180"><nobr><span class="ft1">D( = 1.5)) is linearly separable and the</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:81"><nobr><span class="ft7">Table 1: The effects of scale and overlapping on the<br>time costs of training SVMs (in seconds).</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:91"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:100"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:112"><nobr><span class="ft1">+ n</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:134"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:156"><nobr><span class="ft1">200 2000 4000 8000 20000 40000</span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:394"><nobr><span class="ft1">80000</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:89"><nobr><span class="ft1">time(</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:123"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:134"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:140"><nobr><span class="ft1">) 0.01 0.03</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:226"><nobr><span class="ft1">0.04</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:264"><nobr><span class="ft1">0.07</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:307"><nobr><span class="ft1">0.23</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:352"><nobr><span class="ft1">0.63</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:399"><nobr><span class="ft1">1.32</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:89"><nobr><span class="ft1">time(</span></nobr></DIV>
<DIV style="position:absolute;top:1051;left:123"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:134"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:140"><nobr><span class="ft1">) 0.02 0.70</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:226"><nobr><span class="ft1">3.24 14.01 58.51 201.07 840.60</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:490"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:510"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:535"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:555"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:580"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:600"><nobr><span class="ft24">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:624"><nobr><span class="ft24">3</span></nobr></DIV>
<DIV style="position:absolute;top:205;left:644"><nobr><span class="ft24">3.5</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:482"><nobr><span class="ft24">-2.5</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:486"><nobr><span class="ft24">-2</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:482"><nobr><span class="ft24">-1.5</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:486"><nobr><span class="ft24">-1</span></nobr></DIV>
<DIV style="position:absolute;top:146;left:482"><nobr><span class="ft24">-0.5</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:487"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:483"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:105;left:487"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:483"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:78;left:487"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:567"><nobr><span class="ft24">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:96;left:567"><nobr><span class="ft24">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:668"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:688"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:713"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:733"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:757"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:777"><nobr><span class="ft24">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:801"><nobr><span class="ft24">3</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:821"><nobr><span class="ft24">3.5</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:660"><nobr><span class="ft24">-1.5</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:664"><nobr><span class="ft24">-1</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:660"><nobr><span class="ft24">-0.5</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:666"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:662"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:666"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:662"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:78;left:666"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:91;left:732"><nobr><span class="ft24">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:732"><nobr><span class="ft24">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:557"><nobr><span class="ft3">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:734"><nobr><span class="ft25"><b>(b)</b></span></nobr></DIV>
<DIV style="position:absolute;top:231;left:475"><nobr><span class="ft7">Figure 4: (a) example of non-overlapped balanced<br>data sets, (b) example of overlapped balanced data<br>sets.</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:490"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:510"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:535"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:555"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:579"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:599"><nobr><span class="ft24">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:623"><nobr><span class="ft24">3</span></nobr></DIV>
<DIV style="position:absolute;top:421;left:643"><nobr><span class="ft24">3.5</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:482"><nobr><span class="ft24">-2.5</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:486"><nobr><span class="ft24">-2</span></nobr></DIV>
<DIV style="position:absolute;top:389;left:482"><nobr><span class="ft24">-1.5</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:486"><nobr><span class="ft24">-1</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:482"><nobr><span class="ft24">-0.5</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:487"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:335;left:483"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:487"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:483"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:487"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:561"><nobr><span class="ft24">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:561"><nobr><span class="ft24">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:666"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:686"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:711"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:731"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:756"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:776"><nobr><span class="ft24">2.5</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:800"><nobr><span class="ft24">3</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:821"><nobr><span class="ft24">3.5</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:658"><nobr><span class="ft24">-1.5</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:662"><nobr><span class="ft24">-1</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:658"><nobr><span class="ft24">-0.5</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:663"><nobr><span class="ft24">0</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:659"><nobr><span class="ft24">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:663"><nobr><span class="ft24">1</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:659"><nobr><span class="ft24">1.5</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:663"><nobr><span class="ft24">2</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:738"><nobr><span class="ft24">Positive class</span></nobr></DIV>
<DIV style="position:absolute;top:314;left:738"><nobr><span class="ft24">Negative class</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:557"><nobr><span class="ft3">(a)</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:734"><nobr><span class="ft25"><b>(b)</b></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:475"><nobr><span class="ft7">Figure 5: (a) example of non-overlapped imbalanced<br>data sets, (b) example of overlapped imbalanced<br>data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:475"><nobr><span class="ft1">other (</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:518"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:529"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:542"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:559"><nobr><span class="ft1">D( = 0.6)) is not, as shown in Fig.4. We</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:475"><nobr><span class="ft1">observe the difference of the behaviors of time costs for</span></nobr></DIV>
<DIV style="position:absolute;top:532;left:817"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:538;left:828"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:475"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:502"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:554;left:512"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:551;left:523"><nobr><span class="ft1">when the scale increases. With the same parameter</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:475"><nobr><span class="ft1">settings, the time costs of optimizing the objective for</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:817"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:828"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:475"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:503"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:585;left:514"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:526"><nobr><span class="ft1">are shown in Table 1, from which we can get two</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:475"><nobr><span class="ft7">conclusions, (a) time costs increase with the scale, and (b)<br>in the same scale, the linearly non-separable data will cost<br>more time to converge.</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:479"><nobr><span class="ft23"><i>5.1.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:656;left:529"><nobr><span class="ft23"><i>The Effects of Imbalance</i></span></nobr></DIV>
<DIV style="position:absolute;top:677;left:489"><nobr><span class="ft1">We generate two types of imbalanced data, i.e., n</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:782"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:812"><nobr><span class="ft1">n</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:821"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:677;left:830"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:475"><nobr><span class="ft1">but one (</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:534"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:545"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:693;left:557"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:575"><nobr><span class="ft1">D( = 1.5)) is linearly separable and the</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:475"><nobr><span class="ft1">other (</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:518"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:529"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:542"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:559"><nobr><span class="ft1">D( = 0.6)) is not, as shown in Fig.5. We</span></nobr></DIV>
<DIV style="position:absolute;top:724;left:475"><nobr><span class="ft7">observe the difference of the effects of imbalance for linearly<br>separable data</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:570"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:580"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:592"><nobr><span class="ft1">and linearly non-separable</span></nobr></DIV>
<DIV style="position:absolute;top:737;left:759"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:770"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:776"><nobr><span class="ft1">. For the</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:475"><nobr><span class="ft7">space limitation, we will not describe the detailed results<br>here but only present the major conclusions. For linearly<br>separable data, SVMs can find the non-skew hyperplane if</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:475"><nobr><span class="ft1">C is not too small. In this situation, tuning</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:742"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:751"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:742"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:750"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:765"><nobr><span class="ft1">is meaning-</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:475"><nobr><span class="ft7">less. For linearly non-separable data, the boundary will be<br>skew to positive class if C</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:635"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:648"><nobr><span class="ft1">= C</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:674"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:684"><nobr><span class="ft1">. In this case, increasing</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:477"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:848;left:486"><nobr><span class="ft13">+</span></nobr></DIV>
<DIV style="position:absolute;top:861;left:477"><nobr><span class="ft11">C</span></nobr></DIV>
<DIV style="position:absolute;top:860;left:485"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:855;left:502"><nobr><span class="ft1">dose "push" the skewed separating hyperplane to the</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:475"><nobr><span class="ft1">negative class. For both</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:629"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:640"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:651"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:868;left:679"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:689"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:696"><nobr><span class="ft1">, if the C is too small,</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:475"><nobr><span class="ft7">underfitting occurs, that is, the SVMs simply classify all the<br>samples into negative class.</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:475"><nobr><span class="ft2">5.2</span></nobr></DIV>
<DIV style="position:absolute;top:927;left:515"><nobr><span class="ft2">TRECVID Data Set</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:479"><nobr><span class="ft23"><i>5.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:960;left:529"><nobr><span class="ft23"><i>Experimental Setup</i></span></nobr></DIV>
<DIV style="position:absolute;top:981;left:489"><nobr><span class="ft1">In this section, we evaluate the proposed approach on the</span></nobr></DIV>
<DIV style="position:absolute;top:997;left:475"><nobr><span class="ft7">high level feature extraction task of TRECVID [1]. Four<br>concepts, including "car","maps","sports" and "waterscape",<br>are chosen to model from the data sets. The development<br>data of TRECVID 2005 are employed and divided into train-<br>ing set and validation set in equal size. The detailed statis-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">447</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1256" src="123008.png" alt="background image">
<DIV style="position:absolute;top:89;left:81"><nobr><span class="ft7">Table 2: The details of the training set and valida-<br>tion set of TRECVID 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:117"><nobr><span class="ft1">Concept</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:217"><nobr><span class="ft1">|D</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:232"><nobr><span class="ft11">train</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:259"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:341"><nobr><span class="ft1">|D</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:355"><nobr><span class="ft11">val</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:372"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:186"><nobr><span class="ft1">Positive Negative Positive Negative</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:131"><nobr><span class="ft1">Car</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:196"><nobr><span class="ft1">1097</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:251"><nobr><span class="ft1">28881</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:314"><nobr><span class="ft1">1097</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:369"><nobr><span class="ft1">28881</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:126"><nobr><span class="ft1">Maps</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:199"><nobr><span class="ft1">296</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:251"><nobr><span class="ft1">30462</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:317"><nobr><span class="ft1">296</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:369"><nobr><span class="ft1">30463</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:123"><nobr><span class="ft1">Sports</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:199"><nobr><span class="ft1">790</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:251"><nobr><span class="ft1">29541</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:317"><nobr><span class="ft1">791</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:369"><nobr><span class="ft1">29542</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:107"><nobr><span class="ft1">Waterscape</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:199"><nobr><span class="ft1">293</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:251"><nobr><span class="ft1">30153</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:317"><nobr><span class="ft1">293</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:369"><nobr><span class="ft1">30154</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:81"><nobr><span class="ft7">tics of the data is summarized in Table 2. In our experi-<br>ments, the global 64-dimension color autocorrelogram fea-<br>ture is used to represent the visual content of each image.<br>Conforming to the convention of TRECVID, average preci-<br>sion (AP) is chosen as the evaluation criterion. Totally five<br>algorithms have been implemented for comparison:</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:98"><nobr><span class="ft1">Whole</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:192"><nobr><span class="ft1">All the negative examples are used</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:91"><nobr><span class="ft1">Random</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:166"><nobr><span class="ft1">Random sampling of the negative examples</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:98"><nobr><span class="ft1">Active</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:172"><nobr><span class="ft1">Active sampling of the negative examples</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:94"><nobr><span class="ft1">SCMs I</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:180"><nobr><span class="ft1">SCMs with k-means in the input space</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:100"><nobr><span class="ft1">SCMs</span></nobr></DIV>
<DIV style="position:absolute;top:416;left:216"><nobr><span class="ft1">SCMs with kernel k-means</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:81"><nobr><span class="ft7">In the Active method, we firstly randomly select a sub-<br>set of negative examples.</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:250"><nobr><span class="ft1">With this initial set, we train</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:81"><nobr><span class="ft7">an SVMs model and use this model to classify the whole<br>training data set. Then the maximally misclassified nega-<br>tive examples are added to the training set. This proce-<br>dure iterates until the ratio between the negative and the<br>positive examples exceeding five. Since both the Random<br>and Active methods depend on the initial random chosen<br>data set, we repeat each of them for ten times and calculate<br>their average performances for comparison. Both SCMs I<br>and SCMs methods adopt the Gaussian kernel during the<br>SVMs classification. The only difference is that SCMs I<br>performs data clustering with k-means in the input space<br>while SCMs with k-means in the feature space.</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:85"><nobr><span class="ft23"><i>5.2.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:673;left:134"><nobr><span class="ft23"><i>Parameter Settings</i></span></nobr></DIV>
<DIV style="position:absolute;top:694;left:94"><nobr><span class="ft1">Currently, the experiments focus on the comparative per-</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:81"><nobr><span class="ft7">formance between the different approaches based on the the<br>same parameter settings. Therefore, some of the parameters<br>are heuristically determined and might not be optimal. The<br>current implementation of SCMs involves the following pa-<br>rameter settings: (a) Gaussian kernel is adopted and the pa-<br>rameters are selected via cross-validation, furthermore, the<br>kernel function of kernel k-means clustering is adopted the<br>same as that of SVMs, (b) the threshold for transforming<br>dense graphs to sparse ones is experimentally determined<br>as</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:109"><nobr><span class="ft1">= 0.6, (c) the parameter of shrinking technique is ex-</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:81"><nobr><span class="ft7">perimentally chosen as  = 1.3, (d) for SCMs, the data are<br>imbalanced for each concept, we only carry out data clus-<br>tering for negative classes, therefore, k</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:312"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:898;left:326"><nobr><span class="ft1">always equals</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:411"><nobr><span class="ft1">|D</span></nobr></DIV>
<DIV style="position:absolute;top:893;left:426"><nobr><span class="ft11">+</span></nobr></DIV>
<DIV style="position:absolute;top:895;left:436"><nobr><span class="ft1">|</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:81"><nobr><span class="ft1">and k</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:116"><nobr><span class="ft11">-</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:131"><nobr><span class="ft1">is always chosen as</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:254"><nobr><span class="ft11">|D</span></nobr></DIV>
<DIV style="position:absolute;top:910;left:266"><nobr><span class="ft13">-</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:275"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:924;left:261"><nobr><span class="ft11">10</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:280"><nobr><span class="ft1">, (e) we stop the iteration</span></nobr></DIV>
<DIV style="position:absolute;top:934;left:81"><nobr><span class="ft7">of SCMs when the number of the negative examples are not<br>more than the five times of that of the positive examples.</span></nobr></DIV>
<DIV style="position:absolute;top:975;left:85"><nobr><span class="ft23"><i>5.2.3</i></span></nobr></DIV>
<DIV style="position:absolute;top:975;left:134"><nobr><span class="ft23"><i>Experiment Results</i></span></nobr></DIV>
<DIV style="position:absolute;top:997;left:94"><nobr><span class="ft1">The average performance and time costs of the various</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:81"><nobr><span class="ft1">approaches are in Table 3 and Table 4 respectively.</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:420"><nobr><span class="ft1">We</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:81"><nobr><span class="ft7">can see that both the Random and Active methods use<br>fewer time than the others, but their performances are not<br>as good as the others. Furthermore, the SCMs achieves</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:475"><nobr><span class="ft7">Table 3: The average performance of the approaches<br>on the chosen concepts, measured by average preci-<br>sion.</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:490"><nobr><span class="ft1">Concept</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:558"><nobr><span class="ft1">Whole Random Active SCMs I SCMs</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:503"><nobr><span class="ft1">Car</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:563"><nobr><span class="ft1">0.196</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:626"><nobr><span class="ft1">0.127</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:687"><nobr><span class="ft1">0.150</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:745"><nobr><span class="ft1">0.161</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:801"><nobr><span class="ft1">0.192</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:498"><nobr><span class="ft1">Maps</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:563"><nobr><span class="ft1">0.363</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:626"><nobr><span class="ft1">0.274</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:687"><nobr><span class="ft1">0.311</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:745"><nobr><span class="ft1">0.305</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:801"><nobr><span class="ft1">0.353</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:495"><nobr><span class="ft1">Sports</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:565"><nobr><span class="ft1">0.281</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:626"><nobr><span class="ft1">0.216</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:687"><nobr><span class="ft1">0.253</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:745"><nobr><span class="ft1">0.260</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:799"><nobr><span class="ft1">0.283</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:480"><nobr><span class="ft1">Waterscape</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:563"><nobr><span class="ft1">0.269</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:626"><nobr><span class="ft1">0.143</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:687"><nobr><span class="ft1">0.232</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:745"><nobr><span class="ft1">0.241</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:801"><nobr><span class="ft1">0.261</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:475"><nobr><span class="ft7">Table 4: The average time costs of the approaches<br>on the chosen concepts (in seconds).</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:490"><nobr><span class="ft1">Concept</span></nobr></DIV>
<DIV style="position:absolute;top:275;left:558"><nobr><span class="ft1">Whole Random Active SCMs I SCMs</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:503"><nobr><span class="ft1">Car</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:562"><nobr><span class="ft1">4000.2</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:626"><nobr><span class="ft1">431.0</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:683"><nobr><span class="ft1">1324.6</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:741"><nobr><span class="ft1">1832.0</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:798"><nobr><span class="ft1">2103.4</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:498"><nobr><span class="ft1">Maps</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:565"><nobr><span class="ft1">402.6</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:629"><nobr><span class="ft1">35.2</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:687"><nobr><span class="ft1">164.8</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:745"><nobr><span class="ft1">234.3</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:801"><nobr><span class="ft1">308.5</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:495"><nobr><span class="ft1">Sports</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:562"><nobr><span class="ft1">1384.5</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:626"><nobr><span class="ft1">125.4</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:687"><nobr><span class="ft1">523.8</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:745"><nobr><span class="ft1">732.5</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:801"><nobr><span class="ft1">812.7</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:480"><nobr><span class="ft1">Waterscape</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:565"><nobr><span class="ft1">932.4</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:629"><nobr><span class="ft1">80.1</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:687"><nobr><span class="ft1">400.3</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:745"><nobr><span class="ft1">504.0</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:801"><nobr><span class="ft1">621.3</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:475"><nobr><span class="ft7">the comparable performance with that of Whole while uses<br>fewer time costs. Note that SCMs I also achieves satisfying<br>results. This might be due to the Gaussian kernels, in which</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:475"><nobr><span class="ft1">e</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:482"><nobr><span class="ft11">- x-y</span></nobr></DIV>
<DIV style="position:absolute;top:426;left:523"><nobr><span class="ft13">2</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:529"><nobr><span class="ft1">is monotonic with x</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:656"><nobr><span class="ft1">-y</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:683"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:433;left:689"><nobr><span class="ft1">. Therefore, the order of</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:475"><nobr><span class="ft7">the pair-wise distances is the same for both the input space<br>and feature space, which perhaps leads to similar clustering<br>results.</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:475"><nobr><span class="ft2">6.</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:507"><nobr><span class="ft2">CONCLUSIONS</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:489"><nobr><span class="ft1">In this paper, we have investigated the effects of scale and</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:475"><nobr><span class="ft7">imbalance on SVMs. We highlight the role of data over-<br>lapping in this problem and find that SVMs has no diffi-<br>culties with linear separable large scale imbalanced data.<br>We propose a meta-algorithm named Support Cluster Ma-<br>chines (SCMs) for effectively learning from large scale and<br>imbalanced data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:628;left:623"><nobr><span class="ft1">Different from the previous work,</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft7">we develop the theoretical justifications for the idea and<br>choose the technical component guided by the theoretical<br>results. Finally, experiments on both the synthetic and the<br>TRECVID data are carried out. The results support the<br>previous analysis and show that the SCMs are efficient and<br>effective while dealing with large scale imbalanced data sets.</span></nobr></DIV>
<DIV style="position:absolute;top:738;left:489"><nobr><span class="ft1">However, as a pilot study, there is still some room for im-</span></nobr></DIV>
<DIV style="position:absolute;top:753;left:475"><nobr><span class="ft7">provement. Firstly, we have not incorporated the caching<br>techniques to avoid the kernel reevaluations. Therefore, we<br>have to recalculate the dot product on line whenever it is re-<br>quired. Secondly, the parameters within the algorithms are<br>currently selected heuristically, which depend on the trade-<br>off of efficiency and accuracy.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:475"><nobr><span class="ft2">7.</span></nobr></DIV>
<DIV style="position:absolute;top:863;left:507"><nobr><span class="ft2">ACKNOWLEDGMENTS</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:489"><nobr><span class="ft1">We would like to thank the anonymous reviewers for their</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:475"><nobr><span class="ft7">insightful suggestions. We also thank Dr. Chih-Jen Lin for<br>the code of libSVM, Brian J. Kulis for the code of graclus<br>and National Institute of Standards and Technology for pro-<br>viding the TRECVID data sets. Finally, special thanks go<br>to Dr. Ya-xiang Yuan for his helpful discussions on opti-<br>mization theory.</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:475"><nobr><span class="ft2">8.</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:507"><nobr><span class="ft2">REFERENCES</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:482"><nobr><span class="ft1">[1] TREC Video Retrieval. National Institute of</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:503"><nobr><span class="ft7">Standards and Technology,<br>http://www-nlpir.nist.gov/projects/trecvid/.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">448</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1256" src="123009.png" alt="background image">
<DIV style="position:absolute;top:79;left:88"><nobr><span class="ft1">[2] R. Akbani, S. Kwek, and N. Japkowicz. Applying</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:109"><nobr><span class="ft7">Support Vector Machines to Imbalanced Datasets. In<br>Proceedings of ECML'04, pages 39­50, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:88"><nobr><span class="ft1">[3] D. Boley and D. Cao. Training Support Vector</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:109"><nobr><span class="ft7">Machine using Adaptive Clustering. In Proceeding of<br>2004 SIAM International Conference on Data Mining,<br>April 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:88"><nobr><span class="ft1">[4] S. Boyd and L. Vandenberghe. Convex Optimization.</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:109"><nobr><span class="ft7">Cambridge University Press, New York, NY, USA,<br>2004.</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:88"><nobr><span class="ft1">[5] K. Brinker. Incorporating Diversity in Active Learning</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:109"><nobr><span class="ft7">with Support Vector Machines. In Proceedings of<br>ICML'03, pages 59­66, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:88"><nobr><span class="ft1">[6] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial:</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:109"><nobr><span class="ft7">Special Issue on Learning from Imbalanced Data Sets.<br>SIGKDD Explor. Newsl., 6(1):1­6, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:338;left:88"><nobr><span class="ft1">[7] J. W. Daniel. Stability of the Solution of Definite</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:109"><nobr><span class="ft7">Quadratic Programs. Mathematical Programming,<br>5(1):41­53, December 1973.</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:88"><nobr><span class="ft1">[8] R. Datta, J. Li, and J. Z. Wang. Content-based Image</span></nobr></DIV>
<DIV style="position:absolute;top:402;left:109"><nobr><span class="ft7">Retrieval: Approaches and Trends of the New Age. In<br>Proceedings of ACM SIGMM workshop on MIR'05,<br>pages 253­262, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:88"><nobr><span class="ft1">[9] I. Dhillon, Y. Guan, and B. Kulis. A Fast</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:109"><nobr><span class="ft7">Kernel-based Multilevel Algorithm for Graph<br>Clustering. In Proceeding of ACM SIGKDD'05, pages<br>629­634, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:81"><nobr><span class="ft1">[10] I. S. Dhillon, Y. Guan, and B. Kulis. A Unified View</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:109"><nobr><span class="ft7">of Graph Partitioning and Weighted Kernel k-means.<br>Technical Report TR-04-25, The University of Texas<br>at Austin, Department of Computer Sciences, June<br>2004.</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:81"><nobr><span class="ft1">[11] C. Ding and X. He. K-means clustering via principal</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:109"><nobr><span class="ft7">component analysis. In Proceedings of ICML'04, pages<br>29­36, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:81"><nobr><span class="ft1">[12] K.-S. Goh, E. Y. Chang, and W.-C. Lai. Multimodal</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:109"><nobr><span class="ft7">Concept-dependent Active Learning for Image<br>Retrieval. In Proceedings of ACM MM'04, pages<br>564­571, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:81"><nobr><span class="ft1">[13] A. G. Hauptmann. Towards a Large Scale Concept</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:109"><nobr><span class="ft7">Ontology for Broadcast Video. In Proceedings of<br>CIVR'04, pages 674­675, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:81"><nobr><span class="ft1">[14] A. G. Hauptmann. Lessons for the Future from a</span></nobr></DIV>
<DIV style="position:absolute;top:772;left:109"><nobr><span class="ft7">Decade of Informedia Video Analysis Research. In<br>Proceedings of CIVR'05, pages 1­10, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:81"><nobr><span class="ft1">[15] C.-W. Hsu, C.-C. Chang, and C.-J. Lin. A Practical</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:109"><nobr><span class="ft7">Guide to Support Vector Classification. 2005. available<br>at http://www.csie.ntu.edu.tw/~cjlin/libsvm/.</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:81"><nobr><span class="ft1">[16] T. Joachims. Making Large-scale Support Vector</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:109"><nobr><span class="ft7">Machine Learning Practical. Advances in kernel<br>methods: support vector learning, pages 169­184, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:902;left:81"><nobr><span class="ft1">[17] Y. Lin, Y. Lee, and G. Wahba. Support Vector</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:109"><nobr><span class="ft7">Machines for Classification in Nonstandard Situations.<br>Machine Learning, 46(1-3):191­202, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:81"><nobr><span class="ft1">[18] L. M. Manevitz and M. Yousef. One-class SVMs for</span></nobr></DIV>
<DIV style="position:absolute;top:966;left:109"><nobr><span class="ft7">Document Classification. Journal of Machine Learning<br>Research, 2:139­154, 2002.</span></nobr></DIV>
<DIV style="position:absolute;top:999;left:81"><nobr><span class="ft1">[19] H. T. Nguyen and A. Smeulders. Active Learning</span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:109"><nobr><span class="ft7">Using Pre-clustering. In Proceedings of ICML'04,<br>pages 79­86, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:81"><nobr><span class="ft1">[20] E. Osuna, R. Freund, and F. Girosi. An Improved</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:503"><nobr><span class="ft7">Training Algorithm for Support Vector Machines. In<br>IEEE Workshop on Neural Networks and Signal<br>Processing, September 1997.</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:475"><nobr><span class="ft1">[21] D. Pavlov, J. Mao, and B. Dom. Scaling-Up Support</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:503"><nobr><span class="ft7">Vector Machines Using Boosting Algorithm. In<br>Proceeding of ICPR'00, volume 2, pages 2219­2222,<br>2000.</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:475"><nobr><span class="ft1">[22] J. C. Platt. Fast Training of Support Vector Machines</span></nobr></DIV>
<DIV style="position:absolute;top:208;left:503"><nobr><span class="ft7">using Sequential Minimal Optimization. Advances in<br>kernel methods: support vector learning, pages<br>185­208, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:475"><nobr><span class="ft1">[23] R. C. Prati, G. E. A. P. A. Batista, and M. C.</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:503"><nobr><span class="ft7">Monard. Class Imbalances versus Class Overlapping:<br>an Analysis of a Learning System Behavior. In<br>Proceedings of the MICAI 2004, pages 312­321, 2004.</span></nobr></DIV>
<DIV style="position:absolute;top:321;left:475"><nobr><span class="ft1">[24] G. Schohn and D. Cohn. Less is More: Active</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:503"><nobr><span class="ft7">Learning with Support Vector Machines. In<br>Proceddings of ICML'00, pages 839­846, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:475"><nobr><span class="ft1">[25] A. J. Smola and B. Sch¨</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:646"><nobr><span class="ft1">okopf. Sparse Greedy Matrix</span></nobr></DIV>
<DIV style="position:absolute;top:385;left:503"><nobr><span class="ft7">Approximation for Machine Learning. In Proceedings<br>of ICML'00, pages 911­918, 2000.</span></nobr></DIV>
<DIV style="position:absolute;top:418;left:475"><nobr><span class="ft1">[26] S. Tong and E. Chang. Support Vector Machine</span></nobr></DIV>
<DIV style="position:absolute;top:434;left:503"><nobr><span class="ft7">Active Learning for Image Retrieval. In Proceedings of<br>ACM MM'01, pages 107­118, 2001.</span></nobr></DIV>
<DIV style="position:absolute;top:466;left:475"><nobr><span class="ft1">[27] K. Veropoulos, N. Cristianini, and C. Campbell.</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:503"><nobr><span class="ft7">Controlling the Sensitivity of Support Vector<br>Machines. In Proceedings of IJCAI'99, 1999.</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:475"><nobr><span class="ft1">[28] G. M. Weiss and F. J. Provost. Learning When</span></nobr></DIV>
<DIV style="position:absolute;top:531;left:503"><nobr><span class="ft7">Training Data are Costly: The Effect of Class<br>Distribution on Tree Induction. Journal of Artificial<br>Intelligence Research (JAIR), 19:315­354, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:579;left:475"><nobr><span class="ft1">[29] G. Wu and E. Y. Chang. KBA: Kernel Boundary</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:503"><nobr><span class="ft7">Alignment Considering Imbalanced Data Distribution.<br>IEEE Transactions on Knowledge and Data<br>Engineering, 17(6):786­795, 2005.</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft1">[30] Z. Xu, K. Yu, V. Tresp, X. Xu, and J. Wang.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:503"><nobr><span class="ft7">Representative Sampling for Text Classification Using<br>Support Vector Machines. In Proceedings of ECIR'03,<br>pages 393­407, 2003.</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:475"><nobr><span class="ft1">[31] H. Yu, J. Yang, J. Han, and X. Li. Making SVMs</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:503"><nobr><span class="ft7">Scalable to Large Data Sets using Hierarchical Cluster<br>Indexing. Data Min. Knowl. Discov., 11(3):295­321,<br>2005.</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:475"><nobr><span class="ft2">APPENDIX</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:475"><nobr><span class="ft2">A.</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:511"><nobr><span class="ft2">PROOF OF THEOREM 2</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:489"><nobr><span class="ft1">Firstly, we define ^</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:592"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:839;left:601"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:837;left:612"><nobr><span class="ft1">which satisfies ^</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:699"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:839;left:708"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:724"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:739"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:754"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:760"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:841;left:765"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:779"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:788"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:832;left:796"><nobr><span class="ft17"><br>i</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:803"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:815"><nobr><span class="ft1">c =</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:475"><nobr><span class="ft1">1, . . . , k. It is easy to verify that ^</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:675"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:854;left:684"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:852;left:696"><nobr><span class="ft1">is a feasible solution of</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:475"><nobr><span class="ft1">SCMs. Secondly, we define </span></nobr></DIV>
<DIV style="position:absolute;top:871;left:649"><nobr><span class="ft1"> satisfying </span></nobr></DIV>
<DIV style="position:absolute;top:871;left:726"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:873;left:735"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:746"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:864;left:764"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:862;left:771"><nobr><span class="ft13"></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:771"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:764"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:879;left:774"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:780"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:791"><nobr><span class="ft1">if x</span></nobr></DIV>
<DIV style="position:absolute;top:873;left:814"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:825"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:888;left:475"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:891;left:483"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:489"><nobr><span class="ft1">, i = 1, . . . , n. It is easy to verify that </span></nobr></DIV>
<DIV style="position:absolute;top:888;left:718"><nobr><span class="ft1"> is a feasible solu-</span></nobr></DIV>
<DIV style="position:absolute;top:904;left:475"><nobr><span class="ft1">tion of DSVMs. According to the relation of</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:749"><nobr><span class="ft1">D</span></nobr></DIV>
<DIV style="position:absolute;top:906;left:760"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:904;left:772"><nobr><span class="ft1">and ~</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:799"><nobr><span class="ft1">D, we</span></nobr></DIV>
<DIV style="position:absolute;top:920;left:475"><nobr><span class="ft1">can obtain the following equation</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:530"><nobr><span class="ft14">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:547"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:542"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:570"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:564"><nobr><span class="ft11">j=1</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:585"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:594"><nobr><span class="ft17"><br>i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:601"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:608"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:618"><nobr><span class="ft1">(~</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:633"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:641"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:646"><nobr><span class="ft1">), (~</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:673"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:681"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:687"><nobr><span class="ft1">) </span></nobr></DIV>
<DIV style="position:absolute;top:949;left:707"><nobr><span class="ft17"><br>j</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:713"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:720"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:951;left:729"><nobr><span class="ft1">-</span></nobr></DIV>
<DIV style="position:absolute;top:939;left:750"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:744"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:765"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:774"><nobr><span class="ft17"><br>i</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:784"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:530"><nobr><span class="ft14">1<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:548"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:541"><nobr><span class="ft11">h=1</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:570"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:565"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:587"><nobr><span class="ft1">^</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:586"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:595"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:609"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:616"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:628"><nobr><span class="ft1">(u</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:652"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:660"><nobr><span class="ft1">), (u</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:695"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:700"><nobr><span class="ft1">) ^</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:710"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:719"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:730"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:737"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:745"><nobr><span class="ft1">-</span></nobr></DIV>
<DIV style="position:absolute;top:988;left:766"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:759"><nobr><span class="ft11">h=1</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:783"><nobr><span class="ft1">^</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:781"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:790"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:804"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:475"><nobr><span class="ft1">which means ~</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:554"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:579"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:586"><nobr><span class="ft1">) = G</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:620"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:628"><nobr><span class="ft1">( ^</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:633"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:642"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:650"><nobr><span class="ft1">). Similarly, we can get ~</span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:789"><nobr><span class="ft1">G( </span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:805"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:475"><nobr><span class="ft1">G</span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:486"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:494"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:508"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:516"><nobr><span class="ft1">). Using the fact that </span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:655"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:666"><nobr><span class="ft1">and </span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:701"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:711"><nobr><span class="ft1">are the optimal solu-</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">449</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1256;">
<STYLE type="text/css">
<!--
	.ft26{font-size:6px;line-height:7px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1256" src="123010.png" alt="background image">
<DIV style="position:absolute;top:79;left:81"><nobr><span class="ft1">tions to SCMs and DSVMs respectively, we have G</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:390"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:79;left:398"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:75;left:412"><nobr><span class="ft26"><br></span></nobr></DIV>
<DIV style="position:absolute;top:79;left:419"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:76;left:429"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:81"><nobr><span class="ft1">G</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:91"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:99"><nobr><span class="ft1">( ^</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:105"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:99;left:113"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:121"><nobr><span class="ft1">) and ~</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:159"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:92;left:184"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:190"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:93;left:200"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:216"><nobr><span class="ft1">G( </span></nobr></DIV>
<DIV style="position:absolute;top:97;left:232"><nobr><span class="ft1">). Thus, the equation G</span></nobr></DIV>
<DIV style="position:absolute;top:99;left:389"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:397"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:92;left:411"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:419"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:84"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:81"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:106"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:114;left:112"><nobr><span class="ft1">) holds. For any </span></nobr></DIV>
<DIV style="position:absolute;top:116;left:218"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:110;left:230"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:255"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:265"><nobr><span class="ft1">satisfying</span></nobr></DIV>
<DIV style="position:absolute;top:110;left:325"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:116;left:340"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:357"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:113;left:372"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:386"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:393"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:397"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:122;left:411"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:420"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:109;left:429"><nobr><span class="ft17"><br>i</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:435"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:128;left:86"><nobr><span class="ft1">c = 1, . . . , k}, we know it is a feasible solution to SCMs</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:81"><nobr><span class="ft1">and G</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:119"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:127"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:141"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:149"><nobr><span class="ft1">) = ~</span></nobr></DIV>
<DIV style="position:absolute;top:149;left:175"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:200"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:206"><nobr><span class="ft1">) = G</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:244"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:251"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:144;left:266"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:273"><nobr><span class="ft1">) holds, which means </span></nobr></DIV>
<DIV style="position:absolute;top:151;left:417"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:149;left:430"><nobr><span class="ft1">is</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:81"><nobr><span class="ft1">the optimal solution of SCMs. Similarly, for any </span></nobr></DIV>
<DIV style="position:absolute;top:161;left:405"><nobr><span class="ft1"> R</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:432"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:81"><nobr><span class="ft1">satisfying</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:144"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:151"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:165"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:188;left:172"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:177"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:188;left:191"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:199"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:208"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:219"><nobr><span class="ft1">= </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:246"><nobr><span class="ft17"><br>c</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:258"><nobr><span class="ft1">,</span></nobr></DIV>
<DIV style="position:absolute;top:177;left:264"><nobr><span class="ft1">c = 1, . . . , k} and the con-</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:81"><nobr><span class="ft1">straints of (7), we have ~</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:233"><nobr><span class="ft1">G() = G</span></nobr></DIV>
<DIV style="position:absolute;top:202;left:298"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:306"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:320"><nobr><span class="ft17"><br></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:328"><nobr><span class="ft1">) = ~</span></nobr></DIV>
<DIV style="position:absolute;top:200;left:357"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:382"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:389"><nobr><span class="ft1">), which</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:81"><nobr><span class="ft1">means  is the optimal solution of DSVMs.</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:81"><nobr><span class="ft2">B.</span></nobr></DIV>
<DIV style="position:absolute;top:249;left:115"><nobr><span class="ft2">PROOF OF THEOREM 3</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:94"><nobr><span class="ft1">Note that the feasible regions of (2) and (7) are the same.</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:81"><nobr><span class="ft1">By the fact that </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:191"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:287;left:202"><nobr><span class="ft1">and ~</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:228"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:282;left:237"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:287;left:248"><nobr><span class="ft1">are optimal solutions to (2) and</span></nobr></DIV>
<DIV style="position:absolute;top:303;left:81"><nobr><span class="ft1">(7) respectively, we know that</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:190"><nobr><span class="ft1">( ~</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:196"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:323;left:205"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:326;left:214"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:323;left:237"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:329;left:243"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:249"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:258"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:294"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:329;left:301"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:309"><nobr><span class="ft1"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:329;left:415"><nobr><span class="ft1">(12)</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:190"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:205"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:348;left:214"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:237"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:243"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:249"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:258"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:269"><nobr><span class="ft1">G( ~</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:285"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:294"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:301"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:309"><nobr><span class="ft1"> 0</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:415"><nobr><span class="ft1">(13)</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:81"><nobr><span class="ft1">hold, where the gradients</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:132"><nobr><span class="ft1">G() = Q - e and  ~</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:290"><nobr><span class="ft1">G() = ~</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:339"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:363"><nobr><span class="ft1">- e.</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:415"><nobr><span class="ft1">(14)</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:81"><nobr><span class="ft1">Adding (12) and (13) and then a little arrangement yields</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:81"><nobr><span class="ft1">( ~</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:86"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:95"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:101"><nobr><span class="ft1">-</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:121"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:127"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:133"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:142"><nobr><span class="ft1">[</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:145"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:157"><nobr><span class="ft1">G( ~</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:173"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:182"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:188"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:194"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:216"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:241"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:248"><nobr><span class="ft1">)]</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:261"><nobr><span class="ft1"> (~</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:290"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:296"><nobr><span class="ft1">-</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:316"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:322"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:328"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:336"><nobr><span class="ft1">[</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:340"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:377"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:383"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:389"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:452;left:411"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:436"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:442"><nobr><span class="ft1">)].</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:81"><nobr><span class="ft1">Substituting (14) in the above inequality, we get</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:101"><nobr><span class="ft1">( ~</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:106"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:115"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:500;left:125"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:147"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:503;left:154"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:159"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:170"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:168"><nobr><span class="ft1">Q( ~</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:185"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:194"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:500;left:204"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:226"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:503;left:233"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:242"><nobr><span class="ft1"> (~</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:271"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:500;left:280"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:303"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:503;left:310"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:315"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:324"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:500;left:344"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:358"><nobr><span class="ft1">Q)</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:384"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:503;left:391"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:503;left:415"><nobr><span class="ft1">(15)</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:81"><nobr><span class="ft1">Adding ( ~</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:136"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:145"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:526;left:155"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:525;left:178"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:530;left:185"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:190"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:199"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:526;left:220"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:235"><nobr><span class="ft1">Q)( ~</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:257"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:266"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:526;left:276"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:525;left:300"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:530;left:306"><nobr><span class="ft1">) to the both sides of</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:81"><nobr><span class="ft1">(15), we have</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:101"><nobr><span class="ft1">( ~</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:106"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:566;left:115"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:125"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:147"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:572;left:154"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:159"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:168"><nobr><span class="ft1">Q( ~</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:185"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:566;left:194"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:204"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:226"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:572;left:233"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:242"><nobr><span class="ft1"> (~</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:271"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:280"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:303"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:572;left:310"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:315"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:324"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:344"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:358"><nobr><span class="ft1">Q) ~</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:375"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:566;left:384"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:572;left:391"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:415"><nobr><span class="ft1">(16)</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:81"><nobr><span class="ft1">If  &gt; 0 is the smallest eigenvalue of Q, we have</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:169"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:184"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:616;left:193"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:202"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:616;left:225"><nobr><span class="ft11"> 2</span></nobr></DIV>
<DIV style="position:absolute;top:618;left:249"><nobr><span class="ft1"> (~</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:278"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:288"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:616;left:311"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:621;left:317"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:323"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:331"><nobr><span class="ft1">Q( ~</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:349"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:616;left:357"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:367"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:616;left:390"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:621;left:396"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:111"><nobr><span class="ft1">( ~</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:116"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:638;left:125"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:640;left:134"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:157"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:643;left:163"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:169"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:178"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:198"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:212"><nobr><span class="ft1">Q) ~</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:229"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:638;left:238"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:640;left:249"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:280"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:640;left:290"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:312"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:643;left:332"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:348"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:361"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:388"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:643;left:387"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:638;left:396"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:668;left:81"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:118"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:116"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:663;left:125"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:665;left:145"><nobr><span class="ft1"> ~</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:163"><nobr><span class="ft1">mC. Using (16) we get</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:328"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:327"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:663;left:336"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:665;left:347"><nobr><span class="ft1">- </span></nobr></DIV>
<DIV style="position:absolute;top:663;left:370"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:665;left:391"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:664;left:412"><nobr><span class="ft11">~</span></nobr></DIV>
<DIV style="position:absolute;top:664;left:410"><nobr><span class="ft11">mC</span></nobr></DIV>
<DIV style="position:absolute;top:674;left:419"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:668;left:436"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:81"><nobr><span class="ft1">Now we turn to prove the second result. </span></nobr></DIV>
<DIV style="position:absolute;top:679;left:343"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:684;left:355"><nobr><span class="ft1">is the optimal</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:81"><nobr><span class="ft1">solution of (2), therefore, 0</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:258"><nobr><span class="ft1"> G(~</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:300"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:700;left:307"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:696;left:316"><nobr><span class="ft1">- G(</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:356"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:700;left:363"><nobr><span class="ft1">) is obvious.</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:81"><nobr><span class="ft1">Meanwhile, we have</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:89"><nobr><span class="ft1">G( ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:105"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:114"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:120"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:129"><nobr><span class="ft1">- G(</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:167"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:174"><nobr><span class="ft1">)= 1</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:192"><nobr><span class="ft1">2 ( ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:206"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:215"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:221"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:227"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:235"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:256"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:270"><nobr><span class="ft1">Q) ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:287"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:296"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:305"><nobr><span class="ft1">+ ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:319"><nobr><span class="ft1">G( ~</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:336"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:344"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:351"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:359"><nobr><span class="ft1">- G(</span></nobr></DIV>
<DIV style="position:absolute;top:740;left:398"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:405"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:179"><nobr><span class="ft1"> 12(~</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:215"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:221"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:227"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:235"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:256"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:270"><nobr><span class="ft1">Q) ~</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:287"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:296"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:305"><nobr><span class="ft1">+ ~</span></nobr></DIV>
<DIV style="position:absolute;top:778;left:319"><nobr><span class="ft1">G(</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:344"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:351"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:359"><nobr><span class="ft1">- G(</span></nobr></DIV>
<DIV style="position:absolute;top:773;left:398"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:405"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:179"><nobr><span class="ft1">= 1</span></nobr></DIV>
<DIV style="position:absolute;top:820;left:192"><nobr><span class="ft1">2 ( ~</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:206"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:805;left:215"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:811;left:221"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:227"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:235"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:256"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:270"><nobr><span class="ft1">Q) ~</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:287"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:805;left:296"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:807;left:305"><nobr><span class="ft1">- 12(</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:344"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:811;left:350"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:356"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:365"><nobr><span class="ft1">(Q</span></nobr></DIV>
<DIV style="position:absolute;top:807;left:385"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:399"><nobr><span class="ft1">Q)</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:425"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:179"><nobr><span class="ft1"> 12 Q - ~Q ~</span></nobr></DIV>
<DIV style="position:absolute;top:838;left:271"><nobr><span class="ft11"> 2</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:294"><nobr><span class="ft1">+ 1</span></nobr></DIV>
<DIV style="position:absolute;top:852;left:309"><nobr><span class="ft1">2 Q -</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:356"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:354"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:843;left:379"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:838;left:388"><nobr><span class="ft11"> 2</span></nobr></DIV>
<DIV style="position:absolute;top:875;left:179"><nobr><span class="ft1"> (m</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:210"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:219"><nobr><span class="ft1">+ ~</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:233"><nobr><span class="ft1">m</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:245"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:251"><nobr><span class="ft1">)C</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:267"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:273"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:887;left:232"><nobr><span class="ft1">2</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:81"><nobr><span class="ft2">C.</span></nobr></DIV>
<DIV style="position:absolute;top:936;left:116"><nobr><span class="ft2">PROOF OF THEOREM 4</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:94"><nobr><span class="ft1">Expanding  to be the explicit function of</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:347"><nobr><span class="ft1">{(u</span></nobr></DIV>
<DIV style="position:absolute;top:961;left:378"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:384"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:389"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:954;left:396"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:396"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:959;left:416"><nobr><span class="ft1">, we</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:81"><nobr><span class="ft1">get </span></nobr></DIV>
<DIV style="position:absolute;top:971;left:111"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:122"><nobr><span class="ft1">= YKY</span></nobr></DIV>
<DIV style="position:absolute;top:973;left:185"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:199"><nobr><span class="ft1">Y ~</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:211"><nobr><span class="ft1">K ~</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:224"><nobr><span class="ft1">Y</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:243"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:249"><nobr><span class="ft1">, in which Y and ~</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:361"><nobr><span class="ft1">Y denote di-</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:81"><nobr><span class="ft1">agonal matrices whose diagonal elements are y</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:361"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:368"><nobr><span class="ft1">, . . . , y</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:405"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:417"><nobr><span class="ft1">and</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:82"><nobr><span class="ft1">~</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:81"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:87"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:94"><nobr><span class="ft1">, . . . , ~</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:124"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:131"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:144"><nobr><span class="ft1">respectively. Using the fact that Y equals to ~</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:423"><nobr><span class="ft1">Y,</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:81"><nobr><span class="ft1">we have </span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:142"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:155"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:179"><nobr><span class="ft1">Y(K</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:213"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:228"><nobr><span class="ft1">K)Y</span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:265"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:271"><nobr><span class="ft1">. Since Y only change the</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:81"><nobr><span class="ft1">signs of the elements of K</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:235"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:247"><nobr><span class="ft1">K by Y(K</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:312"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:324"><nobr><span class="ft1">K)Y, we have </span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:419"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:429"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:87"><nobr><span class="ft1">K</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:104"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:118"><nobr><span class="ft1">K</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:137"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:149"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:165"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:180"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:180"><nobr><span class="ft11">h=1</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:203"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:218"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:218"><nobr><span class="ft11">l=1</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:238"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:253"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:259"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:264"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:278"><nobr><span class="ft13">h</span></nobr></DIV>
<DIV style="position:absolute;top:1057;left:288"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:303"><nobr><span class="ft11">x</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:309"><nobr><span class="ft13">j</span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:315"><nobr><span class="ft11"></span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:329"><nobr><span class="ft13">l</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:334"><nobr><span class="ft1">( (x</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:368"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:373"><nobr><span class="ft1">), (x</span></nobr></DIV>
<DIV style="position:absolute;top:1060;left:408"><nobr><span class="ft11">j</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:414"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:1055;left:429"><nobr><span class="ft1">-</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:481"><nobr><span class="ft1">(u</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:505"><nobr><span class="ft11">h</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:512"><nobr><span class="ft1">), (u</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:548"><nobr><span class="ft11">l</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:552"><nobr><span class="ft1">) )</span></nobr></DIV>
<DIV style="position:absolute;top:75;left:568"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:574"><nobr><span class="ft1">. It is a biquadratic function of</span></nobr></DIV>
<DIV style="position:absolute;top:76;left:761"><nobr><span class="ft1">{(u</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:792"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:798"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:76;left:804"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:75;left:810"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:82;left:810"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:79;left:830"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:95;left:475"><nobr><span class="ft7">Therefore, this is an unconstrained convex optimization prob-<br>lem [4]. The necessary and sufficient condition for</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:785"><nobr><span class="ft1">{u</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:801"><nobr><span class="ft17"><br>c</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:807"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:814"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:114;left:814"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:475"><nobr><span class="ft1">to be optimal is</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:577"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:122;left:595"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:601"><nobr><span class="ft1">(</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:607"><nobr><span class="ft1">{(u</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:638"><nobr><span class="ft17"><br>c</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:644"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:650"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:657"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:657"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:126;left:676"><nobr><span class="ft1">) = 0. We can verify that</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:475"><nobr><span class="ft1">(u</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:499"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:505"><nobr><span class="ft1">) =</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:537"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:143;left:548"><nobr><span class="ft13">xic</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:581"><nobr><span class="ft11">(x</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:599"><nobr><span class="ft13">i</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:604"><nobr><span class="ft11">)</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:563"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:573"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:579"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:148;left:610"><nobr><span class="ft1">, c = 1, . . . , k satisfies the condition</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:475"><nobr><span class="ft1">that the gradient is zero.</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:475"><nobr><span class="ft2">D.</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:511"><nobr><span class="ft2">PROOF OF THEOREM 5</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:489"><nobr><span class="ft1">We define a n</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:566"><nobr><span class="ft1">×k matrix Z as Z</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:666"><nobr><span class="ft11">ic</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:680"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:216;left:694"><nobr><span class="ft12">´</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:726"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:714"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:227;left:725"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:735"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:741"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:219;left:759"><nobr><span class="ft1">if x</span></nobr></DIV>
<DIV style="position:absolute;top:221;left:786"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:795"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:221;left:816"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:712"><nobr><span class="ft1">0</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:759"><nobr><span class="ft1">otherwise</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:830"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:475"><nobr><span class="ft7">We can see that Z captures the disjoint cluster memberships.<br>There is only one non-zero entry in each row of Z and Z</span></nobr></DIV>
<DIV style="position:absolute;top:268;left:801"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:273;left:810"><nobr><span class="ft1">Z =</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:475"><nobr><span class="ft1">I</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:481"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:494"><nobr><span class="ft1">holds (I</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:543"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:556"><nobr><span class="ft1">indicates the identity matrix). Suppose  is</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:475"><nobr><span class="ft7">the matrix of the images of the samples in feature space,<br>i.e.,  = [(x</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:559"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:565"><nobr><span class="ft1">), . . . , (x</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:625"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:633"><nobr><span class="ft1">)]. We can verify that the matrix</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:475"><nobr><span class="ft1">ZZ</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:506"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:518"><nobr><span class="ft1">consists of the mean vectors of the clusters containing</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:475"><nobr><span class="ft1">the corresponding sample. Thus, the </span></nobr></DIV>
<DIV style="position:absolute;top:347;left:708"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:719"><nobr><span class="ft1">can be written as</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:536"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:551"><nobr><span class="ft1">- ~</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:565"><nobr><span class="ft1">Q</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:584"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:594"><nobr><span class="ft1">= </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:627"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:636"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:373;left:650"><nobr><span class="ft1">- (ZZ</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:700"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:709"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:714"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:723"><nobr><span class="ft1">ZZ</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:754"><nobr><span class="ft11">T 2</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:776"><nobr><span class="ft1">.</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:475"><nobr><span class="ft1">Using the fact that trace(A</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:646"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:654"><nobr><span class="ft1">A) = A</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:717"><nobr><span class="ft17">2<br>F</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:727"><nobr><span class="ft1">, trace(A + B) =</span></nobr></DIV>
<DIV style="position:absolute;top:417;left:475"><nobr><span class="ft1">trace(A) + trace(B) and trace(AB) = trace(BA), we have</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:495"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:502"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:512"><nobr><span class="ft1">= trace((</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:580"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:589"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:606"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:615"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:626"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:635"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:440;left:649"><nobr><span class="ft1">- (Z</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:678"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:687"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:698"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:707"><nobr><span class="ft1">Z)(Z</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:749"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:758"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:769"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:778"><nobr><span class="ft1">Z)).</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:475"><nobr><span class="ft1">Since trace((</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:564"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:572"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:589"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:598"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:609"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:618"><nobr><span class="ft1">) is constant, minimizing  is equiv-</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:475"><nobr><span class="ft1">alent to maximizing</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:545"><nobr><span class="ft1">J</span></nobr></DIV>
<DIV style="position:absolute;top:510;left:552"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:563"><nobr><span class="ft1">= trace((Z</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:629"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:638"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:502;left:649"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:658"><nobr><span class="ft1">Z)(Z</span></nobr></DIV>
<DIV style="position:absolute;top:502;left:700"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:708"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:502;left:720"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:728"><nobr><span class="ft1">Z)).</span></nobr></DIV>
<DIV style="position:absolute;top:508;left:809"><nobr><span class="ft1">(17)</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:475"><nobr><span class="ft1">With similar procedure, we can see that minimizing J(</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:796"><nobr><span class="ft1">{</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:811"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:529;left:817"><nobr><span class="ft1">}</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:824"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:824"><nobr><span class="ft11">c=1</span></nobr></DIV>
<DIV style="position:absolute;top:533;left:843"><nobr><span class="ft1">)</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:475"><nobr><span class="ft1">amounts to maximizing</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:586"><nobr><span class="ft1">J</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:593"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:603"><nobr><span class="ft1">= trace(Z</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:664"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:673"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:685"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:693"><nobr><span class="ft1">Z).</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:809"><nobr><span class="ft1">(18)</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:475"><nobr><span class="ft1">Matrix K = </span></nobr></DIV>
<DIV style="position:absolute;top:593;left:567"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:576"><nobr><span class="ft1"> is a symmetric matrix. Let </span></nobr></DIV>
<DIV style="position:absolute;top:600;left:771"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:783"><nobr><span class="ft1"> . . . , </span></nobr></DIV>
<DIV style="position:absolute;top:614;left:475"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:616;left:481"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:494"><nobr><span class="ft1"> 0 denote its eigenvalues and (v</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:703"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:709"><nobr><span class="ft1">, . . . , v</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:748"><nobr><span class="ft11">n</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:755"><nobr><span class="ft1">) be the cor-</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:475"><nobr><span class="ft1">responding eigenvectors. Matrix H = Z</span></nobr></DIV>
<DIV style="position:absolute;top:624;left:726"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:735"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:624;left:747"><nobr><span class="ft11">T</span></nobr></DIV>
<DIV style="position:absolute;top:629;left:755"><nobr><span class="ft1">Z is also a</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:475"><nobr><span class="ft1">symmetric matrix.</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:604"><nobr><span class="ft1">Let </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:638"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:653"><nobr><span class="ft1"> . . . ,  </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:723"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:738"><nobr><span class="ft1"> 0 denote its</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:475"><nobr><span class="ft1">eigenvalues.</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:559"><nobr><span class="ft1">According to Poincar´</span></nobr></DIV>
<DIV style="position:absolute;top:660;left:692"><nobr><span class="ft1">e Separation Theorem,</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:475"><nobr><span class="ft1">we know the relations </span></nobr></DIV>
<DIV style="position:absolute;top:678;left:625"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:636"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:678;left:661"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:666"><nobr><span class="ft1">, i = 1, . . . , k hold. There-</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:475"><nobr><span class="ft1">fore, we have J</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:569"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:580"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:691;left:596"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:611"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:611"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:632"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:694;left:638"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:648"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:691;left:664"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:686;left:679"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:697;left:679"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:700"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:694;left:706"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:711"><nobr><span class="ft1">. Similarly, we have</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:475"><nobr><span class="ft1">J</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:483"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:493"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:709;left:507"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:522"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:522"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:543"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:705;left:550"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:550"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:707;left:561"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:709;left:575"><nobr><span class="ft12">È</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:590"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:715;left:590"><nobr><span class="ft11">i=1</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:611"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:705;left:618"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:714;left:617"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:624"><nobr><span class="ft1">. In both cases, the equations hold</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:475"><nobr><span class="ft1">when Z = (v</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:555"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:561"><nobr><span class="ft1">, . . . , v</span></nobr></DIV>
<DIV style="position:absolute;top:728;left:600"><nobr><span class="ft11">k</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:607"><nobr><span class="ft1">)R, where R is an arbitrary k</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:793"><nobr><span class="ft1">×k or-</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:475"><nobr><span class="ft1">thonormal matrix. Actually, the solution to maximizing J</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:828"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:475"><nobr><span class="ft7">is just the well-known theorem of Ky Fan (the Theorem 3.2.<br>of [11]). Note that the optimal Z might no longer conforms</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:475"><nobr><span class="ft1">to the definition of Z</span></nobr></DIV>
<DIV style="position:absolute;top:804;left:606"><nobr><span class="ft11">ic</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:622"><nobr><span class="ft1">=</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:638"><nobr><span class="ft12">´</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:670"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:658"><nobr><span class="ft1"></span></nobr></DIV>
<DIV style="position:absolute;top:798;left:669"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:679"><nobr><span class="ft13">c</span></nobr></DIV>
<DIV style="position:absolute;top:798;left:685"><nobr><span class="ft11">|</span></nobr></DIV>
<DIV style="position:absolute;top:790;left:703"><nobr><span class="ft1">if x</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:730"><nobr><span class="ft11">i</span></nobr></DIV>
<DIV style="position:absolute;top:787;left:739"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:760"><nobr><span class="ft11">c</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:656"><nobr><span class="ft1">0</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:703"><nobr><span class="ft1">otherwise</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:774"><nobr><span class="ft1">, but it is</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:475"><nobr><span class="ft7">still a orthonormal matrix. That is why it is called a relaxed<br>optimal solution.</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft6">450</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
