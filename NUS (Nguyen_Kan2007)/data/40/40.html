<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - f59-hu.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="Administrator">
<META name="date" content="2005-05-31T19:25:21+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
	.ft0{font-size:24px;font-family:Times;color:#000000;}
	.ft1{font-size:15px;font-family:Times;color:#000000;}
	.ft2{font-size:6px;font-family:Times;color:#000000;}
	.ft3{font-size:12px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:6px;font-family:Times;color:#ffffff;}
	.ft6{font-size:8px;font-family:Times;color:#000000;}
	.ft7{font-size:9px;font-family:Times;color:#000000;}
	.ft8{font-size:16px;font-family:Courier;color:#000000;}
	.ft9{font-size:11px;font-family:Times;color:#000000;}
	.ft10{font-size:15px;line-height:20px;font-family:Times;color:#000000;}
	.ft11{font-size:15px;line-height:24px;font-family:Times;color:#000000;}
	.ft12{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft13{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1263" src="40001.png" alt="background image">
<DIV style="position:absolute;top:159;left:113"><nobr><span class="ft0"><b>Automatic Extraction of Titles from General Documents </b></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:309"><nobr><span class="ft0"><b>using Machine Learning </b></span></nobr></DIV>
<DIV style="position:absolute;top:224;left:169"><nobr><span class="ft1">Yunhua Hu</span></nobr></DIV>
<DIV style="position:absolute;top:225;left:255"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:224;left:259"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:116"><nobr><span class="ft3">Computer Science Department </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:136"><nobr><span class="ft3">Xi'an Jiaotong University </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:126"><nobr><span class="ft3">No 28, Xianning West Road </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:148"><nobr><span class="ft3">Xi'an, China, 710049 </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:108"><nobr><span class="ft1">yunhuahu@mail.xjtu.edu.cn  </span></nobr></DIV>
<DIV style="position:absolute;top:337;left:214"><nobr><span class="ft11"> <br> <br> </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:380"><nobr><span class="ft1"> Hang Li, Yunbo Cao </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:382"><nobr><span class="ft3">Microsoft Research Asia </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:403"><nobr><span class="ft3">5F Sigma Center,  </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:362"><nobr><span class="ft3">No. 49 Zhichun Road, Haidian,  </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:388"><nobr><span class="ft3">Beijing, China, 100080  </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:343"><nobr><span class="ft1">{hangli,yucao}@microsoft.com  </span></nobr></DIV>
<DIV style="position:absolute;top:342;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:400"><nobr><span class="ft1">Qinghua Zheng </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:361"><nobr><span class="ft3">Computer Science Department </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:381"><nobr><span class="ft3">Xi'an Jiaotong University </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:371"><nobr><span class="ft3">No 28, Xianning West Road </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:393"><nobr><span class="ft3">Xi'an, China, 710049 </span></nobr></DIV>
<DIV style="position:absolute;top:455;left:358"><nobr><span class="ft1">qhzheng@mail.xjtu.edu.cn </span></nobr></DIV>
<DIV style="position:absolute;top:475;left:459"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:224;left:638"><nobr><span class="ft1">Dmitriy Meyerzon </span></nobr></DIV>
<DIV style="position:absolute;top:244;left:636"><nobr><span class="ft3">Microsoft Corporation </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:644"><nobr><span class="ft3">One Microsoft Way </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:656"><nobr><span class="ft3">Redmond, WA,  </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:666"><nobr><span class="ft3">USA, 98052 </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:610"><nobr><span class="ft1">dmitriym@microsoft.com</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:799"><nobr><span class="ft3">  </span></nobr></DIV>
<DIV style="position:absolute;top:343;left:704"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:366;left:704"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:500;left:91"><nobr><span class="ft1">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:187"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:91"><nobr><span class="ft12">In this paper, we propose a machine learning approach to title <br>extraction from general documents. By general documents, we <br>mean documents that can belong to any one of a number of <br>specific genres, including presentations, book chapters, technical <br>papers, brochures, reports, and letters. Previously, methods have <br>been proposed mainly for title extraction from research papers. It <br>has not been clear whether it could be possible to conduct <br>automatic title extraction from general documents. As a case study, <br>we consider extraction from Office including Word and <br>PowerPoint. In our approach, we annotate titles in sample <br>documents (for Word and PowerPoint respectively) and take them <br>as training data, train machine learning models, and perform title <br>extraction using the trained models. Our method is unique in that <br>we mainly utilize formatting information such as font size as <br>features in the models. It turns out that the use of formatting <br>information can lead to quite accurate extraction from general <br>documents. Precision and recall for title extraction from Word is <br>0.810 and 0.837 respectively, and precision and recall for title <br>extraction from PowerPoint is 0.875 and 0.895 respectively in an <br>experiment on intranet data. Other important new findings in this <br>work include that we can train models in one domain and apply <br>them to another domain, and more surprisingly we can even train <br>models in one language and apply them to another language. <br>Moreover, we can significantly improve search ranking results in <br>document retrieval by using the extracted titles. </span></nobr></DIV>
<DIV style="position:absolute;top:915;left:92"><nobr><span class="ft1">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:919;left:356"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:92"><nobr><span class="ft12"> H.3.3 [Information Storage and Retrieval]: Information Search <br>and Retrieval - Search Process; H.4.1 [Information Systems </span></nobr></DIV>
<DIV style="position:absolute;top:500;left:475"><nobr><span class="ft12">Applications]: Office Automation - Word processing; D.2.8 <br>[Software Engineering]: Metrics - complexity measures, <br>performance measures </span></nobr></DIV>
<DIV style="position:absolute;top:565;left:475"><nobr><span class="ft1">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:569;left:588"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:585;left:475"><nobr><span class="ft4">Algorithms, Experimentation, Performance. </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:475"><nobr><span class="ft1">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:638;left:475"><nobr><span class="ft12">information extraction, metadata extraction, machine learning, <br>search </span></nobr></DIV>
<DIV style="position:absolute;top:687;left:475"><nobr><span class="ft1">1.  INTRODUCTION </span></nobr></DIV>
<DIV style="position:absolute;top:707;left:475"><nobr><span class="ft12">Metadata of documents is useful for many kinds of document <br>processing such as search, browsing, and filtering. Ideally, <br>metadata is defined by the authors of documents and is then used <br>by various systems. However, people seldom define document <br>metadata by themselves, even when they have convenient <br>metadata definition tools [26]. Thus, how to automatically extract <br>metadata from the bodies of documents turns out to be an <br>important research issue. </span></nobr></DIV>
<DIV style="position:absolute;top:839;left:475"><nobr><span class="ft12">Methods for performing the task have been proposed. However, <br>the focus was mainly on extraction from research papers. For <br>instance, Han et al. [10] proposed a machine learning based <br>method to conduct extraction from research papers. They <br>formalized the problem as that of classification and employed <br>Support Vector Machines as the classifier. They mainly used <br>linguistic features in the model.</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:641"><nobr><span class="ft5">1</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:645"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:955;left:475"><nobr><span class="ft12">In this paper, we consider metadata extraction from general <br>documents. By general documents, we mean documents that may <br>belong to any one of a number of specific genres. General <br>documents are more widely available in digital libraries, intranets <br>and the internet, and thus investigation on extraction from them is </span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:475"><nobr><span class="ft4">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:475"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:1068;left:481"><nobr><span class="ft4"> The work was conducted when the first author was visiting </span></nobr></DIV>
<DIV style="position:absolute;top:1083;left:485"><nobr><span class="ft4">Microsoft Research Asia. </span></nobr></DIV>
<DIV style="position:absolute;top:973;left:97"><nobr><span class="ft6"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:994;left:97"><nobr><span class="ft13">Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are <br>not made or distributed for profit or commercial advantage and that copies <br>bear this notice and the full citation on the first page. To copy otherwise, or <br>republish, to post on servers or to redistribute to lists, requires prior specific <br>permission and/or a fee. <br>JCDL'05, June 7­11, 2005, Denver, Colorado, USA <br>Copyright 2005 ACM 1-58113-876-8/05/0006...$5.00. </span></nobr></DIV>
<DIV style="position:absolute;top:1104;left:97"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">145</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1263" src="40002.png" alt="background image">
<DIV style="position:absolute;top:159;left:92"><nobr><span class="ft12">sorely needed. Research papers usually have well-formed styles <br>and noticeable characteristics. In contrast, the styles of general <br>documents can vary greatly. It has not been clarified whether a <br>machine learning based approach can work well for this task. </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:92"><nobr><span class="ft12">There are many types of metadata: title, author, date of creation, <br>etc. As a case study, we consider title extraction in this paper. <br>General documents can be in many different file formats: <br>Microsoft Office, PDF (PS), etc. As a case study, we consider <br>extraction from Office including Word and PowerPoint.  </span></nobr></DIV>
<DIV style="position:absolute;top:315;left:92"><nobr><span class="ft12">We take a machine learning approach. We annotate titles in <br>sample documents (for Word and PowerPoint respectively) and <br>take them as training data to train several types of models, and <br>perform title extraction using any one type of the trained models. <br>In the models, we mainly utilize formatting information such as <br>font size as features. We employ the following models: Maximum <br>Entropy Model, Perceptron with Uneven Margins, Maximum <br>Entropy Markov Model, and Voted Perceptron.  </span></nobr></DIV>
<DIV style="position:absolute;top:447;left:92"><nobr><span class="ft12">In this paper, we also investigate the following three problems, <br>which did not seem to have been examined previously. </span></nobr></DIV>
<DIV style="position:absolute;top:487;left:92"><nobr><span class="ft12">(1) Comparison between models: among the models above, which <br>model performs best for title extraction; </span></nobr></DIV>
<DIV style="position:absolute;top:526;left:92"><nobr><span class="ft12">(2) Generality of model: whether it is possible to train a model on <br>one domain and apply it to another domain, and whether it is <br>possible to train a model in one language and apply it to another <br>language; </span></nobr></DIV>
<DIV style="position:absolute;top:597;left:92"><nobr><span class="ft12">(3) Usefulness of extracted titles: whether extracted titles can <br>improve document processing such as search.  </span></nobr></DIV>
<DIV style="position:absolute;top:636;left:92"><nobr><span class="ft12">Experimental results indicate that our approach works well for <br>title extraction from general documents. Our method can <br>significantly outperform the baselines: one that always uses the <br>first lines as titles and the other that always uses the lines in the <br>largest font sizes as titles. Precision and recall for title extraction <br>from Word are 0.810 and 0.837 respectively, and precision and <br>recall for title extraction from PowerPoint are 0.875 and 0.895 <br>respectively. It turns out that the use of format features is the key <br>to successful title extraction. </span></nobr></DIV>
<DIV style="position:absolute;top:784;left:92"><nobr><span class="ft12">(1) We have observed that Perceptron based models perform <br>better in terms of extraction accuracies. (2) We have empirically <br>verified that the models trained with our approach are generic in <br>the sense that they can be trained on one domain and applied to <br>another, and they can be trained in one language and applied to <br>another. (3) We have found that using the extracted titles we can <br>significantly improve precision of document retrieval (by 10%). </span></nobr></DIV>
<DIV style="position:absolute;top:900;left:92"><nobr><span class="ft12">We conclude that we can indeed conduct reliable title extraction <br>from general documents and use the extracted results to improve <br>real applications. </span></nobr></DIV>
<DIV style="position:absolute;top:955;left:92"><nobr><span class="ft12">The rest of the paper is organized as follows. In section 2, we <br>introduce related work, and in section 3, we explain the <br>motivation and problem setting of our work. In section 4, we <br>describe our method of title extraction, and in section 5, we <br>describe our method of document retrieval using extracted titles. <br>Section 6 gives our experimental results. We make concluding <br>remarks in section 7. </span></nobr></DIV>
<DIV style="position:absolute;top:160;left:475"><nobr><span class="ft10">2.  RELATED WORK <br>2.1  Document Metadata Extraction </span></nobr></DIV>
<DIV style="position:absolute;top:200;left:475"><nobr><span class="ft12">Methods have been proposed for performing automatic metadata <br>extraction from documents; however, the main focus was on <br>extraction from research papers. </span></nobr></DIV>
<DIV style="position:absolute;top:255;left:475"><nobr><span class="ft12">The proposed methods fall into two categories: the rule based <br>approach and the machine learning based approach. </span></nobr></DIV>
<DIV style="position:absolute;top:295;left:475"><nobr><span class="ft12">Giuffrida et al. [9], for instance, developed a rule-based system for <br>automatically extracting metadata from research papers in <br>Postscript. They used rules like "titles are usually located on the <br>upper portions of the first pages and they are usually in the largest <br>font sizes". Liddy et al. [14] and Yilmazel el al. [23] performed <br>metadata extraction from educational materials using rule-based <br>natural language processing technologies. Mao et al. [16] also <br>conducted automatic metadata extraction from research papers <br>using rules on formatting information.  </span></nobr></DIV>
<DIV style="position:absolute;top:442;left:475"><nobr><span class="ft12">The rule-based approach can achieve high performance. However, <br>it also has disadvantages. It is less adaptive and robust when <br>compared with the machine learning approach. </span></nobr></DIV>
<DIV style="position:absolute;top:497;left:475"><nobr><span class="ft12">Han et al. [10], for instance, conducted metadata extraction with <br>the machine learning approach. They viewed the problem as that <br>of classifying the lines in a document into the categories of <br>metadata and proposed using Support Vector Machines as the <br>classifier. They mainly used linguistic information as features. <br>They reported high extraction accuracy from research papers in <br>terms of precision and recall. </span></nobr></DIV>
<DIV style="position:absolute;top:623;left:475"><nobr><span class="ft1">2.2  Information Extraction </span></nobr></DIV>
<DIV style="position:absolute;top:643;left:475"><nobr><span class="ft12">Metadata extraction can be viewed as an application of <br>information extraction, in which given a sequence of instances, we <br>identify a subsequence that represents information in which we <br>are interested. Hidden Markov Model [6], Maximum Entropy <br>Model [1, 4], Maximum Entropy Markov Model [17], Support <br>Vector Machines [3], Conditional Random Field [12], and Voted <br>Perceptron [2] are widely used information extraction models. </span></nobr></DIV>
<DIV style="position:absolute;top:759;left:475"><nobr><span class="ft12">Information extraction has been applied, for instance, to part-of-<br>speech tagging [20], named entity recognition [25] and table <br>extraction [19]. </span></nobr></DIV>
<DIV style="position:absolute;top:824;left:475"><nobr><span class="ft1">2.3  Search Using Title Information  </span></nobr></DIV>
<DIV style="position:absolute;top:844;left:475"><nobr><span class="ft4">Title information is useful for document retrieval.    </span></nobr></DIV>
<DIV style="position:absolute;top:868;left:475"><nobr><span class="ft12">In the system Citeseer, for instance, Giles et al. managed to <br>extract titles from research papers and make use of the extracted <br>titles in metadata search of papers [8]. </span></nobr></DIV>
<DIV style="position:absolute;top:923;left:475"><nobr><span class="ft12">In web search, the title fields (i.e., file properties) and anchor texts <br>of web pages (HTML documents) can be viewed as `titles' of the <br>pages [5]. Many search engines seem to utilize them for web page <br>retrieval [7, 11, 18, 22]. Zhang et al., found that web pages with <br>well-defined metadata are more easily retrieved than those without <br>well-defined metadata [24].  </span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:475"><nobr><span class="ft12">To the best of our knowledge, no research has been conducted on <br>using extracted titles from general documents (e.g., Office <br>documents) for search of the documents. </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">146</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1263" src="40003.png" alt="background image">
<DIV style="position:absolute;top:160;left:92"><nobr><span class="ft10">3.  MOTIVATION AND PROBLEM <br>SETTING </span></nobr></DIV>
<DIV style="position:absolute;top:200;left:92"><nobr><span class="ft12">We consider the issue of automatically extracting titles from <br>general documents.  </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:92"><nobr><span class="ft12">By general documents, we mean documents that belong to one of <br>any number of specific genres. The documents can be <br>presentations, books, book chapters, technical papers, brochures, <br>reports, memos, specifications, letters, announcements, or resumes. <br>General documents are more widely available in digital libraries, <br>intranets, and internet, and thus investigation on title extraction <br>from them is sorely needed. </span></nobr></DIV>
<DIV style="position:absolute;top:356;left:92"><nobr><span class="ft12">Figure 1 shows an estimate on distributions of file formats on <br>intranet and internet [15]. Office and PDF are the main file <br>formats on the intranet. Even on the internet, the documents in the <br>formats are still not negligible, given its extremely large size. In <br>this paper, without loss of generality, we take Office documents as <br>an example. </span></nobr></DIV>
<DIV style="position:absolute;top:600;left:442"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:93"><nobr><span class="ft4">Figure 1. Distributions of file formats in internet and intranet. </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:268"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:663;left:92"><nobr><span class="ft12">For Office documents, users can define titles as file properties <br>using a feature provided by Office. We found in an experiment, <br>however, that users seldom use the feature and thus titles in file <br>properties are usually very inaccurate. That is to say, titles in file <br>properties are usually inconsistent with the `true' titles in the file <br>bodies that are created by the authors and are visible to readers. <br>We collected 6,000 Word and 6,000 PowerPoint documents from <br>an intranet and the internet and examined how many titles in the <br>file properties are correct. We found that surprisingly the accuracy <br>was only 0.265 (cf., Section 6.3 for details). A number of reasons <br>can be considered. For example, if one creates a new file by <br>copying an old file, then the file property of the new file will also <br>be copied from the old file. </span></nobr></DIV>
<DIV style="position:absolute;top:872;left:92"><nobr><span class="ft12">In another experiment, we found that Google uses the titles in file <br>properties of Office documents in search and browsing, but the <br>titles are not very accurate. We created 50 queries to search Word <br>and PowerPoint documents and examined the top 15 results of <br>each query returned by Google. We found that nearly all the titles <br>presented in the search results were from the file properties of the <br>documents. However, only 0.272 of them were correct.  </span></nobr></DIV>
<DIV style="position:absolute;top:989;left:92"><nobr><span class="ft12">Actually, `true' titles usually exist at the beginnings of the bodies <br>of documents. If we can accurately extract the titles from the <br>bodies of documents, then we can exploit reliable title information <br>in document processing. This is exactly the problem we address in <br>this paper. </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">More specifically, given a Word document, we are to extract the <br>title from the top region of the first page. Given a PowerPoint <br>document, we are to extract the title from the first slide. A title <br>sometimes consists of a main title and one or two subtitles. We <br>only consider extraction of the main title.  </span></nobr></DIV>
<DIV style="position:absolute;top:245;left:475"><nobr><span class="ft12">As baselines for title extraction, we use that of always using the <br>first lines as titles and that of always using the lines with largest <br>font sizes as titles. </span></nobr></DIV>
<DIV style="position:absolute;top:546;left:824"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:567;left:516"><nobr><span class="ft4">Figure 2. Title extraction from Word document. </span></nobr></DIV>
<DIV style="position:absolute;top:588;left:651"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:860;left:826"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:881;left:499"><nobr><span class="ft4">Figure 3. Title extraction from PowerPoint document. </span></nobr></DIV>
<DIV style="position:absolute;top:903;left:651"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:923;left:475"><nobr><span class="ft12">Next, we define a `specification' for human judgments in title data <br>annotation. The annotated data will be used in training and testing <br>of the title extraction methods.  </span></nobr></DIV>
<DIV style="position:absolute;top:978;left:475"><nobr><span class="ft12">Summary of the specification: The title of a document should be <br>identified on the basis of common sense, if there is no difficulty in <br>the identification. However, there are many cases in which the <br>identification is not easy. There are some rules defined in the <br>specification that guide identification for such cases. The rules <br>include "a title is usually in consecutive lines in the same format", <br>"a document can have no title", "titles in images are not <br>considered", "a title should not contain words like `draft', </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">147</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
	.ft14{font-size:11px;font-family:Times;color:#ff0000;}
	.ft15{font-size:7px;font-family:Times;color:#000000;}
	.ft16{font-size:14px;font-family:Times;color:#000000;}
	.ft17{font-size:4px;font-family:Times;color:#000000;}
	.ft18{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1263" src="40004.png" alt="background image">
<DIV style="position:absolute;top:159;left:92"><nobr><span class="ft12">`whitepaper', etc", "if it is difficult to determine which is the title, <br>select the one in the largest font size", and "if it is still difficult to <br>determine which is the title, select the first candidate". (The <br>specification covers all the cases we have encountered in data <br>annotation.) </span></nobr></DIV>
<DIV style="position:absolute;top:245;left:92"><nobr><span class="ft12">Figures 2 and 3 show examples of Office documents from which <br>we conduct title extraction. In Figure 2, `Differences in Win32 <br>API Implementations among Windows Operating Systems' is the <br>title of the Word document. `Microsoft Windows' on the top of <br>this page is a picture and thus is ignored. In Figure 3, `Building <br>Competitive Advantages through an Agile Infrastructure' is the <br>title of the PowerPoint document. </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:92"><nobr><span class="ft12">We have developed a tool for annotation of titles by human <br>annotators. Figure 4 shows a snapshot of the tool. </span></nobr></DIV>
<DIV style="position:absolute;top:625;left:441"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:646;left:182"><nobr><span class="ft4">Figure 4. Title annotation tool. </span></nobr></DIV>
<DIV style="position:absolute;top:667;left:268"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:697;left:92"><nobr><span class="ft10">4.  TITLE EXTRACTION METHOD <br>4.1  Outline </span></nobr></DIV>
<DIV style="position:absolute;top:738;left:92"><nobr><span class="ft12">Title extraction based on machine learning consists of training and <br>extraction. The same pre-processing step occurs before training <br>and extraction. </span></nobr></DIV>
<DIV style="position:absolute;top:793;left:92"><nobr><span class="ft12">During pre-processing, from the top region of the first page of a <br>Word document or the first slide of a PowerPoint document a <br>number of units for processing are extracted. If a line (lines are <br>separated by `return' symbols) only has a single format, then the <br>line will become a unit. If a line has several parts and each of <br>them has its own format, then each part will become a unit.</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:413"><nobr><span class="ft14"> </span></nobr></DIV>
<DIV style="position:absolute;top:870;left:417"><nobr><span class="ft4">Each </span></nobr></DIV>
<DIV style="position:absolute;top:885;left:91"><nobr><span class="ft12">unit will be treated as an instance in learning.  A unit contains not <br>only content information (linguistic information) but also <br>formatting information. The input to pre-processing is a document <br>and the output of pre-processing is a sequence of units (instances). <br>Figure 5 shows the units obtained from the document in Figure 2.  </span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:441"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:192"><nobr><span class="ft4">Figure 5. Example of units. </span></nobr></DIV>
<DIV style="position:absolute;top:1086;left:268"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">In learning, the input is sequences of units where each sequence <br>corresponds to a document. We take labeled units (labeled as <br>title_begin, title_end, or other) in the sequences as training data <br>and construct models for identifying whether a unit is title_begin <br>title_end, or other. We employ four types of models: Perceptron, <br>Maximum Entropy (ME), Perceptron Markov Model (PMM), and <br>Maximum Entropy Markov Model (MEMM). </span></nobr></DIV>
<DIV style="position:absolute;top:276;left:475"><nobr><span class="ft12">In extraction, the input is a sequence of units from one document. <br>We employ one type of model to identify whether a unit is <br>title_begin, title_end, or other. We then extract units from the unit <br>labeled with `title_begin' to the unit labeled with `title_end'. The <br>result is the extracted title of the document. </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:475"><nobr><span class="ft12">The unique characteristic of our approach is that we mainly utilize <br>formatting information for title extraction. Our assumption is that <br>although general documents vary in styles, their formats have <br>certain patterns and we can learn and utilize the patterns for title <br>extraction. This is in contrast to the work by Han et al., in which <br>only linguistic features are used for extraction from research <br>papers. </span></nobr></DIV>
<DIV style="position:absolute;top:487;left:475"><nobr><span class="ft1">4.2  Models </span></nobr></DIV>
<DIV style="position:absolute;top:507;left:475"><nobr><span class="ft12">The four models actually can be considered in the same metadata <br>extraction framework. That is why we apply them together to our <br>current problem. </span></nobr></DIV>
<DIV style="position:absolute;top:568;left:475"><nobr><span class="ft4">Each input is a sequence of instances </span></nobr></DIV>
<DIV style="position:absolute;top:575;left:737"><nobr><span class="ft6"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:564;left:729"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:694"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:681"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:565;left:710"><nobr><span class="ft1">L</span></nobr></DIV>
<DIV style="position:absolute;top:575;left:703"><nobr><span class="ft6"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:688"><nobr><span class="ft6"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:568;left:746"><nobr><span class="ft4">together with a </span></nobr></DIV>
<DIV style="position:absolute;top:594;left:475"><nobr><span class="ft4">sequence of labels</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:635"><nobr><span class="ft15"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:627"><nobr><span class="ft16"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:593"><nobr><span class="ft16"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:579"><nobr><span class="ft16"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:608"><nobr><span class="ft16"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:602;left:601"><nobr><span class="ft15"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:602;left:586"><nobr><span class="ft15"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:594;left:642"><nobr><span class="ft4">. </span></nobr></DIV>
<DIV style="position:absolute;top:601;left:660"><nobr><span class="ft6"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:590;left:652"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:666"><nobr><span class="ft4">and</span></nobr></DIV>
<DIV style="position:absolute;top:601;left:697"><nobr><span class="ft6"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:590;left:689"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:704"><nobr><span class="ft4"> represents  an  instance </span></nobr></DIV>
<DIV style="position:absolute;top:615;left:475"><nobr><span class="ft4">and its label, respectively (</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:681"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:624"><nobr><span class="ft3">i</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:676"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:658"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:651"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:647"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:641"><nobr><span class="ft3">1 L</span></nobr></DIV>
<DIV style="position:absolute;top:613;left:631"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:691"><nobr><span class="ft4">). Recall that an instance </span></nobr></DIV>
<DIV style="position:absolute;top:634;left:475"><nobr><span class="ft12">here represents a unit. A label represents title_begin, title_end, or <br>other. Here, k is the number of units in a document. </span></nobr></DIV>
<DIV style="position:absolute;top:674;left:475"><nobr><span class="ft18">In learning, we train a model which can be generally denoted as a <br>conditional probability distribution </span></nobr></DIV>
<DIV style="position:absolute;top:690;left:784"><nobr><span class="ft3">)</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:726"><nobr><span class="ft3">|</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:677"><nobr><span class="ft3">(</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:743"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:688"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:778"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:717"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:767"><nobr><span class="ft3">X</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:733"><nobr><span class="ft3">X</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:709"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:681"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:668"><nobr><span class="ft3">P</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:750"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:692;left:694"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:791"><nobr><span class="ft4"> where </span></nobr></DIV>
<DIV style="position:absolute;top:726;left:488"><nobr><span class="ft2">i</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:478"><nobr><span class="ft3">X</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:493"><nobr><span class="ft4"> and </span></nobr></DIV>
<DIV style="position:absolute;top:726;left:527"><nobr><span class="ft2">i</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:520"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:532"><nobr><span class="ft4"> denote random variables taking instance </span></nobr></DIV>
<DIV style="position:absolute;top:724;left:767"><nobr><span class="ft6"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:713;left:760"><nobr><span class="ft1">x</span></nobr></DIV>
<DIV style="position:absolute;top:717;left:774"><nobr><span class="ft4"> and  label </span></nobr></DIV>
<DIV style="position:absolute;top:750;left:487"><nobr><span class="ft6"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:739;left:479"><nobr><span class="ft1">y</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:494"><nobr><span class="ft4">as values, respectively (</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:679"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:621"><nobr><span class="ft3">i</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:674"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:656"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:649"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:645"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:639"><nobr><span class="ft3">1 L</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:629"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:743;left:689"><nobr><span class="ft4">). </span></nobr></DIV>
<DIV style="position:absolute;top:855;left:614"><nobr><span class="ft15"><i>Learning Tool</i></span></nobr></DIV>
<DIV style="position:absolute;top:932;left:614"><nobr><span class="ft15"><i>Extraction Tool</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:612"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:555"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:540"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:553"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:540"><nobr><span class="ft6"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:553"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:538"><nobr><span class="ft6"><i>  </i></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:582"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:569"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:533"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:512"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:499"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:602"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:577"><nobr><span class="ft17">22</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:564"><nobr><span class="ft17">21</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:533"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:508"><nobr><span class="ft17">22</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:496"><nobr><span class="ft17">21</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:600"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:576"><nobr><span class="ft17">12</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:564"><nobr><span class="ft17">11</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:531"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:506"><nobr><span class="ft17">12</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:495"><nobr><span class="ft17">11</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:605"><nobr><span class="ft17">nk</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:579"><nobr><span class="ft17">n</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:566"><nobr><span class="ft17">n</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:536"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:508"><nobr><span class="ft17">n</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:496"><nobr><span class="ft17">n</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:606"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:796;left:536"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:604"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:534"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:822;left:600"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:574"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:561"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:528"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:503"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:822;left:491"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:597"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:572"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:559"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:528"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:503"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:491"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:596"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:571"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:559"><nobr><span class="ft6"><i>y</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:526"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:502"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:491"><nobr><span class="ft6"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:587"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:516"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:807;left:500"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:807;left:490"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:791;left:585"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:791;left:516"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:774;left:583"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:774;left:514"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:821;left:545"><nobr><span class="ft6"><i></i></span></nobr></DIV>
<DIV style="position:absolute;top:788;left:543"><nobr><span class="ft6"><i></i></span></nobr></DIV>
<DIV style="position:absolute;top:772;left:543"><nobr><span class="ft6"><i></i></span></nobr></DIV>
<DIV style="position:absolute;top:957;left:807"><nobr><span class="ft7">)</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:759"><nobr><span class="ft7">|</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:709"><nobr><span class="ft7">(</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:679"><nobr><span class="ft7">max</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:662"><nobr><span class="ft7">arg</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:774"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:724"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:797"><nobr><span class="ft17">mk</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:770"><nobr><span class="ft17">m</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:748"><nobr><span class="ft17">mk</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:719"><nobr><span class="ft17">m</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:792"><nobr><span class="ft7">x</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:764"><nobr><span class="ft7">x</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:742"><nobr><span class="ft7">y</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:714"><nobr><span class="ft7">y</span></nobr></DIV>
<DIV style="position:absolute;top:957;left:701"><nobr><span class="ft7">P</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:779"><nobr><span class="ft7">L</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:729"><nobr><span class="ft7">L</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:794"><nobr><span class="ft6"><i>)</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:759"><nobr><span class="ft6"><i>|</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:729"><nobr><span class="ft6"><i>(</i></span></nobr></DIV>
<DIV style="position:absolute;top:892;left:769"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:736"><nobr><span class="ft17">1</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:791"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:753"><nobr><span class="ft17">k</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:784"><nobr><span class="ft6"><i>X</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:763"><nobr><span class="ft6"><i>X</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:749"><nobr><span class="ft6"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:731"><nobr><span class="ft6"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:723"><nobr><span class="ft6"><i>P</i></span></nobr></DIV>
<DIV style="position:absolute;top:887;left:773"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:887;left:739"><nobr><span class="ft6"><i>L</i></span></nobr></DIV>
<DIV style="position:absolute;top:842;left:736"><nobr><span class="ft15"><i>Conditional</i></span></nobr></DIV>
<DIV style="position:absolute;top:859;left:736"><nobr><span class="ft15"><i>Distribution</i></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:552"><nobr><span class="ft2">mk</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:518"><nobr><span class="ft2">m</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:501"><nobr><span class="ft2">m</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:546"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:512"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:495"><nobr><span class="ft4">x</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:531"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:524"><nobr><span class="ft2">2</span></nobr></DIV>
<DIV style="position:absolute;top:929;left:507"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:962;left:812"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:983;left:546"><nobr><span class="ft4">Figure 6. Metadata extraction model. </span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:651"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:475"><nobr><span class="ft12">We can make assumptions about the general model in order to <br>make it simple enough for training. </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">148</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
	.ft19{font-size:5px;font-family:Times;color:#000000;}
	.ft20{font-size:13px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1263" src="40005.png" alt="background image">
<DIV style="position:absolute;top:160;left:92"><nobr><span class="ft4">For example, we can assume that </span></nobr></DIV>
<DIV style="position:absolute;top:169;left:333"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:324"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:284"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:319"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:298"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:169;left:292"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:162;left:302"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:160;left:341"><nobr><span class="ft4">are independent of </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:92"><nobr><span class="ft4">each other given</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:239"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:184;left:227"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:182"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:220"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:199"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:192;left:193"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:185;left:204"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:181;left:246"><nobr><span class="ft4">. Thus, we have </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:340"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:314"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:291"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:257"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:233"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:213"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:314"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:253"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:200"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:252"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:241;left:225"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:271"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:213"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:241;left:333"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:241;left:304"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:307"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:243"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:232;left:321"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:296"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:281"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:241"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:218"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:203"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:295"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:260"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:235"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:205"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:190"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:263"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:277"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:219"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:190"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:348"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:260;left:91"><nobr><span class="ft12">In this way, we decompose the model into a number of classifiers. <br>We train the classifiers locally using the labeled data. As the <br>classifier, we employ the Perceptron or Maximum Entropy model. </span></nobr></DIV>
<DIV style="position:absolute;top:315;left:91"><nobr><span class="ft4">We can also assume that the first order Markov property holds for </span></nobr></DIV>
<DIV style="position:absolute;top:341;left:142"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:133"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:93"><nobr><span class="ft3">Y</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:128"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:107"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:101"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:111"><nobr><span class="ft3">L</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:150"><nobr><span class="ft4">given</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:238"><nobr><span class="ft19"><i>k</i></span></nobr></DIV>
<DIV style="position:absolute;top:333;left:227"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:182"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:219"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:198"><nobr><span class="ft4">,</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:193"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:335;left:203"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:246"><nobr><span class="ft4">. Thus, we have </span></nobr></DIV>
<DIV style="position:absolute;top:383;left:349"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:302"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:280"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:247"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:223"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:203"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:303"><nobr><span class="ft4">)</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:242"><nobr><span class="ft4">|</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:191"><nobr><span class="ft4">(</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:325"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:241"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:215"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:260"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:203"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:343"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:316"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:293"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:296"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:233"><nobr><span class="ft2">k</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:331"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:307"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:285"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:270"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:231"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:208"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:383;left:193"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:284"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:249"><nobr><span class="ft4">X</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:225"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:195"><nobr><span class="ft4">Y</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:181"><nobr><span class="ft4">P</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:321"><nobr><span class="ft2">-</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:180"><nobr><span class="ft4">=</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:253"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:266"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:209"><nobr><span class="ft4">L</span></nobr></DIV>
<DIV style="position:absolute;top:368;left:357"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:411;left:91"><nobr><span class="ft12">Again, we obtain a number of classifiers. However, the classifiers <br>are conditioned on the previous label. When we employ the <br>Percepton or Maximum Entropy model as a classifier, the models <br>become a Percepton Markov Model or Maximum Entropy Markov <br>Model, respectively. That is to say, the two models are more <br>precise. </span></nobr></DIV>
<DIV style="position:absolute;top:512;left:91"><nobr><span class="ft12">In extraction, given a new sequence of instances, we resort to one <br>of the constructed models to assign a sequence of labels to the <br>sequence of instances, i.e., perform extraction. </span></nobr></DIV>
<DIV style="position:absolute;top:567;left:91"><nobr><span class="ft12">For Perceptron and ME, we assign labels locally and combine the <br>results globally later using heuristics. Specifically, we first <br>identify the most likely title_begin. Then we find the most likely <br>title_end within three units after the title_begin. Finally, we <br>extract as a title the units between the title_begin and the title_end. </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:91"><nobr><span class="ft12">For PMM and MEMM, we employ the Viterbi algorithm to find <br>the globally optimal label sequence. </span></nobr></DIV>
<DIV style="position:absolute;top:693;left:91"><nobr><span class="ft12">In this paper, for Perceptron, we actually employ an improved <br>variant of it, called Perceptron with Uneven Margin [13]. This <br>version of Perceptron can work well especially when the number <br>of positive instances and the number of negative instances differ <br>greatly, which is exactly the case in our problem. </span></nobr></DIV>
<DIV style="position:absolute;top:779;left:91"><nobr><span class="ft12">We also employ an improved version of Perceptron Markov <br>Model in which the Perceptron model is the so-called Voted <br>Perceptron [2]. In addition, in training, the parameters of the <br>model are updated globally rather than locally. </span></nobr></DIV>
<DIV style="position:absolute;top:858;left:92"><nobr><span class="ft1">4.3  Features </span></nobr></DIV>
<DIV style="position:absolute;top:878;left:92"><nobr><span class="ft12">There are two types of features: format features and linguistic <br>features. We mainly use the former. The features are used for both <br>the title-begin and the title-end classifiers.  </span></nobr></DIV>
<DIV style="position:absolute;top:942;left:92"><nobr><span class="ft16"><i>4.3.1  Format Features </i></span></nobr></DIV>
<DIV style="position:absolute;top:961;left:92"><nobr><span class="ft12">Font Size: There are four binary features that represent the <br>normalized font size of the unit (recall that a unit has only one <br>type of font).  </span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:92"><nobr><span class="ft12">If the font size of the unit is the largest in the document, then the <br>first feature will be 1, otherwise 0. If the font size is the smallest <br>in the document, then the fourth feature will be 1, otherwise 0. If <br>the font size is above the average font size and not the largest in <br>the document, then the second feature will be 1, otherwise 0. If the </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">font size is below the average font size and not the smallest, the <br>third feature will be 1, otherwise 0.  </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:475"><nobr><span class="ft12">It is necessary to conduct normalization on font sizes. For <br>example, in one document the largest font size might be `12pt', <br>while in another the smallest one might be `18pt'.  </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:475"><nobr><span class="ft12">Boldface: This binary feature represents whether or not the <br>current unit is in boldface. </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:475"><nobr><span class="ft12">Alignment: There are four binary features that respectively <br>represent the location of the current unit: `left', `center', `right', <br>and `unknown alignment'. </span></nobr></DIV>
<DIV style="position:absolute;top:349;left:475"><nobr><span class="ft12">The following format features with respect to `context' play an <br>important role in title extraction.  </span></nobr></DIV>
<DIV style="position:absolute;top:389;left:475"><nobr><span class="ft12">Empty Neighboring Unit: There are two binary features that <br>represent, respectively, whether or not the previous unit and the <br>current unit are blank lines. </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:475"><nobr><span class="ft12">Font Size Change: There are two binary features that represent, <br>respectively, whether or not the font size of the previous unit and <br>the font size of the next unit differ from that of the current unit. </span></nobr></DIV>
<DIV style="position:absolute;top:499;left:475"><nobr><span class="ft12">Alignment Change: There are two binary features that represent, <br>respectively, whether or not the alignment of the previous unit and <br>the alignment of the next unit differ from that of the current one. </span></nobr></DIV>
<DIV style="position:absolute;top:554;left:475"><nobr><span class="ft12">Same Paragraph: There are two binary features that represent, <br>respectively, whether or not the previous unit and the next unit are <br>in the same paragraph as the current unit. </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:475"><nobr><span class="ft16"><i>4.3.2  Linguistic Features </i></span></nobr></DIV>
<DIV style="position:absolute;top:637;left:475"><nobr><span class="ft4">The linguistic features are based on key words.  </span></nobr></DIV>
<DIV style="position:absolute;top:661;left:475"><nobr><span class="ft12">Positive Word: This binary feature represents whether or not the <br>current unit begins with one of the positive words. The positive <br>words include `title:', `subject:', `subject line:' For example, in <br>some documents the lines of titles and authors have the same <br>formats. However, if lines begin with one of the positive words, <br>then it is likely that they are title lines. </span></nobr></DIV>
<DIV style="position:absolute;top:762;left:475"><nobr><span class="ft12">Negative Word: This binary feature represents whether or not the <br>current unit begins with one of the negative words. The negative <br>words include `To', `By', `created by', `updated by', etc.  </span></nobr></DIV>
<DIV style="position:absolute;top:817;left:475"><nobr><span class="ft12">There are more negative words than positive words. The above <br>linguistic features are language dependent. </span></nobr></DIV>
<DIV style="position:absolute;top:857;left:475"><nobr><span class="ft12">Word Count: A title should not be too long. We heuristically <br>create four intervals: [1, 2], [3, 6], [7, 9] and [9,</span></nobr></DIV>
<DIV style="position:absolute;top:869;left:728"><nobr><span class="ft20"></span></nobr></DIV>
<DIV style="position:absolute;top:873;left:741"><nobr><span class="ft4">) and define one </span></nobr></DIV>
<DIV style="position:absolute;top:888;left:475"><nobr><span class="ft12">feature for each interval. If the number of words in a title falls into <br>an interval, then the corresponding feature will be 1; otherwise 0.  </span></nobr></DIV>
<DIV style="position:absolute;top:928;left:475"><nobr><span class="ft12">Ending Character: This feature represents whether the unit ends <br>with `:', `-', or other special characters. A title usually does not <br>end with such a character. </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:475"><nobr><span class="ft1">5.  DOCUMENT RETRIEVAL METHOD </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:475"><nobr><span class="ft12">We describe our method of document retrieval using extracted <br>titles. </span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:475"><nobr><span class="ft12">Typically, in information retrieval a document is split into a <br>number of fields including body, title, and anchor text. A ranking <br>function in search can use different weights for different fields of </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">149</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
	.ft21{font-size:3px;font-family:Times;color:#000000;}
	.ft22{font-size:16px;font-family:Times;color:#000000;}
	.ft23{font-size:10px;font-family:Times;color:#000000;}
	.ft24{font-size:11px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1263" src="40006.png" alt="background image">
<DIV style="position:absolute;top:159;left:92"><nobr><span class="ft12">the document. Also, titles are typically assigned high weights, <br>indicating that they are important for document retrieval. As <br>explained previously, our experiment has shown that a significant <br>number of documents actually have incorrect titles in the file <br>properties, and thus in addition of using them we use the extracted <br>titles as one more field of the document. By doing this, we attempt <br>to improve the overall precision. <br>In this paper, we employ a modification of BM25 that allows field <br>weighting [21]. As fields, we make use of body, title, extracted <br>title and anchor. First, for each term in the query we count the <br>term frequency in each field of the document; each field <br>frequency is then weighted according to the corresponding weight <br>parameter: </span></nobr></DIV>
<DIV style="position:absolute;top:368;left:263"><nobr><span class="ft16"><i></i></span></nobr></DIV>
<DIV style="position:absolute;top:372;left:254"><nobr><span class="ft7">=</span></nobr></DIV>
<DIV style="position:absolute;top:388;left:268"><nobr><span class="ft17">f</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:295"><nobr><span class="ft17">tf</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:285"><nobr><span class="ft17">f</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:248"><nobr><span class="ft17">t</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:288"><nobr><span class="ft7">tf</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:276"><nobr><span class="ft7">w</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:234"><nobr><span class="ft7">wtf</span></nobr></DIV>
<DIV style="position:absolute;top:363;left:303"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:403;left:92"><nobr><span class="ft12">Similarly, we compute the document length as a weighted sum of <br>lengths of each field. Average document length in the corpus <br>becomes the average of all weighted document lengths. </span></nobr></DIV>
<DIV style="position:absolute;top:453;left:263"><nobr><span class="ft20"></span></nobr></DIV>
<DIV style="position:absolute;top:456;left:256"><nobr><span class="ft6"><i>=</i></span></nobr></DIV>
<DIV style="position:absolute;top:470;left:268"><nobr><span class="ft21"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:296"><nobr><span class="ft21"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:283"><nobr><span class="ft21"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:457;left:287"><nobr><span class="ft6"><i>dl</i></span></nobr></DIV>
<DIV style="position:absolute;top:457;left:276"><nobr><span class="ft6"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:457;left:235"><nobr><span class="ft6"><i>wdl</i></span></nobr></DIV>
<DIV style="position:absolute;top:445;left:302"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:485;left:91"><nobr><span class="ft24"> <br> <br> </span></nobr></DIV>
<DIV style="position:absolute;top:550;left:91"><nobr><span class="ft4">In our experiments we used</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:320"><nobr><span class="ft3">75</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:317"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:309"><nobr><span class="ft3">0</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:283"><nobr><span class="ft3">,</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:276"><nobr><span class="ft3">8</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:272"><nobr><span class="ft3">.</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:265"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:247"><nobr><span class="ft2">1</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:298"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:255"><nobr><span class="ft3">=</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:287"><nobr><span class="ft3">b</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:240"><nobr><span class="ft3">k</span></nobr></DIV>
<DIV style="position:absolute;top:550;left:337"><nobr><span class="ft4">. Weight for content </span></nobr></DIV>
<DIV style="position:absolute;top:569;left:91"><nobr><span class="ft12">was 1.0, title was 10.0, anchor was 10.0, and extracted title was <br>5.0. </span></nobr></DIV>
<DIV style="position:absolute;top:615;left:92"><nobr><span class="ft1">6.  EXPERIMENTAL RESULTS </span></nobr></DIV>
<DIV style="position:absolute;top:644;left:91"><nobr><span class="ft1">6.1  Data Sets and Evaluation Measures </span></nobr></DIV>
<DIV style="position:absolute;top:664;left:92"><nobr><span class="ft4">We used two data sets in our experiments. </span></nobr></DIV>
<DIV style="position:absolute;top:688;left:92"><nobr><span class="ft12">First, we downloaded and randomly selected 5,000 Word <br>documents and 5,000 PowerPoint documents from an intranet of <br>Microsoft. We call it MS hereafter. </span></nobr></DIV>
<DIV style="position:absolute;top:743;left:92"><nobr><span class="ft12">Second, we downloaded and randomly selected 500 Word and 500 <br>PowerPoint documents from the DotGov and DotCom domains on <br>the internet, respectively. </span></nobr></DIV>
<DIV style="position:absolute;top:798;left:92"><nobr><span class="ft12">Figure 7 shows the distributions of the genres of the documents. <br>We see that the documents are indeed `general documents' as we <br>define them. </span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:435"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:146"><nobr><span class="ft4">Figure 7. Distributions of document genres. </span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:268"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">Third, a data set in Chinese was also downloaded from the internet. <br>It includes 500 Word documents and 500 PowerPoint documents <br>in Chinese. </span></nobr></DIV>
<DIV style="position:absolute;top:214;left:475"><nobr><span class="ft12">We manually labeled the titles of all the documents, on the basis <br>of our specification. </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:475"><nobr><span class="ft12">Not all the documents in the two data sets have titles. Table 1 <br>shows the percentages of the documents having titles. We see that <br>DotCom and DotGov have more PowerPoint documents with titles <br>than MS. This might be because PowerPoint documents published <br>on the internet are more formal than those on the intranet. </span></nobr></DIV>
<DIV style="position:absolute;top:340;left:525"><nobr><span class="ft4">Table 1. The portion of documents with titles </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:499"><nobr><span class="ft4">Domain </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:507"><nobr><span class="ft4">Type </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:604"><nobr><span class="ft4">MS DotCom DotGov </span></nobr></DIV>
<DIV style="position:absolute;top:393;left:506"><nobr><span class="ft4">Word 75.7% </span></nobr></DIV>
<DIV style="position:absolute;top:393;left:681"><nobr><span class="ft4">77.8% 75.6% </span></nobr></DIV>
<DIV style="position:absolute;top:409;left:491"><nobr><span class="ft4">PowerPoint 82.1%  93.4%  96.4% </span></nobr></DIV>
<DIV style="position:absolute;top:425;left:475"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:449;left:475"><nobr><span class="ft12">In our experiments, we conducted evaluations on title extraction in <br>terms of precision, recall, and F-measure. The evaluation <br>measures are defined as follows: </span></nobr></DIV>
<DIV style="position:absolute;top:504;left:475"><nobr><span class="ft4">Precision: </span></nobr></DIV>
<DIV style="position:absolute;top:504;left:580"><nobr><span class="ft4">P = A / ( A + B ) </span></nobr></DIV>
<DIV style="position:absolute;top:528;left:475"><nobr><span class="ft4">Recall:   </span></nobr></DIV>
<DIV style="position:absolute;top:528;left:580"><nobr><span class="ft4">R = A / ( A + C ) </span></nobr></DIV>
<DIV style="position:absolute;top:553;left:475"><nobr><span class="ft4">F-measure: </span></nobr></DIV>
<DIV style="position:absolute;top:553;left:580"><nobr><span class="ft4">F1 = 2PR / ( P + R ) </span></nobr></DIV>
<DIV style="position:absolute;top:577;left:475"><nobr><span class="ft12">Here, A, B, C, and D are numbers of documents as those defined <br>in Table 2. </span></nobr></DIV>
<DIV style="position:absolute;top:617;left:491"><nobr><span class="ft4">Table 2. Contingence table with regard to title extraction </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:534"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:643"><nobr><span class="ft4">Is title </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:750"><nobr><span class="ft4">Is not title </span></nobr></DIV>
<DIV style="position:absolute;top:654;left:509"><nobr><span class="ft4">Extracted A  B </span></nobr></DIV>
<DIV style="position:absolute;top:671;left:499"><nobr><span class="ft4">Not extracted </span></nobr></DIV>
<DIV style="position:absolute;top:671;left:655"><nobr><span class="ft4">C </span></nobr></DIV>
<DIV style="position:absolute;top:671;left:772"><nobr><span class="ft4">D </span></nobr></DIV>
<DIV style="position:absolute;top:687;left:475"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:720;left:475"><nobr><span class="ft1">6.2  Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:740;left:475"><nobr><span class="ft12">We test the accuracies of the two baselines described in section <br>4.2. They are denoted as `largest font size' and `first line' <br>respectively.  </span></nobr></DIV>
<DIV style="position:absolute;top:804;left:475"><nobr><span class="ft1">6.3  Accuracy of Titles in File Properties </span></nobr></DIV>
<DIV style="position:absolute;top:824;left:475"><nobr><span class="ft12">We investigate how many titles in the file properties of the <br>documents are reliable. We view the titles annotated by humans as <br>true titles and test how many titles in the file properties can <br>approximately match with the true titles. We use Edit Distance to <br>conduct the approximate match. (Approximate match is only used <br>in this evaluation). This is because sometimes human annotated <br>titles can be slightly different from the titles in file properties on <br>the surface, e.g., contain extra spaces). </span></nobr></DIV>
<DIV style="position:absolute;top:956;left:475"><nobr><span class="ft4">Given string A and string B: </span></nobr></DIV>
<DIV style="position:absolute;top:980;left:475"><nobr><span class="ft24">if ( (D == 0) or ( D / ( La + Lb ) &lt;  ) ) then string A = string B <br>D: </span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:527"><nobr><span class="ft4">Edit Distance between string A and string B </span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:475"><nobr><span class="ft4">La: </span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:527"><nobr><span class="ft4">length of string A </span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:475"><nobr><span class="ft4">Lb: </span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:527"><nobr><span class="ft4">length of string B </span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:475"><nobr><span class="ft4">:  </span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:527"><nobr><span class="ft4">0.1 </span></nobr></DIV>
<DIV style="position:absolute;top:1083;left:475"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:492;left:216"><nobr><span class="ft22"></span></nobr></DIV>
<DIV style="position:absolute;top:495;left:362"><nobr><span class="ft23">×</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:333"><nobr><span class="ft23">+</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:278"><nobr><span class="ft23">+</span></nobr></DIV>
<DIV style="position:absolute;top:512;left:257"><nobr><span class="ft23">-</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:306"><nobr><span class="ft23">+</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:206"><nobr><span class="ft23">=</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:221"><nobr><span class="ft19"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:496;left:285"><nobr><span class="ft19"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:507;left:393"><nobr><span class="ft23">n</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:392"><nobr><span class="ft23">N</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:342"><nobr><span class="ft23">wtf</span></nobr></DIV>
<DIV style="position:absolute;top:524;left:296"><nobr><span class="ft23">avwdl</span></nobr></DIV>
<DIV style="position:absolute;top:505;left:302"><nobr><span class="ft23">wdl</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:286"><nobr><span class="ft23">b</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:265"><nobr><span class="ft23">b</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:232"><nobr><span class="ft23">k</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:294"><nobr><span class="ft23">k</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:270"><nobr><span class="ft23">wtf</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:194"><nobr><span class="ft23">F</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:161"><nobr><span class="ft23">BM</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:403"><nobr><span class="ft23">)</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:371"><nobr><span class="ft23">log(</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:327"><nobr><span class="ft23">)</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:271"><nobr><span class="ft23">)</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:249"><nobr><span class="ft23">1</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:242"><nobr><span class="ft23">((</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:319"><nobr><span class="ft23">)</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:314"><nobr><span class="ft23">1</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:289"><nobr><span class="ft23">(</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:181"><nobr><span class="ft23">25</span></nobr></DIV>
<DIV style="position:absolute;top:521;left:238"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:496;left:299"><nobr><span class="ft19"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">150</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1263" src="40007.png" alt="background image">
<DIV style="position:absolute;top:159;left:142"><nobr><span class="ft4">Table 3. Accuracies of titles in file properties </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:108"><nobr><span class="ft4">File Type </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:199"><nobr><span class="ft4">Domain </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:268"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:341"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:411"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:212"><nobr><span class="ft4">MS 0.299 </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:343"><nobr><span class="ft4">0.311 </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:403"><nobr><span class="ft4">0.305 </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:198"><nobr><span class="ft4">DotCom 0.210 0.214 0.212 </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:120"><nobr><span class="ft4">Word </span></nobr></DIV>
<DIV style="position:absolute;top:229;left:200"><nobr><span class="ft4">DotGov 0.182 0.177 </span></nobr></DIV>
<DIV style="position:absolute;top:229;left:403"><nobr><span class="ft4">0.180 </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:212"><nobr><span class="ft4">MS 0.229 </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:343"><nobr><span class="ft4">0.245 </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:403"><nobr><span class="ft4">0.237 </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:198"><nobr><span class="ft4">DotCom 0.185 0.186 0.186 </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:104"><nobr><span class="ft4">PowerPoint </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:200"><nobr><span class="ft4">DotGov 0.180 0.182 </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:403"><nobr><span class="ft4">0.181 </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:92"><nobr><span class="ft1">6.4  Comparison with Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:347;left:92"><nobr><span class="ft12">We conducted title extraction from the first data set (Word and <br>PowerPoint in MS). As the model, we used Perceptron.  </span></nobr></DIV>
<DIV style="position:absolute;top:387;left:92"><nobr><span class="ft12">We conduct 4-fold cross validation. Thus, all the results reported <br>here are those averaged over 4 trials. Tables 4 and 5 show the <br>results. We see that Perceptron significantly outperforms the <br>baselines. In the evaluation, we use exact matching between the <br>true titles annotated by humans and the extracted titles. </span></nobr></DIV>
<DIV style="position:absolute;top:473;left:130"><nobr><span class="ft4">Table 4. Accuracies of title extraction with Word </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:119"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:272"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:350"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:418"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:102"><nobr><span class="ft4">Model Perceptron  0.810 0.837 </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:411"><nobr><span class="ft4">0.823</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:164"><nobr><span class="ft4">Largest font size </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:284"><nobr><span class="ft4">0.700 </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:353"><nobr><span class="ft4">0.758 </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:411"><nobr><span class="ft4">0.727</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:94"><nobr><span class="ft4">Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:184"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:284"><nobr><span class="ft4">0.707 </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:353"><nobr><span class="ft4">0.767 </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:411"><nobr><span class="ft4">0.736</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:583;left:114"><nobr><span class="ft4">Table 5. Accuracies of title extraction with PowerPoint </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:119"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:272"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:350"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:418"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:102"><nobr><span class="ft4">Model Perceptron  0.875 0. </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:365"><nobr><span class="ft4">895 </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:411"><nobr><span class="ft4">0.885</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:164"><nobr><span class="ft4">Largest font size </span></nobr></DIV>
<DIV style="position:absolute;top:637;left:284"><nobr><span class="ft4">0.844 </span></nobr></DIV>
<DIV style="position:absolute;top:637;left:353"><nobr><span class="ft4">0.887 </span></nobr></DIV>
<DIV style="position:absolute;top:637;left:411"><nobr><span class="ft4">0.865</span></nobr></DIV>
<DIV style="position:absolute;top:645;left:94"><nobr><span class="ft4">Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:184"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:284"><nobr><span class="ft4">0.639 </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:353"><nobr><span class="ft4">0.671 </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:411"><nobr><span class="ft4">0.655</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:694;left:92"><nobr><span class="ft12">We see that the machine learning approach can achieve good <br>performance in title extraction. For Word documents both <br>precision and recall of the approach are 8 percent higher than <br>those of the baselines. For PowerPoint both precision and recall of <br>the approach are 2 percent higher than those of the baselines.  </span></nobr></DIV>
<DIV style="position:absolute;top:779;left:92"><nobr><span class="ft12">We conduct significance tests. The results are shown in Table 6. <br>Here, `Largest' denotes the baseline of using the largest font size, <br>`First' denotes the baseline of using the first line. The results <br>indicate that the improvements of machine learning over baselines <br>are statistically significant (in the sense p-value &lt; 0.05) </span></nobr></DIV>
<DIV style="position:absolute;top:865;left:199"><nobr><span class="ft4">Table 6. Sign test results </span></nobr></DIV>
<DIV style="position:absolute;top:887;left:94"><nobr><span class="ft4">Documents Type </span></nobr></DIV>
<DIV style="position:absolute;top:887;left:226"><nobr><span class="ft4">Sign test between </span></nobr></DIV>
<DIV style="position:absolute;top:887;left:380"><nobr><span class="ft4">p-value </span></nobr></DIV>
<DIV style="position:absolute;top:912;left:213"><nobr><span class="ft4">Perceptron vs. Largest </span></nobr></DIV>
<DIV style="position:absolute;top:912;left:377"><nobr><span class="ft4">3.59e-26 </span></nobr></DIV>
<DIV style="position:absolute;top:924;left:124"><nobr><span class="ft4">Word  </span></nobr></DIV>
<DIV style="position:absolute;top:937;left:221"><nobr><span class="ft4">Perceptron vs. First </span></nobr></DIV>
<DIV style="position:absolute;top:937;left:377"><nobr><span class="ft4">7.12e-10 </span></nobr></DIV>
<DIV style="position:absolute;top:962;left:213"><nobr><span class="ft4">Perceptron vs. Largest </span></nobr></DIV>
<DIV style="position:absolute;top:962;left:385"><nobr><span class="ft4">0.010 </span></nobr></DIV>
<DIV style="position:absolute;top:974;left:109"><nobr><span class="ft4">PowerPoint  </span></nobr></DIV>
<DIV style="position:absolute;top:987;left:221"><nobr><span class="ft4">Perceptron vs. First </span></nobr></DIV>
<DIV style="position:absolute;top:987;left:377"><nobr><span class="ft4">5.13e-40 </span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:92"><nobr><span class="ft12">We see, from the results, that the two baselines can work well for <br>title extraction, suggesting that font size and position information <br>are most useful features for title extraction. However, it is also <br>obvious that using only these two features is not enough. There </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">are cases in which all the lines have the same font size (i.e., the <br>largest font size), or cases in which the lines with the largest font <br>size only contain general descriptions like `Confidential', `White <br>paper', etc. For those cases, the `largest font size' method cannot <br>work well. For similar reasons, the `first line' method alone <br>cannot work well, either. With the combination of different <br>features (evidence in title judgment), Perceptron can outperform <br>Largest and First. </span></nobr></DIV>
<DIV style="position:absolute;top:291;left:475"><nobr><span class="ft12">We investigate the performance of solely using linguistic features. <br>We found that it does not work well. It seems that the format <br>features play important roles and the linguistic features are <br>supplements.. </span></nobr></DIV>
<DIV style="position:absolute;top:510;left:827"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:531;left:541"><nobr><span class="ft4">Figure 8. An example Word document. </span></nobr></DIV>
<DIV style="position:absolute;top:552;left:475"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:577;left:651"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:759;left:826"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:525"><nobr><span class="ft4">Figure 9. An example PowerPoint document. </span></nobr></DIV>
<DIV style="position:absolute;top:802;left:651"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:823;left:475"><nobr><span class="ft12">We conducted an error analysis on the results of Perceptron. We <br>found that the errors fell into three categories. (1) About one third <br>of the errors were related to `hard cases'. In these documents, the <br>layouts of the first pages were difficult to understand, even for <br>humans. Figure 8 and 9 shows examples. (2) Nearly one fourth of <br>the errors were from the documents which do not have true titles <br>but only contain bullets. Since we conduct extraction from the top <br>regions, it is difficult to get rid of these errors with the current <br>approach. (3). Confusions between main titles and subtitles were <br>another type of error. Since we only labeled the main titles as <br>titles, the extractions of both titles were considered incorrect. This <br>type of error does little harm to document processing like search, <br>however.  </span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:475"><nobr><span class="ft1">6.5  Comparison between Models </span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:475"><nobr><span class="ft12">To compare the performance of different machine learning models, <br>we conducted another experiment. Again, we perform 4-fold cross </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">151</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1263" src="40008.png" alt="background image">
<DIV style="position:absolute;top:159;left:92"><nobr><span class="ft12">validation on the first data set (MS). Table 7, 8 shows the results <br>of all the four models.  </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:92"><nobr><span class="ft12">It turns out that Perceptron and PMM perform the best, followed <br>by MEMM, and ME performs the worst. In general, the <br>Markovian models perform better than or as well as their classifier <br>counterparts. This seems to be because the Markovian models are <br>trained globally, while the classifiers are trained locally. The <br>Perceptron based models perform better than the ME based <br>counterparts. This seems to be because the Perceptron based <br>models are created to make better classifications, while ME <br>models are constructed for better prediction.  </span></nobr></DIV>
<DIV style="position:absolute;top:346;left:101"><nobr><span class="ft4">Table 7. Comparison between different learning models for </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:194"><nobr><span class="ft4">title extraction with Word </span></nobr></DIV>
<DIV style="position:absolute;top:384;left:111"><nobr><span class="ft4">Model Precision Recall  F1 </span></nobr></DIV>
<DIV style="position:absolute;top:400;left:101"><nobr><span class="ft4">Perceptron 0.810  0.837  0.823 </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:108"><nobr><span class="ft4">MEMM 0.797  0.824 0.810 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:114"><nobr><span class="ft4">PMM 0.827 0.823 0.825 </span></nobr></DIV>
<DIV style="position:absolute;top:448;left:119"><nobr><span class="ft4">ME 0.801 0.621 </span></nobr></DIV>
<DIV style="position:absolute;top:448;left:391"><nobr><span class="ft4">0.699 </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:480;left:101"><nobr><span class="ft4">Table 8. Comparison between different learning models for </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:178"><nobr><span class="ft4">title extraction with PowerPoint </span></nobr></DIV>
<DIV style="position:absolute;top:517;left:111"><nobr><span class="ft4">Model Precision Recall  F1 </span></nobr></DIV>
<DIV style="position:absolute;top:536;left:101"><nobr><span class="ft4">Perceptron </span></nobr></DIV>
<DIV style="position:absolute;top:533;left:210"><nobr><span class="ft4">0.875 </span></nobr></DIV>
<DIV style="position:absolute;top:533;left:302"><nobr><span class="ft4">0. 895 </span></nobr></DIV>
<DIV style="position:absolute;top:533;left:391"><nobr><span class="ft4">0. 885 </span></nobr></DIV>
<DIV style="position:absolute;top:555;left:108"><nobr><span class="ft4">MEMM 0.841  0.861 0.851 </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:114"><nobr><span class="ft4">PMM </span></nobr></DIV>
<DIV style="position:absolute;top:571;left:210"><nobr><span class="ft4">0.873 0.896 0.885 </span></nobr></DIV>
<DIV style="position:absolute;top:593;left:119"><nobr><span class="ft4">ME 0.753 0.766 </span></nobr></DIV>
<DIV style="position:absolute;top:593;left:392"><nobr><span class="ft4">0.759 </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:92"><nobr><span class="ft1">6.6  Domain Adaptation </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:92"><nobr><span class="ft12">We apply the model trained with the first data set (MS) to the <br>second data set (DotCom and DotGov). Tables 9-12 show the <br>results. </span></nobr></DIV>
<DIV style="position:absolute;top:693;left:100"><nobr><span class="ft4">Table 9. Accuracies of title extraction with Word in DotGov </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:119"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:272"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:350"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:418"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:734;left:102"><nobr><span class="ft4">Model Perceptron  0.716 0.759 </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:411"><nobr><span class="ft4">0.737</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:164"><nobr><span class="ft4">Largest font size </span></nobr></DIV>
<DIV style="position:absolute;top:753;left:284"><nobr><span class="ft4">0.549 0.619 </span></nobr></DIV>
<DIV style="position:absolute;top:753;left:411"><nobr><span class="ft4">0.582</span></nobr></DIV>
<DIV style="position:absolute;top:759;left:94"><nobr><span class="ft4">Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:775;left:119"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:778;left:184"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:775;left:284"><nobr><span class="ft4">0.462 0.521 </span></nobr></DIV>
<DIV style="position:absolute;top:775;left:411"><nobr><span class="ft4">0.490</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:822;left:103"><nobr><span class="ft4">Table 10. Accuracies of title extraction with PowerPoint in </span></nobr></DIV>
<DIV style="position:absolute;top:837;left:246"><nobr><span class="ft4">DotGov </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:119"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:272"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:350"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:859;left:418"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:878;left:102"><nobr><span class="ft4">Model Perceptron  0.900 0.906 </span></nobr></DIV>
<DIV style="position:absolute;top:875;left:411"><nobr><span class="ft4">0.903</span></nobr></DIV>
<DIV style="position:absolute;top:900;left:164"><nobr><span class="ft4">Largest font size </span></nobr></DIV>
<DIV style="position:absolute;top:897;left:284"><nobr><span class="ft4">0.871 0.888 </span></nobr></DIV>
<DIV style="position:absolute;top:897;left:411"><nobr><span class="ft4">0.879</span></nobr></DIV>
<DIV style="position:absolute;top:903;left:94"><nobr><span class="ft4">Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:918;left:119"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:922;left:184"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:919;left:284"><nobr><span class="ft4">0.554 0.564 </span></nobr></DIV>
<DIV style="position:absolute;top:919;left:411"><nobr><span class="ft4">0.559</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:965;left:95"><nobr><span class="ft4">Table 11. Accuracies of title extraction with Word in DotCom </span></nobr></DIV>
<DIV style="position:absolute;top:995;left:119"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:995;left:211"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:987;left:279"><nobr><span class="ft4">Precisio</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:297"><nobr><span class="ft4">n </span></nobr></DIV>
<DIV style="position:absolute;top:995;left:351"><nobr><span class="ft4">Recall F1 </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:102"><nobr><span class="ft4">Model Perceptron 0.832 0.880 </span></nobr></DIV>
<DIV style="position:absolute;top:1018;left:411"><nobr><span class="ft4">0.855</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:167"><nobr><span class="ft4">Largest font size </span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:286"><nobr><span class="ft4">0.676 0.753 </span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:411"><nobr><span class="ft4">0.712</span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:94"><nobr><span class="ft4">Baselines </span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:119"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:187"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:286"><nobr><span class="ft4">0.577 0.643 </span></nobr></DIV>
<DIV style="position:absolute;top:1062;left:411"><nobr><span class="ft4">0.608</span></nobr></DIV>
<DIV style="position:absolute;top:1084;left:92"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:503"><nobr><span class="ft4">Table 12. Performance of PowerPoint document title </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:590"><nobr><span class="ft4">extraction in DotCom </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:502"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:594"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:197;left:662"><nobr><span class="ft4">Precisio</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:680"><nobr><span class="ft4">n </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:734"><nobr><span class="ft4">Recall F1 </span></nobr></DIV>
<DIV style="position:absolute;top:231;left:485"><nobr><span class="ft4">Model Perceptron 0.910 0.903 </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:794"><nobr><span class="ft4">0.907</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:550"><nobr><span class="ft4">Largest font size</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:669"><nobr><span class="ft4">0.864 0.886 </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:794"><nobr><span class="ft4">0.875</span></nobr></DIV>
<DIV style="position:absolute;top:256;left:477"><nobr><span class="ft4">Baselines</span></nobr></DIV>
<DIV style="position:absolute;top:271;left:502"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:275;left:570"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:272;left:669"><nobr><span class="ft4">0.570 0.585 </span></nobr></DIV>
<DIV style="position:absolute;top:272;left:794"><nobr><span class="ft4">0.577</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:475"><nobr><span class="ft12">From the results, we see that the models can be adapted to <br>different domains well. There is almost no drop in accuracy. The <br>results indicate that the patterns of title formats exist across <br>different domains, and it is possible to construct a domain <br>independent model by mainly using formatting information.</span></nobr></DIV>
<DIV style="position:absolute;top:354;left:791"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:389;left:475"><nobr><span class="ft1">6.7  Language Adaptation </span></nobr></DIV>
<DIV style="position:absolute;top:409;left:475"><nobr><span class="ft12">We apply the model trained with the data in English (MS) to the <br>data set in Chinese.  </span></nobr></DIV>
<DIV style="position:absolute;top:448;left:475"><nobr><span class="ft4">Tables 13-14 show the results. </span></nobr></DIV>
<DIV style="position:absolute;top:473;left:479"><nobr><span class="ft4">Table 13. Accuracies of title extraction with Word in Chinese  </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:502"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:655"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:734"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:801"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:485"><nobr><span class="ft4">Model Perceptron  0.817 0.805 </span></nobr></DIV>
<DIV style="position:absolute;top:511;left:794"><nobr><span class="ft4">0.811</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:547"><nobr><span class="ft4">Largest font size</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:667"><nobr><span class="ft4">0.722 </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:737"><nobr><span class="ft4">0.755 </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:794"><nobr><span class="ft4">0.738</span></nobr></DIV>
<DIV style="position:absolute;top:527;left:477"><nobr><span class="ft4">Baselines</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:502"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:568"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:667"><nobr><span class="ft4">0.743 </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:736"><nobr><span class="ft4">0.777 </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:794"><nobr><span class="ft4">0.760</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:475"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:583;left:487"><nobr><span class="ft4">Table 14. Accuracies of title extraction with PowerPoint in </span></nobr></DIV>
<DIV style="position:absolute;top:599;left:628"><nobr><span class="ft4">Chinese </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:502"><nobr><span class="ft4">  </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:655"><nobr><span class="ft4">Precision </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:734"><nobr><span class="ft4">Recall </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:801"><nobr><span class="ft4">F1 </span></nobr></DIV>
<DIV style="position:absolute;top:637;left:485"><nobr><span class="ft4">Model Perceptron  0.766 0.812 </span></nobr></DIV>
<DIV style="position:absolute;top:637;left:794"><nobr><span class="ft4">0.789</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:547"><nobr><span class="ft4">Largest font size</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:667"><nobr><span class="ft4">0.753 </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:737"><nobr><span class="ft4">0.813 </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:794"><nobr><span class="ft4">0.782</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:477"><nobr><span class="ft4">Baselines</span></nobr></DIV>
<DIV style="position:absolute;top:668;left:502"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:568"><nobr><span class="ft4">First line </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:667"><nobr><span class="ft4">0.627 </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:736"><nobr><span class="ft4">0.676 </span></nobr></DIV>
<DIV style="position:absolute;top:669;left:794"><nobr><span class="ft4">0.650</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:475"><nobr><span class="ft3"> </span></nobr></DIV>
<DIV style="position:absolute;top:711;left:475"><nobr><span class="ft12">We see that the models can be adapted to a different language. <br>There are only small drops in accuracy. Obviously, the linguistic <br>features do not work for Chinese, but the effect of not using them <br>is negligible. The results indicate that the patterns of title formats <br>exist across different languages. </span></nobr></DIV>
<DIV style="position:absolute;top:797;left:475"><nobr><span class="ft12">From the domain adaptation and language adaptation results, we <br>conclude that the use of formatting information is the key to a <br>successful extraction from general documents.</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:719"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:861;left:475"><nobr><span class="ft1">6.8  Search with Extracted Titles </span></nobr></DIV>
<DIV style="position:absolute;top:881;left:475"><nobr><span class="ft12"> We performed experiments on using title extraction for document <br>retrieval.  As a baseline, we employed BM25 without using <br>extracted titles. The ranking mechanism was as described in <br>Section 5. The weights were heuristically set. We did not conduct <br>optimization on the weights. </span></nobr></DIV>
<DIV style="position:absolute;top:967;left:475"><nobr><span class="ft12">The evaluation was conducted on a corpus of 1.3 M documents <br>crawled from the intranet of Microsoft using 100 evaluation <br>queries obtained from this intranet's search engine query logs. 50 <br>queries were from the most popular set, while 50 queries other <br>were chosen randomly. Users were asked to provide judgments of <br>the degree of document relevance from a scale of 1to 5 (1 <br>meaning detrimental, 2 ­ bad, 3 ­ fair, 4 ­ good and 5 ­ excellent).  </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">152</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
	.ft25{font-size:3px;line-height:7px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1263" src="40009.png" alt="background image">
<DIV style="position:absolute;top:159;left:92"><nobr><span class="ft12">Figure 10 shows the results. In the chart two sets of precision <br>results were obtained by either considering good or excellent <br>documents as relevant (left 3 bars with relevance threshold 0.5), or <br>by considering only excellent documents as relevant (right 3 bars <br>with relevance threshold 1.0) </span></nobr></DIV>
<DIV style="position:absolute;top:441;left:105"><nobr><span class="ft21"><i>0</i></span></nobr></DIV>
<DIV style="position:absolute;top:420;left:98"><nobr><span class="ft21"><i>0.05</i></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:101"><nobr><span class="ft21"><i>0.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:380;left:98"><nobr><span class="ft21"><i>0.15</i></span></nobr></DIV>
<DIV style="position:absolute;top:359;left:101"><nobr><span class="ft21"><i>0.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:339;left:98"><nobr><span class="ft21"><i>0.25</i></span></nobr></DIV>
<DIV style="position:absolute;top:319;left:101"><nobr><span class="ft21"><i>0.3</i></span></nobr></DIV>
<DIV style="position:absolute;top:298;left:98"><nobr><span class="ft21"><i>0.35</i></span></nobr></DIV>
<DIV style="position:absolute;top:278;left:101"><nobr><span class="ft21"><i>0.4</i></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:98"><nobr><span class="ft21"><i>0.45</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:124"><nobr><span class="ft21"><i>P@10</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:162"><nobr><span class="ft21"><i>P@5</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:192"><nobr><span class="ft21"><i>Reciprocal</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:234"><nobr><span class="ft21"><i>P@10</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:273"><nobr><span class="ft21"><i>P@5</i></span></nobr></DIV>
<DIV style="position:absolute;top:448;left:303"><nobr><span class="ft21"><i>Reciprocal</i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:164"><nobr><span class="ft21"><i>0.5</i></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:277"><nobr><span class="ft21"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:349;left:344"><nobr><span class="ft25"><i>BM25 Anchor, Title, Body<br>BM25 Anchor, Title, Body, ExtractedTitle</i></span></nobr></DIV>
<DIV style="position:absolute;top:245;left:94"><nobr><span class="ft21"><i>Name All</i></span></nobr></DIV>
<DIV style="position:absolute;top:471;left:193"><nobr><span class="ft21"><i>RelevanceThreshold Data</i></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:339"><nobr><span class="ft21"><i>Description</i></span></nobr></DIV>
<DIV style="position:absolute;top:468;left:441"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:487;left:172"><nobr><span class="ft4">Figure 10. Search ranking results. </span></nobr></DIV>
<DIV style="position:absolute;top:508;left:268"><nobr><span class="ft4"> </span></nobr></DIV>
<DIV style="position:absolute;top:529;left:91"><nobr><span class="ft12">Figure 10 shows different document retrieval results with different <br>ranking functions in terms of precision @10, precision @5 and <br>reciprocal rank: </span></nobr></DIV>
<DIV style="position:absolute;top:581;left:118"><nobr><span class="ft4">· </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:144"><nobr><span class="ft12">Blue bar ­ BM25 including the fields body, title (file <br>property), and anchor text. </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:118"><nobr><span class="ft4">· </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:144"><nobr><span class="ft12">Purple bar ­ BM25 including the fields body, title (file <br>property), anchor text, and extracted title. </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:91"><nobr><span class="ft12">With the additional field of extracted title included in BM25 the <br>precision @10 increased from 0.132 to 0.145, or by ~10%. Thus, <br>it is safe to say that the use of extracted title can indeed improve <br>the precision of document retrieval. </span></nobr></DIV>
<DIV style="position:absolute;top:733;left:92"><nobr><span class="ft1">7.  CONCLUSION </span></nobr></DIV>
<DIV style="position:absolute;top:753;left:92"><nobr><span class="ft12">In this paper, we have investigated the problem of automatically <br>extracting titles from general documents. We have tried using a <br>machine learning approach to address the problem. </span></nobr></DIV>
<DIV style="position:absolute;top:808;left:92"><nobr><span class="ft12">Previous work showed that the machine learning approach can <br>work well for metadata extraction from research papers. In this <br>paper, we showed that the approach can work for extraction from <br>general documents as well. Our experimental results indicated that <br>the machine learning approach can work significantly better than <br>the baselines in title extraction from Office documents. Previous <br>work on metadata extraction mainly used linguistic features in <br>documents, while we mainly used formatting information. It <br>appeared that using formatting information is a key for <br>successfully conducting title extraction from general documents. </span></nobr></DIV>
<DIV style="position:absolute;top:971;left:92"><nobr><span class="ft12">We tried different machine learning models including Perceptron, <br>Maximum Entropy, Maximum Entropy Markov Model, and Voted <br>Perceptron. We found that the performance of the Perceptorn <br>models was the best. We applied models constructed in one <br>domain to another domain and applied models trained in one <br>language to another language. We found that the accuracies did <br>not drop substantially across different domains and across <br>different languages, indicating that the models were generic. We </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft12">also attempted to use the extracted titles in document retrieval. We <br>observed a significant improvement in document ranking <br>performance for search when using extracted title information. All <br>the above investigations were not conducted in previous work, and <br>through our investigations we verified the generality and the <br>significance of the title extraction approach. </span></nobr></DIV>
<DIV style="position:absolute;top:270;left:475"><nobr><span class="ft1">8.  ACKNOWLEDGEMENTS </span></nobr></DIV>
<DIV style="position:absolute;top:290;left:475"><nobr><span class="ft12">We thank Chunyu Wei and Bojuan Zhao for their work on data <br>annotation. We acknowledge Jinzhu Li for his assistance in <br>conducting the experiments. We thank Ming Zhou, John Chen, <br>Jun Xu, and the anonymous reviewers of JCDL'05 for their <br>valuable comments on this paper. </span></nobr></DIV>
<DIV style="position:absolute;top:385;left:475"><nobr><span class="ft1">9.  REFERENCES </span></nobr></DIV>
<DIV style="position:absolute;top:405;left:475"><nobr><span class="ft4">[1]  Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. A </span></nobr></DIV>
<DIV style="position:absolute;top:420;left:501"><nobr><span class="ft12">maximum entropy approach to natural language processing. <br>Computational Linguistics, 22:39-71, 1996. </span></nobr></DIV>
<DIV style="position:absolute;top:457;left:475"><nobr><span class="ft4">[2]  Collins, M. Discriminative training methods for hidden </span></nobr></DIV>
<DIV style="position:absolute;top:472;left:501"><nobr><span class="ft12">markov models: theory and experiments with perceptron <br>algorithms. In Proceedings of Conference on Empirical <br>Methods in Natural Language Processing, 1-8, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:475"><nobr><span class="ft4">[3]  Cortes, C. and Vapnik, V. Support-vector networks. Machine </span></nobr></DIV>
<DIV style="position:absolute;top:540;left:501"><nobr><span class="ft4">Learning, 20:273-297, 1995. </span></nobr></DIV>
<DIV style="position:absolute;top:561;left:475"><nobr><span class="ft4">[4]  Chieu, H. L. and Ng, H. T. A maximum entropy approach to </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:501"><nobr><span class="ft12">information extraction from semi-structured and free text. In <br>Proceedings of the Eighteenth National Conference on <br>Artificial Intelligence, 768-791, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:628;left:475"><nobr><span class="ft4">[5]  Evans, D. K., Klavans, J. L., and McKeown, K. R. Columbia </span></nobr></DIV>
<DIV style="position:absolute;top:644;left:501"><nobr><span class="ft12">newsblaster: multilingual news summarization on the Web. <br>In Proceedings of Human Language Technology conference / <br>North American chapter of the Association for <br>Computational Linguistics annual meeting, 1-4, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:711;left:475"><nobr><span class="ft4">[6]  Ghahramani, Z. and Jordan, M. I. Factorial hidden markov </span></nobr></DIV>
<DIV style="position:absolute;top:727;left:501"><nobr><span class="ft4">models. Machine Learning, 29:245-273, 1997. </span></nobr></DIV>
<DIV style="position:absolute;top:748;left:475"><nobr><span class="ft4">[7]  Gheel, J. and Anderson, T. Data and metadata for finding and </span></nobr></DIV>
<DIV style="position:absolute;top:763;left:501"><nobr><span class="ft12">reminding, In Proceedings of the 1999 International <br>Conference on Information Visualization, 446-451,1999. </span></nobr></DIV>
<DIV style="position:absolute;top:800;left:475"><nobr><span class="ft4">[8]  Giles, C. L., Petinot, Y., Teregowda P. B., Han, H., </span></nobr></DIV>
<DIV style="position:absolute;top:815;left:501"><nobr><span class="ft12">Lawrence, S., Rangaswamy, A., and Pal, N. eBizSearch: a <br>niche search engine for e-Business. In Proceedings of the <br>26th Annual International ACM SIGIR Conference on <br>Research and Development in Information Retrieval, 413-<br>414, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:898;left:475"><nobr><span class="ft4">[9]  Giuffrida, G., Shek, E. C., and Yang, J. Knowledge-based </span></nobr></DIV>
<DIV style="position:absolute;top:914;left:501"><nobr><span class="ft12">metadata extraction from PostScript files. In Proceedings of <br>the Fifth ACM Conference on Digital Libraries, 77-84, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:950;left:475"><nobr><span class="ft4">[10] Han, H., Giles, C. L., Manavoglu, E., Zha, H., Zhang, Z., and </span></nobr></DIV>
<DIV style="position:absolute;top:966;left:501"><nobr><span class="ft12">Fox, E. A. Automatic document metadata extraction using <br>support vector machines. In Proceedings of the Third <br>ACM/IEEE-CS Joint Conference on Digital Libraries, 37-48, <br>2003. </span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:475"><nobr><span class="ft4">[11] Kobayashi, M., and Takeda, K. Information retrieval on the </span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:501"><nobr><span class="ft4">Web. ACM Computing Surveys, 32:144-173, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:1070;left:475"><nobr><span class="ft4">[12] Lafferty, J., McCallum, A., and Pereira, F. Conditional </span></nobr></DIV>
<DIV style="position:absolute;top:1085;left:501"><nobr><span class="ft4">random fields: probabilistic models for segmenting and </span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">153</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1263;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1263" src="40010.png" alt="background image">
<DIV style="position:absolute;top:159;left:118"><nobr><span class="ft12">labeling sequence data. In Proceedings of the Eighteenth <br>International Conference on Machine Learning, 282-289, <br>2001. </span></nobr></DIV>
<DIV style="position:absolute;top:211;left:92"><nobr><span class="ft4">[13] Li, Y., Zaragoza, H., Herbrich, R., Shawe-Taylor J., and </span></nobr></DIV>
<DIV style="position:absolute;top:227;left:118"><nobr><span class="ft12">Kandola, J. S. The perceptron algorithm with uneven margins. <br>In Proceedings of the Nineteenth International Conference <br>on Machine Learning, 379-386, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:279;left:92"><nobr><span class="ft4">[14] Liddy, E. D., Sutton, S., Allen, E., Harwell, S., Corieri, S., </span></nobr></DIV>
<DIV style="position:absolute;top:294;left:118"><nobr><span class="ft12">Yilmazel, O., Ozgencil, N. E., Diekema, A., McCracken, N., <br>and Silverstein, J. Automatic Metadata generation &amp; <br>evaluation. In Proceedings of the 25th Annual International <br>ACM SIGIR Conference on Research and Development in <br>Information Retrieval, 401-402, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:92"><nobr><span class="ft4">[15] Littlefield, A. Effective enterprise information retrieval </span></nobr></DIV>
<DIV style="position:absolute;top:392;left:118"><nobr><span class="ft12">across new content formats. In Proceedings of the Seventh <br>Search Engine Conference, <br>http://www.infonortics.com/searchengines/sh02/02prog.html, <br>2002. </span></nobr></DIV>
<DIV style="position:absolute;top:460;left:92"><nobr><span class="ft4">[16] Mao, S., Kim, J. W., and Thoma, G. R. A dynamic feature </span></nobr></DIV>
<DIV style="position:absolute;top:475;left:118"><nobr><span class="ft12">generation system for automated metadata extraction in <br>preservation of digital materials. In Proceedings of the First <br>International Workshop on Document Image Analysis for <br>Libraries, 225-232, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:92"><nobr><span class="ft4">[17] McCallum, A., Freitag, D., and Pereira, F. Maximum entropy </span></nobr></DIV>
<DIV style="position:absolute;top:558;left:118"><nobr><span class="ft12">markov models for information extraction and segmentation. <br>In Proceedings of the Seventeenth International Conference <br>on Machine Learning, 591-598, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:610;left:92"><nobr><span class="ft4">[18] Murphy, L. D. Digital document metadata in organizations: </span></nobr></DIV>
<DIV style="position:absolute;top:626;left:118"><nobr><span class="ft12">roles, analytical approaches, and future research directions. <br>In Proceedings of the Thirty-First Annual Hawaii <br>International Conference on System Sciences, 267-276, 1998. </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:475"><nobr><span class="ft4">[19] Pinto, D., McCallum, A., Wei, X., and Croft, W. B. Table </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:501"><nobr><span class="ft12">extraction using conditional random fields. In Proceedings of <br>the 26th Annual International ACM SIGIR Conference on <br>Research and Development in Information Retrieval, 235-<br>242, 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:242;left:475"><nobr><span class="ft4">[20] Ratnaparkhi, A. Unsupervised statistical models for </span></nobr></DIV>
<DIV style="position:absolute;top:257;left:501"><nobr><span class="ft12">prepositional phrase attachment. In Proceedings of the <br>Seventeenth International Conference on Computational <br>Linguistics. 1079-1085, 1998. </span></nobr></DIV>
<DIV style="position:absolute;top:309;left:475"><nobr><span class="ft4">[21] Robertson, S., Zaragoza, H., and Taylor, M. Simple BM25 </span></nobr></DIV>
<DIV style="position:absolute;top:325;left:501"><nobr><span class="ft12">extension to multiple weighted fields, In Proceedings of <br>ACM Thirteenth Conference on Information and Knowledge <br>Management, 42-49, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:475"><nobr><span class="ft4">[22] Yi, J. and Sundaresan, N. Metadata based Web mining for </span></nobr></DIV>
<DIV style="position:absolute;top:392;left:501"><nobr><span class="ft12">relevance, In Proceedings of the 2000 International <br>Symposium on Database Engineering &amp; Applications, 113-<br>121, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:444;left:475"><nobr><span class="ft4">[23] Yilmazel, O., Finneran, C. M., and Liddy, E. D. MetaExtract: </span></nobr></DIV>
<DIV style="position:absolute;top:460;left:501"><nobr><span class="ft12">An NLP system to automatically assign metadata. In <br>Proceedings of the 2004 Joint ACM/IEEE Conference on <br>Digital Libraries, 241-242, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:512;left:475"><nobr><span class="ft4">[24] Zhang, J. and Dimitroff, A. Internet search engines' response </span></nobr></DIV>
<DIV style="position:absolute;top:527;left:501"><nobr><span class="ft12">to metadata Dublin Core implementation. Journal of <br>Information Science, 30:310-320, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:564;left:475"><nobr><span class="ft4">[25] Zhang, L., Pan, Y., and Zhang, T. Recognising and using </span></nobr></DIV>
<DIV style="position:absolute;top:579;left:501"><nobr><span class="ft12">named entities: focused named entity recognition using <br>machine learning. In Proceedings of the 27th Annual <br>International ACM SIGIR Conference on Research and <br>Development in Information Retrieval, 281-288, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:475"><nobr><span class="ft4">[26] http://dublincore.org/groups/corporate/Seattle/ </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:92"><nobr><span class="ft0"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1176;left:449"><nobr><span class="ft9">154</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
