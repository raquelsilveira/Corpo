<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Microsoft Word - p105-zhang.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="smadellz">
<META name="date" content="2004-07-08T13:47:34+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:9px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:7px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft5{font-size:16px;font-family:Times;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:11px;font-family:Times;color:#000000;}
	.ft9{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft10{font-size:8px;font-family:Times;color:#000000;}
	.ft11{font-size:7px;font-family:Times;color:#000000;}
	.ft12{font-size:9px;font-family:Times;color:#000000;}
	.ft13{font-size:9px;font-family:Times;color:#000000;}
	.ft14{font-size:16px;font-family:Courier;color:#000000;}
	.ft15{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft16{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft17{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
	.ft18{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213001.png" alt="background image">
<DIV style="position:absolute;top:87;left:112"><nobr><span class="ft0"><b>Web Taxonomy Integration through Co-Bootstrapping </b></span></nobr></DIV>
<DIV style="position:absolute;top:127;left:204"><nobr><span class="ft1">Dell Zhang</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:293"><nobr><span class="ft2">1,2</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:308"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:143;left:141"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:147"><nobr><span class="ft4">Department of Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:164;left:186"><nobr><span class="ft4">School of Computing </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:158"><nobr><span class="ft4">S15-05-24, 3 Science Drive 2 </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:148"><nobr><span class="ft4">National University of Singapore </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:195"><nobr><span class="ft4">Singapore 117543 </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:175"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:180"><nobr><span class="ft4">Singapore-MIT Alliance </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:148"><nobr><span class="ft4">E4-04-10, 4 Engineering Drive 3 </span></nobr></DIV>
<DIV style="position:absolute;top:268;left:195"><nobr><span class="ft4">Singapore 117576 </span></nobr></DIV>
<DIV style="position:absolute;top:285;left:208"><nobr><span class="ft4">+65-68744251 </span></nobr></DIV>
<DIV style="position:absolute;top:310;left:193"><nobr><span class="ft1">dell.z@ieee.org</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:598"><nobr><span class="ft1">Wee Sun Lee</span></nobr></DIV>
<DIV style="position:absolute;top:123;left:709"><nobr><span class="ft2">1,2</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:724"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:143;left:546"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:147;left:552"><nobr><span class="ft4">Department of Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:164;left:591"><nobr><span class="ft4">School of Computing </span></nobr></DIV>
<DIV style="position:absolute;top:181;left:555"><nobr><span class="ft4">SOC1-05-26, 3 Science Drive 2 </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:553"><nobr><span class="ft4">National University of Singapore </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:600"><nobr><span class="ft4">Singapore 117543 </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:580"><nobr><span class="ft3">2</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:585"><nobr><span class="ft4">Singapore-MIT Alliance </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:553"><nobr><span class="ft4">E4-04-10, 4 Engineering Drive 3 </span></nobr></DIV>
<DIV style="position:absolute;top:268;left:600"><nobr><span class="ft4">Singapore 117576 </span></nobr></DIV>
<DIV style="position:absolute;top:285;left:613"><nobr><span class="ft4">+65-68744526 </span></nobr></DIV>
<DIV style="position:absolute;top:310;left:561"><nobr><span class="ft1">leews@comp.nus.edu.sg </span></nobr></DIV>
<DIV style="position:absolute;top:335;left:661"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:81"><nobr><span class="ft5"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:365;left:179"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:81"><nobr><span class="ft15">We address the problem of integrating objects from a source <br>taxonomy into a master taxonomy. This problem is not only <br>currently pervasive on the web, but also important to the <br>emerging semantic web. A straightforward approach to <br>automating this process would be to learn a classifier that can <br>classify objects from the source taxonomy into categories of the <br>master taxonomy. The key insight is that the availability of the <br>source taxonomy data could be helpful to build better classifiers <br>for the master taxonomy if their categorizations have some <br>semantic overlap. In this paper, we propose a new approach, co-<br>bootstrapping, to enhance the classification by exploiting such <br>implicit knowledge. Our experiments with real-world web data <br>show substantial improvements in the performance of taxonomy <br>integration. </span></nobr></DIV>
<DIV style="position:absolute;top:628;left:81"><nobr><span class="ft5"><b>Categories and Subject Descriptors</b></span></nobr></DIV>
<DIV style="position:absolute;top:631;left:352"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:649;left:81"><nobr><span class="ft16">H.2.8 [<b>Database Management</b>]: Database Applications ­ <i>data <br>mining</i>; H.2.5 [<b>Database Management</b>]: Heterogeneous <br>Databases; I.2.6 [<b>Artificial Intelligence</b>]: Learning; I.5.2 <br>[<b>Pattern Recognition</b>]: Design Methodology ­ <i>classifier design <br>and evaluation</i>. </span></nobr></DIV>
<DIV style="position:absolute;top:751;left:81"><nobr><span class="ft5"><b>General Terms</b></span></nobr></DIV>
<DIV style="position:absolute;top:753;left:196"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:770;left:81"><nobr><span class="ft6">Algorithms, Experimentation, Theory. </span></nobr></DIV>
<DIV style="position:absolute;top:806;left:81"><nobr><span class="ft5"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:826;left:81"><nobr><span class="ft15">Semantic Web, Ontology Mapping, Taxonomy Integration, <br>Machine Learning, Classification, Bootstrapping, Boosting. </span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft5"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:877;left:95"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:878;left:105"><nobr><span class="ft5"><b>INTRODUCTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:897;left:81"><nobr><span class="ft15">A taxonomy, or directory or catalog, is a division of a set of <br>objects (documents, images, products, goods, services, etc.) into <br>a set of categories. There are a tremendous number of </span></nobr></DIV>
<DIV style="position:absolute;top:361;left:477"><nobr><span class="ft15">taxonomies on the web, and we often need to integrate objects <br>from a source taxonomy into a master taxonomy.</span></nobr></DIV>
<DIV style="position:absolute;top:379;left:743"><nobr><span class="ft10"> </span></nobr></DIV>
<DIV style="position:absolute;top:402;left:477"><nobr><span class="ft15">This problem is currently pervasive on the web, given that many <br>websites are aggregators of information from various other <br>websites [2]. A few examples will illustrate the scenario. A web <br>marketplace like Amazon</span></nobr></DIV>
<DIV style="position:absolute;top:448;left:627"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:451;left:633"><nobr><span class="ft6"> may want to combine goods from </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:477"><nobr><span class="ft15">multiple vendors' catalogs into its own. A web portal like <br>NCSTRL</span></nobr></DIV>
<DIV style="position:absolute;top:480;left:532"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:483;left:540"><nobr><span class="ft6"> may want to combine documents from multiple </span></nobr></DIV>
<DIV style="position:absolute;top:499;left:477"><nobr><span class="ft15">libraries' directories into its own. A company may want to merge <br>its service taxonomy with its partners'. A researcher may want to <br>merge his/her bookmark taxonomy with his/her peers'. <br>Singapore-MIT Alliance</span></nobr></DIV>
<DIV style="position:absolute;top:545;left:614"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:548;left:620"><nobr><span class="ft6">, an innovative engineering education </span></nobr></DIV>
<DIV style="position:absolute;top:564;left:477"><nobr><span class="ft15">and research collaboration among MIT, NUS and NTU, has a <br>need to integrate the academic resource (courses, seminars, <br>reports, softwares, etc.) taxonomies of these three universities.  </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:477"><nobr><span class="ft15">This problem is also important to the emerging semantic web [4], <br>where data has structures and ontologies describe the semantics <br>of the data, thus better enabling computers and people to work in <br>cooperation. On the semantic web, data often come from many <br>different ontologies, and information processing across <br>ontologies is not possible without knowing the semantic <br>mappings between them. Since taxonomies are central <br>components of ontologies, ontology mapping necessarily involves <br>finding the correspondences between two taxonomies, which is <br>often based on integrating objects from one taxonomy into the <br>other and vice versa [10, 14]. </span></nobr></DIV>
<DIV style="position:absolute;top:809;left:477"><nobr><span class="ft15">If all taxonomy creators and users agreed on a universal standard, <br>taxonomy integration would not be so difficult. But the web has <br>evolved without central editorship. Hence the correspondences <br>between two taxonomies are inevitably noisy and fuzzy. For <br>illustration, consider the taxonomies of two web portals Google</span></nobr></DIV>
<DIV style="position:absolute;top:871;left:832"><nobr><span class="ft11">4</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:890;left:477"><nobr><span class="ft6">and Yahoo</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:545"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:553"><nobr><span class="ft6">: what is "Arts/Music/Styles/" in one may be </span></nobr></DIV>
<DIV style="position:absolute;top:906;left:477"><nobr><span class="ft15">"Entertainment/Music/Genres/" in the other, category <br>"Computers_and_Internet/Software/Freeware" and category </span></nobr></DIV>
<DIV style="position:absolute;top:950;left:477"><nobr><span class="ft6">                                                             </span></nobr></DIV>
<DIV style="position:absolute;top:969;left:477"><nobr><span class="ft11">1</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:481"><nobr><span class="ft6"> http://www.amazon.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:477"><nobr><span class="ft11">2</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:481"><nobr><span class="ft6"> http://www.ncstrl.org/ </span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:477"><nobr><span class="ft11">3</span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:481"><nobr><span class="ft6"> http://web.mit.edu/sma/ </span></nobr></DIV>
<DIV style="position:absolute;top:1036;left:477"><nobr><span class="ft11">4</span></nobr></DIV>
<DIV style="position:absolute;top:1039;left:481"><nobr><span class="ft6"> http://www.google.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:477"><nobr><span class="ft11">5</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:481"><nobr><span class="ft6"> http://www.yahoo.com/ </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:86"><nobr><span class="ft10"> </span></nobr></DIV>
<DIV style="position:absolute;top:969;left:86"><nobr><span class="ft17">Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are <br>not made or distributed for profit or commercial advantage and that copies <br>bear this notice and the full citation on the first page. To copy otherwise, or <br>republish, to post on servers or to redistribute to lists, requires prior specific <br>permission and/or a fee. <br><i>SIGIR'04</i>, July 25-29, 2004, Sheffield, South Yorkshire, UK. <br>Copyright 2004 ACM 1-58113-881-4/04/0007...$5.00. </span></nobr></DIV>
<DIV style="position:absolute;top:1080;left:86"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">410</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft19{font-size:11px;font-family:Symbol;color:#000000;}
	.ft20{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft21{font-size:11px;font-family:Times;color:#000000;}
	.ft22{font-size:5px;font-family:Times;color:#000000;}
	.ft23{font-size:5px;font-family:Times;color:#000000;}
	.ft24{font-size:16px;font-family:Symbol;color:#000000;}
	.ft25{font-size:5px;font-family:Symbol;color:#000000;}
	.ft26{font-size:18px;font-family:Symbol;color:#000000;}
	.ft27{font-size:2px;font-family:Times;color:#000000;}
	.ft28{font-size:12px;font-family:Symbol;color:#000000;}
	.ft29{font-size:19px;font-family:Times;color:#000000;}
	.ft30{font-size:25px;font-family:Symbol;color:#000000;}
	.ft31{font-size:19px;font-family:Symbol;color:#000000;}
	.ft32{font-size:17px;font-family:Symbol;color:#000000;}
	.ft33{font-size:12px;font-family:Times;color:#000000;}
	.ft34{font-size:6px;font-family:Times;color:#000000;}
	.ft35{font-size:12px;font-family:Times;color:#000000;}
	.ft36{font-size:6px;font-family:Symbol;color:#000000;}
	.ft37{font-size:20px;font-family:Times;color:#000000;}
	.ft38{font-size:11px;line-height:18px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213002.png" alt="background image">
<DIV style="position:absolute;top:85;left:81"><nobr><span class="ft15">"Computers/Open_Source/Software" have similar contents but <br>show non-trivial differences, and so on. It is unclear if a <br>universal standard will appear outside specific domains, and <br>even for those domains, there is a need to integrate objects from <br>legacy taxonomy into the standard taxonomy. </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:81"><nobr><span class="ft15">Manual taxonomy integration is tedious, error-prone, and clearly <br>not possible at the web scale. A straightforward approach to <br>automating this process would be to formulate it as a <br>classification problem which has being well-studied in machine <br>learning area [18]. Normally the classifier would be constructed <br>using objects in the master taxonomy as training examples, and <br>the source taxonomy would be completely ignored during <br>learning. However, the availability of the source taxonomy data <br>could be helpful to build better classifiers for the master <br>taxonomy if their categorizations have some semantic overlap, <br>particularly when the number of training examples is not very <br>large. </span></nobr></DIV>
<DIV style="position:absolute;top:378;left:81"><nobr><span class="ft6">Possible useful semantic relationships between a master category  </span></nobr></DIV>
<DIV style="position:absolute;top:395;left:84"><nobr><span class="ft8"><i>C </i> and a source category  <i>S </i> include: </span></nobr></DIV>
<DIV style="position:absolute;top:409;left:81"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:412;left:87"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:414;left:100"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:414;left:123"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:409;left:113"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:413;left:135"><nobr><span class="ft6"> (identical): an object belongs to <i>C </i> if and only if it </span></nobr></DIV>
<DIV style="position:absolute;top:431;left:97"><nobr><span class="ft6">belongs to  <i>S </i>; </span></nobr></DIV>
<DIV style="position:absolute;top:445;left:81"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:87"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:450;left:100"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:450;left:123"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:445;left:135"><nobr><span class="ft19">= </span></nobr></DIV>
<DIV style="position:absolute;top:446;left:112"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:449;left:161"><nobr><span class="ft6"> (mutual exclusion): if an object belongs to <i>S </i> it </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:97"><nobr><span class="ft6">cannot belong to  <i>C </i>; </span></nobr></DIV>
<DIV style="position:absolute;top:481;left:81"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:87"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:486;left:100"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:486;left:126"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:482;left:113"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:486;left:136"><nobr><span class="ft6"> (superset): any object that belonging to <i>S </i> must  also </span></nobr></DIV>
<DIV style="position:absolute;top:505;left:97"><nobr><span class="ft6">belong to  <i>C </i>; </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:81"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:87"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:100"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:524;left:126"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:519;left:112"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:524;left:137"><nobr><span class="ft6"> (subset): any object not belonging to <i>S </i> also  cannot </span></nobr></DIV>
<DIV style="position:absolute;top:543;left:97"><nobr><span class="ft6">belong to  <i>C </i>; </span></nobr></DIV>
<DIV style="position:absolute;top:557;left:81"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:87"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:561;left:97"><nobr><span class="ft6">C and S overlap but neither is a superset of the other. </span></nobr></DIV>
<DIV style="position:absolute;top:578;left:81"><nobr><span class="ft38">In addition, semantic relationships may involve multiple master <br>and source categories. For example, a master category  <i>C </i> may be <br>a subset of the union of two source categories </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:354"><nobr><span class="ft22"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:612;left:347"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:612;left:362"><nobr><span class="ft6"> and </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:399"><nobr><span class="ft22"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:612;left:392"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:612;left:407"><nobr><span class="ft6">, so if </span></nobr></DIV>
<DIV style="position:absolute;top:633;left:81"><nobr><span class="ft6">an object does not belong to either </span></nobr></DIV>
<DIV style="position:absolute;top:641;left:285"><nobr><span class="ft22"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:633;left:278"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:633;left:293"><nobr><span class="ft6"> or </span></nobr></DIV>
<DIV style="position:absolute;top:641;left:322"><nobr><span class="ft22"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:633;left:315"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:633;left:330"><nobr><span class="ft6">, it cannot belong to </span></nobr></DIV>
<DIV style="position:absolute;top:654;left:84"><nobr><span class="ft8"><i>C </i>. The real-world semantic relationships are noisy and fuzzy, </span></nobr></DIV>
<DIV style="position:absolute;top:672;left:81"><nobr><span class="ft15">but they can still provide valuable information for classification. <br>For example, knowing that most (80%) objects in a source <br>category  <i>S </i> belong to one master category </span></nobr></DIV>
<DIV style="position:absolute;top:712;left:326"><nobr><span class="ft22"><i>a</i></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:317"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:334"><nobr><span class="ft6"> and the rest (20%) </span></nobr></DIV>
<DIV style="position:absolute;top:725;left:81"><nobr><span class="ft6">examples belong to another master category </span></nobr></DIV>
<DIV style="position:absolute;top:733;left:359"><nobr><span class="ft22"><i>b</i></span></nobr></DIV>
<DIV style="position:absolute;top:725;left:350"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:725;left:368"><nobr><span class="ft6"> is  obviously </span></nobr></DIV>
<DIV style="position:absolute;top:746;left:81"><nobr><span class="ft15">helpful. The difficulty is that knowledge about those semantic <br>relationships is not explicit but hidden in the data. </span></nobr></DIV>
<DIV style="position:absolute;top:787;left:81"><nobr><span class="ft15">In this paper, we propose a new approach, co-bootstrapping, to <br>enhance the classification by exploiting such implicit knowledge. <br>Our experiments with real-world web data show substantial <br>improvements in the performance of taxonomy integration. </span></nobr></DIV>
<DIV style="position:absolute;top:861;left:81"><nobr><span class="ft15">The rest of this paper is organized as follows. In §2, we give the <br>formal problem statement. In §3, we describe a state-of-the-art <br>solution. In §4, we present our approach in detail. In §5, we <br>conduct experimental evaluations. In §6, we review the related <br>work. In §7, we make concluding remarks. </span></nobr></DIV>
<DIV style="position:absolute;top:961;left:81"><nobr><span class="ft5"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:960;left:95"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:961;left:105"><nobr><span class="ft5"><b>PROBLEM STATEMENT </b></span></nobr></DIV>
<DIV style="position:absolute;top:981;left:81"><nobr><span class="ft15">Taxonomies are often organized as hierarchies. In this work, we <br>assume for simplicity, that any objects assigned to an interior <br>node really belong to a leaf node which is an offspring of that <br>interior node. Since we now have all objects only at leaf nodes, <br>we can flatten the hierarchical taxonomy to a single level and <br>treat it as a set of categories [2]. </span></nobr></DIV>
<DIV style="position:absolute;top:85;left:477"><nobr><span class="ft15">Now we formally define the taxonomy integration problem that <br>we are solving. Given two taxonomies: </span></nobr></DIV>
<DIV style="position:absolute;top:115;left:477"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:119;left:483"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:120;left:493"><nobr><span class="ft6">a master taxonomy </span></nobr></DIV>
<DIV style="position:absolute;top:116;left:604"><nobr><span class="ft21">M with a set of categories </span></nobr></DIV>
<DIV style="position:absolute;top:127;left:775"><nobr><span class="ft23">1</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:793"><nobr><span class="ft23">2</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:780"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:798"><nobr><span class="ft6">,...,</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:826"><nobr><span class="ft22"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:120;left:767"><nobr><span class="ft8"><i>C C</i></span></nobr></DIV>
<DIV style="position:absolute;top:120;left:817"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:120;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:140;left:493"><nobr><span class="ft6">each containing a set of objects, and </span></nobr></DIV>
<DIV style="position:absolute;top:154;left:477"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:158;left:483"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:159;left:493"><nobr><span class="ft6">a source taxonomy </span></nobr></DIV>
<DIV style="position:absolute;top:155;left:607"><nobr><span class="ft21">N with a set of categories </span></nobr></DIV>
<DIV style="position:absolute;top:167;left:778"><nobr><span class="ft23">1</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:795"><nobr><span class="ft23">2</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:783"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:800"><nobr><span class="ft6">,...,</span></nobr></DIV>
<DIV style="position:absolute;top:167;left:827"><nobr><span class="ft22"><i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:772"><nobr><span class="ft8"><i>S S</i></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:819"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:837"><nobr><span class="ft6">  </span></nobr></DIV>
<DIV style="position:absolute;top:179;left:493"><nobr><span class="ft6">each containing a set of objects, </span></nobr></DIV>
<DIV style="position:absolute;top:198;left:477"><nobr><span class="ft6">we need to find the categories in </span></nobr></DIV>
<DIV style="position:absolute;top:194;left:659"><nobr><span class="ft21">M for each object in N.  </span></nobr></DIV>
<DIV style="position:absolute;top:227;left:477"><nobr><span class="ft6">To formulate taxonomy integration as a classification problem, </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:477"><nobr><span class="ft6">we take </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:537"><nobr><span class="ft23">1</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:554"><nobr><span class="ft23">2</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:541"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:560"><nobr><span class="ft6">,...,</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:587"><nobr><span class="ft22"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:528"><nobr><span class="ft8"><i>C C</i></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:578"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:598"><nobr><span class="ft6"> as classes, the objects in </span></nobr></DIV>
<DIV style="position:absolute;top:242;left:752"><nobr><span class="ft21">M as training </span></nobr></DIV>
<DIV style="position:absolute;top:269;left:477"><nobr><span class="ft6">examples, the objects in </span></nobr></DIV>
<DIV style="position:absolute;top:265;left:619"><nobr><span class="ft21">N as test examples, so that taxonomy </span></nobr></DIV>
<DIV style="position:absolute;top:289;left:477"><nobr><span class="ft15">integration can be automatically accomplished by predicting the <br>classes of each test example. Such a classification problem is <br>multi-class and multi-label, in the sense that there are usually <br>more than two possible classes and one object may be relevant to <br>more than one class.  </span></nobr></DIV>
<DIV style="position:absolute;top:390;left:477"><nobr><span class="ft5"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:389;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:390;left:501"><nobr><span class="ft5"><b>A STATE-OF-THE-ART SOLUTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:409;left:477"><nobr><span class="ft15">Agrawal and Srikant recently proposed an elegant approach to <br>taxonomy integration by enhancing the Naïve Bayes algorithm <br>[2]. </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:477"><nobr><span class="ft38">The Naïve Bayes (NB) algorithm is a well-known text <br>classification technique [18]. NB tries to fit a generative model <br>for documents using training examples and apply this model to <br>classify test examples. The generative model of NB assumes that <br>a document is generated by first choosing its class according to a <br>prior distribution of classes, and then producing its words <br>independently according to a (typically multinomial) distribution <br>of terms conditioned on the chosen class [15]. Given a test <br>document  <i>d </i>, NB predicts its class to be  arg max Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:786"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:747"><nobr><span class="ft22"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:599;left:773"><nobr><span class="ft8"><i>C d</i></span></nobr></DIV>
<DIV style="position:absolute;top:599;left:808"><nobr><span class="ft6">. The </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:477"><nobr><span class="ft6">posterior probability  Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:624"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:612"><nobr><span class="ft8"><i>C d</i></span></nobr></DIV>
<DIV style="position:absolute;top:620;left:646"><nobr><span class="ft6">can be computed via Bayes's rule:  </span></nobr></DIV>
<DIV style="position:absolute;top:652;left:480"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:509"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:496"><nobr><span class="ft8"><i>C d</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:531"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:549"><nobr><span class="ft6">Pr[ , ]</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:557"><nobr><span class="ft6">Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:566"><nobr><span class="ft8"><i>C d</i></span></nobr></DIV>
<DIV style="position:absolute;top:663;left:574"><nobr><span class="ft8"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:647;left:537"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:597"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:616"><nobr><span class="ft6">Pr[ ] Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:692"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:663;left:642"><nobr><span class="ft6">Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:642;left:633"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:642;left:666"><nobr><span class="ft8"><i>d C</i></span></nobr></DIV>
<DIV style="position:absolute;top:663;left:658"><nobr><span class="ft8"><i>d</i></span></nobr></DIV>
<DIV style="position:absolute;top:647;left:604"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:700"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:651;left:719"><nobr><span class="ft6">Pr[ ] Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:796"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:736"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:769"><nobr><span class="ft8"><i>d C</i></span></nobr></DIV>
<DIV style="position:absolute;top:647;left:706"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:802"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:679;left:556"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:612"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:621"><nobr><span class="ft23">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:491"><nobr><span class="ft6">Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:562"><nobr><span class="ft6">Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:607"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:617"><nobr><span class="ft22"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:698;left:540"><nobr><span class="ft22"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:687;left:507"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:687;left:580"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:695;left:545"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:683;left:480"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:679;left:521"><nobr><span class="ft26"></span></nobr></DIV>
<DIV style="position:absolute;top:687;left:645"><nobr><span class="ft6">,  </span></nobr></DIV>
<DIV style="position:absolute;top:710;left:477"><nobr><span class="ft6">where  ( , )</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:518"><nobr><span class="ft8"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:710;left:561"><nobr><span class="ft6"> is the number of occurrences of <i>w </i> in  <i>d </i>. The </span></nobr></DIV>
<DIV style="position:absolute;top:729;left:477"><nobr><span class="ft6">probability  Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:729;left:559"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:729;left:576"><nobr><span class="ft6"> can be estimated by the proportion of training </span></nobr></DIV>
<DIV style="position:absolute;top:749;left:477"><nobr><span class="ft6">documents in  <i>C </i>. The probability  Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:714"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:687"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:749;left:721"><nobr><span class="ft6"> can be estimated by </span></nobr></DIV>
<DIV style="position:absolute;top:784;left:516"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:588"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:513"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:529"><nobr><span class="ft6">( ,</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:562"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:806;left:501"><nobr><span class="ft27"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:801;left:558"><nobr><span class="ft22"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:802;left:497"><nobr><span class="ft22"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:802;left:509"><nobr><span class="ft22"><i>V</i></span></nobr></DIV>
<DIV style="position:absolute;top:771;left:506"><nobr><span class="ft8"><i>n C w</i></span></nobr></DIV>
<DIV style="position:absolute;top:792;left:522"><nobr><span class="ft8"><i>n C w</i></span></nobr></DIV>
<DIV style="position:absolute;top:765;left:558"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:787;left:577"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:800;left:504"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:766;left:549"><nobr><span class="ft19">+</span></nobr></DIV>
<DIV style="position:absolute;top:788;left:569"><nobr><span class="ft19">+</span></nobr></DIV>
<DIV style="position:absolute;top:784;left:481"><nobr><span class="ft29"></span></nobr></DIV>
<DIV style="position:absolute;top:780;left:603"><nobr><span class="ft6">, where  ( , )</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:667"><nobr><span class="ft8"><i>n C w</i></span></nobr></DIV>
<DIV style="position:absolute;top:780;left:717"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:721"><nobr><span class="ft6">is the number of </span></nobr></DIV>
<DIV style="position:absolute;top:818;left:477"><nobr><span class="ft38">occurrences of <i>w </i> in training documents in <i>C </i>,  <i>V </i>is the <br>vocabulary of terms, and 0</span></nobr></DIV>
<DIV style="position:absolute;top:836;left:667"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:644"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:831;left:635"><nobr><span class="ft19">&lt; </span></nobr></DIV>
<DIV style="position:absolute;top:836;left:676"><nobr><span class="ft6"> is the Lidstone's smoothing </span></nobr></DIV>
<DIV style="position:absolute;top:855;left:477"><nobr><span class="ft15">parameter [1]. Taking logs, we see that NB is actually a linear <br>classifier: </span></nobr></DIV>
<DIV style="position:absolute;top:894;left:477"><nobr><span class="ft6">  log Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:531"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:519"><nobr><span class="ft8"><i>C d</i></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:554"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:885;left:663"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:719"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:592"><nobr><span class="ft30">(</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:750"><nobr><span class="ft30">)</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:728"><nobr><span class="ft23">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:573"><nobr><span class="ft6">log Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:670"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:698"><nobr><span class="ft6">|</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:714"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:890;left:724"><nobr><span class="ft22"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:904;left:647"><nobr><span class="ft22"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:615"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:687"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:902;left:652"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:890;left:560"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:885;left:629"><nobr><span class="ft31"></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:758"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:913;left:527"><nobr><span class="ft32">(</span></nobr></DIV>
<DIV style="position:absolute;top:913;left:662"><nobr><span class="ft32">)</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:680"><nobr><span class="ft6">log Pr[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:540"><nobr><span class="ft33">( , ) log Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:657"><nobr><span class="ft33">]</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:508"><nobr><span class="ft34"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:923;left:716"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:533"><nobr><span class="ft35"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:628"><nobr><span class="ft35"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:514"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:918;left:480"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:577"><nobr><span class="ft28">×</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:670"><nobr><span class="ft28">+</span></nobr></DIV>
<DIV style="position:absolute;top:914;left:491"><nobr><span class="ft37"></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:733"><nobr><span class="ft6">. </span></nobr></DIV>
<DIV style="position:absolute;top:955;left:477"><nobr><span class="ft15">The enhanced Naïve Bayes (ENB) algorithm [2] uses the <br>categorization of the source taxonomy to get better probability <br>estimations. Given a test document <i>d </i> that is know to be in </span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:477"><nobr><span class="ft6">category  <i>S </i> in </span></nobr></DIV>
<DIV style="position:absolute;top:1004;left:569"><nobr><span class="ft21">N, ENB predicts its category in M to be </span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:479"><nobr><span class="ft6">arg max Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:561"><nobr><span class="ft6">| , ]</span></nobr></DIV>
<DIV style="position:absolute;top:1038;left:522"><nobr><span class="ft22"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:548"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:597"><nobr><span class="ft6">. The posterior probability Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:782"><nobr><span class="ft6">| , ]</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:770"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:818"><nobr><span class="ft6">can </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">411</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft39{font-size:23px;font-family:Symbol;color:#000000;}
	.ft40{font-size:18px;font-family:Times;color:#000000;}
	.ft41{font-size:14px;font-family:Times;color:#000000;}
	.ft42{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft43{font-size:8px;font-family:Times;color:#000000;}
	.ft44{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft45{font-size:8px;font-family:Symbol;color:#000000;}
	.ft46{font-size:25px;font-family:Times;color:#000000;}
	.ft47{font-size:11px;line-height:16px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213003.png" alt="background image">
<DIV style="position:absolute;top:97;left:81"><nobr><span class="ft6">be computed as Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:98;left:207"><nobr><span class="ft6">| , ]</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:195"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:97;left:243"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:88;left:262"><nobr><span class="ft6">Pr[ , , ]</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:270"><nobr><span class="ft6">Pr[ , ]</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:279"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:109;left:287"><nobr><span class="ft8"><i>d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:93;left:250"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:324"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:88;left:343"><nobr><span class="ft6">Pr[ ] Pr[ ,</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:418"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:368"><nobr><span class="ft6">Pr[ , ]</span></nobr></DIV>
<DIV style="position:absolute;top:88;left:361"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:88;left:391"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:109;left:386"><nobr><span class="ft8"><i>d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:93;left:331"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:97;left:441"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:129;left:97"><nobr><span class="ft6">Pr[ ,</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:140"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:129;left:113"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:124;left:84"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:129;left:162"><nobr><span class="ft6">. ENB invokes a simplification that assumes  <i>d </i> </span></nobr></DIV>
<DIV style="position:absolute;top:148;left:81"><nobr><span class="ft6">and  <i>S </i> are independent given  <i>C </i>, therefore  </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:84"><nobr><span class="ft6">Pr[ ,</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:127"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:100"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:159"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:188"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:217"><nobr><span class="ft6">| ] Pr[ | , ]</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:205"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:255"><nobr><span class="ft8"><i>d S C</i></span></nobr></DIV>
<DIV style="position:absolute;top:162;left:177"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:312"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:341"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:370"><nobr><span class="ft6">| ] Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:434"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:357"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:408"><nobr><span class="ft8"><i>d C</i></span></nobr></DIV>
<DIV style="position:absolute;top:162;left:330"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:441"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:182;left:177"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:233"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:243"><nobr><span class="ft23">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:95"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:125"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:184"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:213"><nobr><span class="ft6">|</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:228"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:238"><nobr><span class="ft22"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:200;left:162"><nobr><span class="ft22"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:112"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:201"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:198;left:166"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:84"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:143"><nobr><span class="ft26"></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:266"><nobr><span class="ft6">.  </span></nobr></DIV>
<DIV style="position:absolute;top:213;left:81"><nobr><span class="ft6">The probability  Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:203"><nobr><span class="ft6">|</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:219"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:192"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:213;left:226"><nobr><span class="ft6"> can be estimated in the same way of </span></nobr></DIV>
<DIV style="position:absolute;top:232;left:81"><nobr><span class="ft6">NB. For the probability  Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:269"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:257"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:232;left:296"><nobr><span class="ft6"> , ENB estimates it by </span></nobr></DIV>
<DIV style="position:absolute;top:267;left:111"><nobr><span class="ft39">(</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:201"><nobr><span class="ft39">)</span></nobr></DIV>
<DIV style="position:absolute;top:294;left:106"><nobr><span class="ft27"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:289;left:128"><nobr><span class="ft22"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:289;left:157"><nobr><span class="ft22"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:290;left:101"><nobr><span class="ft22"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:256;left:109"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:256;left:135"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:256;left:166"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:119"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:148"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:182"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:250;left:176"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:274;left:193"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:252;left:123"><nobr><span class="ft19">×</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:149"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:276;left:136"><nobr><span class="ft19">×</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:165"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:273;left:85"><nobr><span class="ft40"></span></nobr></DIV>
<DIV style="position:absolute;top:267;left:210"><nobr><span class="ft6">, where  <i>C </i> is the number of documents </span></nobr></DIV>
<DIV style="position:absolute;top:307;left:81"><nobr><span class="ft6">in  <i>C </i>,  <i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:307;left:153"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:303;left:136"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:307;left:167"><nobr><span class="ft6"> is the number of documents in  <i>S </i> classified into </span></nobr></DIV>
<DIV style="position:absolute;top:328;left:84"><nobr><span class="ft8"><i>C </i> by the NB classifier, and </span></nobr></DIV>
<DIV style="position:absolute;top:329;left:270"><nobr><span class="ft6">0</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:245"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:258"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:328;left:279"><nobr><span class="ft6"> is a parameter reflecting the </span></nobr></DIV>
<DIV style="position:absolute;top:348;left:81"><nobr><span class="ft6">degree of semantic overlap between the categorizations of </span></nobr></DIV>
<DIV style="position:absolute;top:345;left:422"><nobr><span class="ft21">M </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:81"><nobr><span class="ft6">and </span></nobr></DIV>
<DIV style="position:absolute;top:367;left:105"><nobr><span class="ft21">N. The optimal value of </span></nobr></DIV>
<DIV style="position:absolute;top:366;left:246"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:371;left:259"><nobr><span class="ft6"> can be found using a tune set (a </span></nobr></DIV>
<DIV style="position:absolute;top:391;left:81"><nobr><span class="ft15">set of objects whose categories in both taxonomies are known). <br>The tune set can be made available via random sampling or <br>active learning [2]. Taking logs, we see that ENB is still a linear <br>classifier:  </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:84"><nobr><span class="ft6">log Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:132"><nobr><span class="ft6">| , ]</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:120"><nobr><span class="ft8"><i>C d S</i></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:167"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:454;left:293"><nobr><span class="ft24">(</span></nobr></DIV>
<DIV style="position:absolute;top:454;left:349"><nobr><span class="ft24">)</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:205"><nobr><span class="ft30">(</span></nobr></DIV>
<DIV style="position:absolute;top:447;left:380"><nobr><span class="ft30">)</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:359"><nobr><span class="ft23">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:186"><nobr><span class="ft6">log Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:240"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:300"><nobr><span class="ft6">Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:328"><nobr><span class="ft6">|</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:344"><nobr><span class="ft6">]</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:354"><nobr><span class="ft22"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:473;left:278"><nobr><span class="ft22"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:228"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:317"><nobr><span class="ft8"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:471;left:282"><nobr><span class="ft25"></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:173"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:258"><nobr><span class="ft31"></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:388"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:131"><nobr><span class="ft32">(</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:266"><nobr><span class="ft32">)</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:285"><nobr><span class="ft6">log Pr[</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:334"><nobr><span class="ft6">| ]</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:145"><nobr><span class="ft33">( , ) log Pr[ |</span></nobr></DIV>
<DIV style="position:absolute;top:490;left:261"><nobr><span class="ft33">]</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:113"><nobr><span class="ft34"><i>w d</i></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:321"><nobr><span class="ft8"><i>C S</i></span></nobr></DIV>
<DIV style="position:absolute;top:490;left:137"><nobr><span class="ft35"><i>n d w</i></span></nobr></DIV>
<DIV style="position:absolute;top:490;left:232"><nobr><span class="ft35"><i>w C</i></span></nobr></DIV>
<DIV style="position:absolute;top:498;left:118"><nobr><span class="ft36"></span></nobr></DIV>
<DIV style="position:absolute;top:487;left:84"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:181"><nobr><span class="ft28">×</span></nobr></DIV>
<DIV style="position:absolute;top:485;left:274"><nobr><span class="ft28">+</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:95"><nobr><span class="ft37"></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:356"><nobr><span class="ft6">.  </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:81"><nobr><span class="ft15">Comparing the classification functions of NB and ENB, it is <br>obvious that all ENB does is to shift the classification threshold <br>of its base NB classifier, no more and no less. </span></nobr></DIV>
<DIV style="position:absolute;top:581;left:81"><nobr><span class="ft15">To achieve multi-class multi-label classification that is required <br>by taxonomy integration, we use the "one-vs-rest" method to <br>create an ensemble of binary (yes/no) NB or ENB classifiers, one </span></nobr></DIV>
<DIV style="position:absolute;top:632;left:81"><nobr><span class="ft6">for each category  <i>C </i> in </span></nobr></DIV>
<DIV style="position:absolute;top:629;left:211"><nobr><span class="ft21">M.  </span></nobr></DIV>
<DIV style="position:absolute;top:672;left:81"><nobr><span class="ft5"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:671;left:95"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:672;left:105"><nobr><span class="ft5"><b>OUR APPROACH </b></span></nobr></DIV>
<DIV style="position:absolute;top:692;left:81"><nobr><span class="ft15">Here we present our approach in detail. In §4.1, we introduce the <br>boosting technique. In §4.2, we propose the co-bootstrapping <br>method. In §4.3, we discuss the advantages of our approach. </span></nobr></DIV>
<DIV style="position:absolute;top:760;left:81"><nobr><span class="ft5"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:759;left:104"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:760;left:114"><nobr><span class="ft5"><b>Boosting </b></span></nobr></DIV>
<DIV style="position:absolute;top:780;left:81"><nobr><span class="ft15">In our approach to taxonomy integration, we utilize a powerful <br>machine learning method, boosting [17, 23], to build classifiers. <br>The main idea of boosting is to combine many weak hypotheses <br>(simple and moderately accurate classification rules), into a <br>highly accurate classifier. In this paper, we focus on boosting for <br>text classification. Generalization to other kinds of data and <br>learning algorithms would be straightforward.  </span></nobr></DIV>
<DIV style="position:absolute;top:912;left:81"><nobr><span class="ft41"><i>4.1.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:911;left:114"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:912;left:125"><nobr><span class="ft41"><i>Term-Features </i></span></nobr></DIV>
<DIV style="position:absolute;top:931;left:81"><nobr><span class="ft15">Text objects (documents) can be represented using a set of term-<br>features </span></nobr></DIV>
<DIV style="position:absolute;top:953;left:178"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:202"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:159"><nobr><span class="ft6">{</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:183"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:208"><nobr><span class="ft6">,...</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:242"><nobr><span class="ft6">}</span></nobr></DIV>
<DIV style="position:absolute;top:953;left:138"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:953;left:171"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:953;left:194"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:953;left:228"><nobr><span class="ft43"><i>T n</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:131"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:167"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:190"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:225"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:943;left:149"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:251"><nobr><span class="ft6">. The term-feature </span></nobr></DIV>
<DIV style="position:absolute;top:953;left:366"><nobr><span class="ft43"><i>Th</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:362"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:383"><nobr><span class="ft6">(1</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:434"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:406"><nobr><span class="ft8"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:427"><nobr><span class="ft8"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:943;left:395"><nobr><span class="ft19"> </span></nobr></DIV>
<DIV style="position:absolute;top:947;left:441"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:968;left:81"><nobr><span class="ft6">of a given object  <i>x </i> is a binary feature indicating the presence or </span></nobr></DIV>
<DIV style="position:absolute;top:988;left:81"><nobr><span class="ft6">absence of </span></nobr></DIV>
<DIV style="position:absolute;top:996;left:154"><nobr><span class="ft22"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:988;left:145"><nobr><span class="ft8"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:988;left:160"><nobr><span class="ft6"> (the <i>h</i>-th distinct word in the document collection) </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:81"><nobr><span class="ft6">in  <i>x </i>, i.e., </span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:183"><nobr><span class="ft6">1  if  </span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:183"><nobr><span class="ft6">0  if  </span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:219"><nobr><span class="ft43"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:1026;left:150"><nobr><span class="ft43"><i>Th</i></span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:220"><nobr><span class="ft43"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:211"><nobr><span class="ft8"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:239"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:146"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:212"><nobr><span class="ft8"><i>w</i></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:241"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:228"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:176"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:166"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:1019;left:176"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:229"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:176"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:250"><nobr><span class="ft6">.   </span></nobr></DIV>
<DIV style="position:absolute;top:86;left:477"><nobr><span class="ft41"><i>4.1.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:85;left:510"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:86;left:521"><nobr><span class="ft41"><i>Weak Hypotheses </i></span></nobr></DIV>
<DIV style="position:absolute;top:105;left:477"><nobr><span class="ft6">Let </span></nobr></DIV>
<DIV style="position:absolute;top:102;left:500"><nobr><span class="ft21">X  denote the domain of possible objects, and let  Y  be a set </span></nobr></DIV>
<DIV style="position:absolute;top:125;left:477"><nobr><span class="ft6">of  <i>k </i> possible classes. A labeled example is a pair  ( , )</span></nobr></DIV>
<DIV style="position:absolute;top:125;left:773"><nobr><span class="ft8"><i>x Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:800"><nobr><span class="ft6"> where </span></nobr></DIV>
<DIV style="position:absolute;top:145;left:480"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:488"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:141;left:499"><nobr><span class="ft21">X  is an object and <i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:632"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:141;left:645"><nobr><span class="ft21">Y  is the set of classes which  <i>x </i> </span></nobr></DIV>
<DIV style="position:absolute;top:176;left:477"><nobr><span class="ft6">belongs to. We define  [ ]</span></nobr></DIV>
<DIV style="position:absolute;top:176;left:606"><nobr><span class="ft8"><i>Y l</i></span></nobr></DIV>
<DIV style="position:absolute;top:176;left:631"><nobr><span class="ft6"> for  <i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:172;left:662"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:172;left:673"><nobr><span class="ft21">Y  to be </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:775"><nobr><span class="ft6">1  if  </span></nobr></DIV>
<DIV style="position:absolute;top:176;left:734"><nobr><span class="ft6">[ ]</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:775"><nobr><span class="ft6">1  if  </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:803"><nobr><span class="ft8"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:166;left:820"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:176;left:725"><nobr><span class="ft8"><i>Y l</i></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:803"><nobr><span class="ft8"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:820"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:161;left:768"><nobr><span class="ft19">+</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:809"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:163;left:761"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:172;left:750"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:761"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:768"><nobr><span class="ft19">-</span></nobr></DIV>
<DIV style="position:absolute;top:182;left:809"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:761"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:176;left:833"><nobr><span class="ft6">. </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:477"><nobr><span class="ft6">A hypothesis is a real-valued function  :</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:696"><nobr><span class="ft8"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:203;left:725"><nobr><span class="ft19">× </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:765"><nobr><span class="ft21">R</span></nobr></DIV>
<DIV style="position:absolute;top:204;left:711"><nobr><span class="ft21">X Y</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:779"><nobr><span class="ft6">. The sign </span></nobr></DIV>
<DIV style="position:absolute;top:227;left:477"><nobr><span class="ft6">of  ( , )</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:494"><nobr><span class="ft8"><i>h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:227;left:528"><nobr><span class="ft6"> is a prediction of  [ ]</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:631"><nobr><span class="ft8"><i>Y l</i></span></nobr></DIV>
<DIV style="position:absolute;top:227;left:656"><nobr><span class="ft6"> for  <i>x </i>, i.e., whether object  <i>x </i> is </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:477"><nobr><span class="ft6">contained in class  <i>l </i>. The magnitude of   ( , )</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:708"><nobr><span class="ft8"><i>h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:246;left:743"><nobr><span class="ft6"> is interpreted as </span></nobr></DIV>
<DIV style="position:absolute;top:265;left:477"><nobr><span class="ft6">a measure of confidence in the prediction. </span></nobr></DIV>
<DIV style="position:absolute;top:291;left:477"><nobr><span class="ft6">Based on a binary feature  <i>f </i>, we are interested in weak </span></nobr></DIV>
<DIV style="position:absolute;top:310;left:477"><nobr><span class="ft6">hypotheses  <i>h </i> which are simple decision stumps of the form </span></nobr></DIV>
<DIV style="position:absolute;top:336;left:539"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:538"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:330;left:549"><nobr><span class="ft6">  if  </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:590"><nobr><span class="ft6">1</span></nobr></DIV>
<DIV style="position:absolute;top:340;left:486"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:549"><nobr><span class="ft6">  if  </span></nobr></DIV>
<DIV style="position:absolute;top:351;left:591"><nobr><span class="ft6">0</span></nobr></DIV>
<DIV style="position:absolute;top:336;left:544"><nobr><span class="ft43"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:356;left:543"><nobr><span class="ft43"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:330;left:534"><nobr><span class="ft8"><i>c</i></span></nobr></DIV>
<DIV style="position:absolute;top:330;left:571"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:340;left:480"><nobr><span class="ft8"><i>h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:532"><nobr><span class="ft8"><i>c</i></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:570"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:326;left:581"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:526"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:336;left:515"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:526"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:346;left:580"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:351;left:526"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:340;left:601"><nobr><span class="ft6">, where </span></nobr></DIV>
<DIV style="position:absolute;top:346;left:653"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:672"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:662"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:346;left:657"><nobr><span class="ft43"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:346;left:678"><nobr><span class="ft43"><i>l</i></span></nobr></DIV>
<DIV style="position:absolute;top:340;left:648"><nobr><span class="ft8"><i>c</i></span></nobr></DIV>
<DIV style="position:absolute;top:340;left:667"><nobr><span class="ft8"><i>c</i></span></nobr></DIV>
<DIV style="position:absolute;top:336;left:684"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:337;left:696"><nobr><span class="ft21">» . </span></nobr></DIV>
<DIV style="position:absolute;top:392;left:477"><nobr><span class="ft41"><i>4.1.3</i></span></nobr></DIV>
<DIV style="position:absolute;top:391;left:510"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:521"><nobr><span class="ft41"><i>AdaBoost Algorithm </i></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:477"><nobr><span class="ft15">The most popular boosting algorithm is AdaBoost introduced in <br>1995 by Freund and Schapire [12]. Our work is based on a multi-<br>class multi-label version of AdaBoost, AdaBoost.MH [24, 25], <br>which is described in Figure 1. </span></nobr></DIV>
<DIV style="position:absolute;top:810;left:829"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:823;left:477"><nobr><span class="ft6">Given  <i>m </i> training  examples </span></nobr></DIV>
<DIV style="position:absolute;top:829;left:669"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:684"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:659"><nobr><span class="ft6">( , ),...,(</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:733"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:823;left:753"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:724"><nobr><span class="ft43"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:744"><nobr><span class="ft43"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:664"><nobr><span class="ft8"><i>x Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:719"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:737"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:823;left:765"><nobr><span class="ft6"> where  each </span></nobr></DIV>
<DIV style="position:absolute;top:850;left:486"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:480"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:492"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:841;left:503"><nobr><span class="ft21">X , </span></nobr></DIV>
<DIV style="position:absolute;top:850;left:545"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:539"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:552"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:841;left:563"><nobr><span class="ft21">Y , AdaBoost.MH dynamically maintains a </span></nobr></DIV>
<DIV style="position:absolute;top:865;left:477"><nobr><span class="ft6">distribution </span></nobr></DIV>
<DIV style="position:absolute;top:871;left:561"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:865;left:552"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:865;left:572"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:865;left:576"><nobr><span class="ft6">over all objects and classes. Initially this </span></nobr></DIV>
<DIV style="position:absolute;top:886;left:477"><nobr><span class="ft6">distribution </span></nobr></DIV>
<DIV style="position:absolute;top:892;left:555"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:886;left:548"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:564"><nobr><span class="ft6"> is uniform. In the <i>t</i>-th round, the optimal weak </span></nobr></DIV>
<DIV style="position:absolute;top:907;left:477"><nobr><span class="ft6">hypothesis </span></nobr></DIV>
<DIV style="position:absolute;top:912;left:547"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:907;left:541"><nobr><span class="ft8"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:907;left:554"><nobr><span class="ft6"> is selected based on the set of training examples </span></nobr></DIV>
<DIV style="position:absolute;top:928;left:477"><nobr><span class="ft6">and the current distribution </span></nobr></DIV>
<DIV style="position:absolute;top:933;left:644"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:928;left:635"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:928;left:651"><nobr><span class="ft6">. Then a parameter </span></nobr></DIV>
<DIV style="position:absolute;top:933;left:771"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:923;left:762"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:928;left:778"><nobr><span class="ft6"> is chosen, </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:477"><nobr><span class="ft6">and the distribution </span></nobr></DIV>
<DIV style="position:absolute;top:954;left:605"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:949;left:596"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:948;left:612"><nobr><span class="ft6"> is updated in a manner that puts more </span></nobr></DIV>
<DIV style="position:absolute;top:969;left:477"><nobr><span class="ft15">weights on "difficult" examples (object-class pairs) that are <br>misclassified by </span></nobr></DIV>
<DIV style="position:absolute;top:991;left:577"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:986;left:572"><nobr><span class="ft8"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:985;left:584"><nobr><span class="ft6">. Please be referred to [24, 25] for the details </span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:477"><nobr><span class="ft6">on computing optimal </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:613"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:608"><nobr><span class="ft8"><i>h</i></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:620"><nobr><span class="ft6"> and </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:658"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:649"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:1006;left:665"><nobr><span class="ft6">. This procedure repeats for <i>T</i> </span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:477"><nobr><span class="ft6">rounds. The final hypothesis </span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:654"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:643"><nobr><span class="ft8"><i>H x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:683"><nobr><span class="ft6"> is actually a weighted vote </span></nobr></DIV>
<DIV style="position:absolute;top:491;left:489"><nobr><span class="ft20">Given: </span></nobr></DIV>
<DIV style="position:absolute;top:497;left:545"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:560"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:535"><nobr><span class="ft6">( , ),...,(</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:609"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:629"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:600"><nobr><span class="ft43"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:620"><nobr><span class="ft43"><i>m</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:540"><nobr><span class="ft8"><i>x Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:595"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:613"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:636"><nobr><span class="ft20"> where each </span></nobr></DIV>
<DIV style="position:absolute;top:497;left:723"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:717"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:487;left:729"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:739"><nobr><span class="ft21">X</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:756"><nobr><span class="ft20">, </span></nobr></DIV>
<DIV style="position:absolute;top:497;left:772"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:766"><nobr><span class="ft8"><i>Y</i></span></nobr></DIV>
<DIV style="position:absolute;top:487;left:778"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:789"><nobr><span class="ft21">Y</span></nobr></DIV>
<DIV style="position:absolute;top:491;left:804"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:513;left:489"><nobr><span class="ft20">Initialize </span></nobr></DIV>
<DIV style="position:absolute;top:519;left:554"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:559"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:594"><nobr><span class="ft6">1 (</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:626"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:546"><nobr><span class="ft8"><i>D i l</i></span></nobr></DIV>
<DIV style="position:absolute;top:513;left:609"><nobr><span class="ft8"><i>mk</i></span></nobr></DIV>
<DIV style="position:absolute;top:509;left:585"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:634"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:534;left:489"><nobr><span class="ft44"><b>for</b> </span></nobr></DIV>
<DIV style="position:absolute;top:535;left:531"><nobr><span class="ft6">1,...,</span></nobr></DIV>
<DIV style="position:absolute;top:535;left:514"><nobr><span class="ft8"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:535;left:554"><nobr><span class="ft8"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:530;left:521"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:567"><nobr><span class="ft20"> <b>do</b> </span></nobr></DIV>
<DIV style="position:absolute;top:554;left:489"><nobr><span class="ft20">    Pass distribution </span></nobr></DIV>
<DIV style="position:absolute;top:561;left:622"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:555;left:613"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:554;left:629"><nobr><span class="ft20"> to weak learner. </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:489"><nobr><span class="ft20">    Get weak hypothesis </span></nobr></DIV>
<DIV style="position:absolute;top:577;left:651"><nobr><span class="ft6">:</span></nobr></DIV>
<DIV style="position:absolute;top:577;left:638"><nobr><span class="ft8"><i>ht</i></span></nobr></DIV>
<DIV style="position:absolute;top:572;left:671"><nobr><span class="ft19">× </span></nobr></DIV>
<DIV style="position:absolute;top:573;left:711"><nobr><span class="ft21">R</span></nobr></DIV>
<DIV style="position:absolute;top:573;left:657"><nobr><span class="ft21">X Y</span></nobr></DIV>
<DIV style="position:absolute;top:576;left:725"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:596;left:489"><nobr><span class="ft20">    Choose </span></nobr></DIV>
<DIV style="position:absolute;top:602;left:566"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:557"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:592;left:572"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:584"><nobr><span class="ft21">»</span></nobr></DIV>
<DIV style="position:absolute;top:596;left:597"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:617;left:489"><nobr><span class="ft20">    Update: </span></nobr></DIV>
<DIV style="position:absolute;top:644;left:489"><nobr><span class="ft20">        </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:542"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:599"><nobr><span class="ft6">( , ) exp(</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:676"><nobr><span class="ft6">[ ] ( , ))</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:548"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:641;left:595"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:641;left:663"><nobr><span class="ft43"><i>t i</i></span></nobr></DIV>
<DIV style="position:absolute;top:641;left:695"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:641;left:711"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:650;left:533"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:660;left:661"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:635;left:586"><nobr><span class="ft8"><i>D i l</i></span></nobr></DIV>
<DIV style="position:absolute;top:635;left:666"><nobr><span class="ft8"><i>Y l h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:524"><nobr><span class="ft8"><i>D</i></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:552"><nobr><span class="ft8"><i>i l</i></span></nobr></DIV>
<DIV style="position:absolute;top:655;left:653"><nobr><span class="ft8"><i>Z</i></span></nobr></DIV>
<DIV style="position:absolute;top:630;left:654"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:646;left:537"><nobr><span class="ft45">+</span></nobr></DIV>
<DIV style="position:absolute;top:631;left:647"><nobr><span class="ft19">-</span></nobr></DIV>
<DIV style="position:absolute;top:640;left:573"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:644;left:738"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:676;left:489"><nobr><span class="ft20">        where </span></nobr></DIV>
<DIV style="position:absolute;top:682;left:572"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:676;left:564"><nobr><span class="ft8"><i>Z</i></span></nobr></DIV>
<DIV style="position:absolute;top:676;left:579"><nobr><span class="ft20"> is the normalization factor </span></nobr></DIV>
<DIV style="position:absolute;top:697;left:489"><nobr><span class="ft47"><b>end for <br></b>Output the final hypothesis: </span></nobr></DIV>
<DIV style="position:absolute;top:746;left:489"><nobr><span class="ft20">    </span></nobr></DIV>
<DIV style="position:absolute;top:767;left:570"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:519"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:602"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:564"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:753;left:587"><nobr><span class="ft43"><i>t t</i></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:561"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:747;left:508"><nobr><span class="ft8"><i>H x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:747;left:591"><nobr><span class="ft8"><i>h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:742;left:578"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:764;left:565"><nobr><span class="ft45">=</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:548"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:736;left:558"><nobr><span class="ft46"></span></nobr></DIV>
<DIV style="position:absolute;top:746;left:629"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:783;left:489"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:802;left:510"><nobr><span class="ft7"><b>Figure 1: The boosting algorithm AdaBoost.MH. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">412</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft48{font-size:8px;font-family:Times;color:#000000;}
	.ft49{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft50{font-size:11px;line-height:4px;font-family:Times;color:#000000;}
	.ft51{font-size:11px;line-height:2px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213004.png" alt="background image">
<DIV style="position:absolute;top:100;left:81"><nobr><span class="ft6">of weak hypotheses </span></nobr></DIV>
<DIV style="position:absolute;top:121;left:206"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:100;left:237"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:200"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:106;left:223"><nobr><span class="ft43"><i>t t</i></span></nobr></DIV>
<DIV style="position:absolute;top:121;left:197"><nobr><span class="ft43"><i>t</i></span></nobr></DIV>
<DIV style="position:absolute;top:100;left:227"><nobr><span class="ft8"><i>h x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:95;left:213"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:201"><nobr><span class="ft45">=</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:194"><nobr><span class="ft46"></span></nobr></DIV>
<DIV style="position:absolute;top:100;left:265"><nobr><span class="ft6">, and the final prediction can be </span></nobr></DIV>
<DIV style="position:absolute;top:136;left:81"><nobr><span class="ft6">computed according to the sign of </span></nobr></DIV>
<DIV style="position:absolute;top:136;left:284"><nobr><span class="ft6">( , )</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:273"><nobr><span class="ft8"><i>H x l</i></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:313"><nobr><span class="ft6">.  </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:81"><nobr><span class="ft5"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:174;left:104"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:175;left:114"><nobr><span class="ft5"><b>Co-Bootstrapping </b></span></nobr></DIV>
<DIV style="position:absolute;top:197;left:81"><nobr><span class="ft6">Thus far we have completely ignored the categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:193;left:424"><nobr><span class="ft21">N. </span></nobr></DIV>
<DIV style="position:absolute;top:220;left:81"><nobr><span class="ft6">Although </span></nobr></DIV>
<DIV style="position:absolute;top:216;left:145"><nobr><span class="ft21">M and N are usually not identical, their </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:81"><nobr><span class="ft6">categorizations often have some semantic overlap. Therefore the </span></nobr></DIV>
<DIV style="position:absolute;top:259;left:81"><nobr><span class="ft6">categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:255;left:179"><nobr><span class="ft21">N contains valuable implicit knowledge about </span></nobr></DIV>
<DIV style="position:absolute;top:281;left:81"><nobr><span class="ft6">the categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:278;left:196"><nobr><span class="ft21">M. Hereby we propose a new approach, co-</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:81"><nobr><span class="ft15">bootstrapping, to enhance the classification by exploiting such <br>implicit knowledge. </span></nobr></DIV>
<DIV style="position:absolute;top:353;left:81"><nobr><span class="ft41"><i>4.2.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:352;left:114"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:353;left:125"><nobr><span class="ft41"><i>Category-Features </i></span></nobr></DIV>
<DIV style="position:absolute;top:374;left:81"><nobr><span class="ft6">If we have indicator functions for each category in </span></nobr></DIV>
<DIV style="position:absolute;top:371;left:378"><nobr><span class="ft21">N, we can </span></nobr></DIV>
<DIV style="position:absolute;top:394;left:81"><nobr><span class="ft6">imagine taking those indicator functions as features when we </span></nobr></DIV>
<DIV style="position:absolute;top:413;left:81"><nobr><span class="ft6">learn the classifier for </span></nobr></DIV>
<DIV style="position:absolute;top:409;left:205"><nobr><span class="ft21">M. This allows us to exploit the semantic </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:81"><nobr><span class="ft6">relationship among the categories of </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:318"><nobr><span class="ft21">M and N without </span></nobr></DIV>
<DIV style="position:absolute;top:456;left:81"><nobr><span class="ft6">explicitly figuring out what the semantic relationships are. More </span></nobr></DIV>
<DIV style="position:absolute;top:475;left:81"><nobr><span class="ft6">specifically, for each object in </span></nobr></DIV>
<DIV style="position:absolute;top:471;left:266"><nobr><span class="ft21">M, we augment the ordinary </span></nobr></DIV>
<DIV style="position:absolute;top:495;left:81"><nobr><span class="ft6">term-features with a set of category-features </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:142"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:172"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:118"><nobr><span class="ft6">{</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:148"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:178"><nobr><span class="ft6">...,</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:219"><nobr><span class="ft6">}</span></nobr></DIV>
<DIV style="position:absolute;top:520;left:210"><nobr><span class="ft43"><i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:84"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:126"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:155"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:195"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:509;left:108"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:92"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:131"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:160"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:517;left:200"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:228"><nobr><span class="ft6"> derived from </span></nobr></DIV>
<DIV style="position:absolute;top:510;left:306"><nobr><span class="ft21">N. The category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:542;left:101"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:536;left:86"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:539;left:91"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:115"><nobr><span class="ft6">(1</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:170"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:536;left:140"><nobr><span class="ft8"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:536;left:159"><nobr><span class="ft8"><i>N</i></span></nobr></DIV>
<DIV style="position:absolute;top:532;left:127"><nobr><span class="ft19"> </span></nobr></DIV>
<DIV style="position:absolute;top:536;left:182"><nobr><span class="ft6">of a given object  <i>x </i> </span></nobr></DIV>
<DIV style="position:absolute;top:536;left:323"><nobr><span class="ft6">is a binary feature </span></nobr></DIV>
<DIV style="position:absolute;top:560;left:81"><nobr><span class="ft6">indicating whether  <i>x </i> belongs to category </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:333"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:560;left:324"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:560;left:340"><nobr><span class="ft6"> (the  <i>j</i>-th category </span></nobr></DIV>
<DIV style="position:absolute;top:598;left:81"><nobr><span class="ft6">of </span></nobr></DIV>
<DIV style="position:absolute;top:594;left:95"><nobr><span class="ft21">N), i.e., </span></nobr></DIV>
<DIV style="position:absolute;top:585;left:190"><nobr><span class="ft6">1  if  </span></nobr></DIV>
<DIV style="position:absolute;top:608;left:190"><nobr><span class="ft6">0  if  </span></nobr></DIV>
<DIV style="position:absolute;top:591;left:246"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:604;left:165"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:614;left:247"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:585;left:217"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:585;left:237"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:598;left:150"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:608;left:219"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:608;left:238"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:581;left:225"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:582;left:183"><nobr><span class="ft50"><br></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:172"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:183"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:604;left:226"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:609;left:183"><nobr><span class="ft51"><br></span></nobr></DIV>
<DIV style="position:absolute;top:601;left:155"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:256"><nobr><span class="ft6">.   </span></nobr></DIV>
<DIV style="position:absolute;top:632;left:81"><nobr><span class="ft6">In the same way, we can get a set of category-features </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:146"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:178"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:120"><nobr><span class="ft6">{</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:152"><nobr><span class="ft6">,</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:184"><nobr><span class="ft6">...,</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:231"><nobr><span class="ft6">}</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:220"><nobr><span class="ft43"><i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:84"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:128"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:159"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:651;left:201"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:646;left:110"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:91"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:132"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:163"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:654;left:206"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:651;left:243"><nobr><span class="ft6"> derived  from </span></nobr></DIV>
<DIV style="position:absolute;top:647;left:330"><nobr><span class="ft21">M to be used for </span></nobr></DIV>
<DIV style="position:absolute;top:674;left:81"><nobr><span class="ft6">supplementing the features of objects in </span></nobr></DIV>
<DIV style="position:absolute;top:670;left:332"><nobr><span class="ft21">N. The remaining </span></nobr></DIV>
<DIV style="position:absolute;top:694;left:81"><nobr><span class="ft15">problem is to obtain these indicator functions, which are initially <br>not available. </span></nobr></DIV>
<DIV style="position:absolute;top:745;left:81"><nobr><span class="ft41"><i>4.2.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:114"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:745;left:125"><nobr><span class="ft41"><i>Co-Bootstrapping Algorithm </i></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:81"><nobr><span class="ft6">When building the classifier for </span></nobr></DIV>
<DIV style="position:absolute;top:763;left:269"><nobr><span class="ft21">M, the training examples are </span></nobr></DIV>
<DIV style="position:absolute;top:789;left:81"><nobr><span class="ft6">the objects in </span></nobr></DIV>
<DIV style="position:absolute;top:786;left:159"><nobr><span class="ft21">M and the test examples are the objects in N. To </span></nobr></DIV>
<DIV style="position:absolute;top:812;left:81"><nobr><span class="ft6">leverage the categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:808;left:253"><nobr><span class="ft21">N to reinforce classification, our </span></nobr></DIV>
<DIV style="position:absolute;top:832;left:81"><nobr><span class="ft6">classifier uses term-features </span></nobr></DIV>
<DIV style="position:absolute;top:838;left:260"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:833;left:254"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:832;left:273"><nobr><span class="ft6"> as well as category-features </span></nobr></DIV>
<DIV style="position:absolute;top:854;left:84"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:92"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:107"><nobr><span class="ft6">. However, we do not know the exact values of  <i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:390"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:405"><nobr><span class="ft6"> of the </span></nobr></DIV>
<DIV style="position:absolute;top:876;left:81"><nobr><span class="ft6">training examples.  </span></nobr></DIV>
<DIV style="position:absolute;top:901;left:81"><nobr><span class="ft6">Our proposed algorithm overcomes the above obstacle by </span></nobr></DIV>
<DIV style="position:absolute;top:922;left:81"><nobr><span class="ft6">utilizing the bootstrapping idea. Let </span></nobr></DIV>
<DIV style="position:absolute;top:922;left:311"><nobr><span class="ft6">( )</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:299"><nobr><span class="ft43"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:316"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:915;left:300"><nobr><span class="ft48">T</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:289"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:922;left:333"><nobr><span class="ft6"> denote a boosting-</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:81"><nobr><span class="ft6">classifier for taxonomy </span></nobr></DIV>
<DIV style="position:absolute;top:941;left:218"><nobr><span class="ft21">T's categorization based on feature set </span></nobr></DIV>
<DIV style="position:absolute;top:970;left:84"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:970;left:98"><nobr><span class="ft6"> at step  <i>r </i>. Initially we build a classifiers </span></nobr></DIV>
<DIV style="position:absolute;top:976;left:345"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:360"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:381"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:372"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:970;left:365"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:970;left:336"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:964;left:347"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:970;left:388"><nobr><span class="ft6"> based  on </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:81"><nobr><span class="ft6">only term-features, then use it to classify the objects in </span></nobr></DIV>
<DIV style="position:absolute;top:990;left:396"><nobr><span class="ft21">M (the </span></nobr></DIV>
<DIV style="position:absolute;top:1016;left:81"><nobr><span class="ft6">training examples) into the categories of </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:310"><nobr><span class="ft21">N, thus we can predict </span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:81"><nobr><span class="ft6">the value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:306"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:292"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:324"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:312"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:296"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:332"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:1037;left:350"><nobr><span class="ft6"> for each object </span></nobr></DIV>
<DIV style="position:absolute;top:90;left:480"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:86;left:488"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:87;left:499"><nobr><span class="ft21">M . At next step we will be able to build </span></nobr></DIV>
<DIV style="position:absolute;top:96;left:756"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:774"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:90;left:829"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:96;left:785"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:90;left:779"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:90;left:808"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:86;left:796"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:83;left:758"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:93;left:815"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:89;left:748"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:90;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:112;left:477"><nobr><span class="ft6">using the predicted values of  <i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:116;left:667"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:112;left:684"><nobr><span class="ft6"> of the training examples. </span></nobr></DIV>
<DIV style="position:absolute;top:139;left:477"><nobr><span class="ft6">Similarly we can build </span></nobr></DIV>
<DIV style="position:absolute;top:145;left:617"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:634"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:654"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:646"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:639"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:618"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:608"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:139;left:663"><nobr><span class="ft6"> and </span></nobr></DIV>
<DIV style="position:absolute;top:145;left:700"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:716"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:774"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:145;left:728"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:721"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:751"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:739"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:703"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:142;left:757"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:139;left:692"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:139;left:781"><nobr><span class="ft6">. The new </span></nobr></DIV>
<DIV style="position:absolute;top:165;left:477"><nobr><span class="ft6">classifier </span></nobr></DIV>
<DIV style="position:absolute;top:171;left:543"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:559"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:616"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:571"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:564"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:593"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:162;left:581"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:158;left:545"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:168;left:600"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:164;left:535"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:626"><nobr><span class="ft6"> ought to be better than  </span></nobr></DIV>
<DIV style="position:absolute;top:171;left:794"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:809"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:829"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:171;left:821"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:165;left:814"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:785"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:158;left:796"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:165;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:191;left:477"><nobr><span class="ft6">because </span></nobr></DIV>
<DIV style="position:absolute;top:196;left:535"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:551"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:608"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:196;left:562"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:191;left:556"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:191;left:585"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:187;left:573"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:184;left:537"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:194;left:592"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:527"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:191;left:616"><nobr><span class="ft6"> leverages more knowledge. Hence we </span></nobr></DIV>
<DIV style="position:absolute;top:212;left:477"><nobr><span class="ft6">can predict the value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:218;left:774"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:212;left:760"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:212;left:792"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:207;left:780"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:215;left:764"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:800"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:212;left:818"><nobr><span class="ft6"> for </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:477"><nobr><span class="ft6">each object  <i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:236;left:564"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:575"><nobr><span class="ft21">M  more accurately using </span></nobr></DIV>
<DIV style="position:absolute;top:246;left:756"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:772"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:829"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:783"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:777"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:806"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:794"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:234;left:758"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:813"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:240;left:748"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:837"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:266;left:477"><nobr><span class="ft6">instead of </span></nobr></DIV>
<DIV style="position:absolute;top:272;left:567"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:582"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:602"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:593"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:266;left:587"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:265;left:558"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:259;left:568"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:618"><nobr><span class="ft6">, and afterwards we can build </span></nobr></DIV>
<DIV style="position:absolute;top:297;left:489"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:505"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:561"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:517"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:510"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:540"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:527"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:285;left:489"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:547"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:479"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:570"><nobr><span class="ft6">. Also  </span></nobr></DIV>
<DIV style="position:absolute;top:297;left:633"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:649"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:705"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:297;left:661"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:654"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:684"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:672"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:285;left:634"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:295;left:691"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:624"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:291;left:715"><nobr><span class="ft6"> is very likely to be </span></nobr></DIV>
<DIV style="position:absolute;top:318;left:477"><nobr><span class="ft6">better than </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:556"><nobr><span class="ft10">1</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:574"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:629"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:585"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:579"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:608"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:596"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:312;left:558"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:616"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:548"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:639"><nobr><span class="ft6"> because </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:703"><nobr><span class="ft10">2</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:720"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:319;left:775"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:731"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:725"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:754"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:742"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:312;left:704"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:322;left:761"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:318;left:694"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:318;left:785"><nobr><span class="ft6"> is  based </span></nobr></DIV>
<DIV style="position:absolute;top:341;left:477"><nobr><span class="ft6">on a more accurate prediction of  <i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:690"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:341;left:706"><nobr><span class="ft6">. This process can be </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:477"><nobr><span class="ft15">repeated iteratively in a "ping-pong" manner. We name this <br>approach co-bootstrapping since the two classifiers </span></nobr></DIV>
<DIV style="position:absolute;top:401;left:505"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:561"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:489"><nobr><span class="ft43"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:406;left:517"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:510"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:540"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:397;left:527"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:479"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:394;left:489"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:547"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:571"><nobr><span class="ft6"> and </span></nobr></DIV>
<DIV style="position:absolute;top:401;left:627"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:684"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:612"><nobr><span class="ft43"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:406;left:639"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:401;left:632"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:401;left:661"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:397;left:649"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:400;left:603"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:394;left:613"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:403;left:668"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:400;left:695"><nobr><span class="ft6"> collaborate to bootstrap </span></nobr></DIV>
<DIV style="position:absolute;top:422;left:477"><nobr><span class="ft15">themselves together. Figure 2 presents the co-bootstrapping <br>algorithm, and Figure 3 depicts its process. </span></nobr></DIV>
<DIV style="position:absolute;top:474;left:477"><nobr><span class="ft5"><b>4.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:473;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:510"><nobr><span class="ft5"><b>Discussion </b></span></nobr></DIV>
<DIV style="position:absolute;top:504;left:477"><nobr><span class="ft41"><i>4.3.1</i></span></nobr></DIV>
<DIV style="position:absolute;top:503;left:510"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:504;left:521"><nobr><span class="ft41"><i>Why Choose Boosting </i></span></nobr></DIV>
<DIV style="position:absolute;top:523;left:477"><nobr><span class="ft15">We have selected to employ the boosting technique to build <br>classifiers in our co-bootstrapping approach to taxonomy <br>integration because of its following virtues.  </span></nobr></DIV>
<DIV style="position:absolute;top:568;left:477"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:572;left:483"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:572;left:493"><nobr><span class="ft15">Boosting has shown outstanding classification performance on <br>many kinds of data such as text documents [17, 23, 24]. </span></nobr></DIV>
<DIV style="position:absolute;top:601;left:477"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:605;left:483"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:493"><nobr><span class="ft15">Boosting finds the optimal combination of heterogeneous <br>weak hypotheses automatically, therefore alleviates the <br>problem of how to weight ordinary features (e.g. term-features) <br>and category-features appropriately. In contrast, approaches <br>based on other machine learning algorithms like Support <br>Vector Machines (SVMs) [9] would require to adjust relative <br>combination weights, which is a non-trivial problem. </span></nobr></DIV>
<DIV style="position:absolute;top:715;left:477"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:719;left:483"><nobr><span class="ft20"> </span></nobr></DIV>
<DIV style="position:absolute;top:719;left:493"><nobr><span class="ft15">Boosting generates descriptive and human-readable <br>hypotheses as the final classifier, and the learned classifier is <br>usually sparse despite the large feature set. </span></nobr></DIV>
<DIV style="position:absolute;top:777;left:477"><nobr><span class="ft15">Although boosting looks an ideal choice, other machine learning <br>algorithms can also be utilized in the co-bootstrapping approach. <br>We have not investigated this issue yet. </span></nobr></DIV>
<DIV style="position:absolute;top:845;left:477"><nobr><span class="ft41"><i>4.3.2</i></span></nobr></DIV>
<DIV style="position:absolute;top:844;left:510"><nobr><span class="ft42"><i> </i></span></nobr></DIV>
<DIV style="position:absolute;top:845;left:521"><nobr><span class="ft41"><i>Comparison with ENB </i></span></nobr></DIV>
<DIV style="position:absolute;top:863;left:477"><nobr><span class="ft15">Although ENB [2] has been shown to work well for taxonomy <br>integration, we think that a more general approach is still <br>attractive. It has been experimentally shown that AdaBoost is <br>more promising than NB for text classification [24]. The co-<br>bootstrapping approach allows more powerful machine learning <br>algorithms like AdaBoost to be utilized. </span></nobr></DIV>
<DIV style="position:absolute;top:970;left:477"><nobr><span class="ft6">Both ENB and our co-bootstrapping approach exploit the </span></nobr></DIV>
<DIV style="position:absolute;top:988;left:477"><nobr><span class="ft6">categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:985;left:578"><nobr><span class="ft21">N to enhance classification. While all ENB </span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:477"><nobr><span class="ft15">does is to shift the classification threshold of its base NB <br>classifier (see §3), co-bootstrapping has the ability to achieve <br>more complex adjustments on the classification function of its <br>base classifier.  </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">413</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft52{font-size:11px;line-height:17px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213005.png" alt="background image">
<DIV style="position:absolute;top:85;left:81"><nobr><span class="ft15">Furthermore, ENB needs a stand-alone tune set to find the <br>optimal value of parameter </span></nobr></DIV>
<DIV style="position:absolute;top:96;left:244"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:101;left:258"><nobr><span class="ft6"> which controls the influence of </span></nobr></DIV>
<DIV style="position:absolute;top:119;left:81"><nobr><span class="ft15">source categorization information on classification, whereas co-<br>bootstrapping based on boosting does not have such burdens. </span></nobr></DIV>
<DIV style="position:absolute;top:160;left:81"><nobr><span class="ft15">Although co-bootstrapping looks more effective, ENB still holds <br>an advantage in efficiency.  </span></nobr></DIV>
<DIV style="position:absolute;top:689;left:433"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:432"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:86;left:477"><nobr><span class="ft5"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:85;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:86;left:501"><nobr><span class="ft5"><b>EXPERIMENTS </b></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:477"><nobr><span class="ft5"><b>5.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:116;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:117;left:510"><nobr><span class="ft5"><b>Datasets </b></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:477"><nobr><span class="ft15">We have collected 5 datasets from Google and Yahoo. One <br>dataset includes the slice of Google's taxonomy and the slice of <br>Yahoo's taxonomy about websites on one specific topic, as <br>shown in Table 1. </span></nobr></DIV>
<DIV style="position:absolute;top:413;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:426;left:477"><nobr><span class="ft15">In each slice of taxonomy, we take only the top level directories <br>as categories, e.g., the "Movie" slice of Google's taxonomy has <br>categories like "Action", "Comedy", "Horror", etc. </span></nobr></DIV>
<DIV style="position:absolute;top:619;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:632;left:477"><nobr><span class="ft15">For each dataset, we show in Table 2 the number of categories <br>occurred in Google and Yahoo respectively. </span></nobr></DIV>
<DIV style="position:absolute;top:673;left:477"><nobr><span class="ft15">In each category, we take all items listed on the corresponding <br>directory page and its sub-directory pages as its objects. An <br>object (list item) corresponds to a website on the world wide web, <br>which is usually described by its URL, its title, and optionally a <br>short annotation about its content. Here each object is considered <br>as a text document composed of its title and annotation. All <br>documents are pre-processed by removal of stop-words and <br>stemming. </span></nobr></DIV>
<DIV style="position:absolute;top:947;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:960;left:477"><nobr><span class="ft52">For each dataset, we show in Table 3 the number of objects <br>occurred in Google (G), Yahoo (Y), either of them (G</span></nobr></DIV>
<DIV style="position:absolute;top:974;left:780"><nobr><span class="ft21">Y), and </span></nobr></DIV>
<DIV style="position:absolute;top:996;left:477"><nobr><span class="ft6">both of them (G</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:571"><nobr><span class="ft21">Y) respectively. The set of objects in GY </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:477"><nobr><span class="ft15">covers only a small portion (usually less than 10%) of the set of <br>objects in Google or Yahoo alone, which suggests the great <br>benefit of automatically integrating them. This observation is <br>consistent with [2].  </span></nobr></DIV>
<DIV style="position:absolute;top:1029;left:421"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:140"><nobr><span class="ft7"><b>Figure 3: The co-bootstrapping process. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1071;left:92"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:209;left:93"><nobr><span class="ft20">Given: two taxonomies </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:238"><nobr><span class="ft21">M</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:260"><nobr><span class="ft20"> and </span></nobr></DIV>
<DIV style="position:absolute;top:207;left:294"><nobr><span class="ft21">N</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:313"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:229;left:93"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:100"><nobr><span class="ft20"> Build classifier </span></nobr></DIV>
<DIV style="position:absolute;top:239;left:214"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:231"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:251"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:243"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:234;left:236"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:233;left:205"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:227;left:215"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:260"><nobr><span class="ft20">, then use it to predict the </span></nobr></DIV>
<DIV style="position:absolute;top:254;left:109"><nobr><span class="ft20">value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:321"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:255;left:303"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:255;left:339"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:251;left:328"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:307"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:258;left:346"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:364"><nobr><span class="ft20"> for  each </span></nobr></DIV>
<DIV style="position:absolute;top:277;left:109"><nobr><span class="ft20">object </span></nobr></DIV>
<DIV style="position:absolute;top:279;left:153"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:274;left:161"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:275;left:173"><nobr><span class="ft21">N</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:192"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:297;left:93"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:100"><nobr><span class="ft20"> Build classifier </span></nobr></DIV>
<DIV style="position:absolute;top:307;left:215"><nobr><span class="ft10">0</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:230"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:250"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:241"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:302;left:235"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:301;left:206"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:295;left:216"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:301;left:257"><nobr><span class="ft20">, then use it to predict the </span></nobr></DIV>
<DIV style="position:absolute;top:323;left:109"><nobr><span class="ft20">value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:320"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:323;left:306"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:323;left:339"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:319;left:327"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:311"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:346"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:323;left:362"><nobr><span class="ft20"> for  each </span></nobr></DIV>
<DIV style="position:absolute;top:347;left:109"><nobr><span class="ft20">object </span></nobr></DIV>
<DIV style="position:absolute;top:348;left:153"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:344;left:161"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:345;left:172"><nobr><span class="ft21">M</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:195"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:366;left:93"><nobr><span class="ft44"><b>for</b> </span></nobr></DIV>
<DIV style="position:absolute;top:367;left:137"><nobr><span class="ft6">1,...,</span></nobr></DIV>
<DIV style="position:absolute;top:367;left:118"><nobr><span class="ft8"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:367;left:162"><nobr><span class="ft8"><i>R</i></span></nobr></DIV>
<DIV style="position:absolute;top:362;left:127"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:172"><nobr><span class="ft20"> <b>do</b> </span></nobr></DIV>
<DIV style="position:absolute;top:387;left:109"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:116"><nobr><span class="ft20"> Build classifier </span></nobr></DIV>
<DIV style="position:absolute;top:392;left:253"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:308"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:236"><nobr><span class="ft43"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:397;left:264"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:258"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:287"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:388;left:275"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:391;left:227"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:385;left:237"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:395;left:294"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:391;left:318"><nobr><span class="ft20">, then use it to </span></nobr></DIV>
<DIV style="position:absolute;top:413;left:126"><nobr><span class="ft20">predict the value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:438;left:149"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:131"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:167"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:428;left:156"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:436;left:135"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:436;left:174"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:192"><nobr><span class="ft20"> for  each object </span></nobr></DIV>
<DIV style="position:absolute;top:433;left:297"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:429;left:305"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:429;left:316"><nobr><span class="ft21">N</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:335"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:454;left:109"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:116"><nobr><span class="ft20"> Build classifier </span></nobr></DIV>
<DIV style="position:absolute;top:459;left:251"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:459;left:308"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:236"><nobr><span class="ft43"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:262"><nobr><span class="ft43"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:459;left:256"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:459;left:285"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:455;left:273"><nobr><span class="ft21"></span></nobr></DIV>
<DIV style="position:absolute;top:458;left:227"><nobr><span class="ft49"><i><b>B</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:452;left:237"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:462;left:292"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:458;left:318"><nobr><span class="ft20">, then use it to </span></nobr></DIV>
<DIV style="position:absolute;top:479;left:126"><nobr><span class="ft20">predict the value of each category-feature </span></nobr></DIV>
<DIV style="position:absolute;top:503;left:145"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:131"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:164"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:493;left:152"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:501;left:136"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:171"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:187"><nobr><span class="ft20"> for  each object </span></nobr></DIV>
<DIV style="position:absolute;top:498;left:292"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:493;left:300"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:494;left:311"><nobr><span class="ft21">M</span></nobr></DIV>
<DIV style="position:absolute;top:497;left:334"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:521;left:93"><nobr><span class="ft44"><b>end for </b></span></nobr></DIV>
<DIV style="position:absolute;top:536;left:93"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:100"><nobr><span class="ft20">  For each object </span></nobr></DIV>
<DIV style="position:absolute;top:541;left:212"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:537;left:220"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:537;left:231"><nobr><span class="ft21">N</span></nobr></DIV>
<DIV style="position:absolute;top:540;left:250"><nobr><span class="ft20">, if the value of its category-</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:109"><nobr><span class="ft20">feature </span></nobr></DIV>
<DIV style="position:absolute;top:565;left:181"><nobr><span class="ft43"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:559;left:162"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:559;left:199"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:555;left:187"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:562;left:166"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:562;left:206"><nobr><span class="ft48">M</span></nobr></DIV>
<DIV style="position:absolute;top:559;left:224"><nobr><span class="ft20"> is positive, then we classify it </span></nobr></DIV>
<DIV style="position:absolute;top:580;left:109"><nobr><span class="ft20">into </span></nobr></DIV>
<DIV style="position:absolute;top:588;left:147"><nobr><span class="ft22"><i>i</i></span></nobr></DIV>
<DIV style="position:absolute;top:581;left:138"><nobr><span class="ft8"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:577;left:153"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:577;left:164"><nobr><span class="ft21">M</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:185"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:598;left:93"><nobr><span class="ft19">·</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:100"><nobr><span class="ft20">  For each object </span></nobr></DIV>
<DIV style="position:absolute;top:603;left:211"><nobr><span class="ft8"><i>x</i></span></nobr></DIV>
<DIV style="position:absolute;top:599;left:219"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:600;left:230"><nobr><span class="ft21">M</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:252"><nobr><span class="ft20">, if the value of its category-</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:109"><nobr><span class="ft20">feature </span></nobr></DIV>
<DIV style="position:absolute;top:628;left:177"><nobr><span class="ft43"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:622;left:163"><nobr><span class="ft8"><i>f</i></span></nobr></DIV>
<DIV style="position:absolute;top:622;left:196"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:617;left:184"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:625;left:168"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:625;left:203"><nobr><span class="ft48">N</span></nobr></DIV>
<DIV style="position:absolute;top:621;left:221"><nobr><span class="ft20"> is positive, then we classify it </span></nobr></DIV>
<DIV style="position:absolute;top:648;left:109"><nobr><span class="ft20">into </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:147"><nobr><span class="ft22"><i>j</i></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:139"><nobr><span class="ft8"><i>S</i></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:153"><nobr><span class="ft19"></span></nobr></DIV>
<DIV style="position:absolute;top:645;left:165"><nobr><span class="ft21">N</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:184"><nobr><span class="ft20">. </span></nobr></DIV>
<DIV style="position:absolute;top:681;left:133"><nobr><span class="ft7"><b>Figure 2: The co-bootstrapping algorithm. </b></span></nobr></DIV>
<DIV style="position:absolute;top:216;left:588"><nobr><span class="ft7"><b>Table 1: The datasets. </b></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:592"><nobr><span class="ft7"><b>Google Yahoo </b></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:488"><nobr><span class="ft6">Book </span></nobr></DIV>
<DIV style="position:absolute;top:257;left:550"><nobr><span class="ft17">/ Top/ Shopping/ <br>Publications/ Books/ </span></nobr></DIV>
<DIV style="position:absolute;top:257;left:691"><nobr><span class="ft17">/ Business_and_Economy/ <br>Shopping_and_Services/ <br>Books/ Bookstores/ </span></nobr></DIV>
<DIV style="position:absolute;top:300;left:488"><nobr><span class="ft6">Disease </span></nobr></DIV>
<DIV style="position:absolute;top:299;left:550"><nobr><span class="ft17">/ Top/ Health/ <br>Conditions_and_Diseases/ </span></nobr></DIV>
<DIV style="position:absolute;top:299;left:691"><nobr><span class="ft17">/ Health/ <br>Diseases_and_Conditions/ </span></nobr></DIV>
<DIV style="position:absolute;top:328;left:488"><nobr><span class="ft6">Movie </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:550"><nobr><span class="ft17">/ Top/ Arts/ Movies/ <br>Genres/ </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:691"><nobr><span class="ft17">/ Entertainment/ <br>Movies_and_Film/ <br>Genres/ </span></nobr></DIV>
<DIV style="position:absolute;top:370;left:488"><nobr><span class="ft6">Music </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:550"><nobr><span class="ft12">/ Top/ Arts/ Music/ Styles/ </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:691"><nobr><span class="ft17">/ Entertainment/ Music/ <br>Genres/ </span></nobr></DIV>
<DIV style="position:absolute;top:398;left:488"><nobr><span class="ft6">News </span></nobr></DIV>
<DIV style="position:absolute;top:397;left:550"><nobr><span class="ft12">/ Top/ News/ By_Subject/ </span></nobr></DIV>
<DIV style="position:absolute;top:397;left:691"><nobr><span class="ft12">/ News_and_Media/ </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:818;left:559"><nobr><span class="ft7"><b>Table 3: The number of objects. </b></span></nobr></DIV>
<DIV style="position:absolute;top:842;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:842;left:557"><nobr><span class="ft7"><b>Google Yahoo  G</b></span></nobr></DIV>
<DIV style="position:absolute;top:839;left:712"><nobr><span class="ft21"><b>Y G</b><b>Y </b></span></nobr></DIV>
<DIV style="position:absolute;top:862;left:488"><nobr><span class="ft6">Book </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:558"><nobr><span class="ft6">10,842 </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:629"><nobr><span class="ft6">11,268 </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:700"><nobr><span class="ft6">21,111 </span></nobr></DIV>
<DIV style="position:absolute;top:862;left:774"><nobr><span class="ft6">   999 </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:488"><nobr><span class="ft6">Disease </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:558"><nobr><span class="ft6">34,047 </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:629"><nobr><span class="ft6">  9,785 </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:700"><nobr><span class="ft6">41,439 </span></nobr></DIV>
<DIV style="position:absolute;top:879;left:774"><nobr><span class="ft6">2,393 </span></nobr></DIV>
<DIV style="position:absolute;top:896;left:488"><nobr><span class="ft15">Movie 36,787 14,366 49,744  1,409 <br>Music 76,420 24,518 95,971 4,967 <br>News 31,504 </span></nobr></DIV>
<DIV style="position:absolute;top:930;left:629"><nobr><span class="ft6">19,419 </span></nobr></DIV>
<DIV style="position:absolute;top:930;left:700"><nobr><span class="ft6">49,303 1,620 </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:490;left:550"><nobr><span class="ft7"><b>Table 2: The number of categories. </b></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:489"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:514;left:593"><nobr><span class="ft7"><b>Google Yahoo </b></span></nobr></DIV>
<DIV style="position:absolute;top:532;left:489"><nobr><span class="ft6">Book 49 </span></nobr></DIV>
<DIV style="position:absolute;top:532;left:747"><nobr><span class="ft6">41 </span></nobr></DIV>
<DIV style="position:absolute;top:549;left:489"><nobr><span class="ft6">Disease 30 </span></nobr></DIV>
<DIV style="position:absolute;top:549;left:747"><nobr><span class="ft6">51 </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:489"><nobr><span class="ft6">Movie 34 </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:747"><nobr><span class="ft6">25 </span></nobr></DIV>
<DIV style="position:absolute;top:583;left:489"><nobr><span class="ft6">Music 47 </span></nobr></DIV>
<DIV style="position:absolute;top:583;left:747"><nobr><span class="ft6">24 </span></nobr></DIV>
<DIV style="position:absolute;top:600;left:489"><nobr><span class="ft6">News 27 </span></nobr></DIV>
<DIV style="position:absolute;top:600;left:747"><nobr><span class="ft6">34 </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">414</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft53{font-size:7px;font-family:Times;color:#000000;}
	.ft54{font-size:10px;font-family:Helvetica;color:#000000;}
	.ft55{font-size:10px;font-family:Times;color:#000000;}
	.ft56{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213006.png" alt="background image">
<DIV style="position:absolute;top:85;left:81"><nobr><span class="ft15">The number of categories per object in these datasets is 1.54 on <br>average. This observation justifies the necessity of building <br>multi-class multi-label classifiers.  </span></nobr></DIV>
<DIV style="position:absolute;top:153;left:81"><nobr><span class="ft5"><b>5.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:152;left:104"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:153;left:114"><nobr><span class="ft5"><b>Tasks </b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:81"><nobr><span class="ft15">For each dataset, we pose 2 symmetric taxonomy integration <br>tasks: G</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:128"><nobr><span class="ft21">Y (integrating objects from Yahoo into Google) and </span></nobr></DIV>
<DIV style="position:absolute;top:205;left:81"><nobr><span class="ft6">Y</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:91"><nobr><span class="ft21">G (integrating objects from Google into Yahoo). </span></nobr></DIV>
<DIV style="position:absolute;top:230;left:81"><nobr><span class="ft52">As described in §2, we formulate each task as a classification <br>problem. The objects in G</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:237"><nobr><span class="ft21">Y can be used as test examples, </span></nobr></DIV>
<DIV style="position:absolute;top:264;left:81"><nobr><span class="ft15">because their categories in both taxonomies are known to us [2]. <br>We hide the test examples' master categories but expose their <br>source categories to the learning algorithm in training phase, and <br>then compare their hidden master categories with the predictions <br>of the learning algorithm in test phase. Suppose the number of <br>the test examples is  <i>n </i>. For G</span></nobr></DIV>
<DIV style="position:absolute;top:342;left:247"><nobr><span class="ft21">Y tasks, we randomly sample  <i>n </i> </span></nobr></DIV>
<DIV style="position:absolute;top:363;left:81"><nobr><span class="ft6">objects from the set G-Y as training examples. For Y</span></nobr></DIV>
<DIV style="position:absolute;top:359;left:382"><nobr><span class="ft21">G tasks, </span></nobr></DIV>
<DIV style="position:absolute;top:379;left:81"><nobr><span class="ft52">we randomly sample <i>n </i> objects from the set Y-G as training <br>examples. This is to simulate the common situation that the sizes </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:81"><nobr><span class="ft6">of </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:96"><nobr><span class="ft21">M and </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:143"><nobr><span class="ft21">N are roughly in same magnitude. For each task, we </span></nobr></DIV>
<DIV style="position:absolute;top:436;left:81"><nobr><span class="ft15">do such random sampling 5 times, and report the classification <br>performance averaged over these 5 random samplings.  </span></nobr></DIV>
<DIV style="position:absolute;top:488;left:81"><nobr><span class="ft5"><b>5.3</b></span></nobr></DIV>
<DIV style="position:absolute;top:487;left:104"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:114"><nobr><span class="ft5"><b>Measures </b></span></nobr></DIV>
<DIV style="position:absolute;top:507;left:81"><nobr><span class="ft15">As stated in §2, it is natural to accomplish taxonomy integration <br>tasks via building multi-class multi-label classifiers. To measure </span></nobr></DIV>
<DIV style="position:absolute;top:542;left:81"><nobr><span class="ft6">classification performance for each class (category in </span></nobr></DIV>
<DIV style="position:absolute;top:539;left:392"><nobr><span class="ft21">M), we </span></nobr></DIV>
<DIV style="position:absolute;top:563;left:81"><nobr><span class="ft6">use the standard <i>F</i>-score (<i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:567;left:231"><nobr><span class="ft53"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:563;left:235"><nobr><span class="ft6"> measure) [3]. The <i>F</i>-score is defined </span></nobr></DIV>
<DIV style="position:absolute;top:579;left:81"><nobr><span class="ft6">as the harmonic average of precision (<i>p</i>) and recall (<i>r</i>), </span></nobr></DIV>
<DIV style="position:absolute;top:598;left:109"><nobr><span class="ft6">2</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:137"><nobr><span class="ft6">(</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:170"><nobr><span class="ft6">)</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:84"><nobr><span class="ft8"><i>F</i></span></nobr></DIV>
<DIV style="position:absolute;top:598;left:118"><nobr><span class="ft8"><i>pr</i></span></nobr></DIV>
<DIV style="position:absolute;top:598;left:144"><nobr><span class="ft8"><i>p</i></span></nobr></DIV>
<DIV style="position:absolute;top:598;left:164"><nobr><span class="ft8"><i>r</i></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:98"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:593;left:153"><nobr><span class="ft19">+</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:177"><nobr><span class="ft6">, where precision is the proportion of correctly </span></nobr></DIV>
<DIV style="position:absolute;top:618;left:81"><nobr><span class="ft15">predicted positive examples among all predicted positive <br>examples, and recall is the proportion of correctly predicted <br>positive examples among all true positive examples. The <i>F</i>-<br>scores can be computed for the binary decisions on each <br>individual category first and then be averaged over categories. Or <br>they can be computed globally over all the <i>M</i></span></nobr></DIV>
<DIV style="position:absolute;top:700;left:389"><nobr><span class="ft8"><i>n</i></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:379"><nobr><span class="ft19">×</span></nobr></DIV>
<DIV style="position:absolute;top:700;left:403"><nobr><span class="ft6"> binary </span></nobr></DIV>
<DIV style="position:absolute;top:718;left:81"><nobr><span class="ft6">decisions where  <i>M </i> is the number of categories in consideration </span></nobr></DIV>
<DIV style="position:absolute;top:737;left:81"><nobr><span class="ft6">(the number of categories in </span></nobr></DIV>
<DIV style="position:absolute;top:733;left:247"><nobr><span class="ft21">M) and <i>n </i> is the number of total </span></nobr></DIV>
<DIV style="position:absolute;top:760;left:81"><nobr><span class="ft6">test examples (the number of objects in </span></nobr></DIV>
<DIV style="position:absolute;top:756;left:310"><nobr><span class="ft21">N). The former way is </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:81"><nobr><span class="ft15">called  <i>macro-averaging</i> and the latter way is called <i>micro-<br>averaging</i> [27]. It is understood that the micro-averaged <i>F</i>-score <br>(<i>miF</i>) tends to be dominated the classification performance on <br>common categories, and that the macro-averaged <i>F</i>-score (<i>maF</i>) <br>is more influenced by the classification performance on rare <br>categories [27]. Providing both kinds of scores is more <br>informative than providing either alone. </span></nobr></DIV>
<DIV style="position:absolute;top:914;left:81"><nobr><span class="ft5"><b>5.4</b></span></nobr></DIV>
<DIV style="position:absolute;top:913;left:104"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:914;left:114"><nobr><span class="ft5"><b>Settings </b></span></nobr></DIV>
<DIV style="position:absolute;top:934;left:81"><nobr><span class="ft15">We use our own implementation of NB and ENB. The Lidstone's <br>smoothing parameter </span></nobr></DIV>
<DIV style="position:absolute;top:945;left:204"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:950;left:218"><nobr><span class="ft6"> is set to an appropriate value 0.1 [1]. </span></nobr></DIV>
<DIV style="position:absolute;top:969;left:81"><nobr><span class="ft15">The performance of ENB would be greatly affected by its <br>parameter </span></nobr></DIV>
<DIV style="position:absolute;top:980;left:145"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:985;left:160"><nobr><span class="ft6">. We run ENB with a series of exponentially </span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:81"><nobr><span class="ft6">increasing values of </span></nobr></DIV>
<DIV style="position:absolute;top:998;left:197"><nobr><span class="ft28"></span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:210"><nobr><span class="ft6">: (0, 1, 3, 10, 30, 100, 300, 1000) [2] for </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:81"><nobr><span class="ft15">each taxonomy integration task,  and report the best experimental <br>results. We use BoosTexter [24] for the implementation of <br>AdaBoost, taking single words as terms. We set the boosting </span></nobr></DIV>
<DIV style="position:absolute;top:85;left:477"><nobr><span class="ft6">rounds </span></nobr></DIV>
<DIV style="position:absolute;top:86;left:545"><nobr><span class="ft6">1000</span></nobr></DIV>
<DIV style="position:absolute;top:86;left:524"><nobr><span class="ft8"><i>T</i></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:536"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:85;left:580"><nobr><span class="ft6"> and the co-bootstrapping iteration number </span></nobr></DIV>
<DIV style="position:absolute;top:104;left:502"><nobr><span class="ft6">8</span></nobr></DIV>
<DIV style="position:absolute;top:104;left:480"><nobr><span class="ft8"><i>R</i></span></nobr></DIV>
<DIV style="position:absolute;top:99;left:492"><nobr><span class="ft19">=</span></nobr></DIV>
<DIV style="position:absolute;top:103;left:511"><nobr><span class="ft6"> (see Figure 1 &amp; 2). In the following sections, we denote </span></nobr></DIV>
<DIV style="position:absolute;top:121;left:477"><nobr><span class="ft15">the normal AdaBoost approach by AB, and denote the co-<br>bootstrapping approach based on AdaBoost algorithm by CB-AB. </span></nobr></DIV>
<DIV style="position:absolute;top:173;left:477"><nobr><span class="ft5"><b>5.5</b></span></nobr></DIV>
<DIV style="position:absolute;top:172;left:499"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:510"><nobr><span class="ft5"><b>Results </b></span></nobr></DIV>
<DIV style="position:absolute;top:422;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:665;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:828"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:477"><nobr><span class="ft15">The experimental results of NB and ENB are shown in Table 4. <br>We see that ENB really can achieve much better performance <br>than NB for taxonomy integration. </span></nobr></DIV>
<DIV style="position:absolute;top:899;left:507"><nobr><span class="ft54">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:877;left:507"><nobr><span class="ft54">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:507"><nobr><span class="ft54">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:831;left:507"><nobr><span class="ft54">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:808;left:507"><nobr><span class="ft54">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:507"><nobr><span class="ft54">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:763;left:507"><nobr><span class="ft54">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:530"><nobr><span class="ft54">0</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:562"><nobr><span class="ft54">1</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:595"><nobr><span class="ft54">2</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:627"><nobr><span class="ft54">3</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:661"><nobr><span class="ft54">4</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:694"><nobr><span class="ft54">5</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:726"><nobr><span class="ft54">6</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:759"><nobr><span class="ft54">7</span></nobr></DIV>
<DIV style="position:absolute;top:917;left:791"><nobr><span class="ft54">8</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:599"><nobr><span class="ft54">maF(G</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:635"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:648"><nobr><span class="ft54">Y)</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:700"><nobr><span class="ft54">miF(G</span></nobr></DIV>
<DIV style="position:absolute;top:701;left:732"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:704;left:744"><nobr><span class="ft54">Y)</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:599"><nobr><span class="ft54">maF(Y</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:635"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:726;left:648"><nobr><span class="ft54">G)</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:700"><nobr><span class="ft54">miF(Y</span></nobr></DIV>
<DIV style="position:absolute;top:723;left:732"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:726;left:744"><nobr><span class="ft54">G)</span></nobr></DIV>
<DIV style="position:absolute;top:956;left:488"><nobr><span class="ft56"><b>Figure 4: The taxonomy integration performance <br>increases along with the number of co-bootstrapping <br>iterations, on the Book dataset. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:441;left:507"><nobr><span class="ft7"><b>Table 5: Experimental Results of AB and CB-AB. </b></span></nobr></DIV>
<DIV style="position:absolute;top:466;left:643"><nobr><span class="ft7"><b>AB CB-AB </b></span></nobr></DIV>
<DIV style="position:absolute;top:465;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:465;left:543"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:612"><nobr><span class="ft8"><i>maF miF maF miF </i></span></nobr></DIV>
<DIV style="position:absolute;top:501;left:543"><nobr><span class="ft6">Book </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:606"><nobr><span class="ft6">0.1740 0.4499 0.2540 0.6030 </span></nobr></DIV>
<DIV style="position:absolute;top:518;left:543"><nobr><span class="ft15">Disease 0.5375 0.6674 0.6533 0.7703 <br>Movie  0.1930 0.4892 0.3172 0.6716 <br>Music  0.3316 0.5025 0.4851 0.6826 </span></nobr></DIV>
<DIV style="position:absolute;top:534;left:488"><nobr><span class="ft6">G</span></nobr></DIV>
<DIV style="position:absolute;top:530;left:498"><nobr><span class="ft21">Y </span></nobr></DIV>
<DIV style="position:absolute;top:569;left:543"><nobr><span class="ft38">News  0.2150 0.4625 0.3083 0.6218 <br>Book </span></nobr></DIV>
<DIV style="position:absolute;top:587;left:606"><nobr><span class="ft6">0.2436 0.3853 0.3516 0.6341 </span></nobr></DIV>
<DIV style="position:absolute;top:604;left:543"><nobr><span class="ft15">Disease 0.3719 0.6350 0.4371 0.7287 <br>Movie  0.2559 0.5214 0.3922 0.7154 <br>Music  0.4369 0.6397 0.5799 0.7994 </span></nobr></DIV>
<DIV style="position:absolute;top:620;left:488"><nobr><span class="ft6">Y</span></nobr></DIV>
<DIV style="position:absolute;top:616;left:498"><nobr><span class="ft21">G </span></nobr></DIV>
<DIV style="position:absolute;top:655;left:543"><nobr><span class="ft6">News  0.3774 0.4942 0.4340 0.6421 </span></nobr></DIV>
<DIV style="position:absolute;top:198;left:514"><nobr><span class="ft7"><b>Table 4: Experimental Results of NB and ENB. </b></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:643"><nobr><span class="ft7"><b>NB ENB </b></span></nobr></DIV>
<DIV style="position:absolute;top:222;left:488"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:222;left:543"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:240;left:612"><nobr><span class="ft8"><i>maF miF maF miF </i></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:543"><nobr><span class="ft6">Book </span></nobr></DIV>
<DIV style="position:absolute;top:258;left:606"><nobr><span class="ft6">0.1286 0.2384 0.1896 0.5856 </span></nobr></DIV>
<DIV style="position:absolute;top:275;left:543"><nobr><span class="ft15">Disease 0.4386 0.5602 0.5230 0.6895 <br>Movie  0.1709 0.3003 0.2094 0.5331 <br>Music  0.2386 0.3881 0.2766 0.5408 </span></nobr></DIV>
<DIV style="position:absolute;top:291;left:488"><nobr><span class="ft6">G</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:498"><nobr><span class="ft21">Y </span></nobr></DIV>
<DIV style="position:absolute;top:326;left:543"><nobr><span class="ft6">News </span></nobr></DIV>
<DIV style="position:absolute;top:326;left:606"><nobr><span class="ft6">0.2233 0.4450 0.2578 0.5987 </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:543"><nobr><span class="ft6">Book </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:606"><nobr><span class="ft6">0.1508 0.2107 0.2227 0.5471 </span></nobr></DIV>
<DIV style="position:absolute;top:361;left:543"><nobr><span class="ft15">Disease 0.2746 0.4812 0.3415 0.6370 <br>Movie  0.2319 0.4046 0.2884 0.5534 <br>Music  0.3124 0.5359 0.3572 0.6824 </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:488"><nobr><span class="ft6">Y</span></nobr></DIV>
<DIV style="position:absolute;top:373;left:498"><nobr><span class="ft21">G </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:543"><nobr><span class="ft6">News </span></nobr></DIV>
<DIV style="position:absolute;top:412;left:606"><nobr><span class="ft6">0.2966 0.4219 0.3639 0.6007 </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">415</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft57{font-size:10px;line-height:-7px;font-family:Helvetica;color:#000000;}
	.ft58{font-size:10px;line-height:-11px;font-family:Helvetica;color:#000000;}
	.ft59{font-size:10px;line-height:-6px;font-family:Helvetica;color:#000000;}
	.ft60{font-size:10px;line-height:-2px;font-family:Helvetica;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="213007.png" alt="background image">
<DIV style="position:absolute;top:85;left:81"><nobr><span class="ft15">The experimental results of AB and CB-AB are shown in Table 5. <br>Obviously AB beats NB, which is consistent with the conclusion <br>of [24]. Also we find that CB-AB works better than AB for <br>taxonomy integration, which suggests that co-bootstrapping </span></nobr></DIV>
<DIV style="position:absolute;top:152;left:81"><nobr><span class="ft6">makes effective use of the categorization of </span></nobr></DIV>
<DIV style="position:absolute;top:148;left:356"><nobr><span class="ft21">N to enhance </span></nobr></DIV>
<DIV style="position:absolute;top:175;left:81"><nobr><span class="ft6">classification for </span></nobr></DIV>
<DIV style="position:absolute;top:171;left:176"><nobr><span class="ft21">M. </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:81"><nobr><span class="ft15">Figure 4 shows that the taxonomy integration performance <br>increases along with the number of co-bootstrapping iterations, <br>on the Book dataset. This implies that the two boosting-<br>classifiers learned from two taxonomies do mutually boost each <br>other until they become stable.  </span></nobr></DIV>
<DIV style="position:absolute;top:605;left:432"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:929;left:432"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:942;left:81"><nobr><span class="ft15">The experimental results of ENB and CB-AB are compared in <br>Figure 5 and 6. It is clear that CB-AB outperforms ENB <br>consistently and significantly.  </span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:81"><nobr><span class="ft5"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:1009;left:95"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1010;left:105"><nobr><span class="ft5"><b>RELATED WORK </b></span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:81"><nobr><span class="ft15">Most of the recent research efforts related to taxonomy <br>integration are in the context of ontology mapping on semantic <br>web. An ontology specifies a conceptualization of a domain in </span></nobr></DIV>
<DIV style="position:absolute;top:85;left:477"><nobr><span class="ft15">terms of concepts, attributes, and relations [11]. The concepts in <br>an ontology are usually organized into a taxonomy: each concept <br>is represented by a category and associated with a set of objects <br>(called the extension of that concept). The basic goal of ontology <br>mapping is to identify (typically one-to-one) semantic <br>correspondences between the taxonomies of two given ontologies: <br>for each concept (category) in one taxonomy, find the most <br>similar concept (category) in the other taxonomy. Many works in <br>this field use a variety of heuristics to find mappings [7, 16, 19, <br>21]. Recently machine learning techniques have been introduced <br>to further automate the ontology mapping process [10, 13, 14, 20, <br>26]. Some of them derive similarities between concepts <br>(categories) based on their extensions (objects) [10, 13, 14], <br>therefore they need to first integrate objects from one taxonomy <br>into the other and vice versa (i.e., taxonomy integration). So our <br>work can be utilized as a basic component of an ontology <br>mapping system.  </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:477"><nobr><span class="ft15">As stated in §2, taxonomy integration can be formulated as a <br>classification problem. The Rocchio algorithm [3, 22] has been <br>applied to this problem in [14]; and the Naïve Bayes (NB) <br>algorithm [18] has been applied to this problem in [10], without <br>exploiting information in the source taxonomy. To our <br>knowledge, the most advanced approach to taxonomy integration <br>is the enhanced Naïve Bayes (ENB) algorithm proposed by <br>Agrawal and Srikant [2], which we have reviewed and compared <br>with our approach. </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:477"><nobr><span class="ft15">In [6], AdaBoost is selected as the framework to combine term-<br>features and automatically extracted semantic-features in the <br>context of text categorization. We also choose AdaBoost to <br>combine heterogeneous features (term-features and category-<br>features), but it is for a different problem (taxonomy integration) <br>and it works in a more complex way (through co-bootstrapping). </span></nobr></DIV>
<DIV style="position:absolute;top:630;left:477"><nobr><span class="ft15">In [8], an approach called co-boosting is proposed for named <br>entity classification. Essentially co-boosting is a co-training [5] <br>method that attempts to utilize unlabeled data to help <br>classification through exploiting a particular form of redundancy <br>in data: each instance is described by multiple views (disjoint <br>feature sets) which are both compatible and uncorrelated <br>(conditionally independent). However, the multi-view <br>assumption does not hold in the context of taxonomy integration: <br>the set of category features should not be considered as a view <br>because category features alone are not sufficient for <br>classification and they are strongly correlated with term features. <br>In contrast to co-boosting (co-training), co-bootstrapping works <br>with two taxonomies but not two views.  </span></nobr></DIV>
<DIV style="position:absolute;top:860;left:477"><nobr><span class="ft5"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:859;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:860;left:501"><nobr><span class="ft5"><b>CONCLUSION </b></span></nobr></DIV>
<DIV style="position:absolute;top:880;left:477"><nobr><span class="ft15">Our main contribution is to propose a new approach, co-<br>bootstrapping, that can effectively exploit the implicit knowledge <br>in the source taxonomy to improve taxonomy integration.  </span></nobr></DIV>
<DIV style="position:absolute;top:938;left:477"><nobr><span class="ft15">The future work may include: theoretical analysis of the co-<br>bootstrapping approach, incorporating commonsense knowledge <br>and domain constraints into the taxonomy integration process, <br>and so forth. </span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:477"><nobr><span class="ft5"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:490"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1022;left:501"><nobr><span class="ft5"><b>ACKNOWLEDGMENTS </b></span></nobr></DIV>
<DIV style="position:absolute;top:1042;left:477"><nobr><span class="ft15">We would like to thank the anonymous reviewers for their <br>helpful comments and suggestions. </span></nobr></DIV>
<DIV style="position:absolute;top:456;left:121"><nobr><span class="ft54">0</span></nobr></DIV>
<DIV style="position:absolute;top:443;left:111"><nobr><span class="ft54">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:432;left:111"><nobr><span class="ft54">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:420;left:111"><nobr><span class="ft54">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:407;left:111"><nobr><span class="ft54">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:396;left:111"><nobr><span class="ft54">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:111"><nobr><span class="ft54">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:371;left:111"><nobr><span class="ft54">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:360;left:111"><nobr><span class="ft54">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:348;left:111"><nobr><span class="ft54">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:154"><nobr><span class="ft57">B<br>ook</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:181"><nobr><span class="ft59">Di<br>s<br>e<br>a<br>s<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:208"><nobr><span class="ft54">Mo</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:208"><nobr><span class="ft60">v<br>i<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:234"><nobr><span class="ft54">Mu</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:234"><nobr><span class="ft60">s<br>i<br>c</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:261"><nobr><span class="ft54">Ne</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:261"><nobr><span class="ft54">ws</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:288"><nobr><span class="ft57">B<br>ook</span></nobr></DIV>
<DIV style="position:absolute;top:504;left:315"><nobr><span class="ft59">Di<br>s<br>e<br>a<br>s<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:342"><nobr><span class="ft54">Mo</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:342"><nobr><span class="ft60">v<br>i<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:492;left:368"><nobr><span class="ft54">Mu</span></nobr></DIV>
<DIV style="position:absolute;top:475;left:368"><nobr><span class="ft60">s<br>i<br>c</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:395"><nobr><span class="ft54">Ne</span></nobr></DIV>
<DIV style="position:absolute;top:477;left:395"><nobr><span class="ft54">ws</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:190"><nobr><span class="ft54">G</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:199"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:211"><nobr><span class="ft54">Y</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:324"><nobr><span class="ft54">Y</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:333"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:525;left:345"><nobr><span class="ft54">G</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:237"><nobr><span class="ft54">ENB</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:276"><nobr><span class="ft54">CB-AB</span></nobr></DIV>
<DIV style="position:absolute;top:564;left:92"><nobr><span class="ft56"><b>Figure 5: Comparing the macro-averaged F-scores of <br>ENB and CB-AB. </b></span></nobr></DIV>
<DIV style="position:absolute;top:602;left:92"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:121"><nobr><span class="ft54">0</span></nobr></DIV>
<DIV style="position:absolute;top:767;left:111"><nobr><span class="ft54">0.1</span></nobr></DIV>
<DIV style="position:absolute;top:756;left:111"><nobr><span class="ft54">0.2</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:111"><nobr><span class="ft54">0.3</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:111"><nobr><span class="ft54">0.4</span></nobr></DIV>
<DIV style="position:absolute;top:720;left:111"><nobr><span class="ft54">0.5</span></nobr></DIV>
<DIV style="position:absolute;top:708;left:111"><nobr><span class="ft54">0.6</span></nobr></DIV>
<DIV style="position:absolute;top:695;left:111"><nobr><span class="ft54">0.7</span></nobr></DIV>
<DIV style="position:absolute;top:684;left:111"><nobr><span class="ft54">0.8</span></nobr></DIV>
<DIV style="position:absolute;top:672;left:111"><nobr><span class="ft54">0.9</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:154"><nobr><span class="ft57">B<br>ook</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:181"><nobr><span class="ft59">Di<br>s<br>e<br>a<br>s<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:208"><nobr><span class="ft54">Mo</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:208"><nobr><span class="ft60">v<br>i<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:234"><nobr><span class="ft54">Mu</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:234"><nobr><span class="ft60">s<br>i<br>c</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:261"><nobr><span class="ft54">Ne</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:261"><nobr><span class="ft54">ws</span></nobr></DIV>
<DIV style="position:absolute;top:812;left:288"><nobr><span class="ft57">B<br>ook</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:315"><nobr><span class="ft59">Di<br>s<br>e<br>a<br>s<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:342"><nobr><span class="ft54">Mo</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:342"><nobr><span class="ft60">v<br>i<br>e</span></nobr></DIV>
<DIV style="position:absolute;top:816;left:368"><nobr><span class="ft54">Mu</span></nobr></DIV>
<DIV style="position:absolute;top:799;left:368"><nobr><span class="ft60">s<br>i<br>c</span></nobr></DIV>
<DIV style="position:absolute;top:817;left:395"><nobr><span class="ft54">Ne</span></nobr></DIV>
<DIV style="position:absolute;top:801;left:395"><nobr><span class="ft54">ws</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:190"><nobr><span class="ft54">G</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:199"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:849;left:211"><nobr><span class="ft54">Y</span></nobr></DIV>
<DIV style="position:absolute;top:849;left:324"><nobr><span class="ft54">Y</span></nobr></DIV>
<DIV style="position:absolute;top:846;left:333"><nobr><span class="ft55"></span></nobr></DIV>
<DIV style="position:absolute;top:849;left:345"><nobr><span class="ft54">G</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:237"><nobr><span class="ft54">ENB</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:276"><nobr><span class="ft54">CB-AB</span></nobr></DIV>
<DIV style="position:absolute;top:888;left:92"><nobr><span class="ft56"><b>Figure 6: Comparing the micro-averaged F-scores of <br>ENB and CB-AB. </b></span></nobr></DIV>
<DIV style="position:absolute;top:926;left:92"><nobr><span class="ft6"> </span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">416</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="213008.png" alt="background image">
<DIV style="position:absolute;top:86;left:81"><nobr><span class="ft5"><b>9.</b></span></nobr></DIV>
<DIV style="position:absolute;top:85;left:95"><nobr><span class="ft9"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:86;left:105"><nobr><span class="ft5"><b>REFERENCES </b></span></nobr></DIV>
<DIV style="position:absolute;top:106;left:81"><nobr><span class="ft6">[1]  Agrawal, R., Bayardo, R. and Srikant, R. Athena: Mining-</span></nobr></DIV>
<DIV style="position:absolute;top:122;left:108"><nobr><span class="ft15">based Interactive Management of Text Databases. in <br><i>Proceedings of the 7th International Conference on <br>Extending Database Technology (EDBT)</i>, Konstanz, <br>Germany, 2000, 365-379. </span></nobr></DIV>
<DIV style="position:absolute;top:194;left:81"><nobr><span class="ft6">[2]  Agrawal, R. and Srikant, R. On Integrating Catalogs. in </span></nobr></DIV>
<DIV style="position:absolute;top:210;left:108"><nobr><span class="ft16"><i>Proceedings of the 10th International World Wide Web <br>Conference (WWW)</i>, Hong Kong, 2001, 603-612. </span></nobr></DIV>
<DIV style="position:absolute;top:250;left:81"><nobr><span class="ft6">[3]  Baeza-Yates, R. and Ribeiro-Neto, B. <i>Modern Information </i></span></nobr></DIV>
<DIV style="position:absolute;top:266;left:108"><nobr><span class="ft8"><i>Retrieval</i>. Addison-Wesley, New York, NY, 1999. </span></nobr></DIV>
<DIV style="position:absolute;top:289;left:81"><nobr><span class="ft6">[4]  Berners-Lee, T., Hendler, J. and Lassila, O. The Semantic </span></nobr></DIV>
<DIV style="position:absolute;top:305;left:108"><nobr><span class="ft6">Web <i>Scientific American</i>, 2001. </span></nobr></DIV>
<DIV style="position:absolute;top:327;left:81"><nobr><span class="ft6">[5]  Blum, A. and Mitchell, T. Combining Labeled and </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:108"><nobr><span class="ft16">Unlabeled Data with Co-Training. in <i>Proceedings of the <br>11th Annual Conference on Computational Learning Theory <br>(COLT)</i>, Madison, WI, 1998, 92-100. </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:81"><nobr><span class="ft6">[6]  Cai, L. and Hofmann, T. Text Categorization by Boosting </span></nobr></DIV>
<DIV style="position:absolute;top:416;left:108"><nobr><span class="ft16">Automatically Extracted Concepts. in <i>Proceedings of the <br>26th Annual International ACM SIGIR Conference on <br>Research and Development in Information Retrieval <br>(SIGIR)</i>, Toronto, Canada, 2003, 182-189. </span></nobr></DIV>
<DIV style="position:absolute;top:488;left:81"><nobr><span class="ft6">[7]  Chalupsky, H. OntoMorph: A Translation System for </span></nobr></DIV>
<DIV style="position:absolute;top:505;left:108"><nobr><span class="ft15">Symbolic Knowledge. in <i>Proceedings of the 7th <br>International Conference on Principles of Knowledge <br>Representation and Reasoning (KR)</i>, Breckenridge, CO, <br>2000, 471-482. </span></nobr></DIV>
<DIV style="position:absolute;top:576;left:81"><nobr><span class="ft6">[8]  Collins, M. and Singer, Y. Unsupervised Models for Named </span></nobr></DIV>
<DIV style="position:absolute;top:593;left:108"><nobr><span class="ft15">Entity Classification. in <i>Proceedings of the Joint SIGDAT <br>Conference on Empirical Methods in Natural Language <br>Processing and Very Large Corpora (EMNLP)</i>, College <br>Park, MD, 1999, 189-196. </span></nobr></DIV>
<DIV style="position:absolute;top:665;left:81"><nobr><span class="ft6">[9]  Cristianini, N. and Shawe-Taylor, J. <i>An Introduction to </i></span></nobr></DIV>
<DIV style="position:absolute;top:681;left:108"><nobr><span class="ft15"><i>Support Vector Machines</i>. Cambridge University Press, <br>Cambridge, UK, 2000. </span></nobr></DIV>
<DIV style="position:absolute;top:720;left:81"><nobr><span class="ft6">[10]  Doan, A., Madhavan, J., Domingos, P. and Halevy, A. </span></nobr></DIV>
<DIV style="position:absolute;top:736;left:108"><nobr><span class="ft16">Learning to Map between Ontologies on the Semantic Web. <br>in <i>Proceedings of the 11th International World Wide Web <br>Conference (WWW)</i>, Hawaii, USA, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:792;left:81"><nobr><span class="ft6">[11] Fensel, D. <i>Ontologies: A Silver Bullet for Knowledge </i></span></nobr></DIV>
<DIV style="position:absolute;top:808;left:108"><nobr><span class="ft15"><i>Management and Electronic Commerce</i>. Springer-Verlag, <br>2001. </span></nobr></DIV>
<DIV style="position:absolute;top:847;left:81"><nobr><span class="ft6">[12] Freund, Y. and Schapire, R.E. A Decision-theoretic </span></nobr></DIV>
<DIV style="position:absolute;top:863;left:108"><nobr><span class="ft15">Generalization of On-line Learning and an Application to <br>Boosting. <i>Journal of Computer and System Sciences</i>, <i>55</i> (1). <br>119-139. </span></nobr></DIV>
<DIV style="position:absolute;top:918;left:81"><nobr><span class="ft6">[13] Ichise, R., Takeda, H. and Honiden, S. Rule Induction for </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:108"><nobr><span class="ft16">Concept Hierarchy Alignment. in <i>Proceedings of the <br>Workshop on Ontologies and Information Sharing at the <br>17th International Joint Conference on Artificial <br>Intelligence (IJCAI)</i>, Seattle, WA, 2001, 26-29. </span></nobr></DIV>
<DIV style="position:absolute;top:85;left:477"><nobr><span class="ft6">[14]  Lacher, M.S. and Groh, G. Facilitating the Exchange of </span></nobr></DIV>
<DIV style="position:absolute;top:101;left:504"><nobr><span class="ft16">Explicit Knowledge through Ontology Mappings. in <br><i>Proceedings of the Fourteenth International Florida <br>Artificial Intelligence Research Society Conference <br>(FLAIRS)</i>, Key West, FL, 2001, 305-309. </span></nobr></DIV>
<DIV style="position:absolute;top:173;left:477"><nobr><span class="ft6">[15]  McCallum, A. and Nigam, K. A Comparison of Event </span></nobr></DIV>
<DIV style="position:absolute;top:190;left:504"><nobr><span class="ft15">Models for Naive Bayes Text Classification. in <i>AAAI-98 <br>Workshop on Learning for Text Categorization</i>, Madison, <br>WI, 1998, 41-48. </span></nobr></DIV>
<DIV style="position:absolute;top:245;left:477"><nobr><span class="ft6">[16]  McGuinness, D.L., Fikes, R., Rice, J. and Wilder, S. The </span></nobr></DIV>
<DIV style="position:absolute;top:261;left:504"><nobr><span class="ft15">Chimaera Ontology Environment. in <i>Proceedings of the <br>17th National Conference on Artificial Intelligence (AAAI)</i>, <br>Austin, TX, 2000, 1123--1124. </span></nobr></DIV>
<DIV style="position:absolute;top:316;left:477"><nobr><span class="ft6">[17]  Meir, R. and Ratsch, G. An Introduction to Boosting and </span></nobr></DIV>
<DIV style="position:absolute;top:332;left:504"><nobr><span class="ft15">Leveraging. in Mendelson, S. and Smola, A.J. eds. <br><i>Advanced Lectures on Machine Learning, LNCS</i>, Springer-<br>Verlag, 2003, 119-184. </span></nobr></DIV>
<DIV style="position:absolute;top:388;left:477"><nobr><span class="ft6">[18] Mitchell, T. <i>Machine Learning</i>. McGraw Hill, 1997. </span></nobr></DIV>
<DIV style="position:absolute;top:410;left:477"><nobr><span class="ft6">[19] Mitra, P., Wiederhold, G. and Jannink, J. Semi-automatic </span></nobr></DIV>
<DIV style="position:absolute;top:427;left:504"><nobr><span class="ft15">Integration of Knowledge Sources. in <i>Proceedings of The <br>2nd International Conference on Information Fusion</i>, <br>Sunnyvale, CA, 1999. </span></nobr></DIV>
<DIV style="position:absolute;top:482;left:477"><nobr><span class="ft6">[20] Noy, N.F. and Musen, M.A. Anchor-PROMPT: Using Non-</span></nobr></DIV>
<DIV style="position:absolute;top:498;left:504"><nobr><span class="ft16">Local Context for Semantic Matching. in <i>Proceedings of the <br>Workshop on Ontologies and Information Sharing at the <br>17th International Joint Conference on Artificial <br>Intelligence (IJCAI)</i>, Seattle, WA, 2001, 63-70. </span></nobr></DIV>
<DIV style="position:absolute;top:570;left:477"><nobr><span class="ft6">[21] Noy, N.F. and Musen, M.A. PROMPT: Algorithm and Tool </span></nobr></DIV>
<DIV style="position:absolute;top:587;left:504"><nobr><span class="ft16">for Automated Ontology Merging and Alignment. in <br><i>Proceedings of the National Conference on Artificial <br>Intelligence (AAAI)</i>, Austin, TX, 2000, 450-455. </span></nobr></DIV>
<DIV style="position:absolute;top:642;left:477"><nobr><span class="ft6">[22]  Rocchio, J.J. Relevance Feedback in Information Retrieval. </span></nobr></DIV>
<DIV style="position:absolute;top:659;left:504"><nobr><span class="ft15">in Salton, G. ed. <i>The SMART Retrieval System: Experiments <br>in Automatic Document Processing</i>, Prentice-Hall, 1971, <br>313-323. </span></nobr></DIV>
<DIV style="position:absolute;top:714;left:477"><nobr><span class="ft6">[23] Schapire, R.E. The Boosting Approach to Machine </span></nobr></DIV>
<DIV style="position:absolute;top:730;left:504"><nobr><span class="ft16">Learning: An Overview. in <i>MSRI Workshop on Nonlinear <br>Estimation and Classification</i>, Berkeley, CA, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:769;left:477"><nobr><span class="ft6">[24] Schapire, R.E. and Singer, Y. BoosTexter: A Boosting-</span></nobr></DIV>
<DIV style="position:absolute;top:786;left:504"><nobr><span class="ft15">based System for Text Categorization. <i>Machine Learning</i>, <br><i>39</i> (2/3). 135-168. </span></nobr></DIV>
<DIV style="position:absolute;top:825;left:477"><nobr><span class="ft6">[25] Schapire, R.E. and Singer, Y. Improved Boosting </span></nobr></DIV>
<DIV style="position:absolute;top:841;left:504"><nobr><span class="ft16">Algorithms Using Confidence-rated Predictions. <i>Machine <br>Learning</i>, <i>37</i> (3). 297-336. </span></nobr></DIV>
<DIV style="position:absolute;top:880;left:477"><nobr><span class="ft6">[26] Stumme, G. and Maedche, A. FCA-MERGE: Bottom-Up </span></nobr></DIV>
<DIV style="position:absolute;top:897;left:504"><nobr><span class="ft16">Merging of Ontologies. in <i>Proceedings of the 17th <br>International Joint Conference on Artificial Intelligence <br>(IJCAI)</i>, Seattle, WA, 2001, 225-230. </span></nobr></DIV>
<DIV style="position:absolute;top:952;left:477"><nobr><span class="ft6">[27] Yang, Y. and Liu, X. A Re-examination of Text </span></nobr></DIV>
<DIV style="position:absolute;top:969;left:504"><nobr><span class="ft16">Categorization Methods. in <i>Proceedings of the 22nd ACM <br>International Conference on Research and Development in <br>Information Retrieval (SIGIR)</i>, Berkeley, CA, 1999, 42-49. </span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:459"><nobr><span class="ft0"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1116;left:449"><nobr><span class="ft6">417</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
