<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>UIST00 Front-End Matter</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="Bill Schilit">
<META name="date" content="2002-09-18T09:58:49+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:15px;font-family:Times;color:#000000;}
	.ft1{font-size:24px;font-family:Times;color:#000000;}
	.ft2{font-size:13px;font-family:Times;color:#000000;}
	.ft3{font-size:9px;font-family:Times;color:#000000;}
	.ft4{font-size:11px;font-family:Times;color:#000000;}
	.ft5{font-size:8px;font-family:Times;color:#000000;}
	.ft6{font-size:13px;line-height:17px;font-family:Times;color:#000000;}
	.ft7{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="183001.png" alt="background image">
<DIV style="position:absolute;top:82;left:81"><nobr><span class="ft0">   </span></nobr></DIV>
<DIV style="position:absolute;top:102;left:173"><nobr><span class="ft1"><b>StyleCam: Interactive Stylized 3D Navigation </b></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:165"><nobr><span class="ft1"><b> using Integrated Spatial &amp; Temporal Controls </b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:459"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:100"><nobr><span class="ft0">Nicholas Burtnyk</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:225"><nobr><span class="ft3"><i>2,1</i></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:240"><nobr><span class="ft0">, Azam Khan</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:333"><nobr><span class="ft3"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:339"><nobr><span class="ft0">, George Fitzmaurice</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:493"><nobr><span class="ft3"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:499"><nobr><span class="ft0">, Ravin Balakrishnan</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:652"><nobr><span class="ft3"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:182;left:658"><nobr><span class="ft0">, Gordon Kurtenbach</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:812"><nobr><span class="ft3"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:202;left:210"><nobr><span class="ft3"><i>1</i></span></nobr></DIV>
<DIV style="position:absolute;top:205;left:216"><nobr><span class="ft0">Alias|wavefront </span></nobr></DIV>
<DIV style="position:absolute;top:226;left:195"><nobr><span class="ft0">210 King Street East </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:209"><nobr><span class="ft0">Toronto, Ontario </span></nobr></DIV>
<DIV style="position:absolute;top:267;left:207"><nobr><span class="ft0">Canada M5A 1J7 </span></nobr></DIV>
<DIV style="position:absolute;top:288;left:162"><nobr><span class="ft0">akhan, gf, gordo@aw.sgi.com </span></nobr></DIV>
<DIV style="position:absolute;top:202;left:524"><nobr><span class="ft3"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:205;left:530"><nobr><span class="ft0">Department of Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:226;left:569"><nobr><span class="ft0">University of Toronto </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:587"><nobr><span class="ft0">Toronto, Ontario </span></nobr></DIV>
<DIV style="position:absolute;top:267;left:583"><nobr><span class="ft0">Canada M5S 3G5 </span></nobr></DIV>
<DIV style="position:absolute;top:288;left:482"><nobr><span class="ft0">n.burtnyk@utoronto.ca, ravin@cs.toronto.edu </span></nobr></DIV>
<DIV style="position:absolute;top:329;left:81"><nobr><span class="ft4"><b>ABSTRACT </b></span></nobr></DIV>
<DIV style="position:absolute;top:351;left:81"><nobr><span class="ft6"><b>This paper describes StyleCam, an approach for authoring <br>3D viewing experiences that incorporate stylistic elements <br>that are not available in typical 3D viewers. A key aspect of <br>StyleCam is that it allows the author to significantly tailor <br>what the user sees and when they see it. The resulting <br>viewing experience can approach the visual richness and <br>pacing of highly authored visual content such as television <br>commercials or feature films. At the same time, StyleCam <br>allows for a satisfying level of interactivity while avoiding <br>the problems inherent in using unconstrained camera <br>models. The main components of StyleCam are camera <br>surfaces which spatially constrain the viewing camera; <br>animation clips that allow for visually appealing transitions <br>between different camera surfaces; and a simple, unified, <br>interaction technique that permits the user to seamlessly <br>and continuously move between spatial-control of the <br>camera and temporal-control of the animated transitions. <br>Further, the user's focus of attention is always kept on the <br>content, and not on extraneous interface widgets. In <br>addition to describing the conceptual model of StyleCam, <br>its current implementation, and an example authored <br>experience, we also present the results of an evaluation <br>involving real users. </b></span></nobr></DIV>
<DIV style="position:absolute;top:757;left:81"><nobr><span class="ft4"><b>KEYWORDS</b></span></nobr></DIV>
<DIV style="position:absolute;top:756;left:160"><nobr><span class="ft2"><b>: interaction techniques, camera controls, 3D </b></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:81"><nobr><span class="ft2"><b>navigation, 3D viewers, 3D visualization. </b></span></nobr></DIV>
<DIV style="position:absolute;top:799;left:81"><nobr><span class="ft4"><b>1.  INTRODUCTION </b></span></nobr></DIV>
<DIV style="position:absolute;top:821;left:81"><nobr><span class="ft6"><b>Computer graphics has reached the stage where 3D models <br>can be created and rendered, often in real time on <br>commodity hardware, at a fidelity that is almost <br>indistinguishable from the real thing. As such, it should be <br>feasible at the consumer level to use 3D models rather than <br>2D images to represent or showcase various physical <br>artifacts. Indeed, as an example, many product <br>manufacturers' websites are beginning to supply not only </b></span></nobr></DIV>
<DIV style="position:absolute;top:329;left:476"><nobr><span class="ft6"><b>professionally produced 2D images of their products, but <br>also ways to view their products in 3D. Unfortunately, the <br>visual and interactive experience provided by these 3D <br>viewers currently fall short of the slick, professionally <br>produced 2D images of the same items. For example, the <br>quality of 2D imagery in an automobile's sales brochure <br>typically provides a richer and more compelling <br>presentation of that automobile to the user than the <br>interactive 3D experiences provided on the manufacturer's <br>website. If these 3D viewers are to replace, or at the very <br>least be at par with, the 2D imagery, eliminating this </b></span></nobr></DIV>
<DIV style="position:absolute;top:648;left:828"><nobr><span class="ft2"><b>r, </b></span></nobr></DIV>
<DIV style="position:absolute;top:778;left:693"><nobr><span class="ft2"><b> viewpoint in the scene </b></span></nobr></DIV>
<DIV style="position:absolute;top:519;left:476"><nobr><span class="ft2"><b>difference in quality is critical. </b></span></nobr></DIV>
<DIV style="position:absolute;top:545;left:476"><nobr><span class="ft6"><b>The reasons for the poor quality of these 3D viewers fall <br>roughly into two categories. First, 2D imagery is usually <br>produced by professional artists and photographers who are <br>skilled at using this well-established artform to convey <br>information, feelings, or experiences, whereas creators of <br>3D models do not necessarily have the same established <br>skills and are working in an evolving medium. Howeve<br>this problem will work itself out as the medium matures.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:692;left:476"><nobr><span class="ft6"><b>The second issue is more troublesome. In creating 2D <br>images a photographer can carefully control most of the <br>elements that make up the shot including lighting and <br>viewpoint, in an attempt to ensure that a viewer receives the <br>intended message. In contrast, 3D viewers typically allow <br>the user to interactively move their<br>to view any part of the 3D model.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:837"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:541"><nobr><span class="ft2"><b>Figure 1. StyleCam authored elements </b></span></nobr></DIV>
<DIV style="position:absolute;top:118;left:81"><nobr><span class="ft5"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:130;left:81"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:986;left:87"><nobr><span class="ft7"><i>Permission to make digital or hard copies of all or part of this work for <br>personal or classroom use is granted without fee provided that copies are <br>not made or distributed for profit or commercial advantage and that <br>copies bear this notice and the full citation on the first page. To copy <br>otherwise, or republish, to post on servers or to redistribute to lists, <br>requires prior specific permission and/or a fee. <br>UIST'02, October 27-30, 2002, Paris, FRANCE. <br>Copyright 2002 ACM 1-58113-488-6/02/0010...$5.00. </i></span></nobr></DIV>
<DIV style="position:absolute;top:1099;left:87"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:644"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:813"><nobr><span class="ft4"><b>101</b></span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183002.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>This results in a host of problems: a user may "get lost" in <br>the scene, view the model from awkward angles that <br>present it in poor light, miss seeing important features, <br>experience frustration at controlling their navigation, etc. <br>As such, given that the author of the 3D model does not <br>have control over all aspects of what the user eventually <br>sees, they cannot ensure that 3D viewing conveys the <br>intended messages. In the worse case, the problems in 3D <br>viewing produce an experience completely opposite to the <br>authors intentions!  </b></span></nobr></DIV>
<DIV style="position:absolute;top:263;left:81"><nobr><span class="ft6"><b>The goal of our present research is to develop a system, <br>which we call StyleCam (Figure 1), where users viewing <br>3D models can be guaranteed a certain level of quality in <br>terms of their visual and interactive experience. Further, we <br>intend that the system should not only avoid the problems <br>suggested earlier, but also have the capability to make the <br>interactive experience adhere to particular visual styles. For <br>example, with StyleCam one should be able to produce an <br>interactive viewing experience for a 3D model of an <br>automobile "in the style of" the television commercial for <br>that same automobile. Ultimately, a high-level goal of our <br>research is to produce interactive 3D viewing experiences <br>where, to use an old saying from the film industry, "every <br>frame is a Rembrandt". </b></span></nobr></DIV>
<DIV style="position:absolute;top:513;left:81"><nobr><span class="ft4"><b>1.1.  Author vs. User Control </b></span></nobr></DIV>
<DIV style="position:absolute;top:534;left:81"><nobr><span class="ft6"><b>Central to our research is differentiating between the <br>concept of authoring an interactive 3D experience versus <br>authoring a 3D model which the user subsequently views <br>using general controls. If we look at the case of a typical <br>3D viewer on the web, in terms of interaction, the original <br>author of the 3D scene is limited to providing somewhat <br>standard camera controls such as pan, tumble and zoom. <br>Essentially, control of the viewpoint is left up to the user <br>and the author has limited influence on the overall <br>experience. </b></span></nobr></DIV>
<DIV style="position:absolute;top:716;left:81"><nobr><span class="ft6"><b>From an author's perspective this is a significant <br>imbalance. If we view an interactive experience by <br>cinematic standards, an author (or director) of a movie has <br>control over several major elements: content/art direction, <br>shading/lighting, viewpoint, and pacing. It is these elements <br>that determine the overall visual style of a movie. However, <br>in the interactive experience provided by current 3D <br>viewers, by placing control of the viewpoint completely in <br>the hands of the user, the author has surrendered control of <br>two major elements of visual style: viewpoint and pacing. </b></span></nobr></DIV>
<DIV style="position:absolute;top:897;left:81"><nobr><span class="ft6"><b>Thus we desire a method for creating 3D interactive <br>experiences where an author can not only determine the <br>content and shading but also the viewpoints and pacing. <br>However, intrinsic in any interactive system is some degree <br>of user control and therefore, more accurately, our desire is <br>to allow the author to have methods to significantly <br>influence the viewpoints and pacing in order to create <br>particular visual styles. Thus, we hope to strike a better <br>balance between author and user control. In order to <br>achieve this end, StyleCam incorporates an innovative </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>interaction technique that seamlessly integrates spatial <br>camera control with the temporal control of animation <br>playback. </b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:476"><nobr><span class="ft4"><b>2.  CONCEPTUAL MODEL </b></span></nobr></DIV>
<DIV style="position:absolute;top:164;left:476"><nobr><span class="ft6"><b>In order to provide author control or influence over <br>viewpoints and pacing, we need a way for an author to <br>express the viewpoints and the types of pacing they are <br>interested in. Thus we have developed three main elements <br>upon which our StyleCam approach is based. </b></span></nobr></DIV>
<DIV style="position:absolute;top:259;left:476"><nobr><span class="ft2"><b>1.  Camera surfaces ­ an author-created surface used to </b></span></nobr></DIV>
<DIV style="position:absolute;top:276;left:503"><nobr><span class="ft2"><b>constrain the users' movement of the viewpoint </b></span></nobr></DIV>
<DIV style="position:absolute;top:302;left:476"><nobr><span class="ft2"><b>2.  Animation clips ­ an author-created set of visual </b></span></nobr></DIV>
<DIV style="position:absolute;top:319;left:503"><nobr><span class="ft6"><b>sequences and effects whose playback may be <br>controlled by the user. These can include:  <br>·  sophisticated camera movements.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:372;left:503"><nobr><span class="ft2"><b>·  Slates ­ 2D media such as images, movies, </b></span></nobr></DIV>
<DIV style="position:absolute;top:391;left:524"><nobr><span class="ft2"><b>documents, or web pages. </b></span></nobr></DIV>
<DIV style="position:absolute;top:408;left:503"><nobr><span class="ft2"><b>·  visual effects such as fades, wipes, and edits.   </b></span></nobr></DIV>
<DIV style="position:absolute;top:426;left:503"><nobr><span class="ft2"><b>·  animation of elements in the scene. </b></span></nobr></DIV>
<DIV style="position:absolute;top:453;left:476"><nobr><span class="ft2"><b>3.  Unified UI technique ­ The user utilizes a single </b></span></nobr></DIV>
<DIV style="position:absolute;top:471;left:503"><nobr><span class="ft6"><b>method of interaction (dragging) to control the <br>viewpoint, animation clips, and the transitions between <br>camera surfaces. </b></span></nobr></DIV>
<DIV style="position:absolute;top:531;left:476"><nobr><span class="ft4"><b>2.1.  Camera Surfaces </b></span></nobr></DIV>
<DIV style="position:absolute;top:553;left:476"><nobr><span class="ft6"><b>In the motion picture industry a money-shot is a shot with a <br>particular viewpoint that a director has deemed "important" <br>in portraying a story or in setting the visual style of a <br>movie. Similarly, in advertising, money-shots are those <br>which are the most effective in conveying the intended <br>message. We borrow these concepts of a money-shot for <br>our StyleCam system. Our money-shots are viewpoints that <br>an author can use to broadly determine what a user will see. </b></span></nobr></DIV>
<DIV style="position:absolute;top:700;left:476"><nobr><span class="ft6"><b>Further, we use the concept of a camera surface as <br>introduced by Hanson and Wernert </b></span></nobr></DIV>
<DIV style="position:absolute;top:718;left:708"><nobr><span class="ft4"><b>[19, 36]</b></span></nobr></DIV>
<DIV style="position:absolute;top:717;left:754"><nobr><span class="ft2"><b>. When on a </b></span></nobr></DIV>
<DIV style="position:absolute;top:734;left:476"><nobr><span class="ft6"><b>camera surface, the virtual camera's spatial movement is <br>constrained to that surface. Further, each camera surface is <br>defined such that they incorporate a single money-shot. <br>Figure 2 illustrates this notion. </b></span></nobr></DIV>
<DIV style="position:absolute;top:812;left:476"><nobr><span class="ft6"><b>Camera surfaces can be used for various purposes. A small <br>camera surface can be thought of as an enhanced money-<br>shot where the user is allowed to move their viewpoint a bit <br>in order to get a sense of the 3-dimensionality of what they <br>are looking at.  Alternatively, the shape of the surface could <br>be used to provide some dramatic camera movements, for <br>example, sweeping across the front grill of a car. The key <br>idea is that camera surfaces allow authors to conceptualize, <br>visualize, and express particular ranges of viewpoints they <br>deem important. </b></span></nobr></DIV>
<DIV style="position:absolute;top:993;left:476"><nobr><span class="ft6"><b>Intrinsic in our authored interactions is the notion that <br>multiple camera surfaces can be used to capture multiple <br>money-shots. Thus authors have the ability to influence a <br>user's viewpoint broadly, by adding different camera <br>surfaces, or locally by adjusting the shape of a camera </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:81"><nobr><span class="ft4"><b>102</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:166"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft8{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="183003.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>surface to allow a user to navigate through a range of <br>viewpoints which are similar to a single particular money-<br>shot. For example, as shown in Figure 2, camera surfaces at <br>the front and rear of the car provide two authored <br>viewpoints of these parts of the car in which a user can <br>"move around a bit" to get a better sense of the shape of the <br>front grille and rear tail design. </b></span></nobr></DIV>
<DIV style="position:absolute;top:364;left:444"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:386;left:103"><nobr><span class="ft8"><b>Figure 2. Camera surfaces. The active camera is at the <br>money-shot viewpoint on the first camera surface. </b></span></nobr></DIV>
<DIV style="position:absolute;top:417;left:102"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:433;left:81"><nobr><span class="ft6"><b>The rate at which a user moves around on a camera surface <br>(Control-Display gain) can dramatically affect the style of <br>the experience. In order to allow an author some control <br>over visual pacing, we provide the author with the ability to <br>control the rate at which dragging the mouse changes the <br>camera position as it moves across a camera surface. The <br>intention is that increasing/decreasing this gain ratio results <br>in slower/faster camera movement and this will influence <br>how fast a user moves in the scene, which contributes to a <br>sense of pacing and visual style. For example, if small <br>mouse movements cause large changes in viewpoint this <br>may produce a feeling of fast action while large mouse <br>movement and slow changes in movement produce a slow, <br>flowing quality. Figure 3 illustrates an example of variable <br>control-display gain, where the gain increases as the camera <br>gets closer to the right edge of the camera surface. </b></span></nobr></DIV>
<DIV style="position:absolute;top:887;left:443"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:910;left:87"><nobr><span class="ft4"><b>Figure 3. Variable control-display gain on a camera surface </b></span></nobr></DIV>
<DIV style="position:absolute;top:925;left:81"><nobr><span class="ft8"><b> <br>2.2.  Animation Clips </b></span></nobr></DIV>
<DIV style="position:absolute;top:962;left:81"><nobr><span class="ft6"><b>To support transitions between two camera surfaces, we use <br>animation clips as illustrated in Figure 4. An animation clip <br>can be thought of as a "path" between the edges of camera <br>surfaces. When a user navigates to the edge of a camera <br>surface, this triggers an animation. When the animation <br>ends, they resume navigating at the destination camera </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>surface. One obvious type of animation between the camera <br>surfaces would simply be an automatic interpolation of the <br>camera moving from its start location on the first camera <br>surface to its end location on the second camera surface <br>(Figure 4a). This is similar to what systems such as VRML <br>do. While our system supports these automatic interpolated <br>animations, we also allow for authored, stylized, <br>animations. These authored animations can be any visual <br>sequence and pacing, and are therefore opportunities for <br>introducing visual style. For example, in transitioning from <br>one side of the car to the other, the author may create a <br>stylized camera animation which pans across the front of <br>the car, while closing in on a styling detail like a front grille <br>emblem (Figure 4b).  </b></span></nobr></DIV>
<DIV style="position:absolute;top:332;left:476"><nobr><span class="ft6"><b>The generality of using animation clips allows the author <br>the stylistic freedom of completely abandoning the camera-<br>movement metaphor for transitions between surfaces and <br>expressing other types of visual sequences. Thus animation <br>clips are effective mechanisms for introducing slates -- 2D <br>visuals which are not part of the 3D scene but are <br>momentarily placed in front of the viewing camera as it <br>moves from one camera surface to another (Figure 4c). For <br>example, moving from a view of the front of the car to the <br>back of the car may be accomplished using a 2D image <br>showing the name of the car. This mechanism allows the <br>use of visual elements commonly found in advertising such <br>as real action video clips and rich 2D imagery. In the <br>computer realm, slates may also contain elements such as <br>documents or webpages.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:788;left:837"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:811;left:497"><nobr><span class="ft8"><b>Figure 4. Three example animated transitions between <br>camera surfaces. (a) automatic transition, (b) authored <br>stylized transition, (c) slate transition. </b></span></nobr></DIV>
<DIV style="position:absolute;top:857;left:476"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:873;left:476"><nobr><span class="ft6"><b>The use of animation clips also allows for typical visual <br>transitions effects such as cross fades, wipes etc.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:916;left:476"><nobr><span class="ft6"><b>In addition to using animation clips for transitions between <br>camera surfaces, StyleCam also supports the animation of <br>elements in the 3D scene. These scene element animations <br>can occur separately or concurrently with transition <br>animations. For example, while the animation clip for the <br>visual transition may have the camera sweeping down the <br>side of the car, an auxiliary animation may open the trunk <br>to reveal cargo space.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:644"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:813"><nobr><span class="ft4"><b>103</b></span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft9{font-size:6px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="183004.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>The animation of scene elements can also be used to affect <br>extremely broad changes. For example, entire scene <br>transitions (similar to level changes in video games) may <br>occur when a user hits the edge of particular camera <br>surface. </b></span></nobr></DIV>
<DIV style="position:absolute;top:176;left:81"><nobr><span class="ft6"><b>At the author's discretion, temporal control of animation <br>clips can either be under user control or uninterruptable. </b></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:81"><nobr><span class="ft6"><b>Overall, in terms of visual expression, these varying types <br>of animation clips allow an author to provide rich visual <br>experiences and therefore significantly influence the pacing <br>and style of a user's interaction. </b></span></nobr></DIV>
<DIV style="position:absolute;top:298;left:81"><nobr><span class="ft4"><b>2.3.  Unified User Interaction Technique </b></span></nobr></DIV>
<DIV style="position:absolute;top:319;left:81"><nobr><span class="ft6"><b>While animation clips are effective for providing a means <br>to move between camera surfaces and introduce visual <br>styling elements, they also highlight the fundamental issue <br>of arbitrating between user control and system control. At <br>the heart of our system are two distinct types of behavior: <br>1) user control of the viewpoint, and 2) playback of <br>animation clips. In other systems these two types of <br>behavior are treated as distinct interactions. Specifically, <br>the user must stop dragging the camera viewpoint, then <br>click on something in the interface to trigger the animation, <br>dividing their attention and interrupting the visual flow. In <br>our system we wanted to use animations as a seamless way <br>of facilitating movement between camera surfaces. Thus we <br>needed a mechanism for engaging these animations that did <br>not require an explicit mouse click to trigger animation. <br>Ideally we wanted to leave the user with the impression that <br>they "dragged" from one camera surface to another even <br>though the transition between the surfaces was <br>implemented as an authored animation.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:656;left:81"><nobr><span class="ft6"><b>These two behaviors are fundamentally different in that <br>viewpoint control is spatial navigation and animation <br>control is temporal navigation. From a user interaction <br>standpoint, spatial behavior can be thought of as "dragging <br>the camera" while temporal control is "dragging a time <br>slider" or "scrubbing". Given this we required an <br>interaction model which allowed these two types of drags <br>to be combined together in a way that was well defined, <br>controllable, and corresponded to user's expectations.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:820;left:81"><nobr><span class="ft6"><b>Figure 5, which uses the finite-state-machine model to <br>describe interaction as introduced by </b></span></nobr></DIV>
<DIV style="position:absolute;top:839;left:326"><nobr><span class="ft4"><b>[5, 26]</b></span></nobr></DIV>
<DIV style="position:absolute;top:837;left:366"><nobr><span class="ft2"><b>, shows the </b></span></nobr></DIV>
<DIV style="position:absolute;top:854;left:81"><nobr><span class="ft6"><b>interaction model we developed. The key feature of this <br>model is the ability to transition back and forth from spatial <br>to temporal control during a contiguous drag. As a user <br>drags the camera across a camera surface (State 1, Spatial <br>Navigation) and hits the edge of the surface, a transition is <br>made to dragging an invisible time slider (State 2, <br>Temporal Navigation). As the user continues to drag, the <br>drag controls the location in the animation clip, assuming <br>that the author has specified the clip to be under user <br>control. Upon reaching the end of the animation, a <br>transition is made back to dragging the camera, however, <br>on a different, destination camera surface (State 1).  </b></span></nobr></DIV>
<DIV style="position:absolute;top:202;left:707"><nobr><span class="ft9">Button Up</span></nobr></DIV>
<DIV style="position:absolute;top:87;left:744"><nobr><span class="ft9">Clip Finished</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:536"><nobr><span class="ft9">Button Up</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:496"><nobr><span class="ft4"><b>State</b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:508"><nobr><span class="ft4"><b>0</b></span></nobr></DIV>
<DIV style="position:absolute;top:157;left:581"><nobr><span class="ft4"><b>State</b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:593"><nobr><span class="ft4"><b>1</b></span></nobr></DIV>
<DIV style="position:absolute;top:157;left:666"><nobr><span class="ft4"><b>State</b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:678"><nobr><span class="ft4"><b>2</b></span></nobr></DIV>
<DIV style="position:absolute;top:157;left:754"><nobr><span class="ft4"><b>State</b></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:767"><nobr><span class="ft4"><b>3</b></span></nobr></DIV>
<DIV style="position:absolute;top:203;left:541"><nobr><span class="ft9">Button</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:543"><nobr><span class="ft9">Down</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:630"><nobr><span class="ft9">Enter</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:625"><nobr><span class="ft9">Surface</span></nobr></DIV>
<DIV style="position:absolute;top:203;left:632"><nobr><span class="ft9">Exit</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:624"><nobr><span class="ft9">Surface</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:770"><nobr><span class="ft9">Button</span></nobr></DIV>
<DIV style="position:absolute;top:130;left:772"><nobr><span class="ft9">Down</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:494"><nobr><span class="ft9">Tracking</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:578"><nobr><span class="ft9">Dragging</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:579"><nobr><span class="ft9">in Space</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:666"><nobr><span class="ft9">Dragging</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:669"><nobr><span class="ft9">in Time</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:754"><nobr><span class="ft9">Tracking</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:759"><nobr><span class="ft9">during</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:752"><nobr><span class="ft9">Automatic</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:754"><nobr><span class="ft9">Playback</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:716"><nobr><span class="ft9">Stop</span></nobr></DIV>
<DIV style="position:absolute;top:121;left:707"><nobr><span class="ft9">Playback</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:523"><nobr><span class="ft9">Spatial Navigation</span></nobr></DIV>
<DIV style="position:absolute;top:284;left:688"><nobr><span class="ft9">Temporal Navigation</span></nobr></DIV>
<DIV style="position:absolute;top:289;left:833"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:312;left:539"><nobr><span class="ft4"><b>Figure 5. StyleCam interaction model. </b></span></nobr></DIV>
<DIV style="position:absolute;top:328;left:476"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:343;left:476"><nobr><span class="ft6"><b>The interaction model also handles a variety of reasonable <br>variations on this type of dragging behavior. A user may <br>stop moving when dragging an animation clip, thus pausing <br>the animation. If, however, when in State 2 the user <br>releases the mouse button during a drag, automatic <br>playback is invoked to carry the user to the next camera <br>surface (State 3). Should the user press the mouse button <br>during this automatic playback, playback is stopped and <br>temporal control by the user is resumed (return to State 2). <br>We found in practice that this interaction design enhanced <br>the user's feeling of being in control throughout the entire <br>experience.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:559;left:476"><nobr><span class="ft4"><b>3.  DESIGN RATIONALE </b></span></nobr></DIV>
<DIV style="position:absolute;top:581;left:476"><nobr><span class="ft6"><b>At first glance, it may appear that the incorporation of <br>animation clips into StyleCam unnecessarily complicates its <br>authoring and use. After all, without animated transitions, <br>we would not have had to develop an interaction technique <br>that blended between spatial and temporal control. Indeed, <br>when we first began our research, our hope was to create a <br>system that simply involved spatial control of a constrained <br>camera.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:727;left:476"><nobr><span class="ft6"><b>Our first variation used a single camera surface that <br>surrounded the 3D object of interest. The camera was <br>constrained to remain normal to this single camera surface <br>at all times. While this gave the author more control than <br>using a simple unconstrained camera, we found that it was <br>difficult to author a single camera surface that encompassed <br>all the desirable viewpoints and interesting transitions <br>between those viewpoints. In order to guarantee desirable <br>viewpoints, we introduced the concept of money-shots that <br>were placed on the single camera surface. The parameters <br>of the camera were then determined based on its location on <br>the camera surface and a weighted average of the <br>surrounding money-shots. At this point, it was still difficult <br>to author what the user would see when not directly on a <br>money-shot. In other words, while money-shots worked <br>well, the transitions between them worked poorly. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1012;left:476"><nobr><span class="ft6"><b>To address this problem of unsatisfactory transitions, we <br>first replaced the concept of a single global camera surface <br>with separate local camera surfaces for each money-shot. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:81"><nobr><span class="ft4"><b>104</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:166"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183005.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>Then, to define transitions between these local camera <br>surfaces, we introduced the idea of animating the camera. <br>This led to the use of the three types of animation clips as <br>described earlier. Simply playing back the animation clips <br>between camera surfaces gave users the sense that they lost <br>control during this period. To maintain the feeling of <br>continuous control throughout, we developed our integrated <br>spatial-temporal interaction technique. </b></span></nobr></DIV>
<DIV style="position:absolute;top:228;left:81"><nobr><span class="ft4"><b>4.  AN EXAMPLE EXPERIENCE </b></span></nobr></DIV>
<DIV style="position:absolute;top:250;left:81"><nobr><span class="ft6"><b>We illustrate how StyleCam operates by an example. <br>Figure 6 illustrates the system components and how they <br>react to user input, as well as screen shots of what the user <br>actually sees. The user starts by dragging on a camera <br>surface (position A). The path A-B shows the camera being <br>dragged on the surface (spatial navigation). At B, the user <br>reaches the edge of the camera surface and this launches an <br>animation that will transition the user from B to E. The zig-<br>zag path from B to D indicates that the user is scrubbing <br>time on the animation (temporal navigation). Position C <br>simply illustrates an intermediate point in the animation <br>that gets seen three times during the interaction. At position <br>D, the user releases the mouse button, whereupon the <br>system automatically completes playing back the remainder <br>of the animation at the authored pacing. At position E, the </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>user enters another camera surface and resumes spatial <br>navigation of the camera as shown by path E-F. When the <br>user exits this camera surface at position F, another <br>animation is launched that will transition the user to <br>position J. Since the user releases the mouse button at <br>position F, the animation from F to J is played back at the <br>authored pacing. Since this animation is a slate animation, <br>the intermediate shots at positions G, H, and I along the <br>path F to J are of slates containing information on the car <br>fading in and out as the camera pans over the top of the car. <br>The net result of this StyleCam experience is a view of the <br>car that is far more visually rich and influenced by an <br>author who intends to convey a certain message, rather than <br>using simple camera controls as is typical in current 3D <br>viewers. </b></span></nobr></DIV>
<DIV style="position:absolute;top:349;left:476"><nobr><span class="ft4"><b>5.  RELATED WORK </b></span></nobr></DIV>
<DIV style="position:absolute;top:370;left:476"><nobr><span class="ft6"><b>Much prior research has explored camera techniques for 3D <br>virtual environments. Many of the techniques use a 2D <br>mouse or stylus as an input device and introduce metaphors <br>to assist the user. Perhaps the most ubiquitous metaphor, <br>the cinematic camera, enables users to tumble, track and <br>dolly a viewpoint. Various other metaphors have been <br>explored by researchers, including orbiting and flying </b></span></nobr></DIV>
<DIV style="position:absolute;top:475;left:811"><nobr><span class="ft4"><b>[32]</b></span></nobr></DIV>
<DIV style="position:absolute;top:474;left:833"><nobr><span class="ft2"><b>, </b></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:476"><nobr><span class="ft2"><b>through-the-lens control </b></span></nobr></DIV>
<DIV style="position:absolute;top:492;left:629"><nobr><span class="ft4"><b>[18]</b></span></nobr></DIV>
<DIV style="position:absolute;top:491;left:652"><nobr><span class="ft2"><b>, points and areas of interests </b></span></nobr></DIV>
<DIV style="position:absolute;top:1046;left:801"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:114"><nobr><span class="ft4"><b>Figure 6. Example StyleCam experience. Top: system components and their reaction to user input. Bottom: what the user sees. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:644"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:813"><nobr><span class="ft4"><b>105</b></span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183006.png" alt="background image">
<DIV style="position:absolute;top:83;left:81"><nobr><span class="ft4"><b>[22]</b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:103"><nobr><span class="ft2"><b>, using constraints [24, 29], drawing a path </b></span></nobr></DIV>
<DIV style="position:absolute;top:83;left:382"><nobr><span class="ft4"><b>[21]</b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:405"><nobr><span class="ft2"><b>, two-</b></span></nobr></DIV>
<DIV style="position:absolute;top:99;left:81"><nobr><span class="ft2"><b>handed techniques </b></span></nobr></DIV>
<DIV style="position:absolute;top:100;left:199"><nobr><span class="ft4"><b>[1, 38]</b></span></nobr></DIV>
<DIV style="position:absolute;top:99;left:237"><nobr><span class="ft2"><b>, and combinations of techniques </b></span></nobr></DIV>
<DIV style="position:absolute;top:116;left:81"><nobr><span class="ft6"><b>[30, 37]. Bowman et. al. present taxonomies and <br>evaluations of various schemes </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:272"><nobr><span class="ft4"><b>[3, 4]</b></span></nobr></DIV>
<DIV style="position:absolute;top:133;left:301"><nobr><span class="ft2"><b>. </b></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:81"><nobr><span class="ft6"><b>Other techniques involve automatic framing of the areas of <br>interest as typically found in game console based adventure <br>games which use a "chase airplane" metaphor for a third <br>person perspective. Systems that utilize higher degree-of-<br>freedom input devices offer additional control and <br>alternative metaphors have been investigated, including <br>flying </b></span></nobr></DIV>
<DIV style="position:absolute;top:264;left:122"><nobr><span class="ft4"><b>[7, 34]</b></span></nobr></DIV>
<DIV style="position:absolute;top:263;left:158"><nobr><span class="ft2"><b>, eyeball-in-hand </b></span></nobr></DIV>
<DIV style="position:absolute;top:264;left:266"><nobr><span class="ft4"><b>[35]</b></span></nobr></DIV>
<DIV style="position:absolute;top:263;left:288"><nobr><span class="ft2"><b>, and worlds in miniature </b></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:81"><nobr><span class="ft4"><b>[31]</b></span></nobr></DIV>
<DIV style="position:absolute;top:280;left:103"><nobr><span class="ft2"><b>. The major difference between this body of prior </b></span></nobr></DIV>
<DIV style="position:absolute;top:297;left:81"><nobr><span class="ft6"><b>research and our work is that we attempt to give the author <br>substantially more influence over the types of views and <br>transitions between them as the user navigates in the virtual <br>space.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:375;left:81"><nobr><span class="ft6"><b>Beyond techniques for navigating the scene, extra <br>information can also be provided to aid navigation. These <br>include global maps in addition to local views </b></span></nobr></DIV>
<DIV style="position:absolute;top:411;left:368"><nobr><span class="ft4"><b>[12, 14]</b></span></nobr></DIV>
<DIV style="position:absolute;top:410;left:412"><nobr><span class="ft2"><b>, and </b></span></nobr></DIV>
<DIV style="position:absolute;top:427;left:81"><nobr><span class="ft2"><b>various landmarks </b></span></nobr></DIV>
<DIV style="position:absolute;top:428;left:214"><nobr><span class="ft4"><b>[9, 33]</b></span></nobr></DIV>
<DIV style="position:absolute;top:427;left:258"><nobr><span class="ft2"><b>. Others have investigated </b></span></nobr></DIV>
<DIV style="position:absolute;top:444;left:81"><nobr><span class="ft6"><b>integrating global and local views, using various distorted <br>spaces including "fisheye" views </b></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:290"><nobr><span class="ft4"><b>[6, 15]</b></span></nobr></DIV>
<DIV style="position:absolute;top:461;left:328"><nobr><span class="ft2"><b>. At present, in an </b></span></nobr></DIV>
<DIV style="position:absolute;top:478;left:81"><nobr><span class="ft6"><b>attempt to keep the visual space uncluttered, our work does <br>not have mechanisms for providing global information to <br>the user, however, this is something we may incorporate as <br>our system progresses. </b></span></nobr></DIV>
<DIV style="position:absolute;top:556;left:81"><nobr><span class="ft6"><b>Approaches which give the author more influence include <br>guided tours where camera paths are prespecified for the <br>end user to travel along. Galyean [17] proposes a "river <br>analogy" where a user, on a metaphorical boat, can deviate <br>from the guided path, the river, by steering a conceptual <br>"rudder". Fundamental work by Hanson and Wernert </b></span></nobr></DIV>
<DIV style="position:absolute;top:644;left:421"><nobr><span class="ft4"><b>[19, </b></span></nobr></DIV>
<DIV style="position:absolute;top:661;left:81"><nobr><span class="ft4"><b>36]</b></span></nobr></DIV>
<DIV style="position:absolute;top:660;left:99"><nobr><span class="ft2"><b> proposes "virtual sidewalks" which are authored by </b></span></nobr></DIV>
<DIV style="position:absolute;top:677;left:81"><nobr><span class="ft6"><b>constructing virtual surfaces and specifying gaze direction, <br>vistas, and procedural events (e.g., fog and spotlights) along <br>the sidewalk. Our system builds upon the guided tour and <br>virtual sidewalk ideas but differs by providing authoring <br>elements that enable a much more stylized experience. <br>Specifically, we offer a means of presenting 3D, 2D, and <br>temporal media experiences through a simple, unified, <br>singular user interaction technique that supports both <br>spatial and temporal navigation. </b></span></nobr></DIV>
<DIV style="position:absolute;top:841;left:81"><nobr><span class="ft6"><b>Robotic planning algorithms have been used to assist or <br>automatically create a guided tour of a 3D scene, in some <br>cases resulting in specific behaviors trying to satisfy goals <br>and constraints </b></span></nobr></DIV>
<DIV style="position:absolute;top:894;left:183"><nobr><span class="ft4"><b>[10, 11]</b></span></nobr></DIV>
<DIV style="position:absolute;top:893;left:229"><nobr><span class="ft2"><b>. Individual camera framing of a </b></span></nobr></DIV>
<DIV style="position:absolute;top:910;left:81"><nobr><span class="ft6"><b>scene has been used to assist in viewing or manipulation <br>tasks </b></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:125"><nobr><span class="ft4"><b>[27]</b></span></nobr></DIV>
<DIV style="position:absolute;top:927;left:147"><nobr><span class="ft2"><b>. Rules can be defined for cameras to </b></span></nobr></DIV>
<DIV style="position:absolute;top:945;left:81"><nobr><span class="ft6"><b>automatically frame a scene that follow cinematic <br>principles such as keeping the virtual actors visible in the <br>scene; or following the lead actor </b></span></nobr></DIV>
<DIV style="position:absolute;top:980;left:293"><nobr><span class="ft4"><b>[20]</b></span></nobr></DIV>
<DIV style="position:absolute;top:979;left:315"><nobr><span class="ft2"><b>. Yet another system </b></span></nobr></DIV>
<DIV style="position:absolute;top:998;left:81"><nobr><span class="ft4"><b>[2]</b></span></nobr></DIV>
<DIV style="position:absolute;top:996;left:97"><nobr><span class="ft2"><b> allows authors to define storyboard frames and the </b></span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:81"><nobr><span class="ft6"><b>system defines a set of virtual cameras in the 3D scene to <br>support the visual composition. This previous work assists <br>in the authoring aspects by ceding some control to the </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>system. Our work too involves some automatic system <br>control, but we emphasize author control. </b></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:476"><nobr><span class="ft6"><b>Image based virtual reality environments such as <br>QuicktimeVR </b></span></nobr></DIV>
<DIV style="position:absolute;top:143;left:565"><nobr><span class="ft4"><b>[8]</b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:581"><nobr><span class="ft2"><b> utilize camera panning and zooming and </b></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:476"><nobr><span class="ft6"><b>allow users to move to defined vista points. The driving <br>metaphor has also been used for navigating interactive <br>video, as seen in the Movie-Maps system </b></span></nobr></DIV>
<DIV style="position:absolute;top:195;left:768"><nobr><span class="ft4"><b>[23]</b></span></nobr></DIV>
<DIV style="position:absolute;top:194;left:791"><nobr><span class="ft2"><b>. More </b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:476"><nobr><span class="ft2"><b>recently, the Steerable Media project </b></span></nobr></DIV>
<DIV style="position:absolute;top:212;left:719"><nobr><span class="ft4"><b>[25]</b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:741"><nobr><span class="ft2"><b> for interactive </b></span></nobr></DIV>
<DIV style="position:absolute;top:228;left:476"><nobr><span class="ft6"><b>television aims to retain the visual aesthetic of existing <br>television but increase the level of user interactivity. The <br>user is given the ability to control the content progression <br>by seamlessly integrating video with augmented 2D and 3D <br>graphics. While our goals are similar in that we hope to <br>enhance the aesthetics of the visual experience, we differ in <br>that our dominant media type is 3D graphics with <br>augmented temporal media (animations and visual effects) <br>and traditional 2D media (video, still images). </b></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:476"><nobr><span class="ft6"><b>Lastly, we note that widely available 3D viewers or <br>viewing technologies such as VRML, Cult3D, Shockwave, <br>Viewpoint, Virtools, and Pulse3D, are becoming very <br>popular but offer the standard camera controls of vista <br>points, track, tumble, and zoom. We hope our explorations <br>will ultimately assist in offering new experience and <br>interaction approaches for future incarnations of these 3D <br>viewers.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:539;left:476"><nobr><span class="ft4"><b>6.  IMPLEMENTATION </b></span></nobr></DIV>
<DIV style="position:absolute;top:561;left:476"><nobr><span class="ft6"><b>StyleCam is implemented using Alias|wavefront's MAYA <br>3D modeling and animation package. We use MAYA to <br>author the 3D content to be visualized, the required camera <br>surfaces, animation clips, and required associations <br>between them. A custom written MAYA plugin allows the <br>user to control their view of the 3D content based on their <br>mouse input and the authored camera surfaces, animation <br>clips, and associations. </b></span></nobr></DIV>
<DIV style="position:absolute;top:708;left:476"><nobr><span class="ft6"><b>The following description of our implementation assumes <br>some knowledge of MAYA, although we have endeavoured <br>to be as general as possible without sacrificing accuracy. </b></span></nobr></DIV>
<DIV style="position:absolute;top:768;left:476"><nobr><span class="ft4"><b>6.1.  Authoring </b></span></nobr></DIV>
<DIV style="position:absolute;top:790;left:476"><nobr><span class="ft6"><b>First, money-shots are created by defining a MAYA camera <br>with specific position, orientation, and other camera <br>parameters. Then, a camera surface which intersects the <br>position of the money-shot camera is defined by creating an <br>appropriate non-trimmed NURBS surface within MAYA. <br>To include an optional camera look-at point, the author <br>simply defines a point in 3D space (using a MAYA <br>locator). Finally, to make these components easily locatable <br>by the plugin, they are grouped under a named MAYA <br>node within its dependency graph. </b></span></nobr></DIV>
<DIV style="position:absolute;top:971;left:476"><nobr><span class="ft6"><b>Then, StyleCam animation clips are created as one would <br>normally create animations in MAYA, using its TRAX <br>non-linear animation editor. Animation clips at this stage <br>are given meaningful, consistent, names in order to <br>facilitate their identification later when associating them <br>with events. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:81"><nobr><span class="ft4"><b>106</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:166"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183007.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>StyleCam allows the author to create scripts and associate <br>them with events. Supported events are session startup, <br>camera surface entry, camera surface exit, and camera <br>surface timeout (Figure 7).  </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>We implement variable control-display gain on a camera <br>surface (Figure 3) by varying the separation between the <br>isoparms on the NURBS surface.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:476"><nobr><span class="ft6"><b>As shown in Figure 4, StyleCam supports three types of <br>transitions: automatic, authored, and slate. </b></span></nobr></DIV>
<DIV style="position:absolute;top:354;left:417"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:185;left:476"><nobr><span class="ft6"><b>Automatic transitions are those that smoothly move the <br>camera from one camera surface to another without <br>requiring any authored animation clips. This is done by <br>having the system perform quaternion </b></span></nobr></DIV>
<DIV style="position:absolute;top:239;left:715"><nobr><span class="ft4"><b>[28]</b></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:737"><nobr><span class="ft2"><b> interpolation of </b></span></nobr></DIV>
<DIV style="position:absolute;top:254;left:476"><nobr><span class="ft6"><b>camera orientation, combined quaternion and linear <br>interpolation of camera position, and linear interpolation of <br>other camera properties such as focal length. Using <br>quaternion interpolation ensures smooth changes in <br>orientation while defining a smooth arcing path for the <br>position. At each time step in the transition, two <br>quaternions representing the required fractional rotations of <br>the position and orientation vectors of the camera are <br>calculated and applied to the source vectors. In addition, the <br>magnitude of the position vector is adjusted by linear <br>interpolation between the source and destination position <br>vector magnitudes. The result is a series of intermediate <br>camera positions and orientations as Figure 8 illustrates. </b></span></nobr></DIV>
<DIV style="position:absolute;top:719;left:836"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:376;left:186"><nobr><span class="ft4"><b>Figure 7. StyleCam events </b></span></nobr></DIV>
<DIV style="position:absolute;top:392;left:258"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:408;left:81"><nobr><span class="ft6"><b>The session startup event is triggered only once when the <br>user initially begins using StyleCam to view a scene. Exit <br>events are triggered when the user leaves a camera surface <br>from one of four directions. Associated scripts can specify <br>destination camera surfaces and types of transitions to be <br>performed. Time-out events are triggered when the mouse <br>is idle for a given duration while on a particular camera <br>surface, and can be used to launch an automatic <br>presentation. StyleCam's event and script mechanism <br>provides for the use of logic to dynamically alter the <br>presentation. For example, scripts can ensure that some <br>surfaces are only visited once, while others are shown only <br>after certain surfaces have already been visited. </b></span></nobr></DIV>
<DIV style="position:absolute;top:640;left:81"><nobr><span class="ft4"><b>6.2.  Interaction </b></span></nobr></DIV>
<DIV style="position:absolute;top:662;left:81"><nobr><span class="ft6"><b>When the StyleCam plugin is activated, the first money-<br>shot of the first camera surface is used as the initial view. If <br>a look-at point is defined for this camera surface, the <br>orientation of the user camera is set such that the camera <br>points directly at the look-at point. Otherwise, the <br>orientation is set to the normal of the camera surface at the <br>money-shot viewpoint's position.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:742;left:503"><nobr><span class="ft4"><b>Figure 8. Combined quaternion and linear interpolation </b></span></nobr></DIV>
<DIV style="position:absolute;top:757;left:653"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:773;left:476"><nobr><span class="ft6"><b>Authored transitions involve the playback of preauthored <br>animation clips. This gives the author complete control <br>over the user experience during the transition including the <br>pacing, framing and visual effects. </b></span></nobr></DIV>
<DIV style="position:absolute;top:792;left:81"><nobr><span class="ft6"><b>User's mouse movements and button presses are monitored <br>by the StyleCam plugin.  Mouse drags result in the camera <br>moving along the current camera surface. Specifically, for a <br>given mouse displacement (dx, dy), the new position of the <br>camera on the camera surface (in uv-coordinates local to <br>the camera surface) is given by </b></span></nobr></DIV>
<DIV style="position:absolute;top:851;left:476"><nobr><span class="ft6"><b>Slate transitions are a special case of authored transitions. <br>Used to present 2D media, slate transitions are authored by <br>placing an image plane in front of the camera as it <br>transitions between camera surfaces. Various visual effects <br>can be achieved by using multiple image planes <br>simultaneously and by animating transparency and other <br>parameters of these image planes. While the slate transition <br>is in progress, the camera is simultaneously being smoothly <br>interpolated towards the destination camera surface. This <br>essentially allows for a "soft" fade from a camera view, to a <br>slate, and back, as Figure 9 illustrates. </b></span></nobr></DIV>
<DIV style="position:absolute;top:904;left:172"><nobr><span class="ft2"><b>(u1,v1) = (u0,v0) + c*(dx, dy) </b></span></nobr></DIV>
<DIV style="position:absolute;top:930;left:81"><nobr><span class="ft6"><b>where (u0, v0) is the last position of the camera, and c is the <br>gain constant. If either the u or v coordinate of the resulting <br>position is not within the range [0,1], the camera has left <br>the current camera surface. At this point, the author-<br>scripted logic is executed to determine the next step. First, <br>the destination money-shot is resolved. Next, an <br>appropriate transition is performed to move to the next <br>camera surface.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:644"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:813"><nobr><span class="ft4"><b>107</b></span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183008.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:258;left:442"><nobr><span class="ft2"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:188"><nobr><span class="ft4"><b>Figure 9. Slate transitions </b></span></nobr></DIV>
<DIV style="position:absolute;top:296;left:258"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:312;left:81"><nobr><span class="ft6"><b>StyleCam supports temporal control or "scrubbing" of <br>animations. During navigation mode, the user's mouse <br>drags control the camera's position on the camera surface. <br>However, when the user moves off a camera surface into an <br>animated transition, mouse drags control the (invisible) <br>timeslider of the animation. Time is advanced when the <br>mouse is dragged in the same direction that the camera <br>exited the camera surface and reversed if the directions are <br>also reversed. When the mouse button is released, the <br>system takes over time management and smoothly ramps <br>the time steps towards the animation's original playback <br>rate.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:528;left:258"><nobr><span class="ft4"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:543;left:81"><nobr><span class="ft6"><b>Our present implementation supports scrubbing only for <br>automatic transitions. Authored and slate transitions are <br>currently uninterruptible. There is however no technical <br>reason why all  transitions cannot support scrubbing. In <br>future versions we intend to give the author the choice of <br>determining whether or not any given transition is <br>scrubable. This is important since in some cases it may be <br>desirable to force the animation to playback uninterrupted <br>at a certain rate. </b></span></nobr></DIV>
<DIV style="position:absolute;top:707;left:81"><nobr><span class="ft4"><b>7.  EVALUATION </b></span></nobr></DIV>
<DIV style="position:absolute;top:729;left:81"><nobr><span class="ft6"><b>We conducted an informal user study to get a sense of <br>users' initial reactions to using StyleCam. Seven <br>participants, three of whom had experience with 3D <br>graphics applications and camera control techniques, and <br>four who had never used a 3D application or camera <br>controls, were asked to explore a 3D car model using <br>StyleCam. In order to ensure the study resembled our <br>intended casual usage scenario, we gave participants only <br>minimal instructions. We explained the click-and-drag <br>action required to manipulate the camera, a brief rationale <br>for the study, and to imagine they were experiencing an <br>interactive advertisement for that car. We did not identify <br>the various components (camera surfaces, animated <br>transitions, etc) nor give any details on them. This was <br>deliberately done so that the participants could experience <br>these components in action for themselves and give us <br>feedback without knowing in advance of their existence. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:81"><nobr><span class="ft6"><b>One very promising result was that none of the participants <br>realized that they were switching between controlling the </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>camera and controlling the time slider on the animations. <br>They felt that they had the same type of control throughout, <br>indicating that our blending between spatial and temporal <br>control worked remarkably well. Also the simplicity of the <br>interaction technique ­ essentially a single click and drag <br>action ­ was immediately understood and usable by all our <br>users. </b></span></nobr></DIV>
<DIV style="position:absolute;top:211;left:476"><nobr><span class="ft6"><b>Another reaction from all the participants was that, to <br>varying degrees, they sometimes felt that they were not in <br>control of the interaction when the uninterruptable <br>animations occurred. This was particularly acute when the <br>information in the animations seemed unrelated to their <br>current view. In these cases, participants indicated that they <br>had no idea what triggered these animations and were often <br>annoyed at the sudden interruptions. However when the <br>information was relevant the interruptions were not as <br>annoying and often actually appreciated. In some cases <br>participants indicated that they would have liked to be able <br>to replay the animation or to have it last longer. This <br>highlights the importance of carefully authoring the <br>intermingling of uninterruptable animations with the rest of <br>the interaction experience.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:478;left:476"><nobr><span class="ft6"><b>Participants also indicated that they would have liked the <br>ability to click on individual parts of the car model in order <br>to inspect them more closely. This request is not surprising <br>since we made no effort in our current implement to <br>support pointing. However, we believe that in future <br>research StyleCam could be extended to include pointing.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:591;left:476"><nobr><span class="ft6"><b>As we expected, all the participants with prior 3D graphics <br>camera experience stated that they at times would have <br>liked full control of the camera, in addition to the <br>constrained control we provided. Participants without this <br>prior experience, however, did not ask for this directly <br>although they indicated that there were some areas of the <br>car model that they would have liked to see but could not <br>get to. However, this does not necessarily imply full control <br>of the camera is required. We believe that this issue can be <br>largely alleviated at the authoring phase by ascertaining <br>what users want to see for a particular model and ensuring <br>that those features are accessible via the authored camera <br>surfaces. Interestingly, the participant with the most 3D <br>graphics experience commented that  the automatic <br>transitions and smooth camera paths during those <br>transitions were very good and that "for those who don't <br>know 3D and stuff, this would be very good"! </b></span></nobr></DIV>
<DIV style="position:absolute;top:893;left:476"><nobr><span class="ft4"><b>8.  DISCUSSION &amp; CONCLUSIONS </b></span></nobr></DIV>
<DIV style="position:absolute;top:914;left:476"><nobr><span class="ft6"><b>Central to our StyleCam system is the integration of spatial <br>and temporal controls into a single user interaction model. <br>The implications of this interaction model go far beyond a <br>simple interaction technique. The blending of spatial and <br>temporal control presents a completely new issue that an <br>author needs to understand and consider when creating <br>these interactive visual experiences. As evident from the <br>comments of our users, temporal control can feel very <br>much like spatial control even when scrubbing backwards </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:81"><nobr><span class="ft4"><b>108</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:166"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183009.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft6"><b>in an animation when the animation consists of moving the <br>viewing camera around the central object of interest. <br>However, if the animation is not around the central object <br>of interest, for example in some of our slate animations, <br>temporal control can produce very different sensations. <br>These include the feeling of moving backwards in time, <br>interruption of a well paced animation, jarring or ugly <br>visuals, and sometimes even nonsensical content.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:228;left:81"><nobr><span class="ft6"><b>As a result, the author needs to be extremely cognizant of <br>these artefacts and make design decisions as to when and <br>where to relinquish control - and how much control - to the <br>user. At one extreme, the author can specify that certain <br>animations are completely uninterruptible by the user. In <br>the experience we authored for our user study, we included <br>several of these types of transitions. As discussed earlier, <br>whether users favored this depended heavily on the content. <br>In other words, in some cases, as authors, we did not make <br>the right decision. Further improvements could include <br>partially interruptable animations. For example, we may not <br>allow movement backwards in time but allow the user to <br>control the forward pacing. This will largely solve the <br>nonsensical content problem but may still result in <br>occasionally jarring visuals.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:496;left:81"><nobr><span class="ft6"><b>If we intend to support these various types of control, we <br>must also be able to set the users' expectations of what type <br>of control they have at any given time. It is clear that the <br>current StyleCam switching between spatial and temporal <br>control without any explicit indication to the user that a <br>switch is happening works in most cases. In the cases <br>where it fails, either the visual content itself should indicate <br>what control is possible, or some explicit mechanism is <br>required to inform the user of the current or upcoming <br>control possibilities. In addition to the obvious solution of <br>using on-screen visual indicators (e.g., changing cursors) to <br>indicate state, future research could include exploring "hint-<br>ahead" mechanisms that indicate upcoming content if the <br>user chooses to stay on their current course of travel. For <br>example, as the user reaches the edge of a camera surface, a <br>"voice-over" could say something like "now we're heading <br>towards the engine of the car". Alternatively, a visual <br>"signpost" could fade-in near the cursor location to convey <br>this information. These ideas coincide with research that <br>states that navigation routes must be discoverable by the <br>user </b></span></nobr></DIV>
<DIV style="position:absolute;top:842;left:110"><nobr><span class="ft4"><b>[16]</b></span></nobr></DIV>
<DIV style="position:absolute;top:840;left:132"><nobr><span class="ft2"><b>. </b></span></nobr></DIV>
<DIV style="position:absolute;top:867;left:81"><nobr><span class="ft6"><b>It is very clear from our experiences with StyleCam that the <br>user's viewing experience is highly dependent on the talent <br>and skill of the author. It is likely that skills from movie <br>making, game authoring, advertising, and theme park <br>design would all assist in authoring compelling <br>experiences. However, we also realize that authoring skills <br>from these other genres do not necessarily directly translate <br>due to the unique interaction aspects of StyleCam. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:81"><nobr><span class="ft6"><b>While StyleCam has the appropriate components for <br>creating compelling visual experiences, it is still currently a <br>research prototype that requires substantial skills with </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:476"><nobr><span class="ft6"><b>MAYA. We envision a more author-friendly tool that is <br>based on the conceptual model of StyleCam.  </b></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:476"><nobr><span class="ft6"><b>Some future avenues that we intend to explore include <br>supporting soundtracks, extensions to enable pointing to <br>elements in the 3D scene, and mechanisms for authoring <br>animation paths using alternate techniques such as <br>Chameleon </b></span></nobr></DIV>
<DIV style="position:absolute;top:195;left:548"><nobr><span class="ft4"><b>[13]</b></span></nobr></DIV>
<DIV style="position:absolute;top:194;left:570"><nobr><span class="ft2"><b>. </b></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:476"><nobr><span class="ft6"><b>Finally, it is important to note that StyleCam is not limited <br>to product or automobile visualization. Other domains such <br>as visualization of building interiors and medical <br>applications could also utilize the ideas presented in this <br>paper. Figures 10, 11, and 12 illustrate some examples. </b></span></nobr></DIV>
<DIV style="position:absolute;top:315;left:476"><nobr><span class="ft4"><b>ACKNOWLEDGEMENTS </b></span></nobr></DIV>
<DIV style="position:absolute;top:337;left:476"><nobr><span class="ft6"><b>We thank Scott Guy and Miles Menegon for assistance in <br>figure and video creation. </b></span></nobr></DIV>
<DIV style="position:absolute;top:380;left:476"><nobr><span class="ft4"><b>REFERENCES </b></span></nobr></DIV>
<DIV style="position:absolute;top:402;left:476"><nobr><span class="ft2"><b>1.  Balakrishnan, R., &amp; Kurtenbach, G. (1999). Exploring </b></span></nobr></DIV>
<DIV style="position:absolute;top:419;left:497"><nobr><span class="ft6"><b>bimanual camera control and object manipulation in 3D <br>graphics interfaces. ACM CHI 1999 Conference on <br>Human Factors in Computing Systems. p. 56-63. </b></span></nobr></DIV>
<DIV style="position:absolute;top:480;left:476"><nobr><span class="ft2"><b>2.  Bares, W., McDermott, S., Boudreaux, C., &amp; Thainimit, </b></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:497"><nobr><span class="ft6"><b>S. (2000). Virtual 3D camera composition from frame <br>constraints. ACM Multimedia. p. 177-186. </b></span></nobr></DIV>
<DIV style="position:absolute;top:540;left:476"><nobr><span class="ft2"><b>3.  Bowman, D.A., Johnson, D.B., &amp; Hodges, L.F. (1997). </b></span></nobr></DIV>
<DIV style="position:absolute;top:557;left:497"><nobr><span class="ft6"><b>Travel in immersive virtual environments. IEEE <br>VRAIS'97 Virtual Reality Annual International <br>Symposium. p. 45-52. </b></span></nobr></DIV>
<DIV style="position:absolute;top:618;left:476"><nobr><span class="ft2"><b>4.  Bowman, D.A., Johnson, D.B., &amp; Hodges, L.F. (1999). </b></span></nobr></DIV>
<DIV style="position:absolute;top:635;left:497"><nobr><span class="ft6"><b>Testbed environment of virtual environment interaction. <br>ACM VRST'99 Symposium on Virtual Reality Software <br>and Technologies. p. 26-33. </b></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:476"><nobr><span class="ft2"><b>5.  Buxton, W., ed. Three-state model of graphical input. </b></span></nobr></DIV>
<DIV style="position:absolute;top:713;left:497"><nobr><span class="ft6"><b>Human-computer interaction - INTERACT'90, ed. D. <br>Diaper. 1990, Elsevier Science Publishers B. V. (North-<br>Holland): Amsterdam. 449-456. </b></span></nobr></DIV>
<DIV style="position:absolute;top:774;left:476"><nobr><span class="ft2"><b>6.  Carpendale, M.S.T., &amp; Montagnese, C.A. (2001). A </b></span></nobr></DIV>
<DIV style="position:absolute;top:791;left:497"><nobr><span class="ft6"><b>framework for unifying presentation space. ACM <br>UIST'2001 Symposium on User Interface Software and <br>Technology. p. 61-70. </b></span></nobr></DIV>
<DIV style="position:absolute;top:852;left:476"><nobr><span class="ft2"><b>7.  Chapman, D., &amp; Ware, C. (1992). Manipulating the </b></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:497"><nobr><span class="ft6"><b>future: predictor based feedback for velocity control in <br>virtual environment navigation. ACM I3D'92 <br>Symposium on Interactive 3D Graphics. p. 63-66. </b></span></nobr></DIV>
<DIV style="position:absolute;top:930;left:476"><nobr><span class="ft2"><b>8.  Chen, S.E. (1995). QuickTime VR: An image-based </b></span></nobr></DIV>
<DIV style="position:absolute;top:947;left:497"><nobr><span class="ft6"><b>approach to virtual environment navigation. ACM <br>SIGGRAPH'95 Conference on Computer Graphics and <br>Interactive Techniques. p. 29-38. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:476"><nobr><span class="ft2"><b>9.  Darken, R., &amp; Sibert, J. (1996). Wayfinding strategies </b></span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:497"><nobr><span class="ft6"><b>and behaviours in large virtual worlds. ACM CHI'96 <br>Conference on Human Factors in Computing Systems. <br>p. 142-149. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:644"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:813"><nobr><span class="ft4"><b>109</b></span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="183010.png" alt="background image">
<DIV style="position:absolute;top:81;left:81"><nobr><span class="ft2"><b>10. Drucker, S.M., Galyean, T.A., &amp; Zeltzer, D. (1992). </b></span></nobr></DIV>
<DIV style="position:absolute;top:99;left:103"><nobr><span class="ft6"><b>CINEMA: A system for procedural camera movements. <br>ACM  Symposium on Interactive 3D Graphics. p. 67-70. </b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:81"><nobr><span class="ft2"><b>11. Drucker, S.M., &amp; Zeltzer, D. (1994). Intelligent camera </b></span></nobr></DIV>
<DIV style="position:absolute;top:159;left:103"><nobr><span class="ft6"><b>control in a virtual environment. Graphics Interface. p. <br>190-199. </b></span></nobr></DIV>
<DIV style="position:absolute;top:203;left:81"><nobr><span class="ft2"><b>12. Elvins, T., Nadeau, D., Schul, R., &amp; Kirsh, D. (1998). </b></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:103"><nobr><span class="ft6"><b>Worldlets: 3D thumbnails for 3D browsing. ACM <br>CHI'98 Conf. on Human Factors in Computing Systems. <br>p. 163-170. </b></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:81"><nobr><span class="ft2"><b>13. Fitzmaurice, G.W. (1993). Situated information spaces </b></span></nobr></DIV>
<DIV style="position:absolute;top:298;left:103"><nobr><span class="ft6"><b>and spatially aware palmtop computers. <br>Communications of the ACM, 36(7). p. 38-49. </b></span></nobr></DIV>
<DIV style="position:absolute;top:341;left:81"><nobr><span class="ft2"><b>14. Fukatsu, S., Kitamura, Y., Masaki, T., &amp; Kishino, F. </b></span></nobr></DIV>
<DIV style="position:absolute;top:359;left:103"><nobr><span class="ft6"><b>(1998). Intuitive control of bird's eye overview images <br>for navigation in an enormous virtual environment. <br>ACM VRST'98 Sympoisum on Virtual Reality Software <br>and Technology. p. 67-76. </b></span></nobr></DIV>
<DIV style="position:absolute;top:437;left:81"><nobr><span class="ft2"><b>15. Furnas, G. (1986). Generalized fisheye views. ACM </b></span></nobr></DIV>
<DIV style="position:absolute;top:454;left:103"><nobr><span class="ft6"><b>CHI 1986 Conference on Human Factors in Computing <br>Systems. p. 16-23. </b></span></nobr></DIV>
<DIV style="position:absolute;top:497;left:81"><nobr><span class="ft2"><b>16. Furnas, G. (1997). Effective view navigation. ACM </b></span></nobr></DIV>
<DIV style="position:absolute;top:514;left:103"><nobr><span class="ft6"><b>CHI'97 Conference on Human Factors in Computing <br>Systems. p. 367-374. </b></span></nobr></DIV>
<DIV style="position:absolute;top:558;left:81"><nobr><span class="ft2"><b>17. Galyean, T.A. (1995). Guided navigation of virtual </b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:103"><nobr><span class="ft6"><b>environments.  ACM I3D'95 Symposium on Interactive <br>3D Graphics. p. 103-104. </b></span></nobr></DIV>
<DIV style="position:absolute;top:619;left:81"><nobr><span class="ft2"><b>18. Gliecher, M., &amp; Witkin, A. (1992). Through-the-lens </b></span></nobr></DIV>
<DIV style="position:absolute;top:636;left:103"><nobr><span class="ft6"><b>camera control. ACM SIGGRAPH' Conf. on Computer <br>Graphics and Interactive Techniques. p. 331-340. </b></span></nobr></DIV>
<DIV style="position:absolute;top:679;left:81"><nobr><span class="ft2"><b>19. Hanson, A.J., &amp; Wernet, E. (1997). Constrained 3D </b></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:103"><nobr><span class="ft2"><b>navigation with 2D controllers. p. 175-182. </b></span></nobr></DIV>
<DIV style="position:absolute;top:723;left:81"><nobr><span class="ft2"><b>20. He, L., Cohen, M.F., &amp; Salesin, D. (1996). The virtual </b></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:103"><nobr><span class="ft6"><b>cinematographer: a paradigm for automatic real-time <br>camera control and directing. ACM SIGGRAPH'96 <br>Conference on Computer Graphics and Interactive <br>Techniques. p. 217-224. </b></span></nobr></DIV>
<DIV style="position:absolute;top:818;left:81"><nobr><span class="ft2"><b>21. Igarashi, T., Kadobayashi, R., Mase, K., &amp; Tanaka, H. </b></span></nobr></DIV>
<DIV style="position:absolute;top:835;left:103"><nobr><span class="ft6"><b>(1998). Path drawing for 3D walkthrough. ACM UIST <br>1998 Symposium on User Interface Software and <br>Technology. p. 173-174. </b></span></nobr></DIV>
<DIV style="position:absolute;top:896;left:81"><nobr><span class="ft2"><b>22. Jul, S., &amp; Furnas, G. (1998). Critical zones in desert </b></span></nobr></DIV>
<DIV style="position:absolute;top:913;left:103"><nobr><span class="ft6"><b>fog: aids to multiscale navigation. ACM Symposium on <br>User Interface Software and Technology. p. 97-106. </b></span></nobr></DIV>
<DIV style="position:absolute;top:957;left:81"><nobr><span class="ft2"><b>23. Lippman, A. (1980). Movie-maps: an application of the </b></span></nobr></DIV>
<DIV style="position:absolute;top:974;left:103"><nobr><span class="ft6"><b>optical videodisc to computer graphics. ACM <br>SIGGRAPH'80 Conference on Computer Graphics and <br>Interactive Techniques. p. 32-42. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1034;left:81"><nobr><span class="ft2"><b>24. Mackinlay, J., Card, S., &amp; Robertson, G. (1990). Rapid </b></span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:103"><nobr><span class="ft2"><b>controlled movement through a virtual 3D workspace. </b></span></nobr></DIV>
<DIV style="position:absolute;top:81;left:497"><nobr><span class="ft6"><b>ACM SIGGRAPH 1990 Conference on Computer <br>Graphics and Interactive Techniques. p. 171-176. </b></span></nobr></DIV>
<DIV style="position:absolute;top:125;left:476"><nobr><span class="ft2"><b>25. Marrin, C., Myers, R., Kent, J., &amp; Broadwell, P. (2001). </b></span></nobr></DIV>
<DIV style="position:absolute;top:142;left:497"><nobr><span class="ft6"><b>Steerable media: interactive television via video <br>synthesis. ACM Conference on 3D Technologies for the <br>World Wide Web. p. 7-14. </b></span></nobr></DIV>
<DIV style="position:absolute;top:203;left:476"><nobr><span class="ft2"><b>26. Newman, W. (1968). A system for interactive graphical </b></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:497"><nobr><span class="ft2"><b>programming. </b></span></nobr></DIV>
<DIV style="position:absolute;top:220;left:604"><nobr><span class="ft2"><b>AFIPS Spring Joint Computer </b></span></nobr></DIV>
<DIV style="position:absolute;top:237;left:497"><nobr><span class="ft2"><b>Conference. p. 47-54. </b></span></nobr></DIV>
<DIV style="position:absolute;top:263;left:476"><nobr><span class="ft2"><b>27. Phillips, C.B., Badler, N.I., &amp; Granieri, J. (1992). </b></span></nobr></DIV>
<DIV style="position:absolute;top:281;left:497"><nobr><span class="ft6"><b>Automatic viewing control for 3D direct manipulation. <br>ACM Symposium on Interactive 3D Graphics. p. 71-74. </b></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:476"><nobr><span class="ft2"><b>28. </b></span></nobr></DIV>
<DIV style="position:absolute;top:324;left:497"><nobr><span class="ft6"><b>Shoemake, K. (1985). Animating rotation with <br>quartenion curves. ACM SIGGRAPH Conf Computer <br>Graphics &amp; Interactive Techniques. p. 245-254. </b></span></nobr></DIV>
<DIV style="position:absolute;top:385;left:476"><nobr><span class="ft2"><b>29. Smith, G., Salzman, T., &amp; Stuerzlinger, W. (2001). 3D </b></span></nobr></DIV>
<DIV style="position:absolute;top:402;left:497"><nobr><span class="ft6"><b>Scene manipulation with 2D devices and constraints. <br>Graphics Interface. p. 135-142. </b></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:476"><nobr><span class="ft2"><b>30. Steed, A. (1997). Efficient navigation around complex </b></span></nobr></DIV>
<DIV style="position:absolute;top:463;left:497"><nobr><span class="ft6"><b>virtual environments. ACM VRST'97 Conference on <br>Virtual Reality Software and Technology. p. 173-180. </b></span></nobr></DIV>
<DIV style="position:absolute;top:506;left:476"><nobr><span class="ft2"><b>31. Stoakley, R., Conway, M., &amp; Pausch, R. (1995). Virtual </b></span></nobr></DIV>
<DIV style="position:absolute;top:523;left:497"><nobr><span class="ft6"><b>reality on a WIM: Interactive worlds in miniature. ACM <br>CHI 1995 Conference on Human Factors in Computing <br>Systems. p. 265-272. </b></span></nobr></DIV>
<DIV style="position:absolute;top:584;left:476"><nobr><span class="ft2"><b>32. Tan, D., Robertson, G., &amp; Czerwinski, M. (2001). </b></span></nobr></DIV>
<DIV style="position:absolute;top:601;left:497"><nobr><span class="ft6"><b>Exploring 3D navigation: combining speed-coupled <br>flying with orbiting. ACM CHI'2001 Conference on <br>Human Factors in Computing Systems. p. 418-425. </b></span></nobr></DIV>
<DIV style="position:absolute;top:662;left:476"><nobr><span class="ft2"><b>33. Vinson, N. (1999). Design guidelines for landmarks to </b></span></nobr></DIV>
<DIV style="position:absolute;top:679;left:497"><nobr><span class="ft6"><b>support navigation in virtual environments. ACM <br>CHI'99 Conference on Human Factors in Computing <br>Systems. p. 278-285. </b></span></nobr></DIV>
<DIV style="position:absolute;top:740;left:476"><nobr><span class="ft2"><b>34. Ware, C., &amp; Fleet, D. (1997). Context sensitve flying </b></span></nobr></DIV>
<DIV style="position:absolute;top:757;left:497"><nobr><span class="ft6"><b>interface.  ACM I3D'97 Symposium on Interactive 3D <br>Graphics. p. 127-130. </b></span></nobr></DIV>
<DIV style="position:absolute;top:801;left:476"><nobr><span class="ft2"><b>35. Ware, C., &amp; Osborne, S. (1990). Exploration and virtual </b></span></nobr></DIV>
<DIV style="position:absolute;top:818;left:497"><nobr><span class="ft6"><b>camera control in virtual three dimensional <br>environments.  ACM I3D'90 Symposium on Interactive <br>3D Graphics. p. 175-183. </b></span></nobr></DIV>
<DIV style="position:absolute;top:879;left:476"><nobr><span class="ft2"><b>36. Wernert, E.A., &amp; Hanson, A.J. (1999). A framework for </b></span></nobr></DIV>
<DIV style="position:absolute;top:896;left:497"><nobr><span class="ft6"><b>assisted exploration with collaboration. IEEE <br>Visualization. p. 241-248. </b></span></nobr></DIV>
<DIV style="position:absolute;top:939;left:476"><nobr><span class="ft2"><b>37. Zeleznik, R., &amp; Forsberg, A. (1999). UniCam - 2D </b></span></nobr></DIV>
<DIV style="position:absolute;top:957;left:497"><nobr><span class="ft6"><b>Gestural Camera Controls for 3D Environments. ACM <br>Symposium on Interactive 3D Graphics. p. 169-173. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:476"><nobr><span class="ft2"><b>38. Zeleznik, R., Forsberg, A., &amp; Strauss, P. (1997). Two </b></span></nobr></DIV>
<DIV style="position:absolute;top:1017;left:497"><nobr><span class="ft6"><b>pointer input for 3D interaction. ACM I3D Symposium <br>on Interactive 3D Graphics. p. 115-120. </b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:81"><nobr><span class="ft4"><b>110</b></span></nobr></DIV>
<DIV style="position:absolute;top:1121;left:166"><nobr><span class="ft4"><b>Volume 4, Issue 2</b></span></nobr></DIV>
</DIV>
</BODY>
</HTML>
