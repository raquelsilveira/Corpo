<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>dai-khoo.sigir.doc</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="author" content="Unknown">
<META name="keywords" content="">
<META name="date" content="1999-12-08T11:31:15+00:00">
<META name="subject" content="">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Helvetica;color:#000000;}
	.ft1{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft2{font-size:11px;font-family:Helvetica;color:#000000;}
	.ft3{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft4{font-size:16px;font-family:Times;color:#000000;}
	.ft5{font-size:11px;font-family:Times;color:#000000;}
	.ft6{font-size:11px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;font-family:Helvetica;color:#000000;}
	.ft8{font-size:11px;font-family:Symbol;color:#000000;}
	.ft9{font-size:9px;font-family:Times;color:#000000;}
	.ft10{font-size:16px;font-family:Courier;color:#000000;}
	.ft11{font-size:13px;font-family:Helvetica;color:#000000;}
	.ft12{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft13{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft14{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
	.ft15{font-size:9px;line-height:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15001.png" alt="background image">
<DIV style="position:absolute;top:114;left:89"><nobr><span class="ft0"><b>A New Statistical Formula for Chinese Text Segmentation</b></span></nobr></DIV>
<DIV style="position:absolute;top:147;left:220"><nobr><span class="ft0"><b>Incorporating Contextual Information</b></span></nobr></DIV>
<DIV style="position:absolute;top:186;left:230"><nobr><span class="ft1">Yubin Dai</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:179"><nobr><span class="ft1">Christopher S.G. Khoo</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:177"><nobr><span class="ft2">Division of Information Studies</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:191"><nobr><span class="ft2">School of Applied Science</span></nobr></DIV>
<DIV style="position:absolute;top:259;left:167"><nobr><span class="ft2">Nanyang Technological University</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:214"><nobr><span class="ft2">Singapore 639798</span></nobr></DIV>
<DIV style="position:absolute;top:290;left:227"><nobr><span class="ft2">(65) 790-4602</span></nobr></DIV>
<DIV style="position:absolute;top:313;left:190"><nobr><span class="ft3">dyb_lte@hotmail.com</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:186"><nobr><span class="ft3">assgkhoo@ntu.edu.sg</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:597"><nobr><span class="ft1">Teck Ee Loh</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:576"><nobr><span class="ft2">10 Kent Ridge Crescent</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:582"><nobr><span class="ft2">Data Storage Institute</span></nobr></DIV>
<DIV style="position:absolute;top:237;left:592"><nobr><span class="ft2">Singapore 119260</span></nobr></DIV>
<DIV style="position:absolute;top:253;left:605"><nobr><span class="ft2">(65) 874-8413</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:557"><nobr><span class="ft3">dsilohte@dsi.nus.edu.sg</span></nobr></DIV>
<DIV style="position:absolute;top:372;left:81"><nobr><span class="ft4"><b>ABSTRACT</b></span></nobr></DIV>
<DIV style="position:absolute;top:395;left:81"><nobr><span class="ft12">A new statistical formula for identifying 2-character words in<br>Chinese text, called the <i>contextual information formula</i>, was<br>developed empirically by performing stepwise logistic regression<br>using a sample of sentences that had been manually segmented.<br>Contextual information in the form of the frequency of characters<br>that are adjacent to the bigram being processed as well as the<br>weighted document frequency of the overlapping bigrams were<br>found to be significant factors for predicting the probablity that<br>the bigram constitutes a word. Local information (the number of<br>times the bigram occurs in the document being segmented) and<br>the position of the bigram in the sentence were not found to be<br>useful in determining words. The <i>contextual information formula<br></i>was found to be significantly and substantially better than the<br><i>mutual information formula</i> in identifying 2-character words.<br>The method can also be used for identifying multi-word terms in<br>English text.</span></nobr></DIV>
<DIV style="position:absolute;top:659;left:81"><nobr><span class="ft4"><b>Keywords</b></span></nobr></DIV>
<DIV style="position:absolute;top:679;left:81"><nobr><span class="ft12">Chinese text segmentation, word boundary identification, logistic<br>regression, multi-word terms</span></nobr></DIV>
<DIV style="position:absolute;top:722;left:81"><nobr><span class="ft4"><b>1.</b></span></nobr></DIV>
<DIV style="position:absolute;top:721;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:722;left:105"><nobr><span class="ft4"><b>INTRODUCTION</b></span></nobr></DIV>
<DIV style="position:absolute;top:742;left:81"><nobr><span class="ft12">Chinese text is different from English text in that there is no<br>explicit word boundary. In English text, words are separated by<br>spaces. Chinese text (as well as text of other Oriental languages)<br>is made up of ideographic characters, and a word can comprise<br>one, two or more such characters, without explicit indication<br>where one word ends and another begins.</span></nobr></DIV>
<DIV style="position:absolute;top:845;left:81"><nobr><span class="ft12">This has implications for natural language processing and<br>information retrieval with Chinese text. Text processing<br>techniques that have been developed for Western languages deal<br>with words as meaningful text units and assume that words are<br>easy to identify. These techniques may not work well for Chinese<br>text without some adjustments. To apply these techniques to</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:477"><nobr><span class="ft12">Chinese text, automatic methods for identifying word boundaries<br>accurately have to be developed. The process of identifying word<br>boundaries has been referred to as text segmentation or, more<br>accurately, word segmentation.</span></nobr></DIV>
<DIV style="position:absolute;top:441;left:477"><nobr><span class="ft12">Several techniques have been developed for Chinese text<br>segmentation. They can be divided into:<br>1.</span></nobr></DIV>
<DIV style="position:absolute;top:474;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:474;left:497"><nobr><span class="ft12"><i>statistical methods,</i> based on statistical properties and<br>frequencies of characters and character strings in a corpus<br>(e.g. [13] and [16]).</span></nobr></DIV>
<DIV style="position:absolute;top:523;left:477"><nobr><span class="ft5">2.</span></nobr></DIV>
<DIV style="position:absolute;top:522;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:523;left:497"><nobr><span class="ft12"><i>dictionary-based methods,</i> often complemented with<br>grammar rules. This approach uses a dictionary of words to<br>identify word boundaries. Grammar rules are often used to<br>resolve conflicts (choose between alternative segmentations)<br>and to improve the segmentation (e.g. [4], [8], [19] and [20]).</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:477"><nobr><span class="ft5">3.</span></nobr></DIV>
<DIV style="position:absolute;top:604;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:604;left:497"><nobr><span class="ft12"><i>syntax-based methods, </i>which integrate the word<br>segmentation process with syntactic parsing or part-of-speech<br>tagging (e.g. [1]).</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:477"><nobr><span class="ft5">4.</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:653;left:497"><nobr><span class="ft12"><i>conceptual methods, </i>that make use of some kind of semantic<br>processing to extract information and store it in a knowledge<br>representation scheme. Domain knowledge is used for<br>disambiguation (e.g. [9]).</span></nobr></DIV>
<DIV style="position:absolute;top:718;left:477"><nobr><span class="ft5">Many researchers use a combination of methods (e.g. [14]).</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:477"><nobr><span class="ft12">The objective of this study was to empirically develop a<br>statistical formula for Chinese text segmentation. Researchers<br>have used different statistical methods in segmentation, most of<br>which were based on theoretical considerations or adopted from<br>other fields. In this study, we developed a statistical formula<br>empirically by performing stepwise logistic regression using a<br>sample of sentences that had been manually segmented. This<br>paper reports the new formula developed for identifying 2-<br>character words, and the effectiveness of this formula compared<br>with the <i>mutual information formula</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:909;left:477"><nobr><span class="ft5">This study has the following novel aspects:</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:477"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:483"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:926;left:497"><nobr><span class="ft12">The statistical formula was derived empirically using<br>regression analysis.</span></nobr></DIV>
<DIV style="position:absolute;top:955;left:477"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:483"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:959;left:497"><nobr><span class="ft5">The manual segmentation was performed to identify</span></nobr></DIV>
<DIV style="position:absolute;top:975;left:510"><nobr><span class="ft5">meaningful</span></nobr></DIV>
<DIV style="position:absolute;top:975;left:586"><nobr><span class="ft5"> words rather than simple words.</span></nobr></DIV>
<DIV style="position:absolute;top:991;left:510"><nobr><span class="ft5">Meaningful</span></nobr></DIV>
<DIV style="position:absolute;top:991;left:588"><nobr><span class="ft5"> words include phrasal words and multi-</span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:497"><nobr><span class="ft5">word terms.</span></nobr></DIV>
<DIV style="position:absolute;top:1020;left:477"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:483"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:497"><nobr><span class="ft12">In addition to the relative frequencies of bigrams and<br>characters often used in other studies, our study also<br>investigated the use of <i>document frequencies</i> and <i>weighted</i></span></nobr></DIV>
<DIV style="position:absolute;top:975;left:78"><nobr><span class="ft14">Permission to make digital or hard copies of all or part of this work for<br>personal or classroom use is granted without fee provided that copies are<br>not made or distributed for profit or commercial advantage and that<br>copies bear this notice and the full citation on the first page.  To copy<br>otherwise, to republish, to post on servers or to redistribute to lists,<br>requires prior specific permission and/or a fee.<br>SIGIR '99   8/99 Berkley, CA  USA<br>Copyright 1999 ACM 1-58113-096-1/99/0007 . . . $5.00</span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">82</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft16{font-size:10px;font-family:Times;color:#000000;}
	.ft17{font-size:5px;font-family:Times;color:#000000;}
	.ft18{font-size:10px;font-family:Times;color:#000000;}
	.ft19{font-size:7px;font-family:Times;color:#000000;}
	.ft20{font-size:7px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15002.png" alt="background image">
<DIV style="position:absolute;top:942;left:289"><nobr><span class="ft16">)</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:275"><nobr><span class="ft16">(</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:243"><nobr><span class="ft16">*</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:237"><nobr><span class="ft16">)</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:223"><nobr><span class="ft16">(</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:267"><nobr><span class="ft16">)</span></nobr></DIV>
<DIV style="position:absolute;top:923;left:245"><nobr><span class="ft16">(</span></nobr></DIV>
<DIV style="position:absolute;top:931;left:175"><nobr><span class="ft16">log</span></nobr></DIV>
<DIV style="position:absolute;top:938;left:192"><nobr><span class="ft17">2</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:279"><nobr><span class="ft18"><i>C</i></span></nobr></DIV>
<DIV style="position:absolute;top:942;left:253"><nobr><span class="ft18"><i>freq</i></span></nobr></DIV>
<DIV style="position:absolute;top:942;left:228"><nobr><span class="ft18"><i>B</i></span></nobr></DIV>
<DIV style="position:absolute;top:942;left:202"><nobr><span class="ft18"><i>freq</i></span></nobr></DIV>
<DIV style="position:absolute;top:923;left:250"><nobr><span class="ft18"><i>BC</i></span></nobr></DIV>
<DIV style="position:absolute;top:923;left:223"><nobr><span class="ft18"><i>freq</i></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:101"><nobr><span class="ft12"><i>document frequencies</i>. Weighted document frequencies are<br>similar to document frequencies but each document is<br>weighted by the square of the number of times the character<br>or bigram occurs in the document.</span></nobr></DIV>
<DIV style="position:absolute;top:180;left:81"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:87"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:184;left:101"><nobr><span class="ft12">Contextual information was included in the study. To predict<br>whether the bigram <i>BC</i> in the character string </span></nobr></DIV>
<DIV style="position:absolute;top:201;left:387"><nobr><span class="ft5"> A B C D</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:129"><nobr><span class="ft5"> constitutes a word, we investigated whether the</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:101"><nobr><span class="ft12">frequencies for <i>AB</i>, <i>CD</i>, <i>A</i> and <i>D</i> should be included in the<br>formula.</span></nobr></DIV>
<DIV style="position:absolute;top:262;left:81"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:266;left:87"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:267;left:101"><nobr><span class="ft12">Local frequencies were included in the study. We<br>investigated character and bigram frequencies <i>within the<br>document</i> in which the sentence occurs (i.e. the number of<br>times the character or bigram appears in the document being<br>segmented).</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:81"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:87"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:349;left:101"><nobr><span class="ft12">We investigated whether the position of the bigram (at the<br>beginning of the sentence, before a punctuation mark, or after<br>a punctuation mark) had a significant effect.</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:81"><nobr><span class="ft8">·</span></nobr></DIV>
<DIV style="position:absolute;top:398;left:87"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:101"><nobr><span class="ft12">We developed a segmentation algorithm to apply the<br>statistical formula to segment sentences and resolve conflicts.</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:81"><nobr><span class="ft5">In this study, our objective was to segment text into</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:95"><nobr><span class="ft5">meaningful words</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:208"><nobr><span class="ft5"> rather than </span></nobr></DIV>
<DIV style="position:absolute;top:453;left:293"><nobr><span class="ft5">simple words</span></nobr></DIV>
<DIV style="position:absolute;top:453;left:381"><nobr><span class="ft5">. A simple</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:81"><nobr><span class="ft12">word is the smallest independent unit of a sentence that has<br>meaning on its own. A <i>meaningful word</i> can be a simple word or<br>a compound word comprising 2 or more simple words ­<br>depending on the context. In many cases, the meaning of a<br>compound word is more than just a combination of the meanings<br>of the constituent simple words, i.e. some meaning is lost when<br>the compound word is segmented into simple words.<br>Furthermore, some phrases are used so often that native speakers<br>perceive them and use them as a unit. Admittedly, there is some<br>subjectivity in the manual segmentation of text. But the fact that<br>statistical models can be developed to predict the manually<br>segmented words substantially better than chance indicates some<br>level of consistency in the manual segmentation.</span></nobr></DIV>
<DIV style="position:absolute;top:687;left:81"><nobr><span class="ft12">The problem of identifying meaningful words is not limited to<br>Chinese and oriental languages. Identifying multi-word terms is<br>also a problem in text processing with English and other Western<br>languages, and researchers have used the <i>mutual information<br>formula</i> and other statistical approaches for identifying such<br>terms (e.g. [3], [6] and [7]).</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:81"><nobr><span class="ft4"><b>2.</b></span></nobr></DIV>
<DIV style="position:absolute;top:794;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:795;left:105"><nobr><span class="ft4"><b>PREVIOUS STUDIES</b></span></nobr></DIV>
<DIV style="position:absolute;top:815;left:81"><nobr><span class="ft13">There are few studies using a purely statistical approach to<br>Chinese text segmentation. One statistical formula that has been<br>used by other researchers (e.g. [11] and [16]) is the <i>mutual<br>information formula</i>. Given a character string  </span></nobr></DIV>
<DIV style="position:absolute;top:864;left:381"><nobr><span class="ft5"> A B C D</span></nobr></DIV>
<DIV style="position:absolute;top:881;left:108"><nobr><span class="ft5">, the mutual information for the bigram <i>BC</i> is given by the</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:81"><nobr><span class="ft5">formula:</span></nobr></DIV>
<DIV style="position:absolute;top:935;left:95"><nobr><span class="ft5">MI(BC)  =</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:135"><nobr><span class="ft5">   =      log</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:192"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:196"><nobr><span class="ft5"> freq(BC) ­ log</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:280"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:284"><nobr><span class="ft5"> freq(B) ­ log</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:359"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:364"><nobr><span class="ft5"> freq(C)</span></nobr></DIV>
<DIV style="position:absolute;top:986;left:81"><nobr><span class="ft12">where <i>freq</i> refers to the relative frequency of the character or<br>bigram in the corpus (i.e. the number of times the character or<br>bigram occurs in the corpus divided by the number of characters<br>in the corpus).</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:81"><nobr><span class="ft12"><i>Mutual information</i> is a measure of how strongly the two<br>characters are associated, and can be used as a measure of how</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:477"><nobr><span class="ft13">likely the pair of characters constitutes a word. Sproat &amp; Shih<br>[16] obtained recall and precision values of 94% using <i>mutual<br>information</i> to identify words. This study probably segmented<br>text into simple words rather than meaningful words. In our<br>study, text was segmented into meaningful words and we<br>obtained much poorer results for the <i>mutual information<br>formula</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:477"><nobr><span class="ft12">Lua [12] and Lua &amp; Gan [13] applied information theory to the<br>problem of Chinese text segmentation. They calculated the<br>information content of characters and words using the<br>information entropy formula <i>I = - log</i></span></nobr></DIV>
<DIV style="position:absolute;top:293;left:711"><nobr><span class="ft20"><i>2</i></span></nobr></DIV>
<DIV style="position:absolute;top:288;left:715"><nobr><span class="ft6"><i> P</i>, where P is the</span></nobr></DIV>
<DIV style="position:absolute;top:305;left:477"><nobr><span class="ft12">probability of occurrence of the character or word. If the<br>information content of a character string is less than the sum of<br>the information content of the constituent characters, then the<br>character string is likely to constitute a word. The formula for<br>calculating this </span></nobr></DIV>
<DIV style="position:absolute;top:369;left:580"><nobr><span class="ft5">loss</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:615"><nobr><span class="ft5"> of information content when a word is</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:477"><nobr><span class="ft12">formed is identical to the mutual information formula. Lua &amp;<br>Gan [13] obtained an accuracy of 99% (measured in terms of the<br>number of errors per 100 characters).</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:477"><nobr><span class="ft12">Tung &amp; Lee [18] also used information entropy to identify<br>unknown words in a corpus. However, instead of calculating the<br>entropy value for the character string that is hypothesized to be a<br>word (i.e. the candidate word), they identified all the characters<br>that occurred to the left of the candidate word in the corpus. For<br>each left character, they calculated the probability and entropy<br>value for that character given that it occurs to the left of the<br>candidate word. The same is done for the characters to the right<br>of the candidate word. If the sum of the entropy values for the<br>left characters and the sum of the entropy values for the right<br>characters are both high, than the candidate word is considered<br>likely to be a word. In other words, a character string is likely to<br>be a word if it has several different characters to the left and to<br>the right of it in the corpus, and none of the left and right<br>characters predominate (i.e. not strongly associated with the<br>character string).</span></nobr></DIV>
<DIV style="position:absolute;top:706;left:477"><nobr><span class="ft12">Ogawa &amp; Matsuda [15] developed a statistical method to<br>segment Japanese text. Instead of attempting to identify words<br>directly, they developed a formula to estimate the probability that<br>a bigram straddles a word boundary. They referred to this as the<br>segmentation probability. This was complemented with some<br>syntactic information about which class of characters could be<br>combined with which other class.</span></nobr></DIV>
<DIV style="position:absolute;top:825;left:477"><nobr><span class="ft12">All the above mathematical formulas used for identifying words<br>and word boundaries were developed based on theoretical<br>considerations and not derived empirically.</span></nobr></DIV>
<DIV style="position:absolute;top:880;left:477"><nobr><span class="ft12">Other researchers have developed statistical methods to find the<br>best segmentation for the whole sentence rather than focusing on<br>identifying individual words. Sproat et al. [17] developed a<br>stochastic finite state model for segmenting text. In their model,<br>a word dictionary is represented as a weighted finite state<br>transducer. Each weight represents the estimated cost of the<br>word (calculated using the negative log probability). Basically,<br>the system selects the sentence segmentation that has the<br>smallest total cost. Chang &amp; Chen [1] developed a method for<br>word segmentation and part-of-speech tagging based on a first-<br>order hidden Markov model.</span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">83</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft21{font-size:16px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15003.png" alt="background image">
<DIV style="position:absolute;top:120;left:81"><nobr><span class="ft4"><b>3.</b></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:120;left:105"><nobr><span class="ft4"><b>RESEARCH METHOD</b></span></nobr></DIV>
<DIV style="position:absolute;top:140;left:81"><nobr><span class="ft12">The purpose of this study was to empirically develop a statistical<br>formula for identifying 2-character words as well as to<br>investigate the usefulness of various factors for identifying the<br>words. A sample of 400 sentences was randomly selected from 2<br>months (August and September 1995) of news articles from the<br><i>Xin Hua News Agency</i>, comprising around 2.3 million characters.<br>The sample sentences were manually segmented. The<br>segmentation rules described in [10] were followed fairly closely.<br>More details of the manual segmentation process, especially with<br>regard to identifying meaningful words will be given in [5].</span></nobr></DIV>
<DIV style="position:absolute;top:308;left:81"><nobr><span class="ft12">300 sentences were used for model building, i.e. using regression<br>analysis to develop a statistical formula. 100 sentences were set<br>aside for model validation to evaluate the formula developed in<br>the regression analysis. The sample sentences were broken up<br>into overlapping bigrams. In the regression analysis, the<br>dependent variable was whether a bigram was a two-character<br>word according to the manual segmentation. The independent<br>variables were various corpus statistics derived from the corpus<br>(2 months of news articles).</span></nobr></DIV>
<DIV style="position:absolute;top:460;left:81"><nobr><span class="ft12">The types of frequency information investigated were:<br>1. <i>Relative frequency</i> of individual characters and bigrams</span></nobr></DIV>
<DIV style="position:absolute;top:493;left:102"><nobr><span class="ft12">(character pairs) in the corpus, i.e. the number of times the<br>character or bigram occurs in the corpus divided by the total<br>number of characters in the corpus.</span></nobr></DIV>
<DIV style="position:absolute;top:542;left:81"><nobr><span class="ft5">2. <i>Document frequency</i> of characters and bigrams, i.e. the</span></nobr></DIV>
<DIV style="position:absolute;top:558;left:102"><nobr><span class="ft12">number of documents in the corpus containing the character<br>or bigram divided by the total number of documents in the<br>corpus.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:81"><nobr><span class="ft5">3. <i>Weighted document frequency</i> of characters and bigrams. To</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:102"><nobr><span class="ft12">calculate the weighted document frequency of a character<br>string, each document containing the character string is<br>assigned a score equal to the square of the number of times<br>the character string occurs in the document. The scores for all<br>the documents containing the character string are then<br>summed and divided by the total number of documents in the<br>corpus to obtain the weighted document frequency for the<br>character string.  The rationale is that if a character string<br>occurs several times within the same document, this is<br>stronger evidence that the character string constitutes a word,<br>than if the character string occurs once in several documents.<br>Two or more characters can occur together by chance in<br>several different documents. It is less likely for two<br>characters to occur together several times within the same<br>document by chance.</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:81"><nobr><span class="ft5">4. <i>Local frequency</i> in the form of <i>within-document frequency</i> of</span></nobr></DIV>
<DIV style="position:absolute;top:883;left:102"><nobr><span class="ft12">characters and bigrams, i.e. the number of times the character<br>or bigram occurs in the document being segmented.</span></nobr></DIV>
<DIV style="position:absolute;top:916;left:81"><nobr><span class="ft5">5. <i>Contextual information</i>. Frequency information of characters</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:102"><nobr><span class="ft12">adjacent to a bigram is used to help determine whether the<br>bigram is a word. For the character string </span></nobr></DIV>
<DIV style="position:absolute;top:948;left:379"><nobr><span class="ft5"> A B C D</span></nobr></DIV>
<DIV style="position:absolute;top:964;left:129"><nobr><span class="ft5">, to determine whether the bigram <i>BC</i> is a word,</span></nobr></DIV>
<DIV style="position:absolute;top:981;left:102"><nobr><span class="ft12">frequency information for the adjacent characters <i>A</i> and <i>D</i>, as<br>well as the overlapping bigrams <i>AB</i> and <i>BC</i> were considered.</span></nobr></DIV>
<DIV style="position:absolute;top:1014;left:81"><nobr><span class="ft5">6. <i>Positional information</i>. We studied whether the position of a</span></nobr></DIV>
<DIV style="position:absolute;top:1030;left:101"><nobr><span class="ft12">character string (at the beginning, middle or end of a<br>sentence) gave some indication of whether the character<br>string was a word.</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:477"><nobr><span class="ft12">The statistical model was developed using forward stepwise<br>logistic regression, using the Proc Logistic function in the SAS<br>v.6.12 statistical package for Windows. Logistic regression is an<br>appropriate regression technique when the dependent variable is<br>binary valued (takes the value 0 or 1). The formula developed<br>using logistic regression predicts the probability (more<br>accurately, the log of the odds) that a bigram is a meaningful<br>word.</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:477"><nobr><span class="ft13">In the stepwise regression, the threshold for a variable to enter<br>the model was set at the 0.001 significance level and the<br>threshold for retaining a variable in the model was set at 0.01. In<br>addition, preference was given to <i>relative frequencies</i> and <i>local<br>frequencies</i> because they are easier to calculate than <i>document<br>frequencies</i> and <i>weighted document frequencies</i>. Also, <i>relative<br>frequencies</i> are commonly used in previous studies.</span></nobr></DIV>
<DIV style="position:absolute;top:375;left:477"><nobr><span class="ft12">Furthermore, a variable was entered in a model only if it gave a<br>noticeable improvement to the effectiveness of the model. During<br>regression analysis, the effectiveness of the model was estimated<br>using the measure of concordance that was automatically output<br>by the SAS statistical program. A variable was accepted into the<br>model only if the measure of concordance improved by at least<br>2% when the variable was entered into the model.</span></nobr></DIV>
<DIV style="position:absolute;top:495;left:477"><nobr><span class="ft12">We evaluated the accuracy of the segmentation using measures of<br>recall and precision. Recall and precision in this context are<br>defined as follows:</span></nobr></DIV>
<DIV style="position:absolute;top:549;left:477"><nobr><span class="ft5">Recall = No. of 2-character words identified in the automatic</span></nobr></DIV>
<DIV style="position:absolute;top:566;left:524"><nobr><span class="ft5">              segmentation that are correct                         </span></nobr></DIV>
<DIV style="position:absolute;top:582;left:531"><nobr><span class="ft5">No. of 2-character words identified in the manual</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:639"><nobr><span class="ft5">segmentation</span></nobr></DIV>
<DIV style="position:absolute;top:620;left:477"><nobr><span class="ft5">Precision = No. of 2-character words identified in the automatic</span></nobr></DIV>
<DIV style="position:absolute;top:637;left:531"><nobr><span class="ft12">             segmentation that are correct                        <br>   No. of 2-character words identified in the automatic</span></nobr></DIV>
<DIV style="position:absolute;top:669;left:618"><nobr><span class="ft5">segmentation</span></nobr></DIV>
<DIV style="position:absolute;top:699;left:477"><nobr><span class="ft4"><b>4.</b></span></nobr></DIV>
<DIV style="position:absolute;top:698;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:699;left:504"><nobr><span class="ft21"><b>STATISTICAL FORMULAS<br>DEVELOPED</b></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:477"><nobr><span class="ft4"><b>4.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:743;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:510"><nobr><span class="ft4"><b>The Contextual Information Formula</b></span></nobr></DIV>
<DIV style="position:absolute;top:764;left:477"><nobr><span class="ft12">The formula that was developed for 2-character words is as<br>follows. Given a character string </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:706"><nobr><span class="ft5"> A B C D </span></nobr></DIV>
<DIV style="position:absolute;top:780;left:809"><nobr><span class="ft5">, the</span></nobr></DIV>
<DIV style="position:absolute;top:797;left:477"><nobr><span class="ft5">association strength for bigram <i>BC</i> is:</span></nobr></DIV>
<DIV style="position:absolute;top:819;left:477"><nobr><span class="ft5"> Assoc(BC) = 0.35 * log</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:613"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:819;left:617"><nobr><span class="ft5"> freq(BC) + 0.37 * log</span></nobr></DIV>
<DIV style="position:absolute;top:824;left:740"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:819;left:744"><nobr><span class="ft5"> freq(A) +</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:558"><nobr><span class="ft5">0.32 log</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:602"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:607"><nobr><span class="ft5"> freq(D) ­ 0.36 * log</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:720"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:725"><nobr><span class="ft5"> docfreq</span></nobr></DIV>
<DIV style="position:absolute;top:840;left:769"><nobr><span class="ft19">wt</span></nobr></DIV>
<DIV style="position:absolute;top:835;left:778"><nobr><span class="ft5">(AB) ­</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:558"><nobr><span class="ft5">0.29 * log</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:613"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:617"><nobr><span class="ft5"> docfreq</span></nobr></DIV>
<DIV style="position:absolute;top:856;left:662"><nobr><span class="ft19">wt</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:671"><nobr><span class="ft5">(CD) + 5.91</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:477"><nobr><span class="ft12">where <i>freq</i> refers to the relative frequency in the corpus and<br><i>docfreq</i></span></nobr></DIV>
<DIV style="position:absolute;top:895;left:518"><nobr><span class="ft20"><i>wt</i></span></nobr></DIV>
<DIV style="position:absolute;top:891;left:527"><nobr><span class="ft5"> refers to the weighted document frequency. We refer to</span></nobr></DIV>
<DIV style="position:absolute;top:907;left:477"><nobr><span class="ft12">this formula as the <i>contextual information formula</i>. More details<br>of the regression model are given in Table 1.</span></nobr></DIV>
<DIV style="position:absolute;top:946;left:477"><nobr><span class="ft12">The formula indicates that contextual information is helpful in<br>identifying word boundaries. <i>A</i> in the formula refers to the<br>character preceding the bigram that is being processed, whereas<br><i>D</i> is the character following the bigram. The formula indicates<br>that if the character preceding and the character following the<br>bigram have high relative frequencies, then the bigram is more<br>likely to be a word.</span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">84</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft22{font-size:11px;font-family:Courier;color:#000000;}
	.ft23{font-size:7px;font-family:Courier;color:#000000;}
	.ft24{font-size:11px;font-family:Courier;color:#000000;}
	.ft25{font-size:7px;font-family:Courier;color:#000000;}
	.ft26{font-size:11px;font-family:Times;color:#000000;}
	.ft27{font-size:11px;line-height:14px;font-family:Courier;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15004.png" alt="background image">
<DIV style="position:absolute;top:447;left:81"><nobr><span class="ft12">Contextual information involving the <i>weighted document<br>frequency</i> was also found to be significant. The formula indicates<br>that if the overlapping bigrams <i>AB</i> and <i>CD</i> have high <i>weighted<br>document frequencies</i>, then the bigram <i>BC</i> is less likely to be a<br>word. We tried replacing the <i>weighted document frequencies<br></i>with the unweighted <i>document frequencies </i>as well as the<i> relative<br>frequencies. </i>These were found to give a lower concordance score.<br>Even with <i>docfreq (AB)</i> and <i>docfreq (CD)</i> in the model, <i>docfreq</i></span></nobr></DIV>
<DIV style="position:absolute;top:567;left:432"><nobr><span class="ft20"><i>wt</i></span></nobr></DIV>
<DIV style="position:absolute;top:579;left:81"><nobr><span class="ft6"><i>(AB)</i> and <i>docfreq</i></span></nobr></DIV>
<DIV style="position:absolute;top:584;left:184"><nobr><span class="ft20"><i>wt </i></span></nobr></DIV>
<DIV style="position:absolute;top:579;left:200"><nobr><span class="ft6"><i>(CD)</i> were found to improve the model</span></nobr></DIV>
<DIV style="position:absolute;top:595;left:81"><nobr><span class="ft12">significantly. However, local frequencies were surprisingly not<br>found to be useful in predicting 2-character words.</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:81"><nobr><span class="ft12">We investigated whether the position of the bigram in the<br>sentence was a significant factor. We included a variable to<br>indicate whether the bigram occurred just after a punctuation<br>mark or at the beginning of the sentence, and another variable to<br>indicate whether the bigram occurred just before a punctuation<br>mark or at the end of a sentence. The interaction between each of<br>the </span></nobr></DIV>
<DIV style="position:absolute;top:731;left:117"><nobr><span class="ft5">position</span></nobr></DIV>
<DIV style="position:absolute;top:731;left:175"><nobr><span class="ft5"> variables and the various relative frequencies</span></nobr></DIV>
<DIV style="position:absolute;top:747;left:81"><nobr><span class="ft12">were not significant. However, it was found that whether or not<br>the bigram was at the end of a sentence or just before a<br>punctuation mark was a significant factor. Bigrams at the end of<br>a sentence or just before a punctuation mark tend to be words.<br>However, since this factor did not improve the concordance score<br>by 2%, the effect was deemed too small to be included in the<br>model.</span></nobr></DIV>
<DIV style="position:absolute;top:867;left:81"><nobr><span class="ft12">It should be noted that the contextual information used in the<br>study already incorporates some positional information. The<br>frequency of character A (the character preceding the bigram)<br>was given the value 0 if the bigram was preceded by a<br>punctuation mark or was at the beginning of a sentence.<br>Similarly, the frequency of character D (the character following<br>the bigram) was given the value 0 if the bigram preceded a<br>punctuation mark.</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:81"><nobr><span class="ft12">We also investigated whether the model would be different for<br>high and low frequency words. We included in the regression<br>analysis the interaction between the relative frequency of the<br>bigram and the other relative frequencies. The interaction terms<br>were not found to be significant. Finally, it is noted that the</span></nobr></DIV>
<DIV style="position:absolute;top:446;left:477"><nobr><span class="ft12">coefficients for the various factors are nearly the same, hovering<br>around 0.34.</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:477"><nobr><span class="ft4"><b>4.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:488;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:489;left:510"><nobr><span class="ft4"><b>Improved Mutual Information Formula</b></span></nobr></DIV>
<DIV style="position:absolute;top:509;left:477"><nobr><span class="ft12">In this study, the <i>contextual information formula</i> (CIF) was<br>evaluated by comparing it with the <i>mutual information formula<br></i>(MIF). We wanted to find out whether the segmentation results<br>using the CIF was better than the segmentation results using the<br>MIF.</span></nobr></DIV>
<DIV style="position:absolute;top:597;left:477"><nobr><span class="ft13">In the CIF model, the coefficients of the variables were<br>determined using regression analysis. If CIF was found to give<br>better results than MIF, it could be because the coefficients for<br>the variables in CIF had been determined empirically ­ and not<br>because of the types of variables in the formula. To reject this<br>explanation, regression analysis was used to determine the<br>coefficients for the factors in the <i>mutual information formula</i>.<br>We refer to this new version of the formula as the <i>improved<br>mutual information formula</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:477"><nobr><span class="ft5">Given a character string </span></nobr></DIV>
<DIV style="position:absolute;top:750;left:652"><nobr><span class="ft5"> A B C D </span></nobr></DIV>
<DIV style="position:absolute;top:750;left:751"><nobr><span class="ft5">, the <i>improved</i></span></nobr></DIV>
<DIV style="position:absolute;top:767;left:477"><nobr><span class="ft6"><i>mutual information formula</i> is:</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:477"><nobr><span class="ft5"> Improved MI(BC) =  0.39 * log</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:654"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:658"><nobr><span class="ft5"> freq(BC) - 0.28 * log</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:780"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:789;left:784"><nobr><span class="ft5"> freq(B) -</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:598"><nobr><span class="ft5">0.23 log</span></nobr></DIV>
<DIV style="position:absolute;top:810;left:643"><nobr><span class="ft19">2</span></nobr></DIV>
<DIV style="position:absolute;top:805;left:647"><nobr><span class="ft5"> freq(C) - 0.32</span></nobr></DIV>
<DIV style="position:absolute;top:828;left:477"><nobr><span class="ft12">The coefficients are all close to 0.3. The formula is thus quite<br>similar to the <i>mutual information formula</i>, except for a<br>multiplier of 0.3.</span></nobr></DIV>
<DIV style="position:absolute;top:887;left:477"><nobr><span class="ft4"><b>5.</b></span></nobr></DIV>
<DIV style="position:absolute;top:886;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:887;left:501"><nobr><span class="ft4"><b>SEGMENTATION ALGORITHMS</b></span></nobr></DIV>
<DIV style="position:absolute;top:907;left:477"><nobr><span class="ft12">The automatic segmentation process has the following steps:<br>1.</span></nobr></DIV>
<DIV style="position:absolute;top:922;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:923;left:497"><nobr><span class="ft12">The statistical formula is used to calculate a score for each<br>bigram to indicate its association strength (or how likely the<br>bigram is a word).</span></nobr></DIV>
<DIV style="position:absolute;top:972;left:477"><nobr><span class="ft5">2.</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:972;left:497"><nobr><span class="ft12">A threshold value is then set and used to decide which<br>bigram is a word. If a bigram obtains a score above the<br>threshold value, then it is selected as a word. Different<br>threshold values can be used, depending on whether the user<br>prefers high recall or high precision.</span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:477"><nobr><span class="ft5">3.</span></nobr></DIV>
<DIV style="position:absolute;top:1052;left:487"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:1053;left:497"><nobr><span class="ft12">A segmentation algorithm is used to resolve conflict. If two<br>overlapping bigrams both have association scores above the</span></nobr></DIV>
<DIV style="position:absolute;top:127;left:152"><nobr><span class="ft27">                     Parameter Standard    Wald      Pr &gt;     Standardized<br>Variable         DF  Estimate  Error    Chi-Square Chi-Square Estimate</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:152"><nobr><span class="ft22">INTERCPT</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:328"><nobr><span class="ft22">5.9144</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:402"><nobr><span class="ft22">0.1719</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:476"><nobr><span class="ft22">1184.0532</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:658"><nobr><span class="ft22">   .</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:152"><nobr><span class="ft22">Log freq(BC)</span></nobr></DIV>
<DIV style="position:absolute;top:184;left:251"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:185;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:328"><nobr><span class="ft22">0.3502</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:402"><nobr><span class="ft22">0.0106</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:476"><nobr><span class="ft22">1088.7291</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:658"><nobr><span class="ft22">0.638740</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:152"><nobr><span class="ft22">Log freq(A)</span></nobr></DIV>
<DIV style="position:absolute;top:198;left:243"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:328"><nobr><span class="ft22">0.3730</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:402"><nobr><span class="ft22">0.0113</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:476"><nobr><span class="ft22">1092.1382</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:658"><nobr><span class="ft22">0.709621</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:152"><nobr><span class="ft22">Log freq(D)</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:243"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:214;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:328"><nobr><span class="ft22">0.3171</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:402"><nobr><span class="ft22">0.0107</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:476"><nobr><span class="ft22">886.4446</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:658"><nobr><span class="ft22">0.607326</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:152"><nobr><span class="ft22">Log docfreq</span></nobr></DIV>
<DIV style="position:absolute;top:232;left:243"><nobr><span class="ft23">wt</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:254"><nobr><span class="ft22">(AB)</span></nobr></DIV>
<DIV style="position:absolute;top:227;left:287"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:328"><nobr><span class="ft22">-0.3580</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:402"><nobr><span class="ft22">0.0111</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:476"><nobr><span class="ft22">1034.0948</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:658"><nobr><span class="ft22">-0.800520</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:152"><nobr><span class="ft22">Log docfreq</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:243"><nobr><span class="ft23">wt</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:254"><nobr><span class="ft22">(CD)</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:287"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:242;left:301"><nobr><span class="ft22">1</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:328"><nobr><span class="ft22">-0.2867</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:402"><nobr><span class="ft22">0.0104</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:476"><nobr><span class="ft22">754.2276</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:571"><nobr><span class="ft22">0.0001</span></nobr></DIV>
<DIV style="position:absolute;top:242;left:658"><nobr><span class="ft22">-0.635704</span></nobr></DIV>
<DIV style="position:absolute;top:272;left:152"><nobr><span class="ft22">Note: <i>freq</i> refers to the relative frequency, and <i>docfreq</i></span></nobr></DIV>
<DIV style="position:absolute;top:277;left:616"><nobr><span class="ft25"><i>wt</i></span></nobr></DIV>
<DIV style="position:absolute;top:272;left:626"><nobr><span class="ft22"> refers to the</span></nobr></DIV>
<DIV style="position:absolute;top:287;left:152"><nobr><span class="ft22">weighted document frequency.</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:152"><nobr><span class="ft22">Association of Predicted Probabilities and Observed Responses</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:152"><nobr><span class="ft27">            Concordant = 90.1%          Somers' D = 0.803<br>            Discordant =  9.8%          Gamma     = 0.803<br>            Tied       =  0.1%          Tau-a     = 0.295<br>            (23875432 pairs)            c         = 0.901</span></nobr></DIV>
<DIV style="position:absolute;top:410;left:306"><nobr><span class="ft26"><b>Table 1. Final regression model for 2-character words</b></span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">85</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft28{font-size:11px;font-family:Times;color:#000000;}
	.ft29{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15005.png" alt="background image">
<DIV style="position:absolute;top:490;left:101"><nobr><span class="ft12">threshold value, then there is conflict or ambiguity. The<br>frequency of such conflicts will rise as the threshold value is<br>lowered. The segmentation algorithm resolves the conflict<br>and selects one of the bigrams as a word.</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:81"><nobr><span class="ft13">One simple segmentation algorithm is the <i>forward match<br>algorithm</i>. Consider the sentence </span></nobr></DIV>
<DIV style="position:absolute;top:577;left:296"><nobr><span class="ft5">A B C D E </span></nobr></DIV>
<DIV style="position:absolute;top:577;left:408"><nobr><span class="ft5">. The</span></nobr></DIV>
<DIV style="position:absolute;top:594;left:81"><nobr><span class="ft12">segmentation process proceeds from the beginning of the<br>sentence to the end. First the bigram <i>AB</i> is considered. If the<br>association score is above the threshold, then <i>AB</i> is taken as a<br>word, and the bigram <i>CD</i> is next considered. If the association<br>score of <i>AB</i> is below the threshold, the character <i>A</i>  is taken as a<br>1-character word. And the bigram <i>BC</i> is next considered. In<br>effect, if the association score of both <i>AB</i> and <i>BC</i> are above<br>threshold, the forward match algorithm selects <i>AB</i> as a word and<br>not <i>BC</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:749;left:81"><nobr><span class="ft13">The <i>forward match </i>method for resolving ambiguity is somewhat<br>arbitrary and not satisfactory. When overlapping bigrams exceed<br>the threshold value, it simply decides in favour of the earlier<br>bigram. Another segmentation algorithm was developed in this<br>study which we refer to as the <i>comparative forward match<br>algorithm</i>. This has an additional step:</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:95"><nobr><span class="ft13">If 2 overlapping bigrams <i>AB</i> and <i>BC</i> both have scores above<br>the threshold value then their scores are compared. If <i>AB</i> has a<br>higher value, then it is selected as a word, and the program<br>next considers the bigrams <i>CD</i> and <i>DE</i>. On the other hand, if<br><i>AB</i> has a lower value, then character <i>A</i> is selected as a 1-<br>character word, and the program next considers bigrams <i>BC<br></i>and <i>CD</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:975;left:81"><nobr><span class="ft12">The <i>comparative forward match</i> method (CFM) was compared<br>with the <i>forward match</i> method (FM) by applying them to the 3<br>statistical formulas (the <i>contextual information formula</i>, the<br><i>mutual information formula</i> and the <i>improved mutual<br>information formula</i>). One way to compare the effectiveness of<br>the 2 segmentation algorithms is by comparing their precision<br>figures at the same recall levels. The precision figures for</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:477"><nobr><span class="ft12">selected recall levels are given in Table 2. The results are based<br>on the sample of 300 sentences.</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:477"><nobr><span class="ft13">The <i>comparative forward match</i> algorithm gave better results for<br>the <i>mutual information </i>and <i>improved mutual information<br>formulas</i> ­ especially at low threshold values when a large<br>number of conflicts are likely. Furthermore, for the <i>forward<br>match</i> method, the recall didn</span></nobr></DIV>
<DIV style="position:absolute;top:411;left:666"><nobr><span class="ft5">t go substantially higher than</span></nobr></DIV>
<DIV style="position:absolute;top:427;left:477"><nobr><span class="ft5">80% even at low threshold values.</span></nobr></DIV>
<DIV style="position:absolute;top:450;left:477"><nobr><span class="ft12">For the <i>contextual information formula</i>, the <i>comparative forward<br>match</i> method did not perform better than <i>forward match</i>, except<br>at very low threshold values when the recall was above 90%.<br>This was expected because the <i>contextual information formula<br></i>already incorporates information about neighboring characters<br>within the formula. The formula gave very few conflicting<br>segmentations. There were very few cases of overlapping<br>bigrams both having association scores above the threshold ­<br>except when threshold values were below ­1.5.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:477"><nobr><span class="ft4"><b>6.</b></span></nobr></DIV>
<DIV style="position:absolute;top:606;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:501"><nobr><span class="ft4"><b>EVALUATION</b></span></nobr></DIV>
<DIV style="position:absolute;top:631;left:477"><nobr><span class="ft4"><b>6.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:630;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:631;left:510"><nobr><span class="ft21"><b>Comparing the Contextual Information<br>Formula with the Mutual Information<br>Formula</b></span></nobr></DIV>
<DIV style="position:absolute;top:694;left:477"><nobr><span class="ft13">In this section we compare the effectiveness of the <i>contextual<br>information formula</i> with the <i>mutual information formula</i> and<br>the <i>improved mutual information formula</i> using the 100<br>sentences that had been set aside for evaluation purposes. For the<br><i>contextual information formula</i>, the <i>forward match</i> segmentation<br>algorithm was used. The <i>comparative forward match</i> algorithm<br>was used for the <i>mutual information</i> and the <i>improved mutual<br>information</i> formulas.</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:477"><nobr><span class="ft12">The three statistical formulas were compared by comparing their<br>precision figures at 4 recall levels ­ at 60%, 70%, 80% and 90%.<br>For each of the three statistical formulas, we identified the<br>threshold values that would give a recall of 60%, 70%, 80% and<br>90%. We then determined the precision values at these threshold<br>values to find out whether the <i>contextual information formula<br></i>gave better precision than the other two formulas at 60%, 70%,<br>80% and 90% recall. These recall levels were selected because a<br>recall of 50% or less is probably unacceptable for most<br>applications.</span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:477"><nobr><span class="ft13">The precision figures for the 4 recall levels are given in Table 3.<br>The recall-precision graphs for the 3 formulas are given in Fig. 1.<br>The <i>contextual information formula</i> substantially outperforms<br>the <i>mutual information</i> and the <i>improved mutual information<br></i>formulas. At the 90% recall level, the <i>contextual information</i></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:270"><nobr><span class="ft26"><b>Precision</b></span></nobr></DIV>
<DIV style="position:absolute;top:135;left:109"><nobr><span class="ft26"><b>Recall</b></span></nobr></DIV>
<DIV style="position:absolute;top:135;left:181"><nobr><span class="ft5">Comparative</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:174"><nobr><span class="ft5">Forward Match</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:280"><nobr><span class="ft5">Forward</span></nobr></DIV>
<DIV style="position:absolute;top:152;left:286"><nobr><span class="ft5">Match</span></nobr></DIV>
<DIV style="position:absolute;top:136;left:349"><nobr><span class="ft6"><i>Improvement</i></span></nobr></DIV>
<DIV style="position:absolute;top:173;left:98"><nobr><span class="ft5">Mutual Information</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:113"><nobr><span class="ft26"><b>90%</b></span></nobr></DIV>
<DIV style="position:absolute;top:189;left:203"><nobr><span class="ft5">51%</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:301"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:383"><nobr><span class="ft6"><i>-</i></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:113"><nobr><span class="ft26"><b>80%</b></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:203"><nobr><span class="ft5">52%</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:291"><nobr><span class="ft5">47%</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:376"><nobr><span class="ft6"><i>5%</i></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:113"><nobr><span class="ft26"><b>70%</b></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:203"><nobr><span class="ft5">53%</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:291"><nobr><span class="ft5">51%</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:376"><nobr><span class="ft6"><i>2%</i></span></nobr></DIV>
<DIV style="position:absolute;top:240;left:113"><nobr><span class="ft26"><b>60%</b></span></nobr></DIV>
<DIV style="position:absolute;top:239;left:203"><nobr><span class="ft5">54%</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:291"><nobr><span class="ft5">52%</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:376"><nobr><span class="ft6"><i>2%</i></span></nobr></DIV>
<DIV style="position:absolute;top:260;left:98"><nobr><span class="ft5">Improved Mutual Information</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:113"><nobr><span class="ft26"><b>90%</b></span></nobr></DIV>
<DIV style="position:absolute;top:277;left:203"><nobr><span class="ft5">51%</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:301"><nobr><span class="ft5">-</span></nobr></DIV>
<DIV style="position:absolute;top:277;left:383"><nobr><span class="ft6"><i>-</i></span></nobr></DIV>
<DIV style="position:absolute;top:294;left:113"><nobr><span class="ft26"><b>80%</b></span></nobr></DIV>
<DIV style="position:absolute;top:293;left:203"><nobr><span class="ft5">53%</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:291"><nobr><span class="ft5">46%</span></nobr></DIV>
<DIV style="position:absolute;top:293;left:376"><nobr><span class="ft6"><i>7%</i></span></nobr></DIV>
<DIV style="position:absolute;top:310;left:113"><nobr><span class="ft26"><b>70%</b></span></nobr></DIV>
<DIV style="position:absolute;top:310;left:203"><nobr><span class="ft5">54%</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:291"><nobr><span class="ft5">52%</span></nobr></DIV>
<DIV style="position:absolute;top:310;left:376"><nobr><span class="ft6"><i>2%</i></span></nobr></DIV>
<DIV style="position:absolute;top:327;left:113"><nobr><span class="ft26"><b>60%</b></span></nobr></DIV>
<DIV style="position:absolute;top:326;left:203"><nobr><span class="ft5">55%</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:291"><nobr><span class="ft5">54%</span></nobr></DIV>
<DIV style="position:absolute;top:327;left:376"><nobr><span class="ft6"><i>1%</i></span></nobr></DIV>
<DIV style="position:absolute;top:347;left:98"><nobr><span class="ft5">Contextual Information Formula</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:113"><nobr><span class="ft26"><b>90%</b></span></nobr></DIV>
<DIV style="position:absolute;top:364;left:203"><nobr><span class="ft5">55%</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:291"><nobr><span class="ft5">54%</span></nobr></DIV>
<DIV style="position:absolute;top:364;left:376"><nobr><span class="ft6"><i>1%</i></span></nobr></DIV>
<DIV style="position:absolute;top:381;left:113"><nobr><span class="ft26"><b>80%</b></span></nobr></DIV>
<DIV style="position:absolute;top:380;left:203"><nobr><span class="ft5">62%</span></nobr></DIV>
<DIV style="position:absolute;top:380;left:291"><nobr><span class="ft5">62%</span></nobr></DIV>
<DIV style="position:absolute;top:381;left:376"><nobr><span class="ft6"><i>0%</i></span></nobr></DIV>
<DIV style="position:absolute;top:397;left:113"><nobr><span class="ft26"><b>70%</b></span></nobr></DIV>
<DIV style="position:absolute;top:397;left:203"><nobr><span class="ft5">65%</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:291"><nobr><span class="ft5">65%</span></nobr></DIV>
<DIV style="position:absolute;top:397;left:376"><nobr><span class="ft6"><i>0%</i></span></nobr></DIV>
<DIV style="position:absolute;top:414;left:113"><nobr><span class="ft26"><b>60%</b></span></nobr></DIV>
<DIV style="position:absolute;top:413;left:203"><nobr><span class="ft5">68%</span></nobr></DIV>
<DIV style="position:absolute;top:413;left:291"><nobr><span class="ft5">68%</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:376"><nobr><span class="ft6"><i>0%</i></span></nobr></DIV>
<DIV style="position:absolute;top:441;left:98"><nobr><span class="ft29"><b>Table 2. Recall and precision values for the <i>comparative<br>forward match</i> segmentation algorithm vs. <i>forward match</b></i></span></nobr></DIV>
<DIV style="position:absolute;top:119;left:665"><nobr><span class="ft26"><b>Precision</b></span></nobr></DIV>
<DIV style="position:absolute;top:135;left:499"><nobr><span class="ft26"><b>Recall</b></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:571"><nobr><span class="ft6"><i>Mutual</i></span></nobr></DIV>
<DIV style="position:absolute;top:152;left:558"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:645"><nobr><span class="ft6"><i>Improved Mutual</i></span></nobr></DIV>
<DIV style="position:absolute;top:152;left:660"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:136;left:764"><nobr><span class="ft6"><i>Contextual</i></span></nobr></DIV>
<DIV style="position:absolute;top:152;left:761"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:174;left:503"><nobr><span class="ft26"><b>90%</b></span></nobr></DIV>
<DIV style="position:absolute;top:174;left:562"><nobr><span class="ft5">57%  (0.0)</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:661"><nobr><span class="ft5">57%  (-2.5)</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:762"><nobr><span class="ft5">61%  (-1.5)</span></nobr></DIV>
<DIV style="position:absolute;top:191;left:503"><nobr><span class="ft26"><b>80%</b></span></nobr></DIV>
<DIV style="position:absolute;top:190;left:562"><nobr><span class="ft5">59%  (3.7)</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:661"><nobr><span class="ft5">59%  (-1.5)</span></nobr></DIV>
<DIV style="position:absolute;top:190;left:762"><nobr><span class="ft5">66%  (-0.8)</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:503"><nobr><span class="ft26"><b>70%</b></span></nobr></DIV>
<DIV style="position:absolute;top:206;left:562"><nobr><span class="ft5">59%  (4.7)</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:661"><nobr><span class="ft5">60%  (-1.0)</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:762"><nobr><span class="ft5">70%  (-0.3)</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:503"><nobr><span class="ft26"><b>60%</b></span></nobr></DIV>
<DIV style="position:absolute;top:223;left:562"><nobr><span class="ft5">60%  (5.6)</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:661"><nobr><span class="ft5">62%  (-0.7)</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:764"><nobr><span class="ft5">74%  (0.0)</span></nobr></DIV>
<DIV style="position:absolute;top:243;left:495"><nobr><span class="ft5">* Threshold values are given in parenthesis.</span></nobr></DIV>
<DIV style="position:absolute;top:267;left:487"><nobr><span class="ft26"><b>Table 3. Recall and precision for three statistical formulas</b></span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">86</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft30{font-size:8px;font-family:Helvetica;color:#000000;}
	.ft31{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15006.png" alt="background image">
<DIV style="position:absolute;top:652;left:81"><nobr><span class="ft12"><i>formula</i> was better by about 4%. At the 60% recall level, it<br>outperformed the <i>mutual information formula</i> by 14% (giving a<br>relative improvement of 23%). The results also indicate that the<br><i>improved mutual information formula</i> does not perform better<br>than the <i>mutual information formula</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:745;left:81"><nobr><span class="ft4"><b>6.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:744;left:104"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:745;left:114"><nobr><span class="ft4"><b>Statistical Test of Significance</b></span></nobr></DIV>
<DIV style="position:absolute;top:764;left:81"><nobr><span class="ft12">In order to perform a statistical test, recall and precision figures<br>were calculated for each of the 100 sentences used in the<br>evaluation. The average recall and the average precision across<br>the 100 sentences were then calculated for the three statistical<br>formulas. In the previous section, recall and precision were<br>calculated for all the 100 sentences combined. Here, recall and<br>precision were obtained for individual sentences and then the<br>average across the 100 sentences was calculated. The average<br>precision for 60%, 70%, 80% and 90% average recall are given<br>in Table 4.</span></nobr></DIV>
<DIV style="position:absolute;top:932;left:81"><nobr><span class="ft13">For each recall level, an analysis of variance with repeated<br>measures was carried out to find out whether the differences in<br>precision were significant. Pairwise comparisons using Tukey s<br>HSD test was also carried out. The <i>contextual information<br>formula</i> was significantly better (</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:284"><nobr><span class="ft8"></span></nobr></DIV>
<DIV style="position:absolute;top:998;left:293"><nobr><span class="ft5">=0.001) than the <i>mutual</i></span></nobr></DIV>
<DIV style="position:absolute;top:1015;left:81"><nobr><span class="ft12"><i>information</i> and the <i>improved mutual information </i>formulas at all<br>4 recall levels. The <i>improved mutual information formula</i> was<br>not found to be significantly better than <i>mutual information</i>.</span></nobr></DIV>
<DIV style="position:absolute;top:829;left:477"><nobr><span class="ft4"><b>7.</b></span></nobr></DIV>
<DIV style="position:absolute;top:828;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:829;left:501"><nobr><span class="ft4"><b>ANALYSIS OF ERRORS</b></span></nobr></DIV>
<DIV style="position:absolute;top:849;left:477"><nobr><span class="ft12">The errors that arose from using the <i>contextual information<br>formula</i> were analyzed to gain insights into the weaknesses of<br>the model and how the model can be improved. There are two<br>types of errors: errors of commission and errors of omission.<br>Errors of commission are bigrams that are identified by the<br>automatic segmentation to be words when in fact they are not<br>(according to the manual segmentation). Errors of omission are<br>bigrams that are not identified by the automatic segmentation to<br>be words but in fact they are.</span></nobr></DIV>
<DIV style="position:absolute;top:1001;left:477"><nobr><span class="ft12">The errors depend of course on the threshold values used. A high<br>threshold (e.g. 1.0) emphasizes precision and a low threshold<br>(e.g. ­1.0) emphasizes recall. 50 sentences were selected from<br>the 100 sample sentences to find the distribution of errors at<br>different regions of threshold values.</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:517"><nobr><span class="ft26"><b>Association Score &gt;1.0 (definite errors)</b></span></nobr></DIV>
<DIV style="position:absolute;top:485;left:538"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:485;left:623"><nobr><span class="ft5">will </span></nobr></DIV>
<DIV style="position:absolute;top:485;left:662"><nobr><span class="ft5"> through</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:538"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:623"><nobr><span class="ft5">telegraph [on the] day [31 July]</span></nobr></DIV>
<DIV style="position:absolute;top:534;left:517"><nobr><span class="ft31"><b>Association Score Between ­1.0 and 1.0<br>(borderline errors)</b></span></nobr></DIV>
<DIV style="position:absolute;top:566;left:538"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:623"><nobr><span class="ft5">still </span></nobr></DIV>
<DIV style="position:absolute;top:566;left:662"><nobr><span class="ft5"> to</span></nobr></DIV>
<DIV style="position:absolute;top:582;left:623"><nobr><span class="ft5">will be</span></nobr></DIV>
<DIV style="position:absolute;top:598;left:538"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:598;left:623"><nobr><span class="ft5">people etc.</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:538"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:615;left:623"><nobr><span class="ft5">I want</span></nobr></DIV>
<DIV style="position:absolute;top:635;left:517"><nobr><span class="ft5">Person's name</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:538"><nobr><span class="ft5">  (</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:582"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:652;left:625"><nobr><span class="ft5">Wan Wen Ju</span></nobr></DIV>
<DIV style="position:absolute;top:673;left:517"><nobr><span class="ft5">Place name</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:538"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:575"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:689;left:625"><nobr><span class="ft5">a village name in China</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:538"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:575"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:705;left:625"><nobr><span class="ft5">Canada</span></nobr></DIV>
<DIV style="position:absolute;top:726;left:517"><nobr><span class="ft5">Name of an organization/institution</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:538"><nobr><span class="ft5"> (</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:579"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:625"><nobr><span class="ft5">Xin Hua Agency</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:538"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:575"><nobr><span class="ft5">)</span></nobr></DIV>
<DIV style="position:absolute;top:758;left:625"><nobr><span class="ft5">The State Department</span></nobr></DIV>
<DIV style="position:absolute;top:785;left:508"><nobr><span class="ft26"><b>Table 6. Bigrams incorrectly identified as words</b></span></nobr></DIV>
<DIV style="position:absolute;top:560;left:104"><nobr><span class="ft30"><b>55</b></span></nobr></DIV>
<DIV style="position:absolute;top:505;left:104"><nobr><span class="ft30"><b>60</b></span></nobr></DIV>
<DIV style="position:absolute;top:449;left:104"><nobr><span class="ft30"><b>65</b></span></nobr></DIV>
<DIV style="position:absolute;top:395;left:104"><nobr><span class="ft30"><b>70</b></span></nobr></DIV>
<DIV style="position:absolute;top:339;left:104"><nobr><span class="ft30"><b>75</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:117"><nobr><span class="ft30"><b>60</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:163"><nobr><span class="ft30"><b>65</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:208"><nobr><span class="ft30"><b>70</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:254"><nobr><span class="ft30"><b>75</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:300"><nobr><span class="ft30"><b>80</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:345"><nobr><span class="ft30"><b>85</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:391"><nobr><span class="ft30"><b>90</b></span></nobr></DIV>
<DIV style="position:absolute;top:575;left:437"><nobr><span class="ft30"><b>95</b></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:261"><nobr><span class="ft30"><b>Recall(%)</b></span></nobr></DIV>
<DIV style="position:absolute;top:477;left:98"><nobr><span class="ft30"><b>Precision(%)</b></span></nobr></DIV>
<DIV style="position:absolute;top:352;left:335"><nobr><span class="ft9">Contextual information</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:335"><nobr><span class="ft9">Mutual information</span></nobr></DIV>
<DIV style="position:absolute;top:401;left:335"><nobr><span class="ft14">Improved mutual<br>information</span></nobr></DIV>
<DIV style="position:absolute;top:615;left:87"><nobr><span class="ft31"><b>Fig. 1. Recall-precision graph for the three statistical<br>models.</b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:506"><nobr><span class="ft26"><b>Association Score&gt;1.0 (definite errors) </b></span></nobr></DIV>
<DIV style="position:absolute;top:134;left:735"><nobr><span class="ft5">   </span></nobr></DIV>
<DIV style="position:absolute;top:150;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:150;left:575"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:150;left:612"><nobr><span class="ft12">university (agricultural<br>university)</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:183;left:575"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:183;left:614"><nobr><span class="ft5">geology (geologic age)</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:575"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:614"><nobr><span class="ft5">plant (upland plant)     </span></nobr></DIV>
<DIV style="position:absolute;top:215;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:215;left:575"><nobr><span class="ft5">) </span></nobr></DIV>
<DIV style="position:absolute;top:215;left:614"><nobr><span class="ft5">sovereignty (sovereign state)</span></nobr></DIV>
<DIV style="position:absolute;top:248;left:506"><nobr><span class="ft31"><b>Association Score Between ­1.0 and 1.0<br>(borderline errors)</b></span></nobr></DIV>
<DIV style="position:absolute;top:264;left:617"><nobr><span class="ft5">    </span></nobr></DIV>
<DIV style="position:absolute;top:280;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:575"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:280;left:614"><nobr><span class="ft5">statistics (statistical data)</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:296;left:575"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:296;left:614"><nobr><span class="ft5">calamity (natural calamity)  </span></nobr></DIV>
<DIV style="position:absolute;top:312;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:575"><nobr><span class="ft5">) </span></nobr></DIV>
<DIV style="position:absolute;top:312;left:614"><nobr><span class="ft5">resources (manpower resources)</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:328;left:564"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:328;left:614"><nobr><span class="ft5">professor (associate professor)</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:564"><nobr><span class="ft5">)  </span></nobr></DIV>
<DIV style="position:absolute;top:345;left:614"><nobr><span class="ft5">poor (pauperization)</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:564"><nobr><span class="ft5">) </span></nobr></DIV>
<DIV style="position:absolute;top:361;left:614"><nobr><span class="ft5">fourteen (the 14</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:702"><nobr><span class="ft19">th</span></nobr></DIV>
<DIV style="position:absolute;top:361;left:709"><nobr><span class="ft5"> day)  </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:527"><nobr><span class="ft5">(</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:564"><nobr><span class="ft5">) </span></nobr></DIV>
<DIV style="position:absolute;top:377;left:614"><nobr><span class="ft5">twenty (twenty pieces)</span></nobr></DIV>
<DIV style="position:absolute;top:404;left:498"><nobr><span class="ft31"><b>Table 5. Simple words that are part of a longer<br>meaningful word</b></span></nobr></DIV>
<DIV style="position:absolute;top:130;left:252"><nobr><span class="ft26"><b>Avg Precision</b></span></nobr></DIV>
<DIV style="position:absolute;top:146;left:106"><nobr><span class="ft26"><b>Avg</b></span></nobr></DIV>
<DIV style="position:absolute;top:162;left:99"><nobr><span class="ft26"><b>Recall</b></span></nobr></DIV>
<DIV style="position:absolute;top:147;left:172"><nobr><span class="ft6"><i>Mutual</i></span></nobr></DIV>
<DIV style="position:absolute;top:163;left:159"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:147;left:245"><nobr><span class="ft6"><i>Improved Mutual</i></span></nobr></DIV>
<DIV style="position:absolute;top:163;left:260"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:147;left:364"><nobr><span class="ft6"><i>Contextual</i></span></nobr></DIV>
<DIV style="position:absolute;top:163;left:361"><nobr><span class="ft6"><i>Information</i></span></nobr></DIV>
<DIV style="position:absolute;top:185;left:104"><nobr><span class="ft26"><b>90%</b></span></nobr></DIV>
<DIV style="position:absolute;top:185;left:164"><nobr><span class="ft5">57% (1.0)</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:263"><nobr><span class="ft5">58% (-2.3)</span></nobr></DIV>
<DIV style="position:absolute;top:185;left:364"><nobr><span class="ft5">61% (-1.5)</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:104"><nobr><span class="ft26"><b>80%</b></span></nobr></DIV>
<DIV style="position:absolute;top:201;left:164"><nobr><span class="ft5">60% (3.8)</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:263"><nobr><span class="ft5">60% (-1.4)</span></nobr></DIV>
<DIV style="position:absolute;top:201;left:364"><nobr><span class="ft5">67% (-0.7)</span></nobr></DIV>
<DIV style="position:absolute;top:218;left:104"><nobr><span class="ft26"><b>70%</b></span></nobr></DIV>
<DIV style="position:absolute;top:217;left:164"><nobr><span class="ft5">59% (4.8)</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:263"><nobr><span class="ft5">60% (-1.0)</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:364"><nobr><span class="ft5">70% (-0.3)</span></nobr></DIV>
<DIV style="position:absolute;top:234;left:104"><nobr><span class="ft26"><b>60%</b></span></nobr></DIV>
<DIV style="position:absolute;top:233;left:164"><nobr><span class="ft5">60% (5.6)</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:263"><nobr><span class="ft5">63% (-0.6)</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:366"><nobr><span class="ft5">73% (0.0)</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:95"><nobr><span class="ft5">* Threshold values are given in parenthesis.</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:87"><nobr><span class="ft31"><b>Table 4. Average recall and average precision for the three<br>statistical formulas</b></span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">87</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft32{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="15007.png" alt="background image">
<DIV style="position:absolute;top:360;left:81"><nobr><span class="ft12">We divide the errors of commission (bigrams that are incorrectly<br>identified as words by the automatic segmentation) into 2 groups:<br>1.</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:91"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:392;left:101"><nobr><span class="ft12">Definite errors: bigrams with association scores above 1.0 but<br>are not words</span></nobr></DIV>
<DIV style="position:absolute;top:425;left:81"><nobr><span class="ft5">2.</span></nobr></DIV>
<DIV style="position:absolute;top:424;left:91"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:425;left:101"><nobr><span class="ft12">Borderline errors: bigrams with association scores between ­<br>1.0 and 1.0 and are not words</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:81"><nobr><span class="ft12">We also divide the errors of omission (bigrams that are words<br>but are not identified by the automatic segmentation) into 2<br>groups:<br>1.  Definite errors: bigrams with association scores below ­1.0</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:101"><nobr><span class="ft5">but are words</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:81"><nobr><span class="ft5">2.  Borderline errors: bigrams with association scores between ­</span></nobr></DIV>
<DIV style="position:absolute;top:561;left:101"><nobr><span class="ft5">1.0 and 1.0 and are words.</span></nobr></DIV>
<DIV style="position:absolute;top:587;left:81"><nobr><span class="ft4"><b>7.1</b></span></nobr></DIV>
<DIV style="position:absolute;top:586;left:104"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:587;left:114"><nobr><span class="ft4"><b>Errors of Commission</b></span></nobr></DIV>
<DIV style="position:absolute;top:607;left:81"><nobr><span class="ft5">Errors of commission can be divided into 2 types:</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:86"><nobr><span class="ft5">1.</span></nobr></DIV>
<DIV style="position:absolute;top:623;left:96"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:623;left:113"><nobr><span class="ft12">The bigram is a simple word that is part of a longer<br>meaningful word.</span></nobr></DIV>
<DIV style="position:absolute;top:656;left:86"><nobr><span class="ft5">2.</span></nobr></DIV>
<DIV style="position:absolute;top:655;left:96"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:656;left:113"><nobr><span class="ft12">The bigram is not a word (neither simple word nor<br>meaningful word).</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:81"><nobr><span class="ft12">Errors of the first type are illustrated in Table 5. The words<br>within parenthesis are actually meaningful words but segmented<br>as simple words (words on the left). The words lose part of the<br>meaning when segmented as simple words. These errors<br>occurred mostly with 3 or 4-character meaningful words.</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:81"><nobr><span class="ft12">Errors of the second type are illustrated in Table 6. Many of the<br>errors are caused by incorrectly linking a character with a<br>function word or pronoun. Some of the errors can easily be</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:477"><nobr><span class="ft12">removed by using a list of function words and pronouns to<br>identify these characters.</span></nobr></DIV>
<DIV style="position:absolute;top:161;left:477"><nobr><span class="ft4"><b>7.2</b></span></nobr></DIV>
<DIV style="position:absolute;top:160;left:499"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:161;left:510"><nobr><span class="ft4"><b>Errors of Omission</b></span></nobr></DIV>
<DIV style="position:absolute;top:181;left:477"><nobr><span class="ft12">Examples of <i>definite errors</i> of omission (bigrams with<br>association scores below ­1.0 but are words) are given in Table<br>7. Most of the errors are rare words and time words. Some are<br>ancient names, rare and unknown place names, as well as<br>technical terms. Since our corpus comprises general news<br>articles, these types of words are not frequent in the corpus. Time<br>words like dates usually have low association values because<br>they change everyday! These errors can be reduced by<br>incorporating a separate algorithm for recognizing them.</span></nobr></DIV>
<DIV style="position:absolute;top:333;left:477"><nobr><span class="ft5">The proportion of errors of the various types are given in Table 8.</span></nobr></DIV>
<DIV style="position:absolute;top:382;left:477"><nobr><span class="ft4"><b>8.</b></span></nobr></DIV>
<DIV style="position:absolute;top:381;left:490"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:382;left:501"><nobr><span class="ft4"><b>CONCLUSION</b></span></nobr></DIV>
<DIV style="position:absolute;top:402;left:477"><nobr><span class="ft13">A new statistical formula for identifying 2-character words in<br>Chinese text, called the <i>contextual information formula</i>, was<br>developed empirically using regression analysis. The focus was<br>on identifying meaningful words (including multi-word terms<br>and idioms) rather than simple words. The formula was found to<br>give significantly and substantially better results than the <i>mutual<br>information formula.</i></span></nobr></DIV>
<DIV style="position:absolute;top:523;left:477"><nobr><span class="ft12">Contextual information in the form of the frequency of characters<br>that are adjacent to the bigram being processed as well as the<br>weighted document frequency of the overlapping bigrams were<br>found to be significant factors for predicting the probablity that<br>the bigram constitutes a word. Local information (e.g. the<br>number of times the bigram occurs in the document being<br>segmented) and the position of the bigram in the sentence were<br>not found to be useful in determining words.</span></nobr></DIV>
<DIV style="position:absolute;top:658;left:477"><nobr><span class="ft12">Of the bigrams that the formula erroneously identified as words,<br>about 80% of them were actually simple words. Of the rest,<br>many involved incorrect linking with a function words. Of the<br>words that the formula failed to identify as words, more than a<br>third of them were rare words or time words. The proportion of<br>rare words increased as the threshold value used was lowered.<br>These rare words cannot be identified using statistical<br>techniques.</span></nobr></DIV>
<DIV style="position:absolute;top:794;left:477"><nobr><span class="ft5">This study investigated a purely statistical approach to text</span></nobr></DIV>
<DIV style="position:absolute;top:134;left:98"><nobr><span class="ft26"><b>Association Score between  -1.0 and -2.0</b></span></nobr></DIV>
<DIV style="position:absolute;top:150;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:150;left:152"><nobr><span class="ft5">the northern section of a construction project</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:152"><nobr><span class="ft5">fragments of  ancient books</span></nobr></DIV>
<DIV style="position:absolute;top:199;left:98"><nobr><span class="ft26"><b>Association Score  &lt; -2.0</b></span></nobr></DIV>
<DIV style="position:absolute;top:215;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:215;left:152"><nobr><span class="ft5">September</span></nobr></DIV>
<DIV style="position:absolute;top:231;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:231;left:152"><nobr><span class="ft5">3rd day</span></nobr></DIV>
<DIV style="position:absolute;top:247;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:247;left:152"><nobr><span class="ft5">(name of a district in China )</span></nobr></DIV>
<DIV style="position:absolute;top:264;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:264;left:152"><nobr><span class="ft5">(name of an institution)</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:120"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:280;left:152"><nobr><span class="ft5">the Book of Changes</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:98"><nobr><span class="ft31"><b>Table 7. 2-character words with association score<br>below -1.0</b></span></nobr></DIV>
<DIV style="position:absolute;top:853;left:151"><nobr><span class="ft32"><b>Errors of Commission<br></b>Association score &gt; 1.0</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:165"><nobr><span class="ft5">(No. of errors=34)</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:334"><nobr><span class="ft26"><b>Borderline Cases</b></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:305"><nobr><span class="ft5">Association score: ­1.0 to1.0</span></nobr></DIV>
<DIV style="position:absolute;top:885;left:332"><nobr><span class="ft5">(No. of cases: 210)</span></nobr></DIV>
<DIV style="position:absolute;top:853;left:557"><nobr><span class="ft26"><b>Errors of Omission</b></span></nobr></DIV>
<DIV style="position:absolute;top:869;left:546"><nobr><span class="ft5">Association score &lt; ­1.0</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:496"><nobr><span class="ft5">Association score:</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:513"><nobr><span class="ft5">­1.0 to ­2.0</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:496"><nobr><span class="ft5">(No. of errors=43)</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:633"><nobr><span class="ft5">Association score</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:663"><nobr><span class="ft5">&lt; ­2.0</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:631"><nobr><span class="ft5">(No. of errors=22)</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:159"><nobr><span class="ft5">Simple</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:162"><nobr><span class="ft5">words</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:161"><nobr><span class="ft5">82.3%</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:224"><nobr><span class="ft5">Not words</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:235"><nobr><span class="ft5">17.7%</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:301"><nobr><span class="ft5">Simple</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:303"><nobr><span class="ft5">words</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:302"><nobr><span class="ft5">55.2%</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:367"><nobr><span class="ft5">Not</span></nobr></DIV>
<DIV style="position:absolute;top:925;left:361"><nobr><span class="ft5">words</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:360"><nobr><span class="ft5">20.5%</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:415"><nobr><span class="ft12">Meaning-<br>ful words</span></nobr></DIV>
<DIV style="position:absolute;top:989;left:424"><nobr><span class="ft5">24.3%</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:488"><nobr><span class="ft5">Rare words</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:500"><nobr><span class="ft5">&amp; time</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:503"><nobr><span class="ft5">words</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:501"><nobr><span class="ft5">23.2%</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:568"><nobr><span class="ft5">Others</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:569"><nobr><span class="ft5">76.8%</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:623"><nobr><span class="ft5">Rare words</span></nobr></DIV>
<DIV style="position:absolute;top:979;left:635"><nobr><span class="ft5">&amp; time</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:638"><nobr><span class="ft5">words</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:636"><nobr><span class="ft5">63.6%</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:703"><nobr><span class="ft5">Others</span></nobr></DIV>
<DIV style="position:absolute;top:1028;left:704"><nobr><span class="ft5">36.4%</span></nobr></DIV>
<DIV style="position:absolute;top:1061;left:311"><nobr><span class="ft26"><b>Table 8. Proportion of errors of different types</b></span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">88</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="15008.png" alt="background image">
<DIV style="position:absolute;top:118;left:81"><nobr><span class="ft12">segmentation. The advantage of the statistical approach is that it<br>can be applied to any domain, provided that the document<br>collection is sufficiently large to provide frequency information.<br>A domain-specific dictionary of words is not required. In fact, the<br>statistical formula can be used to generate a shortlist of candidate<br>words for such a dictionary. On the other hand, the statistical<br>method cannot identify rare words and proper names. It is also<br>fooled by combinations of function words that occur frequently<br>and by function words that co-occur with other words.</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:81"><nobr><span class="ft12">It is well-known that a combination of methods is needed to give<br>the best segmentation results. The segmentation quality in this<br>study can be improved by using a list of function words and<br>segmenting the function words as single character words. A<br>dictionary of common and well-known names (including names<br>of persons, places, institutions, government bodies and classic<br>books) could be used by the system to identify proper names that<br>occur infrequently in the corpus.  Chang et al. [2] developed a<br>method for recognizing proper nouns using a dictionary of family<br>names in combination with a statistical method for identifying<br>the end of the name. An algorithm for identifying time and dates<br>would also be helpful. It is not clear whether syntactic processing<br>can be used to improve the segmentation results substantially.</span></nobr></DIV>
<DIV style="position:absolute;top:487;left:81"><nobr><span class="ft12">Our current work includes developing statistical formulas for<br>identifying 3 and 4-character words, as well as investigating<br>whether the statistical formula developed here can be used with<br>other corpora. The approach adopted in this study can also be<br>used to develop statistical models for identifying multi-word<br>terms in English text. It would be interesting to see whether the<br>regression model developed for English text is similar to the one<br>developed in this study for Chinese text. Frantzi, Ananiadou &amp;<br>Tsujii [7], using a different statistical approach, found that<br>contextual information could be used to improve the<br>identification of multi-word terms in English text.</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:81"><nobr><span class="ft4"><b>9.</b></span></nobr></DIV>
<DIV style="position:absolute;top:675;left:95"><nobr><span class="ft7"><b> </b></span></nobr></DIV>
<DIV style="position:absolute;top:676;left:105"><nobr><span class="ft4"><b>REFERENCES</b></span></nobr></DIV>
<DIV style="position:absolute;top:696;left:81"><nobr><span class="ft5">[1] Chang, C.-H., and Chen, C.-D. A study of integrating</span></nobr></DIV>
<DIV style="position:absolute;top:712;left:108"><nobr><span class="ft12">Chinese word segmentation and part-of-speech tagging.<br>Communications of COLIPS, 3, 1 (1993), 69-77.</span></nobr></DIV>
<DIV style="position:absolute;top:750;left:81"><nobr><span class="ft5">[2] Chang, J.-S., Chen, S.-D., Ker, S.-J., Chen, Y., and Liu, J.S.</span></nobr></DIV>
<DIV style="position:absolute;top:766;left:108"><nobr><span class="ft12">A multiple-corpus approach to recognition of proper names<br>in Chinese texts. Computer Processing of Chinese and<br>Oriental Languages, 8, 1 (June 1994), 75-85.</span></nobr></DIV>
<DIV style="position:absolute;top:821;left:81"><nobr><span class="ft5">[3] Church, K.W., and Hanks, P. Word association norms,</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:108"><nobr><span class="ft12">mutual information and lexicography. In Proceedings of the<br>27</span></nobr></DIV>
<DIV style="position:absolute;top:851;left:122"><nobr><span class="ft19">th</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:129"><nobr><span class="ft5"> Annual Meeting of the Association for Computational</span></nobr></DIV>
<DIV style="position:absolute;top:870;left:108"><nobr><span class="ft5">Linguistics (Vancouver, June 1989), 76-83.</span></nobr></DIV>
<DIV style="position:absolute;top:892;left:81"><nobr><span class="ft5">[4] Dai, J.C., and Lee, H.J. A generalized unification-based LR</span></nobr></DIV>
<DIV style="position:absolute;top:908;left:108"><nobr><span class="ft12">parser for Chinese. Computer Processing of Chinese and<br>Oriental Languages, 8, 1 (1994), 1-18.</span></nobr></DIV>
<DIV style="position:absolute;top:947;left:81"><nobr><span class="ft5">[5] Dai, Y. Developing a new statistical method for Chinese</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:108"><nobr><span class="ft5">text segmentation. (Master</span></nobr></DIV>
<DIV style="position:absolute;top:963;left:269"><nobr><span class="ft5">s thesis in preparation)</span></nobr></DIV>
<DIV style="position:absolute;top:985;left:81"><nobr><span class="ft5">[6] Damerau, F.J. Generating and evaluating domain-oriented</span></nobr></DIV>
<DIV style="position:absolute;top:1002;left:108"><nobr><span class="ft12">multi-word terms from texts. Information Processing &amp;<br>Management, 29, 4 (1993), 433-447.</span></nobr></DIV>
<DIV style="position:absolute;top:1040;left:81"><nobr><span class="ft5">[7] Frantzi, K.T., Ananiadou, S., and Tsujii, J. The C-</span></nobr></DIV>
<DIV style="position:absolute;top:1056;left:108"><nobr><span class="ft12">value/NC-value method of automatic recognition for multi-<br>word terms. In C. Nikolaou and C. Stephanidis (eds.),</span></nobr></DIV>
<DIV style="position:absolute;top:118;left:504"><nobr><span class="ft12">Research and Advanced Technology for Digital Libraries,<br>2</span></nobr></DIV>
<DIV style="position:absolute;top:132;left:510"><nobr><span class="ft19">nd</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:520"><nobr><span class="ft5"> European Conference, ECDL</span></nobr></DIV>
<DIV style="position:absolute;top:135;left:709"><nobr><span class="ft5">98 (Heraklion, Crete,</span></nobr></DIV>
<DIV style="position:absolute;top:151;left:504"><nobr><span class="ft5">September 1998), Springer-Verlag, 585-604.</span></nobr></DIV>
<DIV style="position:absolute;top:173;left:477"><nobr><span class="ft5">[8] Liang, N.Y. The knowledge of Chinese words segmentation</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:504"><nobr><span class="ft12">[in Chinese]. Journal of Chinese Information Processing, 4,<br>2 (1990), 42-49.</span></nobr></DIV>
<DIV style="position:absolute;top:228;left:477"><nobr><span class="ft5">[9] Liu, I.M. Descriptive-unit analysis of sentences: Toward a</span></nobr></DIV>
<DIV style="position:absolute;top:244;left:504"><nobr><span class="ft12">model natural language processing. Computer Processing of<br>Chinese &amp; Oriental Languages, 4, 4 (1990), 314-355.</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:477"><nobr><span class="ft5">[10] Liu, Y., Tan, Q., and Shen, X.K. Xin xi chu li yong xian dai</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:504"><nobr><span class="ft5">han yu fen ci gui fan ji zi dong fen ci fang fa [</span></nobr></DIV>
<DIV style="position:absolute;top:299;left:793"><nobr><span class="ft5">Modern</span></nobr></DIV>
<DIV style="position:absolute;top:315;left:504"><nobr><span class="ft12">Chinese Word Segmentation Rules and Automatic Word<br>Segmentation Methods for Information Processing</span></nobr></DIV>
<DIV style="position:absolute;top:331;left:797"><nobr><span class="ft5">]. Qing</span></nobr></DIV>
<DIV style="position:absolute;top:347;left:504"><nobr><span class="ft5">Hua University Press, Beijing, 1994.</span></nobr></DIV>
<DIV style="position:absolute;top:370;left:477"><nobr><span class="ft5">[11] Lua, K.T. Experiments on the use of bigram mutual</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:504"><nobr><span class="ft12">information in Chinese natural language processing.<br>Presented at the 1995 International Conference on Computer<br>Processing of Oriental Languages (ICCPOL) (Hawaii,<br>November 1995). Available: http://137.132.89.143/luakt/<br>publication.html</span></nobr></DIV>
<DIV style="position:absolute;top:473;left:477"><nobr><span class="ft5">[12] Lua, K.T. From character to word - An application of</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:504"><nobr><span class="ft12">information theory. Computer Processing of Chinese &amp;<br>Oriental Languages, 4, 4 (1990), 304-312.</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:477"><nobr><span class="ft5">[13] Lua, K.T., and Gan, G.W. An application of information</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:504"><nobr><span class="ft12">theory in Chinese word segmentation. Computer Processing<br>of Chinese &amp; Oriental Languages, 8, 1 (1994), 115-124.</span></nobr></DIV>
<DIV style="position:absolute;top:583;left:477"><nobr><span class="ft5">[14] Nie, J.Y., Hannan, M.L., and Jin, W.Y. Unknown word</span></nobr></DIV>
<DIV style="position:absolute;top:599;left:504"><nobr><span class="ft12">detection and segmentation of Chinese using statistical and<br>heuristic knowledge. Communications of COLIPS, 5, 1&amp;2<br>(1995), 47-57.</span></nobr></DIV>
<DIV style="position:absolute;top:653;left:477"><nobr><span class="ft5">[15] Ogawa, Y., and Matsuda, T. Overlapping statistical word</span></nobr></DIV>
<DIV style="position:absolute;top:670;left:504"><nobr><span class="ft12">indexing: A new indexing method for Japanese text. In<br>Proceedings of the 20th Annual International ACM SIGIR<br>Conference on Research and Development in Information<br>Retrieval (Philadelphia, July 1997), ACM, 226-234.</span></nobr></DIV>
<DIV style="position:absolute;top:741;left:477"><nobr><span class="ft5">[16] Sproat, R., and Shih, C.L. A statistical method for finding</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:504"><nobr><span class="ft12">word boundaries in Chinese text. Computer Processing of<br>Chinese &amp; Oriental Languages, 4, 4 (1990), 336-351.</span></nobr></DIV>
<DIV style="position:absolute;top:795;left:477"><nobr><span class="ft5">[17] Sproat, R., Shih, C., Gale, W., and Chang, N. A stochastic</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:504"><nobr><span class="ft12">finite-state word-segmentation algorithm for Chinese.<br>Computational Lingustics, 22, 3 (1996), 377-404.</span></nobr></DIV>
<DIV style="position:absolute;top:850;left:477"><nobr><span class="ft5">[18] Tung, C.-H., and Lee, H.-J. Identification of unknown words</span></nobr></DIV>
<DIV style="position:absolute;top:866;left:504"><nobr><span class="ft12">from a corpus. Computer Processing of Chinese and<br>Oriental Languages, 8 (Supplement, Dec. 1994), 131-145.</span></nobr></DIV>
<DIV style="position:absolute;top:905;left:477"><nobr><span class="ft5">[19] Wu, Z., and Tseng, G. ACTS: An automatic Chinese text</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:504"><nobr><span class="ft12">segmentation system for full text retrieval. Journal of the<br>American Society for Information Science, 46, 2 (1995), 83-<br>96.</span></nobr></DIV>
<DIV style="position:absolute;top:976;left:477"><nobr><span class="ft5">[20] Yeh, C.L., and Lee, H.J. Rule-based word identification for</span></nobr></DIV>
<DIV style="position:absolute;top:992;left:504"><nobr><span class="ft12">mandarin Chinese sentences: A unification approach.<br>Computer Processing of Chinese and Oriental Languages, 5,<br>2 (1991), 97-118.</span></nobr></DIV>
<DIV style="position:absolute;top:1138;left:450"><nobr><span class="ft11">89</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
