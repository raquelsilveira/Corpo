<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>D:\Paper\HTML\48</TITLE>
<META http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<META name="generator" content="pdftohtml 0.35beta">
<META name="date" content="2006-06-20T16:37:58+00:00">
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:25px;font-family:Times;color:#000000;}
	.ft1{font-size:11px;font-family:Times;color:#000000;}
	.ft2{font-size:16px;font-family:Times;color:#000000;}
	.ft3{font-size:13px;font-family:Times;color:#000000;}
	.ft4{font-size:7px;font-family:Times;color:#000000;}
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;font-family:Courier;color:#000000;}
	.ft7{font-size:11px;font-family:Times;color:#000000;}
	.ft8{font-size:14px;font-family:Helvetica;color:#000000;}
	.ft9{font-size:11px;line-height:15px;font-family:Times;color:#000000;}
	.ft10{font-size:11px;line-height:16px;font-family:Times;color:#000000;}
	.ft11{font-size:16px;line-height:24px;font-family:Times;color:#000000;}
	.ft12{font-size:11px;line-height:21px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="48001.png" alt="background image">
<DIV style="position:absolute;top:109;left:116"><nobr><span class="ft0"><b>Building a Research Library for the History of the Web </b></span></nobr></DIV>
<DIV style="position:absolute;top:144;left:81"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:145"><nobr><span class="ft2">William Y. Arms </span></nobr></DIV>
<DIV style="position:absolute;top:187;left:106"><nobr><span class="ft3">Computer Science Department </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:150"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:221;left:150"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:238;left:155"><nobr><span class="ft3">1-607-255-3046 </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:128"><nobr><span class="ft2">wya@cs.cornell.edu </span></nobr></DIV>
<DIV style="position:absolute;top:283;left:207"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:303;left:158"><nobr><span class="ft2">Blazej J. Kot </span></nobr></DIV>
<DIV style="position:absolute;top:324;left:112"><nobr><span class="ft3">Information Science Program </span></nobr></DIV>
<DIV style="position:absolute;top:341;left:150"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:358;left:150"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:376;left:155"><nobr><span class="ft3">1-607-255-4654 </span></nobr></DIV>
<DIV style="position:absolute;top:399;left:134"><nobr><span class="ft2">bjk45@cornell.edu </span></nobr></DIV>
<DIV style="position:absolute;top:166;left:415"><nobr><span class="ft2">Selcuk Aya </span></nobr></DIV>
<DIV style="position:absolute;top:187;left:358"><nobr><span class="ft3">Computer Science Department </span></nobr></DIV>
<DIV style="position:absolute;top:204;left:402"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:221;left:402"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:238;left:407"><nobr><span class="ft3">1-607-255-7316 </span></nobr></DIV>
<DIV style="position:absolute;top:262;left:377"><nobr><span class="ft2">ayas@cs.cornell.edu </span></nobr></DIV>
<DIV style="position:absolute;top:287;left:459"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:312;left:408"><nobr><span class="ft2">Ruth Mitchell </span></nobr></DIV>
<DIV style="position:absolute;top:333;left:387"><nobr><span class="ft3">Cornell Theory Center </span></nobr></DIV>
<DIV style="position:absolute;top:350;left:402"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:367;left:402"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:385;left:407"><nobr><span class="ft3">1-607-254-8689 </span></nobr></DIV>
<DIV style="position:absolute;top:408;left:368"><nobr><span class="ft2">mitchell@tc.cornell.edu</span></nobr></DIV>
<DIV style="position:absolute;top:166;left:654"><nobr><span class="ft2">Pavel Dmitriev </span></nobr></DIV>
<DIV style="position:absolute;top:191;left:610"><nobr><span class="ft3">Computer Science Department </span></nobr></DIV>
<DIV style="position:absolute;top:208;left:654"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:226;left:654"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:243;left:659"><nobr><span class="ft3">1-607-255-5431 </span></nobr></DIV>
<DIV style="position:absolute;top:266;left:617"><nobr><span class="ft2">dmitriev@cs.cornell.edu </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:711"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:317;left:666"><nobr><span class="ft2">Lucia Walle </span></nobr></DIV>
<DIV style="position:absolute;top:337;left:639"><nobr><span class="ft3">Cornell Theory Center </span></nobr></DIV>
<DIV style="position:absolute;top:355;left:654"><nobr><span class="ft3">Cornell University </span></nobr></DIV>
<DIV style="position:absolute;top:372;left:654"><nobr><span class="ft3">Ithaca, NY 14853 </span></nobr></DIV>
<DIV style="position:absolute;top:389;left:659"><nobr><span class="ft3">1-607-254-8775 </span></nobr></DIV>
<DIV style="position:absolute;top:413;left:628"><nobr><span class="ft2">lwalle@tc.cornell.edu </span></nobr></DIV>
<DIV style="position:absolute;top:438;left:711"><nobr><span class="ft2"> </span></nobr></DIV>
<DIV style="position:absolute;top:464;left:81"><nobr><span class="ft2">ABSTRACT</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:177"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:484;left:81"><nobr><span class="ft9">This paper describes the building of a research library for studying <br>the Web, especially research on how the structure and content of the <br>Web change over time. The library is particularly aimed at <br>supporting social scientists for whom the Web is both a fascinating <br>social phenomenon and a mirror on society. </span></nobr></DIV>
<DIV style="position:absolute;top:572;left:81"><nobr><span class="ft9">The library is built on the collections of the Internet Archive, which <br>has been preserving a crawl of the Web every two months since <br>1996. The technical challenges in organizing this data for research <br>fall into two categories: high-performance computing to transfer and <br>manage the very large amounts of data, and human-computer <br>interfaces that empower research by non-computer specialists. </span></nobr></DIV>
<DIV style="position:absolute;top:686;left:81"><nobr><span class="ft2">Categories and Subject Descriptors</span></nobr></DIV>
<DIV style="position:absolute;top:690;left:347"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:707;left:81"><nobr><span class="ft10">H.3.7 [Information Storage and Retrieval]: Digital Libraries ­ <br>collection, systems issues, user issues. J.4 [Social and Behavioral <br>Sciences]: sociology.  </span></nobr></DIV>
<DIV style="position:absolute;top:773;left:81"><nobr><span class="ft2">General Terms</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:195"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:794;left:81"><nobr><span class="ft9">Algorithms, Management, Measurement, Performance, Design, <br>Human Factors. </span></nobr></DIV>
<DIV style="position:absolute;top:844;left:81"><nobr><span class="ft2">Keywords</span></nobr></DIV>
<DIV style="position:absolute;top:865;left:81"><nobr><span class="ft9">history of the Web, digital libraries, computational social science, <br>Internet Archive. </span></nobr></DIV>
<DIV style="position:absolute;top:915;left:81"><nobr><span class="ft11">1.  BACKGROUND <br>1.1  Research in the History of the Web </span></nobr></DIV>
<DIV style="position:absolute;top:959;left:81"><nobr><span class="ft9">The Web is one of the most interesting artifacts of our time. For <br>social scientists, it is a subject of study both for itself and for the </span></nobr></DIV>
<DIV style="position:absolute;top:463;left:477"><nobr><span class="ft9">manner in which it illuminates contemporary social phenomena. Yet <br>a researcher who wishes to study the Web is faced with major <br>difficulties. <br>An obvious problem is that the Web is huge. Any study of the Web <br>as a whole must be prepared to analyze billions of pages and <br>hundreds of terabytes of data. Furthermore, the Web changes <br>continually. It is never possible to repeat a study on the actual Web <br>with quite the same data. Any snapshot of the whole Web requires a <br>crawl that will take several weeks to gather data. Because the size <br>and boundaries of the Web are ill defined, basic parameters are hard <br>to come by and it is almost impossible to generate random samples <br>for statistical purposes. <br>But the biggest problem that social scientists face in carrying out <br>Web research is historical: the desire to track activities across time. <br>The Web of today can be studied by direct Web crawling, or via <br>tools such as the Google Web API</span></nobr></DIV>
<DIV style="position:absolute;top:711;left:673"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:713;left:677"><nobr><span class="ft1">, while Amazon has recently </span></nobr></DIV>
<DIV style="position:absolute;top:729;left:477"><nobr><span class="ft9">made its older Alexa corpus commercially available for the <br>development of searching and related services</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:725"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:744;left:730"><nobr><span class="ft1">. However, the only </span></nobr></DIV>
<DIV style="position:absolute;top:760;left:477"><nobr><span class="ft9">collection that can be used for more general research into the history <br>of the Web is the Web collection of the Internet Archive</span></nobr></DIV>
<DIV style="position:absolute;top:774;left:772"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:777"><nobr><span class="ft1">.  </span></nobr></DIV>
<DIV style="position:absolute;top:802;left:477"><nobr><span class="ft2">1.2  The Internet Archive </span></nobr></DIV>
<DIV style="position:absolute;top:822;left:477"><nobr><span class="ft9">Everybody with an interest in the history of the Web must be <br>grateful to Brewster Kahle for his foresight in preserving the content <br>of the Web for future generations, through the not-for-profit Internet <br>Archive and through Alexa Internet, Inc., which he also founded.  </span></nobr></DIV>
<DIV style="position:absolute;top:920;left:477"><nobr><span class="ft1">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:940;left:477"><nobr><span class="ft4">1</span></nobr></DIV>
<DIV style="position:absolute;top:942;left:481"><nobr><span class="ft1"> The Google Web Search API allows a client to submit a limited </span></nobr></DIV>
<DIV style="position:absolute;top:958;left:488"><nobr><span class="ft9">number of search requests, using the SOAP and WSDL <br>standards. See: http://www.google.com/apis/. </span></nobr></DIV>
<DIV style="position:absolute;top:993;left:477"><nobr><span class="ft4">2</span></nobr></DIV>
<DIV style="position:absolute;top:995;left:481"><nobr><span class="ft1"> See http://websearch.alexa.com/welcome.html for the Alexa </span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:488"><nobr><span class="ft9">corpus made available by Amazon. This site also has a <br>description of the relationship between Alexa Internet and the <br>Internet Archive. </span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:477"><nobr><span class="ft4">3</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:481"><nobr><span class="ft1"> The Internet Archive's Web site is http://www.archive.org/.  </span></nobr></DIV>
<DIV style="position:absolute;top:1000;left:87"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:87"><nobr><span class="ft5">Copyright is held by the author/owner(s). </span></nobr></DIV>
<DIV style="position:absolute;top:1044;left:87"><nobr><span class="ft5">JCDL'06, June 11-15, 2006, Chapel Hill, North Carolina, USA. </span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:87"><nobr><span class="ft5">ACM 1-5959</span></nobr></DIV>
<DIV style="position:absolute;top:1066;left:149"><nobr><span class="ft1">3-354-9/06/0006.</span></nobr></DIV>
<DIV style="position:absolute;top:1067;left:240"><nobr><span class="ft5"> </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:452"><nobr><span class="ft7">95</span></nobr></DIV>
<DIV style="position:absolute;top:60;left:312"><nobr><span class="ft8"><i><b>Vannevar Bush Best Paper Candidate</b></i></span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:14px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="48002.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">The Internet Archive began to collect and preserve the Web in 1996. <br>With a few gaps in the early years, the collection has added a full <br>crawl of the Web every two months since then. Most but not all of <br>this data comes from the Alexa crawls. Statistics of the sizes of the <br>separate crawls are complicated by the fact that a single crawl may <br>contain several variants of the same URL, but in August 2005 the <br>total volume of data was 544 Terabytes (TB). This is the size of the <br>compressed data.  As discussed below, the overall compression ratio <br>is about 10:1, so that the total size of the collection is approximately <br>5 to 6 Petabytes uncompressed. Table 1 gives estimates of the size <br>of the individual crawls for each year. </span></nobr></DIV>
<DIV style="position:absolute;top:289;left:132"><nobr><span class="ft1">Table 1. Estimates of crawl sizes (compressed)  </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:152"><nobr><span class="ft1">Year </span></nobr></DIV>
<DIV style="position:absolute;top:314;left:213"><nobr><span class="ft1">Web pages </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:193"><nobr><span class="ft1">(TB per crawl) </span></nobr></DIV>
<DIV style="position:absolute;top:314;left:313"><nobr><span class="ft1">Metadata </span></nobr></DIV>
<DIV style="position:absolute;top:330;left:284"><nobr><span class="ft1">(TB per crawl) </span></nobr></DIV>
<DIV style="position:absolute;top:357;left:152"><nobr><span class="ft1">1996 1 </span></nobr></DIV>
<DIV style="position:absolute;top:357;left:346"><nobr><span class="ft1">0.2 </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:152"><nobr><span class="ft1">1997 2 </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:346"><nobr><span class="ft1">0.4 </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:152"><nobr><span class="ft1">1998 3 </span></nobr></DIV>
<DIV style="position:absolute;top:407;left:346"><nobr><span class="ft1">0.6 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:152"><nobr><span class="ft1">1999 4 </span></nobr></DIV>
<DIV style="position:absolute;top:432;left:346"><nobr><span class="ft1">0.8 </span></nobr></DIV>
<DIV style="position:absolute;top:458;left:152"><nobr><span class="ft1">2000 10 1.2 </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:152"><nobr><span class="ft1">2001 15  2 </span></nobr></DIV>
<DIV style="position:absolute;top:508;left:152"><nobr><span class="ft1">2002 25  3 </span></nobr></DIV>
<DIV style="position:absolute;top:534;left:152"><nobr><span class="ft1">2003 30  4 </span></nobr></DIV>
<DIV style="position:absolute;top:559;left:152"><nobr><span class="ft1">2004 45  6 </span></nobr></DIV>
<DIV style="position:absolute;top:584;left:152"><nobr><span class="ft1">2005 60 10 </span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft9"> <br>We are working with the Internet Archive to build a research library <br>based on this collection. In summer 2005, we began work on the <br>system that is being used to transfer a major subset of the data to <br>Cornell and to organize it for researchers, with a particular emphasis <br>on supporting social science research. This paper describes the <br>technical design, performance testing, and progress in <br>implementation. <br>The overall goals of the library and plans for its use in research are <br>described in a separate paper [1].  </span></nobr></DIV>
<DIV style="position:absolute;top:782;left:81"><nobr><span class="ft2">1.3  User Studies </span></nobr></DIV>
<DIV style="position:absolute;top:803;left:81"><nobr><span class="ft9">In building any library, the objective is to organize the collections <br>and services so that they provide the greatest range of opportunities <br>for users, both now and in the future. Inevitably the design is a <br>trade-off between predictions of what users will find helpful and the <br>practicalities of building and maintaining the library. This trade-off <br>is particularly important for a library of the whole Web because of <br>the computing challenges of managing very large amounts of data. <br>Therefore, the design of the library began with interviews of <br>potential users to identify how the collections might be organized to <br>be most valuable to them. Two users studies were carried out, with <br>sociologists and with computer science researchers. </span></nobr></DIV>
<DIV style="position:absolute;top:992;left:81"><nobr><span class="ft13"><i>1.3.1  Sociology </i></span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:81"><nobr><span class="ft9">In fall 2005, Cornell received support from the National Science <br>Foundation's Next Generation Cybertools program for a project that <br>combines sociology research with continuing development of the </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft1">Web library</span></nobr></DIV>
<DIV style="position:absolute;top:107;left:544"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:548"><nobr><span class="ft1">. In this project, the specific areas of research are </span></nobr></DIV>
<DIV style="position:absolute;top:124;left:477"><nobr><span class="ft9">diffusion of ideas, including polarization of opinions and the spread <br>of urban legends. Conventionally, sociologists have studied such <br>phenomena by analysis of small surveys with hand-coded data. One <br>aim of the project is to develop a new methodology for such <br>research built around very large-scale collections of Web data, with <br>automated tools used to extract, encode and analyze the data.  <br>Social science researchers identified a number of specific studies <br>that they would like to carry out using historical Web data. Many of <br>the studies have the same general structure: (a) extract a subset of <br>the Web for detailed analysis, (b) encode selected attributes of the <br>pages in that subset, (c) repeat for the corresponding subsets at <br>several different dates, (d) analyze the changes over time.  <br>The criteria by which a portion of the Web is chosen for analysis are <br>extremely varied. Some desirable criteria are impossible with <br>today's computing, e.g., they require understanding of the content of <br>a page. However, simple criteria such as domain names provide a <br>good starting point for many purposes, particularly when combined <br>with focused Web crawling to refine the subsets for analysis. Once a <br>subset has been extracted, social science researchers want to analyze <br>the text, for which full text indexes are important. They also wish to <br>analyze the structure of links between pages for the social <br>relationship that they represent.  <br>Such research requires interdisciplinary efforts by computer <br>scientists and social scientists. Some of the analysis tools already <br>exist, e.g., using full text indexes of Web pages to trace the <br>movement of individuals. Others tools are themselves subjects of <br>computer science research in natural language processing and <br>machine learning, e.g., to analyze the text of Web pages for <br>sentiments, opinions, and other features of interest to social <br>scientists.  </span></nobr></DIV>
<DIV style="position:absolute;top:627;left:477"><nobr><span class="ft13"><i>1.3.2  Computer Science </i></span></nobr></DIV>
<DIV style="position:absolute;top:646;left:477"><nobr><span class="ft9">Ten computer scientists who carry out research on the Web <br>contributed to the user studies. Their research areas include the <br>structure and evolution of the Web, data mining, digital libraries, <br>machine learning, and natural language processing. Most of their <br>interest focuses on the textual content of Web pages and the <br>structure of the Web as revealed by the graph of links between <br>pages. Several of the researchers commented that they expend <br>ninety percent of their effort gathering test data; even then they have <br>difficulty in determining how robust the results are across time. <br>A fundamental tool for such research is the Web graph of links <br>between pages. Studies of the graph are very important in <br>understanding the structure of the Web, and the graph is the basis of <br>practical tools such as PageRank [3] or Hubs and Authorities [9]. <br>Despite its importance, there have been few studies that have looked <br>at changes in the Web graph over time. Many of the classical studies <br>of the Web graph were based on early AltaVista crawls and have <br>never been repeated. Algorithmic research needs graphs of at least <br>one billion pages, preferably stored in the main memory of a single <br>computer. <br>For textual research on the Web there are two additional <br>requirements. The first is snapshots that are repeated across time </span></nobr></DIV>
<DIV style="position:absolute;top:1011;left:477"><nobr><span class="ft1">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:1031;left:477"><nobr><span class="ft4">4</span></nobr></DIV>
<DIV style="position:absolute;top:1033;left:481"><nobr><span class="ft1"> Michael Macy (principal investigator), et al., &quot;Very Large Semi-</span></nobr></DIV>
<DIV style="position:absolute;top:1049;left:488"><nobr><span class="ft9">Structured Datasets for Social Science Research&quot;. NSF grant <br>SES-0537606. http://www.infosci.cornell.edu/SIN/cybertools/ </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:452"><nobr><span class="ft7">96</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="48003.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">that can be used for burst analysis, and other time based research. <br>The second is full text indexes of substantial numbers of Web pages. <br>Focused Web crawling is of particular importance in digital libraries <br>research. Part of the original motivation for developing this library <br>was an interest in automatic selection of library materials from the <br>Web [2, 10]. Using the actual Web for research in focused crawling <br>is technically difficult and the results are often hard to interpret <br>since no experiment can ever be repeated with exactly the same <br>data. </span></nobr></DIV>
<DIV style="position:absolute;top:267;left:81"><nobr><span class="ft11">2.  ARCHITECTURE  <br>2.1  Data Management  </span></nobr></DIV>
<DIV style="position:absolute;top:311;left:81"><nobr><span class="ft9">The Internet Archive uses highly compressed file formats developed <br>in conjunction with Alexa Internet. Compressed Web pages are <br>packed together in large files using the ARC format [4]. The pages <br>in each ARC file are in essentially random order, usually the <br>sequence in which the Web crawler originally captured them. Every <br>ARC file has an associated DAT file, which contains metadata for <br>the pages including URL, IP address, crawl date and time, and <br>hyperlinks from the page. The files are compressed with gzip. Ten <br>years ago the decision was made that ARC files should be <br>approximately 100 MB, which seemed big at the time, but this size <br>is now too small for efficiency and will need to be increased. The <br>sizes of the DAT files depend on the number of pages in the <br>associated ARC files, but average about 15 MB. The compression <br>ratios also vary widely. The ratio is more than 20:1 for text files but <br>close to 1:1 for files that are already compressed efficiently, such as <br>videos. The overall ratio for ARC files is about 10:1. </span></nobr></DIV>
<DIV style="position:absolute;top:574;left:81"><nobr><span class="ft13"><i>2.1.1  The Database </i></span></nobr></DIV>
<DIV style="position:absolute;top:593;left:81"><nobr><span class="ft9">The Cornell Web library uses a relational database to store metadata <br>about the Web pages and a separate Page Store to store the actual <br>pages. In addition, the unprocessed ARC and DAT files received <br>from the Internet Archive are copied to a tape archive. In choosing a <br>relational database, we considered but rejected two approaches that <br>have been successful in related applications.  <br>The first option was to use a modern digital library repository with <br>support for rich data models, such as XML and RDF, and search <br>services that support semi-structured data, such as XQuery. Such <br>capabilities are appealing, but we know of no repository system that <br>can manage tens of billions of objects. The scale of the Web <br>precludes such an approach. <br>The second option was to follow the model of production services <br>for the Web, such as Google [7] and Yahoo. They provide low cost <br>processing and data storage by spreading their systems across very <br>large numbers of small, commodity computers used as servers. This <br>is the approach used by the Internet Archive to store its collections <br>and for its very popular Wayback Machine</span></nobr></DIV>
<DIV style="position:absolute;top:872;left:332"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:874;left:337"><nobr><span class="ft1">. We rejected this </span></nobr></DIV>
<DIV style="position:absolute;top:890;left:81"><nobr><span class="ft9">architecture for a research library for two principal reasons: (a) there <br>are many algorithmic computations on large datasets where a single <br>large computer is intrinsically more efficient than a distributed <br>cluster of smaller machines, and (b) even when the research can be <br>done effectively, clusters of computers are more difficult to program <br>by researchers who are carrying out Web-scale research. As an <br>example, each server at the Internet Archive has an index of the files <br>stored on it, but there is only a very limited central index. The <br>Wayback Machine allows a user to retrieve all the pages in the </span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:81"><nobr><span class="ft1">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:81"><nobr><span class="ft4">5</span></nobr></DIV>
<DIV style="position:absolute;top:1065;left:86"><nobr><span class="ft1"> The Wayback Machine is accessible at http://www.archive.org/. </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft9">entire collection that have a given URL. It relies on a protocol in <br>which an identifier is broadcast and each server responds with a list <br>of matches. This is very efficient for this specific purpose, but it <br>would be extremely difficult to extract the flexible subsets required <br>by social science researchers with this organization of data.  <br>A relational database has many advantages for the Web library and <br>one major disadvantage. Foremost among the advantages is <br>scalability. Commercial relational database systems are highly <br>optimized for storing, loading, indexing, extracting, backing-up, and <br>restoring huge volumes of data. Usability is another important <br>advantage. A relational schema provides a single image of the <br>collection, expressed in a manner that is familiar to many <br>researchers. The disadvantage is a loss of flexibility. The design and <br>implementation of the database attempt to reconcile the expected <br>uses that will be made of the library against scalability constraints, <br>but it will be difficult to make major changes to the schema without <br>rebuilding the entire database.  <br>The actual Web pages are stored in a separate Page Store. At the <br>Internet Archive, if two Web pages are identical they are stored <br>twice. With the new Page Store, duplicate pages are stored only <br>once. Rather surprisingly, there is as yet very little data about how <br>many pages remain unchanged between crawls, but we expect that <br>elimination of duplicates will save significant online storage, <br>especially with large audio and video files. <br>The Page Store is implemented as a set of compressed files, one file <br>for each page received in the ARC files. Since many pages on the <br>Web do not change between crawls, the Preload subsystem checks <br>for content duplicates using an MD5 check sum of the content. <br>Thus, a copy of the content is stored only once however many pages <br>have that content. In order to guarantee fast access to the stored <br>content, each page's content is compressed individually. <br>The architecture of the Page Store allows decisions to be made <br>about which pages to store online at any given time. For example, <br>the library might decide not to store large audio and video files <br>online. While all metadata will be online at all times, an individual <br>Web page could be accessed from the online Page Store, the off-line <br>tape archive, or over the Internet from the Internet Archive.    </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:477"><nobr><span class="ft2">2.2  Equipment </span></nobr></DIV>
<DIV style="position:absolute;top:749;left:477"><nobr><span class="ft9">The library is housed at the Cornell Theory Center, which is the <br>university's high-performance computing center. The choice of <br>equipment and the use of a relational database were closely related <br>decisions. The Theory Center has expertise in distributed cluster <br>computing, but because of the very high data rates, a symmetric <br>multi-processor configuration was chosen instead. Figure 1 shows <br>part of the configuration of the central computer. </span></nobr></DIV>
<DIV style="position:absolute;top:1063;left:508"><nobr><span class="ft1">Figure 1. Configuration of the main computer system </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:452"><nobr><span class="ft7">97</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="48004.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">The system is shared with another data-intensive program of <br>research, the analysis of data from the Arecibo radio telescope in <br>Puerto Rico. Each group has use of a dedicated Unisys ES7000/430 <br>server, with 16 Itanium2 processors running at 1.5 Gigahertz.  The <br>memory can be shared between the servers, but in practice each <br>project has sole use of 32 GB. Each server has a RAID disk <br>subsystem attached via a dual-ported fiber-channel. The operating <br>system is Microsoft Windows Server 2003. <br>For the Web library the disk subsystem provides an initial 45 TB of <br>disk storage. We plan to extend the capacity to 240 TB by 2007. <br>There are no technical barriers to adding additional fiber channels <br>and disk capacity.  In the longer-term, disk prices are falling faster <br>than the growth in the size of Web crawls, which gives confidence <br>that the library will be able to keep up with the growth of the Web. <br>By using a symmetric multi-processor configuration with a high <br>performance disk subsystem, we are able to balance processing and <br>disk access requirements. Since the data sets are local to the system <br>on which the database is located, the system can perform bulk-<br>loading tasks without incurring any networking penalties. <br>The large real memory is an added attraction of this configuration. It <br>allows researchers to carry out substantial computation entirely in <br>memory. For instance, it is possible to process a Web graph of one <br>billion pages within memory.  </span></nobr></DIV>
<DIV style="position:absolute;top:501;left:81"><nobr><span class="ft2">2.3  The Human Interface </span></nobr></DIV>
<DIV style="position:absolute;top:521;left:81"><nobr><span class="ft9">The design of the human interface is perhaps the most challenging <br>aspect of developing the library. The social science research groups <br>that we are working with have the technical skills to write scripts <br>and simple programs. Many are experts in statistical calculations. <br>But they should not be expected to write large or complex computer <br>programs. The current design supports three categories of users. </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:91"><nobr><span class="ft1">  The  Basic Access Service provides a Web Services API that </span></nobr></DIV>
<DIV style="position:absolute;top:638;left:92"><nobr><span class="ft9">allows a client to access pages in the collection by any metadata <br>that is indexed in the database, e.g., by URL and date. The Retro <br>Browser, which is described below, uses this API to allow a user <br>to browse the collection as it was at a certain date. </span></nobr></DIV>
<DIV style="position:absolute;top:707;left:91"><nobr><span class="ft1">  The  Subset Extraction Service supports users who wish to </span></nobr></DIV>
<DIV style="position:absolute;top:723;left:92"><nobr><span class="ft9">download sets of partially analyzed data to their own computers <br>for further analysis. A Web form is provided to define a subset of <br>the data (e.g., by date, URL, domain, etc.), extract subsets of the <br>collection, and store them as virtual views in the database. Sets of <br>analysis tools, many of which are already under development, can <br>be applied to the subset and the results downloaded to a client <br>computer. </span></nobr></DIV>
<DIV style="position:absolute;top:840;left:91"><nobr><span class="ft1">  Technically advanced users can be authorized to run their own </span></nobr></DIV>
<DIV style="position:absolute;top:856;left:92"><nobr><span class="ft1">programs on the central computer. </span></nobr></DIV>
<DIV style="position:absolute;top:878;left:81"><nobr><span class="ft9">To support the Basic Access Service and the Subset Extraction <br>Service, we provide a dedicated Web server, which is housed next <br>to the main system. </span></nobr></DIV>
<DIV style="position:absolute;top:935;left:81"><nobr><span class="ft2">3.  SCALABILITY EXPERIMENTS  </span></nobr></DIV>
<DIV style="position:absolute;top:955;left:81"><nobr><span class="ft9">Although the library has been generously funded by the National <br>Science Foundation, we do not yet have sufficient capacity to <br>download and mount online the entire Web collection of the Internet <br>Archive. This is the long term goal, but during the initial phase, care <br>has been taken to balance the several parts of the system: online <br>storage, network bandwidth, processing of the incoming data, <br>database, performance, and the need to archive, back-up, and restore </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft9">the data. In spring 2005, several undergraduate and masters students <br>carried out independent projects to estimate sizes and processing <br>requirements</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:545"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:550"><nobr><span class="ft1">. </span></nobr></DIV>
<DIV style="position:absolute;top:162;left:477"><nobr><span class="ft9">To test database performance before large amounts of actual data <br>were available, we used the R-MAT algorithm to generate a <br>synthetic graph with properties similar to the Web graph  [5]. This <br>test graph has one billion nodes with more than seven billion links, <br>and domain names generated according to their distribution on the <br>real Web [11]. <br>Based on these benchmarks, the decision was made to install a 100 <br>Mb/sec network connection to the Internet Archive and to load data <br>at a sustained rate of 250 GB/day, beginning January 2006. This rate <br>will enable the library to acquire and mount online by the end of <br>2007 a complete crawl of the Web for each year since 1996. This <br>phase will require approximately 240 TB of disk storage. Note that <br>the disk requirement differs from the estimates of raw data shown in <br>Table 1. The database with its indexes is less highly compressed <br>than the raw data, but savings are made in the storage of duplicate <br>data, both in the database and the Page Store. <br>During fall 2005, first generation software was written for (a) the <br>data flow system that brings data to the library, and (b) the user API <br>and tool sets. They are described in the next two sections. </span></nobr></DIV>
<DIV style="position:absolute;top:485;left:477"><nobr><span class="ft2">4.  DATA FLOW </span></nobr></DIV>
<DIV style="position:absolute;top:505;left:477"><nobr><span class="ft9">Figure 2 shows the flow of data into the library. When ARC and <br>DAT files are received from the Internet Archive, the first step is to <br>store them in the tape archive. The Preload system then unpacks the <br>raw data, extracts metadata, and prepares batch files for loading into <br>the database and Page Store.  </span></nobr></DIV>
<DIV style="position:absolute;top:796;left:549"><nobr><span class="ft1"> Figure 2. Flow of data into the library </span></nobr></DIV>
<DIV style="position:absolute;top:818;left:477"><nobr><span class="ft9">Figure 2 does not show the data tracking system. This is a major <br>subsystem that manages the data transfers, monitors all errors, and <br>tracks the tens of millions of files within the library.  </span></nobr></DIV>
<DIV style="position:absolute;top:875;left:477"><nobr><span class="ft2">4.1  Networking </span></nobr></DIV>
<DIV style="position:absolute;top:895;left:477"><nobr><span class="ft9">Internet2 is used to transfer data from the Internet Archive in San <br>Francisco, California to Cornell University in Ithaca, New York. For <br>this purpose, a 100Mbit/sec link has been established from the <br>Internet Archive to Internet2. Both Cornell and the Internet Archive <br>have internal networks with 1 Gbit/sec or greater performance.    <br>In the future, the National LambdaRail and the TeraGrid are <br>intriguing possibilities. These new networks have the capacity to go </span></nobr></DIV>
<DIV style="position:absolute;top:1021;left:477"><nobr><span class="ft1">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:1041;left:477"><nobr><span class="ft4">6</span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:481"><nobr><span class="ft1"> These student reports are available at </span></nobr></DIV>
<DIV style="position:absolute;top:1059;left:488"><nobr><span class="ft1">http://www.infosci.cornell.edu/SIN/WebLib/papers.html. </span></nobr></DIV>
<DIV style="position:absolute;top:683;left:760"><nobr><span class="ft1">User tools</span></nobr></DIV>
<DIV style="position:absolute;top:683;left:535"><nobr><span class="ft1">Preload system</span></nobr></DIV>
<DIV style="position:absolute;top:746;left:704"><nobr><span class="ft1">Database</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:660"><nobr><span class="ft1">Page store</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:532"><nobr><span class="ft1">Tape</span></nobr></DIV>
<DIV style="position:absolute;top:771;left:526"><nobr><span class="ft1">archive</span></nobr></DIV>
<DIV style="position:absolute;top:602;left:490"><nobr><span class="ft1">Internet 2</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:452"><nobr><span class="ft7">98</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="48005.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">beyond bulk data transfer and support genuine distributed <br>processing between the Web library and the Internet Archive.  For <br>example, if large audio and video files are not stored online, an <br>application could use the TeraGrid to retrieve individual large files <br>from the Internet Archive on demand.  <br>At the end of December 2005, a series of experiments were run to <br>measure the sustained throughput of multi-threaded FTP transfers <br>over Internet2, using Transport Layer Security. These measurements <br>showed transfer rates of 280 GB per day before system tuning, or <br>rather better than 30 percent of the theoretical maximum throughput <br>of the link to the Internet Archive. This is sufficient for the planned <br>rate of 250 GB per day. If greater bandwidth proves necessary, the <br>link from the Internet Archive to Internet2 can be upgraded to <br>500Mbps inexpensively, while the Cornell Theory Center will soon <br>have enormous bandwidth available via the TeraGrid.  </span></nobr></DIV>
<DIV style="position:absolute;top:362;left:81"><nobr><span class="ft2">4.2   Preload Subsystem </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:81"><nobr><span class="ft9">The Preload subsystem takes incoming ARC and DAT files, <br>uncompresses them, parses them to extract metadata, and generates <br>two types of output files: metadata for loading into the database and <br>the actual content of the Web pages to be stored in the Page Store. <br>Metadata for loading into the database is output in the form of 40GB <br>text files, a separate file for every database table.  <br>To satisfy the speed and flexibility requirements, the Preload system <br>is designed to run as a set of independent single-thread processes, <br>avoiding all inter-process communication and locking over input or <br>output data. Likewise, each process writes its own output files. This <br>design allows for easy configuration of the system to run a required <br>number of processes on a given number of processors, on one or <br>more machines. To determine each process's input, input files are <br>partitioned by the first k bits of the MD5 hash sum of the filename, <br>where 2k is the total number of processes in the system. The design <br>of the subsystem does not require the corresponding ARC and DAT <br>files to be processed together. <br>A series of experiments were run to test the performance of the <br>Preload system, using the metadata from the synthetic Web graph. <br>The experiments used 1, 2, 4 and 8 processors, with the data <br>partitioned into 16 parts, according to the first 4 bits of the hash <br>sum. Separate experiments were made for ARC and DAT files. <br>Figure 3 shows results for the ARC files. The x-axis shows the <br>number of CPUs used, and the y-axis shows the throughput in <br>KB/sec. The white bar shows throughput per processor and the <br>shaded bar shows total throughput. Adding more processors slightly <br>decreases the throughput per processor due to contention for random <br>disk accesses. The total throughput increases steadily up to four <br>processors. After that, disk contention becomes too high, and the <br>throughput actually declines. The results for DAT files are similar, <br>with the total throughput flattening after four processors. <br>From these experiments, we conclude that four processors are <br>optimal. The corresponding throughputs are 73 MB/sec (about 6 <br>TB/day) for ARC files, and 12 MB/sec  (about 1 TB/day) for DAT <br>files.  <br>When metadata from the DAT files is uncompressed its size <br>increases by a factor of about 11:1. Fortunately, much of this data is </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft12">duplicated. For example, a given URL may occur many times in a <br>crawl and be replicated in many crawls. Therefore duplicate <br>elimination has been a major consideration in refining the database <br>design. <br> </span></nobr></DIV>
<DIV style="position:absolute;top:487;left:509"><nobr><span class="ft1">Figure 3. Performance of Preload system (ARC files)   </span></nobr></DIV>
<DIV style="position:absolute;top:509;left:477"><nobr><span class="ft9">Note that during the above experiments no other processes were <br>running on the system. The overall performance will be lower when <br>the Preload system is run in production mode at the same time as <br>other subsystems. Also, during the preliminary phase, only text and <br>HTML files were fully analyzed to extract links and anchor text. <br>Processing of other file formats will be added in the future. Some of <br>these formats will be computationally intensive, e.g., PDF files. </span></nobr></DIV>
<DIV style="position:absolute;top:629;left:477"><nobr><span class="ft2">4.3  Database Design </span></nobr></DIV>
<DIV style="position:absolute;top:650;left:477"><nobr><span class="ft12">The relational database uses Microsoft SQL Server 2000. Three <br>important goals when designing the database schema and deciding <br>how to load the data to the database were: (a) minimize the storage <br>requirements, (b) maximize the load throughput, and (c) support <br>efficient logging, backup, and restore. <br>Conceptually, for each crawl, the database stores metadata about <br>each page (e.g., information about the content of the page, URL of <br>the page) and about the links between them (including anchor text <br>and text surrounding the anchor text). However, to avoid storing <br>redundant data, the schema is denormalized. The denormalized <br>schema is shown below in Figure 4. Information about URL and <br>domain names is stored in a look-up table, since the same URLs and <br>domain names appear many times in a single crawl and across <br>crawls. For similar reasons, anchor text, text surrounding the anchor <br>text, and information about page content are stored in the look-up <br>tables Dictionary and Page Content respectively, as shown in the <br>schema in Figure 4. To make the loading of the data faster, separate <br>tables for each of Page, Page Content and Link are created for each <br>crawl while the other tables (e.g., the URL table) are shared among <br>crawls. <br> </span></nobr></DIV>
<DIV style="position:absolute;top:228;left:488"><nobr><span class="ft3">80</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:488"><nobr><span class="ft3">70</span></nobr></DIV>
<DIV style="position:absolute;top:280;left:488"><nobr><span class="ft3">60</span></nobr></DIV>
<DIV style="position:absolute;top:306;left:488"><nobr><span class="ft3">50</span></nobr></DIV>
<DIV style="position:absolute;top:332;left:488"><nobr><span class="ft3">40</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:488"><nobr><span class="ft3">30</span></nobr></DIV>
<DIV style="position:absolute;top:384;left:488"><nobr><span class="ft3">20</span></nobr></DIV>
<DIV style="position:absolute;top:409;left:488"><nobr><span class="ft3">10</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:495"><nobr><span class="ft3">Throughput (MB/s)</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:584"><nobr><span class="ft3">1</span></nobr></DIV>
<DIV style="position:absolute;top:449;left:638"><nobr><span class="ft3">    2                4                 8</span></nobr></DIV>
<DIV style="position:absolute;top:463;left:630"><nobr><span class="ft3">Number of CPUs</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:452"><nobr><span class="ft7">99</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft14{font-size:12px;font-family:Times;color:#000000;}
-->
</STYLE>
<IMG width="918" height="1188" src="48006.png" alt="background image">
<DIV style="position:absolute;top:266;left:81"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:288;left:112"><nobr><span class="ft1">Figure 4. Database design: the de-normalized schema   </span></nobr></DIV>
<DIV style="position:absolute;top:310;left:81"><nobr><span class="ft9">The Preload subsystem outputs separate files conforming to the <br>schema described above and these files are bulk loaded into the <br>database. There are many parameters that affect the bulk load <br>performance. These parameters include: batch size, file size, degree <br>of parallelism, and interaction with the logging, backup and <br>recovery system. The synthetic Web data was used to understand <br>how these parameters affect the loading performance and to tune <br>them. The first sets of experiments were used to determine the <br>optimal file size for bulk loading and the number of CPUs used. In <br>these experiments, default, recovery and backup mechanisms were <br>used [6]. <br>The results of the first set of experiments indicate that it is optimal <br>to load each file as a single batch; a file size of 40GB and 4 CPUs <br>gave the best performance, and around 800GB could be loaded in <br>one day. However, the experiments to determine the file size and <br>degree of parallelism showed significant variability. Using the <br>default logging provided by MS SQL Server 2000, checkpointing <br>and nightly backups were consuming enormous resources. They <br>interfered with the bulk loading process and are a probable cause of <br>the variance seen in the performance benchmarks. <br>Two observations were made to overcome the performance penalty. <br>First, data is append-only while being bulk loaded and is read-only <br>after the bulk load is complete; logging, recovery and backup <br>mechanisms can be customized to increase the performance and <br>decrease the variance in loading times. Second, tables in the schema <br>can reside on different disks and thus can be written in parallel. <br>Following these two observations, in the current design each table <br>can be put onto a separate disk as shown in Figure 5. Moreover, <br>Page, Page Content and Links information for each crawl are put <br>into separate files. This partitioning according to crawls is easy in <br>MS SQL Server as separate tables for each of Page, Page Content <br>and Link are created for each crawl.  <br>The database load subsystem is divided into two programs: a high-<br>level program that organizes the processes and a low level program <br>that runs separate loads in parallel. The workflow for loading each <br>table in each crawl consists of five major steps: (a) Get the files <br>produced by the Preload subsystem for the current table in the <br>current crawl and write the relevant log information to an <br>administrative database; commit the transaction writing the log <br>information. (b) Write files of the table to the disk corresponding to <br>the current table via the low level program; files corresponding to <br>different tables can be written in parallel. (c) Create the necessary <br>indexes. (d) Back-up the newly written data. (e) Write to the log the <br>relevant information to indicate that processing of the files for the <br>current table in the current crawl is complete and commit the <br>transaction writing the log information. In MS SQL Server 2000, <br>backups and index creation are all atomic.  </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft1"> </span></nobr></DIV>
<DIV style="position:absolute;top:393;left:508"><nobr><span class="ft1">Figure 5. Database design: organization of file system </span></nobr></DIV>
<DIV style="position:absolute;top:415;left:477"><nobr><span class="ft9">This new design is being implemented and tested, as of January <br>2006. First indications are that the performance will comfortably <br>meet the required performance goals. Extensive benchmarking is <br>required to tune many parameters, such as batch size, file size, <br>degree of parallelism, and the index management. </span></nobr></DIV>
<DIV style="position:absolute;top:504;left:477"><nobr><span class="ft2">4.4  Archiving </span></nobr></DIV>
<DIV style="position:absolute;top:524;left:477"><nobr><span class="ft9">Archiving and back-up are expensive operations with complex <br>trade-offs. Without care, the networking bandwidth and disk <br>throughput used in logging, back-up, and writing to the tape library <br>could have a major impact on the system throughput.   <br>As described above, the database design allows the database files to <br>be backed up incrementally. This provides two options for restoring <br>the database, by reprocessing the raw data or from the back-up. The <br>Page Store is not backed-up. If parts of it were ever corrupted, they <br>would have to be restored by reprocessing the raw data. A current <br>design project is to reorganize the Page Store to permit efficient <br>restoration. <br>The library uses a robotic tape library with LTO3 tape drives. This <br>is shared with other systems at the center. All unprocessed ARC and <br>DAT files are copied to the tape library to be stored indefinitely. <br>This preserves another copy of the Internet Archive's data for the <br>long-term. This data is unique and could never be replaced. The <br>industry standard life of these tapes is thirty years but our <br>expectation is that six years is a more probable time before the tape <br>library is replaced and all the data will have to be copied onto fresh <br>media. </span></nobr></DIV>
<DIV style="position:absolute;top:863;left:477"><nobr><span class="ft2">5.  SUPPORT FOR THE USER </span></nobr></DIV>
<DIV style="position:absolute;top:883;left:477"><nobr><span class="ft9">Figure 6 shows the architecture of the interface that the library <br>offers to users. This is a three-tier architecture. The data tier consists <br>of the relational database of metadata and the Page Store; the <br>middleware tier provides services to access the data, tools to analyze <br>it, and a choice of Web Services APIs; clients interact with the <br>middleware tools either through the APIs, or directly. </span></nobr></DIV>
<DIV style="position:absolute;top:988;left:477"><nobr><span class="ft2">5.1  Clients </span></nobr></DIV>
<DIV style="position:absolute;top:1008;left:477"><nobr><span class="ft9">The user tools system was designed to be extensible and scalable. <br>Specifically, it supports two categories of users: (a) users who <br>analyze the data remotely from their own computers, perhaps at <br>another institution or even in another country, and (b) </span></nobr></DIV>
<DIV style="position:absolute;top:123;left:136"><nobr><span class="ft2">Page</span></nobr></DIV>
<DIV style="position:absolute;top:214;left:91"><nobr><span class="ft14">Page Content</span></nobr></DIV>
<DIV style="position:absolute;top:223;left:235"><nobr><span class="ft2">Link</span></nobr></DIV>
<DIV style="position:absolute;top:241;left:235"><nobr><span class="ft14">- Destination URL</span></nobr></DIV>
<DIV style="position:absolute;top:133;left:245"><nobr><span class="ft14">URL Path</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:334"><nobr><span class="ft14">Dictionary</span></nobr></DIV>
<DIV style="position:absolute;top:131;left:354"><nobr><span class="ft14">Domain</span></nobr></DIV>
<DIV style="position:absolute;top:222;left:579"><nobr><span class="ft14">Page</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:663"><nobr><span class="ft14">Link</span></nobr></DIV>
<DIV style="position:absolute;top:339;left:505"><nobr><span class="ft14">Page Content</span></nobr></DIV>
<DIV style="position:absolute;top:302;left:749"><nobr><span class="ft14">Custom</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:760"><nobr><span class="ft14">Log</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:704"><nobr><span class="ft14">Everything</span></nobr></DIV>
<DIV style="position:absolute;top:213;left:724"><nobr><span class="ft14">Else</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">100</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="48007.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">computationally intensive users, who may wish to run very heavy <br>analyses of the data using the Cornell Theory Center computing <br>environment. Corresponding to these two categories of users the <br>architecture supports two types of clients: Web services clients and <br>clients that execute their own programs on the library's servers. The <br>first category requires little technical sophistication from the user, <br>while the second category trades complexity against the flexibility <br>of being able to write custom programs and to use the full <br>processing power available. </span></nobr></DIV>
<DIV style="position:absolute;top:515;left:82"><nobr><span class="ft1">Figure 6. Architecture of the interfaces provided for users of the </span></nobr></DIV>
<DIV style="position:absolute;top:531;left:241"><nobr><span class="ft1">library </span></nobr></DIV>
<DIV style="position:absolute;top:553;left:81"><nobr><span class="ft9">Web services clients are intended to be used remotely, with <br>moderate demands on their access to the data. Examples of these <br>clients include running queries using a full text index, or fetching <br>specific pages using existing indexes. Web service clients may also <br>be used to start, control, and retrieve results from experiments run <br>by high-performance clients. Web services are implemented by <br>using Microsoft's ATL server libraries. They run on a dedicated <br>Web server. The clients themselves can be implemented in any <br>language or environment that supports Web services standards. <br>Users of these clients do not need to know how the data is stored. <br>They are provided with forms that are automatically converted to <br>SQL commands by the middleware.  <br>The high-performance clients will for the most part run within the <br>Cornell Theory Center. They will usually have a high bandwidth <br>connection to the database. Clients of this form may carry out <br>research that need lots of computation power, e.g., experiments that <br>process very large subsets or analyze how the structure of the Web <br>changes over time. These clients are implemented by linking against <br>dynamic link libraries (DLLs) provided by the application server <br>tier. </span></nobr></DIV>
<DIV style="position:absolute;top:885;left:81"><nobr><span class="ft2">5.2  Access to the Database </span></nobr></DIV>
<DIV style="position:absolute;top:906;left:81"><nobr><span class="ft9">The application server tier accesses the database using Microsoft <br>database tools. Two main areas of functionality have been <br>implemented in this tier: Basic Access Services (BAS), and the <br>Subset Extraction Services (SES). Each consists of two parts: a set <br>of services, implemented as a series of dynamic link libraries <br>(DLLs) written in C++, and a Web Services API.  <br>Basic Access Services are for clients that interact directly with the <br>database. They allow a client to fetch pages given a combination of <br>URL and date of crawl. They also allow a client to check within <br>which crawls a given page is available. For example, a focused Web </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft9">crawler can retrieve pages from a specified crawl, using a simple <br>client script that interfaces to the BAS Web services API. <br>Subset Extraction Services allow a client to select a part of the data <br>as a subset. Once created, this subset is stored in the database as a <br>view. Such subsets are useful for running experiments over a <br>smaller, perhaps random, sample of the Web, as well as selecting <br>relevant pages for a particular experiments, such as those from a <br>given domain. For example, a researcher studying government Web <br>sites might extract textual pages from the .gov domain for a selected <br>range of dates.  </span></nobr></DIV>
<DIV style="position:absolute;top:282;left:477"><nobr><span class="ft13"><i>5.2.1  Users Beyond Cornell </i></span></nobr></DIV>
<DIV style="position:absolute;top:301;left:477"><nobr><span class="ft9">This digital library is intended for the use of all academic <br>researchers, not only those based at Cornell. Technically, this is <br>straightforward. The library is connected to the Internet, including <br>Internet2, and will soon be available via the TeraGrid. <br>We are currently developing a code of use policies for researchers. <br>Mining this data has potential for abuses of privacy. Cornell <br>researchers need to follow the university's procedures for such <br>research and we need to find convenient ways to extend the code of <br>practice to all users of the library.  </span></nobr></DIV>
<DIV style="position:absolute;top:459;left:477"><nobr><span class="ft2">5.3  User Tools </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:477"><nobr><span class="ft13"><i>5.3.1  The Retro Browser </i></span></nobr></DIV>
<DIV style="position:absolute;top:502;left:477"><nobr><span class="ft9">The Retro Browser is an example of a Web services client that is <br>already implemented and running in a test environment [12]. To a <br>user, it appears to be a regular Web browser, except that it browses <br>an historical snapshot of the Web. <br>The Retro Browser is designed with the non-technical user in mind. <br>The design assumes that the user may not be technically proficient <br>and should not be expected to install new software or run special <br>scripts. After the user has made an initial choice of a date in Web <br>history, the Retro Bowser behaves like any other Web browser. The <br>user uses a standard browser to carry out all the standard Web tasks, <br>such as download an applet, run a script, submit a forms, etc. The <br>only difference is that every URL is resolved to the record in the <br>Web library for the specified date. <br>The major component of the Retro Browser is a Web server <br>configured to be used as a proxy server. To obtain the data from the <br>database, the proxy server utilizes the Basic Access Web Service <br>API. <br>The Retro Browser client interacts with the Retro Browser proxy in <br>a standard HTTP client-server fashion. To fetch the appropriate <br>page from the database requires a URL and the date of the crawl, <br>which is represented by a crawl ID. The proxy server expects a <br>session cookie specifying a crawl ID with every request. If such a <br>cookie is not found with the request, the user is asked to specify the <br>crawl ID. Further requests may or may not contain a cookie since <br>cookies are tied to a domain. However, the Retro Browser proxy <br>ensures that the cookie is replicated for all domains using a series of <br>redirects. In this manner, the architecture ensures that the user is <br>asked to specify the crawl date for the first request only.  </span></nobr></DIV>
<DIV style="position:absolute;top:972;left:477"><nobr><span class="ft13"><i>5.3.2  Analysis of the Web Graph </i></span></nobr></DIV>
<DIV style="position:absolute;top:991;left:477"><nobr><span class="ft9">A set of analysis tools is under development that will provide more <br>complex access and analysis functions. These tools are part of the <br>application server tier. They are applied to subsets of the data and <br>accessed either directly or through the Subset Extraction Services <br>API. </span></nobr></DIV>
<DIV style="position:absolute;top:272;left:94"><nobr><span class="ft1">Data Tier</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:219"><nobr><span class="ft9">Application<br>Server Tier</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:345"><nobr><span class="ft1">Client Tier</span></nobr></DIV>
<DIV style="position:absolute;top:437;left:94"><nobr><span class="ft1">Page Store</span></nobr></DIV>
<DIV style="position:absolute;top:312;left:94"><nobr><span class="ft1">Metadata</span></nobr></DIV>
<DIV style="position:absolute;top:309;left:189"><nobr><span class="ft9">Basic Access<br>Services</span></nobr></DIV>
<DIV style="position:absolute;top:414;left:191"><nobr><span class="ft10">Subset Extraction<br>Services</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:242"><nobr><span class="ft10">BAS Web<br>Service</span></nobr></DIV>
<DIV style="position:absolute;top:468;left:232"><nobr><span class="ft9">SES Web<br>Service</span></nobr></DIV>
<DIV style="position:absolute;top:465;left:372"><nobr><span class="ft1">Clients</span></nobr></DIV>
<DIV style="position:absolute;top:311;left:349"><nobr><span class="ft9">High<br>Performance<br>Clients</span></nobr></DIV>
<DIV style="position:absolute;top:392;left:358"><nobr><span class="ft10">Retro-<br>Browser</span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">101</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<IMG width="918" height="1188" src="48008.png" alt="background image">
<DIV style="position:absolute;top:109;left:81"><nobr><span class="ft9">One group of tools operates on the Web graph of a subset. <br>Hyperlinks from each Web page are stored in the database. <br>Representation of the graph is by its adjacency matrix using a <br>compressed sparse row representation. Preliminary software has <br>been written to read all the links from a given subset of the data and <br>construct the adjacency matrix. The matrix is then stored in the file <br>system in a compressed form, which allows performing the basic <br>operations, such as matrix addition and multiplication. The Cuthill-<br>McKee algorithm is used to reorder the nodes to create dense blocks <br>within the matrix to increase the compression ratio and allow in-<br>memory processing [8].  </span></nobr></DIV>
<DIV style="position:absolute;top:292;left:81"><nobr><span class="ft13"><i>5.3.3  Full Text Indexes </i></span></nobr></DIV>
<DIV style="position:absolute;top:311;left:81"><nobr><span class="ft9">Full text indexes are a vital tool for many researchers. For instance a <br>social science researcher may wish to track the Web pages that refer <br>to a named individual or may identify trends by burst analysis of <br>terms used on the Web. <br> Our initial approach is to provide an indexing service for data <br>subsets, using the Nutch search engine. It is straightforward to <br>extract a subset, which is represented by a database view, and create <br>a full text index of all textual pages in the subset. The only problem <br>is the processing necessary to index a very large subset. <br>We are in discussions with Cutting, the principal developer of the <br>Lucene and Nutch search engines</span></nobr></DIV>
<DIV style="position:absolute;top:479;left:264"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:481;left:268"><nobr><span class="ft1">. He has been working with the </span></nobr></DIV>
<DIV style="position:absolute;top:497;left:81"><nobr><span class="ft9">Internet Archive to build indexes of very large collections of Web <br>pages in ARC format. For this purpose, they are developing a <br>modified version of Nutch, known as Nutch WAX (Web Archive <br>eXtensions). Rather than duplicate this effort, we are exploring the <br>possibility of providing access to these indexes through the Basic <br>Access Service. </span></nobr></DIV>
<DIV style="position:absolute;top:602;left:81"><nobr><span class="ft2">6.  ACKNOWLEDGEMENTS </span></nobr></DIV>
<DIV style="position:absolute;top:622;left:81"><nobr><span class="ft9">We wish to thank Brewster Kahle, Tracey Jaquith, John Berry and <br>their colleagues at the Internet Archive for their support of this <br>work. </span></nobr></DIV>
<DIV style="position:absolute;top:679;left:81"><nobr><span class="ft9">The following Cornell students have contributed to the development <br>of the library described in this paper: Mayank Gandhi, Nicholas <br>Gerner, Min-Daou Gu, Wei Guo, Parul Jain, Karthik Jeyabalan, <br>Jerrin Kallukalam, Serena Kohli, Ari Rabkin, Patrick Colin Reilly, <br>Lipi Sanghi, Shantanu Shah, Dmitriy Shtokman, Chris Sosa, Samuel <br>Benzaquen Stern, Jimmy Yanbo Sun, Harsh Tiwari, Nurwati <br>Widodo, Yu Richard Wang. </span></nobr></DIV>
<DIV style="position:absolute;top:799;left:81"><nobr><span class="ft9">This work has been funded in part by the National Science <br>Foundation, grants CNS-0403340, DUE-0127308, and SES-<br>0537606, with equipment support from Unisys and by an E-Science <br>grant and a gift from Microsoft Corporation. </span></nobr></DIV>
<DIV style="position:absolute;top:1005;left:81"><nobr><span class="ft1">                                                                  </span></nobr></DIV>
<DIV style="position:absolute;top:1025;left:81"><nobr><span class="ft4">7</span></nobr></DIV>
<DIV style="position:absolute;top:1027;left:86"><nobr><span class="ft1"> The Lucene search engine is described at </span></nobr></DIV>
<DIV style="position:absolute;top:1043;left:92"><nobr><span class="ft9">http://lucene.apache.org/. Nutch is described at  <br>http://lucene.apache.org/nutch/. </span></nobr></DIV>
<DIV style="position:absolute;top:109;left:477"><nobr><span class="ft2">7.  REFERENCES </span></nobr></DIV>
<DIV style="position:absolute;top:130;left:477"><nobr><span class="ft1">[1]  Arms, W., Aya, S., Dmitriev, P., Kot, B., Mitchell, R., Walle, </span></nobr></DIV>
<DIV style="position:absolute;top:145;left:504"><nobr><span class="ft9">L., A Research Library for the Web based on the Historical <br>Collections of the Internet Archive. D-Lib Magazine. February <br>2006. http://www.dlib.org/dlib/february06/arms/02arms.html </span></nobr></DIV>
<DIV style="position:absolute;top:199;left:477"><nobr><span class="ft1">[2]  Bergmark, D., Collection synthesis. ACM/IEEE-CS Joint </span></nobr></DIV>
<DIV style="position:absolute;top:215;left:504"><nobr><span class="ft1">Conference on Digital Libraries, 2002. </span></nobr></DIV>
<DIV style="position:absolute;top:237;left:477"><nobr><span class="ft1">[3]  Brin, S., and Page. L., The anatomy of a large-scale </span></nobr></DIV>
<DIV style="position:absolute;top:253;left:504"><nobr><span class="ft9">hypertextual Web search engine. Seventh International World <br>Wide Web Conference. Brisbane, Australia, 1998.  </span></nobr></DIV>
<DIV style="position:absolute;top:290;left:477"><nobr><span class="ft1">[4]  Burner, M., and Kahle, B., Internet Archive ARC File Format, </span></nobr></DIV>
<DIV style="position:absolute;top:306;left:504"><nobr><span class="ft1">1996. http://archive.org/web/researcher/ArcFileFormat.php </span></nobr></DIV>
<DIV style="position:absolute;top:328;left:477"><nobr><span class="ft1">[5]  Chakrabarti, D., Zhan, Y., and Faloutsos, C., R-MAT: </span></nobr></DIV>
<DIV style="position:absolute;top:344;left:504"><nobr><span class="ft9">recursive model for graph mining. SIAM International <br>Conference on Data Mining, 2004. </span></nobr></DIV>
<DIV style="position:absolute;top:382;left:477"><nobr><span class="ft1">[6]  Gerner, N., Sosa, C., Fall 2005 Semester Report for Web Lab </span></nobr></DIV>
<DIV style="position:absolute;top:397;left:504"><nobr><span class="ft9">Database Load Group. M.Eng. report, Computer Science <br>Department, Cornell University, 2005. <br>http://www.infosci.cornell.edu/SIN/WebLib/papers/Gerner200<br>5.doc. </span></nobr></DIV>
<DIV style="position:absolute;top:467;left:477"><nobr><span class="ft1">[7]  Ghemawat, S., Gobioff, H. and Leung, S., The Google File </span></nobr></DIV>
<DIV style="position:absolute;top:483;left:504"><nobr><span class="ft9">System. 19th ACM Symposium on Operating Systems <br>Principles, October 2003. </span></nobr></DIV>
<DIV style="position:absolute;top:520;left:477"><nobr><span class="ft1">[8]  Jeyabalan, K., Kallukalam, J., Representation of Web Graph </span></nobr></DIV>
<DIV style="position:absolute;top:536;left:504"><nobr><span class="ft9">for in Memory Computation. M.Eng. report, Computer Science <br>Department, Cornell University, 2005. <br>http://www.infosci.cornell.edu/SIN/WebLib/papers/Jeyabalan<br>Kallukalam2005.doc.  </span></nobr></DIV>
<DIV style="position:absolute;top:606;left:477"><nobr><span class="ft1">[9]  J. Kleinberg. Authoritative sources in a hyperlinked </span></nobr></DIV>
<DIV style="position:absolute;top:621;left:504"><nobr><span class="ft9">environment. Ninth ACM-SIAM Symposium on Discrete <br>Algorithms, 1998.  </span></nobr></DIV>
<DIV style="position:absolute;top:659;left:477"><nobr><span class="ft1">[10]  Mitchell, S., Mooney, M., Mason, J., Paynter, G., Ruscheinski, </span></nobr></DIV>
<DIV style="position:absolute;top:675;left:504"><nobr><span class="ft9">J., Kedzierski, A., Humphreys, K., iVia Open Source Virtual <br>Library System. D-Lib Magazine, 9 (1), January 2003. <br>http://www.dlib.org/dlib/january03/mitchell/01mitchell.html </span></nobr></DIV>
<DIV style="position:absolute;top:728;left:477"><nobr><span class="ft1">[11]  Shah, S., Generating a web graph. M.Eng. report, Computer </span></nobr></DIV>
<DIV style="position:absolute;top:744;left:504"><nobr><span class="ft9">Science Department, Cornell University, 2005. <br>http://www.infosci.cornell.edu/SIN/WebLib/papers/Shah2005a<br>.doc. </span></nobr></DIV>
<DIV style="position:absolute;top:798;left:477"><nobr><span class="ft1">[12]  Shah, S., Retro Browser. M.Eng. report, Computer Science </span></nobr></DIV>
<DIV style="position:absolute;top:814;left:504"><nobr><span class="ft9">Department, Cornell University, 2005. <br>http://www.infosci.cornell.edu/SIN/WebLib/papers/Shah2005b<br>.pdf. </span></nobr></DIV>
<DIV style="position:absolute;top:1128;left:449"><nobr><span class="ft7">102</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
